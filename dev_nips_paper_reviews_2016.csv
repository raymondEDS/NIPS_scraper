Title,Author,Link,Reviewer 1 Summary,Reviewer 1 Qualitative Assessment,Reviewer 1 Confidence in this Review,Reviewer 2 Summary,Reviewer 2 Qualitative Assessment,Reviewer 2 Confidence in this Review,Reviewer 3 Summary,Reviewer 3 Qualitative Assessment,Reviewer 3 Confidence in this Review,Reviewer 4 Summary,Reviewer 4 Qualitative Assessment,Reviewer 4 Confidence in this Review,Reviewer 5 Summary,Reviewer 5 Qualitative Assessment,Reviewer 5 Confidence in this Review,Reviewer 6 Summary,Reviewer 6 Qualitative Assessment,Reviewer 6 Confidence in this Review,Reviewer 7 Summary,Reviewer 7 Qualitative Assessment,Reviewer 7 Confidence in this Review,Reviewer 8 Summary,Reviewer 8 Qualitative Assessment,Reviewer 8 Confidence in this Review
Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models,"Tomoharu Iwata, Makoto Yamada",https://proceedings.neurips.cc/paper/2016/hash/0f96613235062963ccde717b18f97592-Abstract.html,"This paper provides an approach, based on PPCA and PCCA, to finding outlier observations in multi-view data sets. The model embeds observations in a latent space shared across views and finds projections from that latent space to each view. Observations that do not agree with the shared latent space and projection (common to all observations in a particular view) are recognized as outliers. The approach uses stochastic EM to approximate the random variables of the model conditioned on data. ","The paper presents an intriguing approach to outlier detection. Using the probability of the number of clusters being greater than 1 is an interesting innovation that I have not seen before. One limitation of that approach I see is whether to interpret instances with more than one latent vector as ""outliers"" or just multimodal. The description at the end of Section 5 bears this out: does the fact that a movie has more than one genre make it an outlier? In general, this means that the proposed method will work best when each instance can be represented with a single vector across multiple views.  Presentation was rated ""sub-standard"" mostly due to the very dense blocks of text in some places (e.g., the related work section). Also, it would be better to have subsections in the paper to structure the text more. The derivations starting line 99 needed more detail for how the marginal distributions were arrived at (perhaps a large chunk of this part can be pushed to the appendix?).  The experiments covered a large number of data sets but the only meaningful benchmark was PCCA (and SVM for one experiment). I thought the method of creating anomalous instances was clever and could be useful to others. The paragraph starting line 219 describes results for multi-view anomaly detection but it would be helpful in pointing toward specific data sets in the evaluation when doing this. The purpose of the paragraph starting line 238 was not clear to me, I could not see why it was needed.   I noticed a typo on line 89 (missing space).",2-Confident (read it all; understood it all reasonably well),"The authors propose a generative model approach for multi-view anomaly detection. The method is based on estimation of the number of latent vectors that generate each instance with Dirichlet process priors. The estimation itself is performed using a stochastic EM algorithm. The paper consists of the proposed algorithm, with its underlying model and background, and a set of experiments / simulations to compare its performance with existing anomaly detection algorithms.","The paper is relevant to anomaly detetction and the method appears novel. The technical contribution is sound. There are no theoretical results and in fact the paper's claim is solely based on empirical results. The empirical part appears carefully done and well reported. The model is reasonable and the estimation algorithm based on the model also appears reasonable.  The empirical part aims to capture took much ground I feel. The main goal is anomaly detection yet the empirical part also discusses missing value imputation and latent dimension reduction, which are outside the immediate scope of the paper. This comes at the expense of anomaly detection. Since we are dealing with a detection algorithm, it would be beneficial to explain when the algorithm makes false positives and false negatives; to explain when the detection problem is easy or hard; and to try to use simulations, generating data according to the assumed model itself, in order to evaluate whether the algorithms works as expected - even under the assumed model.   Since the empirical part is all the paper has to offer in terms to evidence for merit of the proposed method, I recommend to remove the parts discussing problems other than anomaly detection, and to expand the empirical work along the lines above.   ",1-Less confident (might not have understood significant parts),"The paper presents a simple multi-view linear Gaussian latent variable model, where latent factors for each data point are clustered. Also, a heuristic multi-view anomaly detection criterion is proposed.","I find the connections to PCCA far-fetched and confusing. In the experiments, it is a serious flaw to use PCCA abbreviation to a model that is not PCCA.  The insight of the real application needs to be elaborated. As such, it is difficult to grasp utility of the model and what anomaly means.  I have some doubts regarding the anomaly score. It depends on the parameters of the DP base distribution and I would expect to see a sensitivity analysis (i.e. how r affects clusterings/anomaly scores). Different cluster assignments may not be sufficient for detecting anomaly because the actual realisations may still be similar. Additionally, the component number K affects the performance as verified in the experiments. Why cross-validation or some other method for model selection (for each data collection) was not used?  I doubt how useful the DP formulation is for small number of views. The number of views corresponds to the maximum number of clusters.  The assumption of same noise variance for all views is very limiting in practice; I would recommend exploring performance for alternative choices.  How many views were used in the experiments?  How missing value imputation is related with anomaly detection? What is the rationale?  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper uses a nonparametric Bayesian model for detecting anomalies in Multiview data. The idea is to determine the number of latent vectors that is needed to generate Multiview vectors that associated with the same entity. Ideally, the number should be one since the same data point in different views correspond to the same latent representation. If the number exceeds one with high probability, the data can be considered as an outlier. Overall, the proposed approach is professional and demonstrates good performance. But the writing is a bit vague. In terms of algorithm, the contribution seems not strong."," My two concerns are as follows: 1)It is not clear which part does the major contribution lie in. The major components of the method, i.e., generative modeling, EM algorithm, and Gibbs sampling, are well developed methods. It is hard to see any innovative aspect in terms of method. It is suggested that the authors itemize the contributions in the introduction. 2)The main objective is to detect outliers, but the detection process is rather vaguely described. In fact, there is only one sentence regarding outlier detection in lines 133-134. “Embedding” the detection process in Gibbs sampling looks quite heuristic and seems that there’s no theory backing it. This part may be explained in detail. ",2-Confident (read it all; understood it all reasonably well),The article looks at the problem of anomaly detection. The article wants to be able to solve this problem by looking at more than two views at the same time and to be able to combine in a way multiple view to use all the information available.  The article introduces a probabilistic model. One would like to do inference. This inference problem is then solved by something that looks like the cavity method. But the reviewer is not sure and has stopped understanding what was happening in that article from there.    Novelty/originality : The reviewer can not judge. Clarity and presentation : The reviewer can not judge. Technical quality : The reviewer can not judge. ,If the part 3 on inference is using the cavity method (Belief propagation) to solve this inference problem This should show in the article since these two names will appeal to people coming from different background.   I not then this means that the reviewer hasn't understand anything to this article.,1-Less confident (might not have understood significant parts),"This paper presents a probabilistic model general to multi-view data.   The model assumes each view of a data instance is generated from a single latent component and that ""anomalous"" instances are those for which not all views are generated from the same component.  A stochastic EM algorithm is given that alternates between Gibbs sampling the component assignments and maximum-likelihood estimation of the projection matrices with all other parameters analytically integrated out.  The proposed model is shown to consistently outperform 4 baseline models on 11 anomaly detection datasets (in which the ground-truth anomalies are known) that are artificially made to be multiview datasets by randomly splitting features.","This paper is well-written.  The proposed model is elegant in the way it defines multiview anomalies and seems to yield a simple and practical inference algorithm.   The authors do a good job of motivating multiview anomaly detection as a worthy task and covering some of the relevant prior work.  The experiments are described thoroughly and the proposed model is shown to consistently outperform the four chosen baselines.   While I think this paper is well-written and well-executed, I think the model itself does not represent a major conceptual leap from prior work.  In the past couple years, there has been a lot of work on matrix and tensor factorization methods for anomaly detection (see this recent survey [1]).  I would have liked to see more coverage of recent anomaly detection methods in the paper.  Also, for general multiview data, the Gaussian likelihood is too restrictive since we might expect binary, count or positive real data in some of the views.    [1] Fanaee-T, Hadi, and João Gama. ""Tensor-based anomaly detection: An interdisciplinary survey."" Knowledge-Based Systems 98 (2016): 130-147.",2-Confident (read it all; understood it all reasonably well),,,,,,
Matching Networks for One Shot Learning,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, Daan Wierstra",https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html,"In this paper the authors phrase one-shot learning as an instance of a “set-to-set” neural network, that has a very similar structure as various recently published “sequence-to-sequence” models; or memory- / pointer networks. The idea and proposed architecture is simple and compelling. The paper also contains an extensive experimental part where multiple new one-show problems are introduced.  "," The basic idea behind this paper is simple and seems compelling. Until about line 110, the authors do a good job motivating and explaining their model. The paragraph about LSTM equipped set-embeddings (lines ~116  to 123) is not very detailed though. I can imagine many different architectures that fit with the description in this paragraph. Furthermore, I could not find detailed descriptions of the models and hyperparameters used for the experiments -- neither in the experimental section nor in the (non-existing) supplemental material. From this perspective it seems hard to independently reproduce the presented results.  Besides of introducing a new model, the paper also introduces various new one-shot learning tasks. The tasks seem reasonable hard and the authors evaluated various baseline models against these.  In summary: this paper introduces a simple and interesting method to train one-shot learning classifiers. The experimental section is extensive and the proposed model seems to outperform previous state-of-the-art methods considerably; but given that many of the tasks are new, it is very hard for me to assess the actual (empirical) progress archived by the method.  ",2-Confident (read it all; understood it all reasonably well),"The article describes a one-shot learning model called matching network. The basic idea of the model is to compute the output category of one given test example by computing a kind of ‘similarity’ with all the training examples that are stored in a memory. This similarity (denoted a(x,x_i)) is based on two representation functions f and g which goals are to encode both the new example, but also the examples stored in memory. The proposed model is thus very close to siamese networks, but using an asymmetric architectures since g and f can be different functions. The main originality is contained in two particular aspects: first, instead of computing the g function on each training example, the model proposes a sequential model (bidirectional LSTM) to learn to encode the training examples based on the previously seen examples. Second, the set of examples is also used to learn to encode any incoming datapoint. These two components will be learned over a set of one-shot learning tasks, resulting in a ‘learning to learn’ problem. Experiments are made on different images datasets and one language dataset. The proposed approach is compared to baseline models, MANN and siamese nets (not on all the datasets…) and shows interesting results","Comments: the idea contained in the paper is not a very big contribution to the field but still remains interesting, and the experiments on different datasets provide important information concerning the behavior of the model. But the paper has many different problems that make it difficult to understand. The first problem is in the structure of the paper since the problem formulation is in fact only given in Section 2.2 while I think it is important to well define the one-shot learning problem at the beginning of the paper. It would make the article easier to read and to follow. Sections  2.1 and 2.1.1 are well written but could include more precise definitions. For example, in Equation 1, y_i is not defined : is it a real number, a one hot vector (I suppose it is a one hot vector). The core of the contributions in Section 2.1.2 is very difficult to well understand. It is not clear how the bidirectional LSTM is used, but the more difficult part concerns the way f(\hat(x)) is defined. The authors could clearly give more details since this section is in fact the main contribution of the article. Written as it is, it makes the model difficult to understand, and almost impossible to reproduce. At last, if these sections can be understood by researchers very familiar to deep learning, it will not be the case for ML researchers used to read more ‘classical’ papers while the principle of the paper is more general than deep learning and can be for example understood from a metric learning point of view. I clearly advise the authors to improve the way the model is described. Concerning the experiments, it is really interesting to see that this type of approach improves the quality of the system w.r.t Siamese networks. But the Siamese net results are only provided for omniglot and not for imagenet which is a little bit strange. The language model part also does not contain any table/figure showing a summary of the results which make the discourse difficult to follow in this section.   To conclude, if the article provides a simple but interesting idea for one-shot learning, the way it is written must be drastically improved to allow a publication in a top conference.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper proposes a non-parametric method for one-shot learning where the weight (or, distance metric) between the test item and its neighbors (one-shot set) can be learnt by back-propagation. For this, it also proposes a meta-learning training strategy where a mini-batch to mini-batch learning is performed. The proposed method is tested on classification tasks with three different datasets. The proposed method seems to work reasonably well compared to the results of previous one-shot learning methods.","Note that after reading the submitted version, I have also found and read the arXiv version. My comments are based on the arXiv version unless mentioning the source explicitly.   Overall, I enjoyed the paper and the performance is interesting. Below are my comments:  - It seems good to put the model description in the Appendix to the main description of the paper. Without it, I felt the exposition a bit abstractive and not so clear. Also, I'm wondering the meaning of eqn 4. in the Appendix.  - The effect of ordering for items in S. It seems that the FCE for g(x) requires to order the elements in S. What kind of criteria did you use for the ordering and how sensitive the performance by different ordering schemes.  - In lines around 80-82 (in the submission version), it says ""Unlike other attentional memory mechcanisms [2], (1) is non-parameteric in nature: as the support set size grows, so does the memory used"". -> I'm not so sure about this. Doesn't the memory size also change depending on e.g., the input sentence length (e.g., in NMT)?  - I would add more detailed description on the convolutional siamese networks because it is one of the main algorithms compared in the experiments.  - I would still like to see the actual numbers for the FCE version in the Table 1. although the authors mention that ""it did not seem to help much"" and some analysis on why the FCE was not helpful for omniglot is required.  - For the language modeling experiments, I would like to see at least one other one-shot learning method in the comparison.  Minor comments   - Some explanation is required in the caption of Fig. 1 which is also too large. - More informative notation would be helpful. E.g. in Eqn. (1) I guess the label y_i will be the softmax output vector for classification. Current notation makes it look like weighted average of the integer class labels.  - Also, notation, sometimes use Equantion X and other times Eq. X. etc. - Ref. for meta-learning in line 139. ",2-Confident (read it all; understood it all reasonably well),"This paper presents Matching Nets, a one-shot learning framework that adopts advances in attention and memory that enable rapid learning. It maps a small labelled support set to a classifier, where the mapping is parameterized by a neural network. They also define one-shot tasks that can benchmark other approaches on ImageNet and small scale language modeling.","Matching Networks aims to incorporate the best characteristics from both parametric and non-parametric models, providing rapid acquisition of new examples as well as generalization from common examples. They cast the problem of one-shot learning within the set-to-set framework, which seems an extension of their previous work.  The main novelty of MN, as claimed in the paper, is reinterpreting a well studied framework to do one-shot learning. However, it is kind of hard to claim this as novelty. The difference with simple KDE/NN is that MN adopts the parameterized mapping as feature extractor. MN tries to train two embeddings (CNN/LSTM) to minimize the classification error between support set and unseen examples. Given the limits mentioned in the conclusion, this paper seems like a preliminary work on framework/task definition, which does not solve real challenges, e.g. speed/scalability issues with large training data.   Some minor problems: 1. Figure 1 is not referred in the text and needs better explanation. For example, what is the definition of g_theta and f_theta? They are not mentioned until sec 2.1.1. 2. The meaning of “switching the task from minibatch to minibatch” in line 35 is unclear . 3. The organization of sec 4 is a little wired, should section of 4.1.3 be 4.2?",1-Less confident (might not have understood significant parts),"Deep supervised networks do not typically adapt rapidly to new concepts from sparse data. This paper alleviates this problem by proposing a neural network based model with a non-parametric structure.  One of the main high-level insights in this paper is that for one-shot learning problems, train and test both on one-shot like settings. Also the use of non-parametric ideas in deep learning is an interesting way to deal with catastrophic forgetting and to robustly adapt to fast changes in the concept space.   For a new test item, the model predicts the label as the weighted linear sum of labels in the training support set S. This weighting is computed by taking the cosine distance between features of the test item and features of items in S. Two separate neural networks are used for the test items (g) and items in S (f) respectively. Both g and f are recurrent neural networks which embed data-points in the context of S (which enables data-dependent embeddings).   I really like this paper as it proposes a very simple and elegant approach to one shot learning by combining neural networks with non-parametric structures.   The contributions can be summarized as: (1) To do one shot learning during test, train the network to do one-shot learning (2) Incorporate non-parametric structure into neural networks via an attention mechanism (3) Authors demonstrate state of the art classification accuracy on Omniglot ","- Are the omniglot results on the same train-test split as reported in Lake et al ? A head on comparison with this paper would be interesting and important to see.  - What happens with the quantitative results when the embedding functions f and g are same architectures/networks.  - One of the most unsatisfying things about this model is that it won't perform well when the training and test tasks come from a significantly different distribution. The authors do present some analysis in 4.2.1 that this limitation might be causing performance degradation in L_dogs. I would have liked to see more of such test cases.   - For reproducibility, authors should list network architecture details for each setup  Overall I really like the direction of adapting neural nets for one-shot learning and this paper makes meaningful scientific contributions towards it.   ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper presents a  non-parametric approach to one shot learning. Novelties claimed are two-fold. First, the network itself utilizes ideas from attention and memory and second the training procedure keeps switching the task every minibatch. The model and training strategy is explained well on most parts, however, there are a few important that currently seem unanswered. Experimental results show that the model is in fact able to learn the underlying structure of the training classes and is able to generalize well enough to novel classes. ","Major comments:  1) The model utilizes a bidirectional LSTM to encode a sample along with the context of an entire support set S. In line 120, the paper mentions that this allows the model to ""ignore"" some elements in the support set S. Further, ""depth"" is added to the computation. This reviewer feels it would be helpful to have more discussion on both these points, further motivating the use of sequence modelling. Along the same lines, line 122 could be extended to include more discussion, strengthening the connections to the ""learning to learn"" paradigm.  2) Although the network performance is good on the tasks that the paper defines, it is unclear how ""similar"" networks would perform. For examples, exploratory experiments could be performed to investigate how important is the application of sequence modelling to the task. How do other models that take in a set along with attention but do not utilize sequence modelling perform ? Investigative studies prove to be more useful to the overall understanding of the phenomenon that drives the best performing model. Perhaps the authors can simply add in some discussion into what alternate (but valid) models can be used and hypothesize how would they behave, although ideally, atleast one exploratory experiment could be reported.  These were the main reasons this reviewer believes that the the clarity/presentation of the paper is sub-standard for NIPS. The paper is clear on many significant parts, but given the importance of the problem addressed, and the novelties, more clarity/understanding of the overall model (alternate valid models) might be required.  Minor comments:  1) Fig. 1 caption could give more details on the algorithm. It helps to have an informative figure near  the beginning of the paper.",2-Confident (read it all; understood it all reasonably well),,,,,,
A Multi-Batch L-BFGS Method for Machine Learning,"Albert S. Berahas, Jorge Nocedal, Martin Takac",https://proceedings.neurips.cc/paper/2016/hash/8ebda540cbcc4d7336496819a46a1b68-Abstract.html,"In supervised learning, one is interested in minimizing the empirical risk where efficient optimization algorithms become the key. First-order methods such as stochastic gradient descent and its variants are reasonably well understood admitting efficient implementation and parallelization techniques. However there has been a recent interest in making second-order methods such as Newton's method or L-BFGS method efficient for such large-scale problems. This paper is along this direction, presenting a new variant of the stochastic L-BFGS method  that is efficient and robust in mainly two settings: The first arises in the presence of node failures in a distributed computing environment, the second occurs when one uses an adaptive batch size that varies over iterations for accelerating learning. The main idea is to form the Hessian estimate based on the overlap between consecutive batches (the intuition why this works is that we have less limitation in choosing the second-order information matrix compared to an estimate of the true gradient). This way, the Hessian corrections and updates with L-BFGS become more stable. The results are also illustrated and supported by numerical experiments.    I would recommend this paper to be accepted. My minor concerns/questions are below:  1) If the strong convexity lower-bound mu_1 is very small, then it seems that the resulting limit in Theorem 3.2 will be worse than the limit obtained by a first-order version of your algorithm when one would choose simply H_k = I. Could you please add a few lines to clarify this point?   2) I would suggest to add a few more references to the intro regarding recent work on stochastic L-BFGS for the sake of completeness of the literature survey. Examples include [Moritz, Nishihara, Jordan, 2016], [Mokhtari, Ribeiro, 2014]   3) To my knowledge, [Schmidt, Roux, Bach] paper appeared in Math. Programming recently. I would suggest to give a reference to the Math. Programming journal instead of the reference to the arxiv version.   4) Typo in line 59: use  --> used    ",The paper is readable and well-written on an interesting topic that has received much recent attention. ,2-Confident (read it all; understood it all reasonably well),The paper studies a multi-batch L-BFGS method that is robust in the absence of sample consistency. Their experiments show its effectiveness on logistic regression problems. ,"The proposed robust Quasi-Newton method is interesting and new with regard to using overlapping consecutive samples. The methods works both in node failures or multi-batch settings. The paper suggests two multi-batch sampling strategies, also analyzes the convergence properties of the multi-batch L-BFGS method. The numerical results show the proposed method is more efficient than comparing algorithms including SGD for logistic problems. The paper is clearly written. ",2-Confident (read it all; understood it all reasonably well),"In this paper, the author considered the a multi-batch L-BFGS method for optimizing logistic regression problem. In stand stochastic L-BFGS, the algorithm can be instable due to the calculation of gradient difference when the batch changes. The authors present a robust multi-batch L-BFGS, in which each computation node contain a subset of the whole data set with some overlapping elements. In this way, a robust quasi-Newton updating can be proposed. Two sample strategies were proposed to constructing subsets with overlapping elements. A convergence analysis regarding strongly convex objective functions was conducted. Extensive experiments (included in the supplementary file) demonstrate the superiority of the proposed method over some baselines. ","This paper is easy to follow and interesting. My major concerns are on the experiments.   First, the comparison to SGD in the paper is not very fair. A mini-batch SGD with parallel implementation is required for the comparison. Recently, there are many improved mini-bath (dual-primal) SGD methods, e.g. those attached below. Otherwise, I am not sure the superiority over the SGD methods.  Lee J, Ma T, Lin Q. Distributed stochastic variance reduced gradient methods[J]. arXiv preprint arXiv:1507.07595, 2015.  Shalev-Shwartz S, Zhang T. Accelerated mini-batch stochastic dual coordinate ascent[C]//Advances in Neural Information Processing Systems. 2013: 378-385.  Since the comparison is on classification tasks, a comparison of prediction accuracy w.r.t. running time (not epochs) is required.  The paper analyzed the nonconvex cases, but no experiment is performed to verify the theoretical results.   Another question is about convergence speed on convex cases (but not strongly convex). For example, the convergence on the L1-norm regularized SVM problem.  Line 164: randomly selecting elements from S_k?        ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper proposes a multi-batch L-BFGS method to obtain fast learning processes. One main issue for the method is instability, led by the sample inconsistency between the batches for two consecutive iterations. The authors proposed overlapping batches. Then stable second-order information is captured by computing the gradient difference for the overlapping subset of samples.  This paper also provides theoretical guarantees, where given strongly convex f and reasonable assumptions on the batches, the iterates linearly converge to a neighborhood of the optimum. For non-convex f, the average norm of the gradient over iterations decreases as a constant plus O(1/t) term. (not converging to zero).  The numerical results show that the convergence of the algorithm over epochs is competitive against SGD. It is also shown that the proposed algorithm is much more stable than just using multi-batch L-BFGS without batch overlapping.","This paper provides an interesting attempt to give competitive practical performance of second-order methods against first-order ones in the distributed and stochastic setting. The convergence analysis is neat but not surprising.  As stated in the beginning, when the communication cost is limited, the proposed algorithm is more likely to outperform SGD. It would have been impressive if the authors conducted experiments with such settings.",2-Confident (read it all; understood it all reasonably well),This paper proposes a variant of L-BFGS that can be used in a distributed computing environment. ,"This paper is well-written and effectively conveys the core idea. Unlike the first order method, subsampling of ""inverse Hessian matrix"" in the second order method is too noisy, and problematic, which makes a second-order method hard to be applied in distributed computing environment. This paper proposes to handle ""noiseness"" of the inverse Hessian matrix by using overlapped version of estimates, instead of using non-overlapped version of estimates.  I found this idea to be interesting, and preliminary results look promising. ",2-Confident (read it all; understood it all reasonably well),"The article tackles an original approach to ease the parallelism from the batch method L-BFGS. The authors explain the heuristics behind the algorithm, provide a convergence analysis in both the convex and the nonconvex settings, and show the performance of their method on a real dataset.","I liked the article which presented clear ideas and focus on a new side of stochastic descent algorithm.  The article aims at designing stochastic version of L-BFGS. At each iteration, you sample a large set of index (while SGD-based algorithms with mini-batchs use small mini-batchs), and select some indices to update the estimate of the inverse Hessian matrix (just like the original L-BFGS) The goal of the paper is to show that using quasi-Newton updates with overlap between consecutive batches ensure stability. I liked the convergence analysis: even though the optimality gap (in the convex case) doesn't converge to 0, the speed of convergence if fast. I would have appreciated more comments about the bounds (especially the denominator of the bound in Theorem 4),.",2-Confident (read it all; understood it all reasonably well),,,,,,
How Deep is the Feature Analysis underlying Rapid Visual Categorization?,"Sven Eberhardt, Jonah G. Cader, Thomas Serre",https://proceedings.neurips.cc/paper/2016/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html,The paper compares performance of deep networks to psychophysical performance of humans performing classification on the same images.  From the correlation in performance it is claimed that human performance is best modeled based on intermediate levels of a deep net. ,"Overall this is an interesting study, and I do appreciate the method of using Mechanical Turk to collect data. As far as I know the result is novel, that humans correlate most strongly with intermediate layers of a variety of different deep nets. I think the Discussion section is a bit to grandiose in claiming that layer 10 of their fine-tuned VGG-16 model presents some sort of supporting evidence of the Serre et al claim about how many processing stages humans have. I also would have liked to see an investigation of what the representations at 70% total depth were for the different networks. It would have supported their argument better if they could have asserted that the features at 70% depth in a smaller network, such as AlexNet corresponded strongly with the features found in a larger net, such as VGG-19.   Specific  comments:  As far as I could tell they did not use a mask for the human subjects.  Why?   If we assume that the mask serves the purpose it is intended to serve, then it should stop contributions of recurrent and top-down processing from influencing the human's response. I think using a mask would increase support for their argument that error correlations can be used to assess the ""depth"" of the human recognition system.  The authors compared the human results to results from three staple deep nets: AlexNet, VGG16, and VGG19. For convolutional layers they grab a random sample of features from each layer. They fix the number of features they use, which I find a little concerning. Given that deeper layers in these networks have fewer features, if they fix the sample size then they are going to get a more complete set of features as they ascend the hierarchy, and thus a more complete description of the layer's representation. Given the features from a certain Deep Net layer, they use a SVM to perform the 2AFC task. This surprises me, I would think that they would have just used a couple of fully connected layers as is done in the ImageNet challenge.   They find that the humans and deep nets have the highest response correlation when using features from layers at 70% of the model's depth. It seems like this is consistent across models, which is a bit confusing. The conclusion they draw from the result is that humans use intermediate-level features. However, it is based on the assumption that the features at whatever layer corresponds to 70% of the model's depth are uniform across models. They do not discuss this directly, and I would be very interested to see if it is true at all. Does modifying the total model depth have any influence on the features learned at a relative depth?  They further note that the maximum correlation observed with one of their models (fined tuned VGG16) occurs with the 10th layer of processing, which is about how many ""processing stages"" Serre, Oliva, & Poggio claim are in the ventral stream. I find this whole argument misleading, given that the specific layer depth that correlates with humans is going to vary monotonically with the total depth of the network. If you consider their result that the highest correlation is always at around 70% of the total depth, it seems reasonable to just choose a network that has the correct number of total layers to match the Serre et al.'s findings. Also, they claim that they use VGG-16 because it is the best performing network, but that is not true according to Figure 3a. VGG-19 performs best out of the box, only after fine-tuning VGG-16 do they do the best. They do not comment on how well a fine-tuned VGG-19 net would do.  ",2-Confident (read it all; understood it all reasonably well),"The paper conducts a psychophysics study to assess the correlation between human performance on rapid visual categorization tasks and that predicted by features learnt by recent deep neural networks when trained on large data. In particular, the study evaluated correlation vis-a-vis features learnt at various depths by such networks. The study concludes that with a suggestion that the feature representations used by the human visual system may be of intermediate complexity, similar to those learnt by deep learning networks at intermediate depths (around layer 10). ","The paper addresses an important question - regarding the complexity of the features computed by the human visual system for rapid visual categorization. The paper is also written well and as far as I can tell, the experiment conducted is a novel one and may add value to the knowledge regarding the human visual system. However, I am not convinced about the value of the particular experiments conducted and the results reported in the paper.      In particular,  (a) There is a mismatch between the relative depths at which these networks obtain human level accuracy and the relative depths at which the correlation with human performance on individual images peaks. This is an important artifact that goes unexplained in the paper.     (b) Further, the correlation with human performance peaks at 35 percent and hence I am not sure if the features computed at those depths or the complexity of these features says anything about (or predicts) the kinds of features computed by the human visual system.  (c) It is well known that the human neural mechanisms as well as the network topology are very different from those used by ANNs.   (d) Further, there is an issue with response time cutoff being kept at 500 ms. As discussed in lines 192-196, increasing the response time to 1000 ms increases human accuracy significantly. Though this does not differentially impact conclusions about the relative depth at which the correlation peaks, it would perhaps reduce the artifact mentioned in point (a) here. These performance curves are not presented.     In light of all of this, I am not sure there is a useful takeaway from the experiments and the results reported in the paper. I am willing to be persuaded otherwise by the authors in the rebuttal phase.  ",2-Confident (read it all; understood it all reasonably well),"This paper compares the performance of humans and deep networks in fast categorization. The aim is not to improve performance but rather, provide some insight as to the extent to which natural processes in the brain may match current vision recognition architectures. The meat of the paper is an experiment of animal vs. non-animal fast categorization. On the human side, the experiment is run on MTurk. On the machine side, several deep networks (AlexNet, VGG16 and VGG19) are trained and the representations learned at each layer are extracted and used as input for classification with a linear classifier (SVM or logistic regression). Correlating human and machine decision scores show that human decision profile corresponds best to intermediate representations of deep networks, and that the extra layers afford artificial nets an added complexity (that indeed may explain their super-human performance).","This is an interesting paper for people who are interested in the link between artificial nets and human brain functioning. The paper reads well, is clear. I would expect a subset of the nips audience to be interested in those results. The discussion sees the fact that the maximum correlation is at about 10 layers, as consistent with the number of processing stages found in the ventral stream of the visual cortex in ref. 16 (Serre et al 2007). I find this claim a bit strange -- ""around 10"" can be 70% of 19, which is 13.3 and a margin of about one third, which doesn't seem much of a claim for networks of a maximum depth of 19 layers -- it all seems to be the same order of magnitude. What would the result be with even deeper networks, like the approx. 150 layers of this year's ImageNet? Would the authors expect the maximum correlation to then be at around 10% of the total layer depth?","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The work describes a classic psych-phsyical experiment, where the subject is asked to answer the question if an animal can be seen in the picture. Results are then compared with the output of several out of the box available deep learning systems. The decision process is then done by using a linear support vector machine. Main result of the paper is a comparison of human results with the performance of the ANNs for each layer, where human performance is outperformed at 30% of the layer depth of the used depth of the deep learning algorithms.","One issue is that the paper still lacks real clarity one the meaning of the relative layer depth, that is how that corresponds to what kind computational processing complexity. So still it is hard to interpret the data, i.e. that humans are outperformed on what level of the layered natwork doing what.",1-Less confident (might not have understood significant parts),"The paper provided a large-scale experiment to measure visual classification performance of human being, to compare with state-of-the art deep learning methods. The methodology to assess the visual processing mechanism is well organized. However, there is no novel techniques proposed in this paper. Psychologically, it is found that human's visual processing may be comparable to 8-12 layers of current deep networks in terms of correlation.","There is no discussion on the reason why the artificial networks with layers having peak correlation can outperform human's behavior. There is no counterintuitive result. In other words, it is better to add more important insights to assist further understanding of psychological and technological advancement. It is also better to propose a bit new signal processing method rather than using existing methods.",2-Confident (read it all; understood it all reasonably well),,,,,,,,,
Generating Videos with Scene Dynamics,"Carl Vondrick, Hamed Pirsiavash, Antonio Torralba",https://proceedings.neurips.cc/paper/2016/hash/04025959b191f8f9de3f924f0940515f-Abstract.html,"This problem tackles the problem of generating short (with very few frames) tiny (64x64) videos. The approach uses a ""deconvolutional"" neural network that is split into two parts: the first predicts a foreground ""motion"", and a mask. The second predicts the motion of the background. The mask is then used to combine the two and obtain the desired generated video. In order to ensure that the videos generated are plausible, the networks are trained in an adversarial setup.","Overall this paper is very clearly laid out, and it is very easy to follow. Given that the authors are basing much of their method on existing methods for image generation, the novelty of the method lies in the way they adapted such methods to generate video.  It is important to emphasize that I am not familiar with any other papers that attempt to do this (and the authors also didn't seem to be able to find other such papers).  The problem with video, unlike images is that low frequencies are not only spanning space, but also time. Therefore, when generating video, typical methods will attempt to generate the temporal low frequencies first, resulting in very jarring outputs. The authors tackled this problem by explicitly decomposing the ""background"" from the ""foreground"". The background network's task is to generate the ""low frequencies"" while the foreground can focus (and will focus) on generating the more interesting parts (the high frequencies -- or motions of the ""small"" objects).  In order to generate ""plausible"" images, the authors employ an adversarial critic network (or discriminator).   In terms of the technical contents, outside the high level ideas, I have some questions for the authors: 1) why did you not use batch norm in the discriminator in the first layer? 2) how come your generator/discriminator don't use the same activation functions (i.e., ReLU vs Leaky ReLU)? In the final version of the paper, please attempt to describe the reasoning for these decisions, and possibly provide results showing a more consistent setup.    Regarding the human eval results, I appreciate the honesty -- most of the videos don't seem ""real"", and some are rather jarring (especially the baby videos).",2-Confident (read it all; understood it all reasonably well),"The paper describes a generative adversarial convolutional neural network for video (a block of 32 frames x 64x64 pixels). The architecture is divided into two streams one for static background and one for the moving foreground that are combined together using a mask that is also generated. The loss is a standard generative adversarial loss. The paper shows video generation experiments on four scene categories with qualitative results and quantitative user-study evaluating the realism of the generated videos. In addition, the paper uses trained representation on 5000 hours of Flickr videos for action classification demonstrating that the resulting representation serves as a better initialization for training  action classification models.","Strengths:  - The addressed problem of generative models of videos is interesting, timely and difficult. - Novelty: The proposed two stream architecture (static background / moving foreground + mask) for video generation is novel (albeit somewhat incremental over the previous static image generation GANs). Nevertheless, I like the proposed extension. - Experiments: The set of experiments is fairly comprehensive (generation, classification, user study). The results are encouraging, but but the visual quality of the generated results is quite poor and the action recognition results are much below the current state-of-the-art on the considered UCF dataset.  Weaknesses:  - The paper is somewhat incremental. The developed model is a fairly straighforward extension of the GAN for static images. - The generated videos have significant artifacts. Only some of the beach videos are kind of convincing.  The action recognition performance is much below the current state-of-the-art on the UCF dataset, which uses more complex (deeper, also processing optic flow) architectures.  Questions: - What is the size of the beach/golf course/train station/hospital datasets? - How do the video generation results from the network trained on 5000 hours of video look?  Summary: While somewhat incremental, the paper seems to have enough novelty for a poster. The visual results encouraging but with many artifacts. The action classification results demonstrate benefits of the learnt representation compared with random weights but are significantly below state-of-the-art results on the considered dataset.",2-Confident (read it all; understood it all reasonably well),"The paper to propose to use a two stream architecture to generate videos, where one stream generates the background frames and another one generates the foreground frames and a set of masks. By having two streams it allows to generate different motions for the background and the foreground. The model is trained using a VGAN approach and trained small video clips, the authors show promising results for video generation, video representation learning and for animating images.","After so much recent work in image generation using Generative Adversarial Networks it is nice to see a novel proposal for videos. The paper is well written with nice examples.    It is not very clear how much of the performance is due to the data cleaning, for instead filtered videos with a pre-trained image model and camera stabilization. There are no experiments using unfiltered unlabeled videos for generation, or without stabilization.  The results in action recognition in UC-101 are ok, but a comparison with a model using a pre-trained on image classification is missing.  There is not evaluation of the animated images.",2-Confident (read it all; understood it all reasonably well),"In this paper the authors use the generative adversarial network (GAN), specifically DC-GAN in order to generate tiny videos. They do so by using a two stream architecture. One generates the background still image and the other generates the foreground motion and mask. The mask comprises of a separate last layer of the foreground generator that uses sigmoidal activation.  The authors in addition to generating videos from noise, show applications for action classification that is competitive and extrapolation of single frame image.  ","The proposed work is novel and the authors have provided sufficiently convincing evaluation of their method. The results for specific class videos look reasonable ( sometimes confusing human evaluators). Overall I think that the method and evaluation are good enough to justify acceptance of the paper.   One point could be that the evaluation could perhaps consider an additional architecture where one initially generates a background frame that is animated in a one stream architecture. Right now the motion is separately generated from the background and is combined. In this case, the generation would be more coupled.  In any case, this architecture is partially evaluated when the authors consider extrapolation of single frame images.  A minor point is that few videos are available for actual evaluation. A thorough evaluation with more videos would be appreciated.",2-Confident (read it all; understood it all reasonably well),"This paper proposed a tiny video generative model via GAN. The problem is challenging due to its huge dimensions. The authors propose a two-stream architecture by modeling foreground and background, respectively. The motion is modeled through 3D convolutional layers, which is a direct extension to the approaches of image generation through GAN [30, 23]. The results are visually pleasant in terms of appearance and motion. Moreover, this work also explores the task of unsupervised video representation learning through a well-trained discriminator. ","   The paper is novel in terms of exploring large-scale video generative model. It is generally well written and easy to understand.   The main concern is that, the proposed model is too general and mid to high level structured contents of a video are not well modeled even they are trained in each scene category respectively.   As a proposition of video generation problem, understanding the contents of both frames and sequences (e.g. objects, actions, etc.) is essential. However from the proposed architecture, the discriminator can hardly catch any mid to high level visual information since no such regularizations are enforced. The results show numerous unrealistic object shapes and irregular motions, which are far from realistic ones. This also applies to the results of animating images.  Although the problem is interesting, some results leave much room for improvement before it can be presented (with convincing results) in NIPS.  ",2-Confident (read it all; understood it all reasonably well),"The paper aims at generating short (32 frames) video segments with resolution 64x64. The proposed method relies on Generative Adversarial Network approach. There are 2 proposed architectures: straight-forward one and the one that generates static background and dynamic foreground separately. The training data is generated in a way that tries to ""stabilize"" images by compensating camera motion. This results in ignoring samples for which it can't be done reliably.  There are 3 types of experiments: 1. video generation. Samples, generated from different architectures and self-implemented baseline, are compared with real videos by asking Amazon Mechanical Turk assessors to determine which of the two videos is real. Assessors prefer generated videos to real ones in 18% of the cases on average. 2. pre-training for action classification. The algorithm does not outperform current state-of-the-art Temporal Ordering. 3. generation of video conditioned on the first frame. ","Experiments demonstrating large motion of large objects at larger resolution would be very important for understanding if the method works reasonably. Many scenes shown in the results either almost do not move (like hospital 1,3,5,6,7,8,9,11,12,13,14,15) or provide strange-looking movements (like oscillating face in hospital 2,10 and many train videos). Larger resolution would certainly be helpful as assessors would be able to better see what is actually happening in the video. At the current resolution it is very hard to see what exactly is happening to moving objects. Most of the sea videos just show flickering of the water, how natural it looks is also only possible to say for larger resolution. ",2-Confident (read it all; understood it all reasonably well),,,,,,
LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain,"Zeyuan Allen-Zhu, Yuanzhi Li",https://proceedings.neurips.cc/paper/2016/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html,"The main result seems to be an improvement of [14] for spectral-error low rank approximation, from time k*nnz(A)/eps^{1/2} + k^2 d / eps + k^3/eps^{3/2} to k nnz(A)/eps^{1/2} + k^2 d / eps^{1/2}. This is interesting in certain regimes, if say, eps is very small and nnz(A) is about d. Note here the time is worse than nnz(A) times one can achieve with Frobenius norm error, though in certain applications one may need spectral error.   My understanding of the savings - and I hope the authors can clarify this - is that [14] was a bit crude with the k/eps^{1/2} vectors in R^d they find via a block Krylov iteration. I think they just compute an orthonormal basis for such vectors which gives k^2 d/eps time (ignoring fast matrix multi optimizations). This paper achieves k^2 d / eps^{1/2} time essentially because they never have to perform such an orthogonalization; they instead iteratively find a very good rank-1 approx, then they deflate off what they just found, and repeat by finding a very good rank-1 approx on what's left, in total k times. They still need O~(1/eps^{1/2}) iterations for each rank-1 approx, and they need to deflate off what they just found, but given that the vectors they find anyway are almost already orthogonal they just pretend that they are and the deflation takes O(kd/eps^{1/2}) per iteration, giving the O(k^2 d/eps^{1/2}) in total. Another key difference with [14] is that the 1/eps^{1/2} is coming from the shift-and-invert algorithm of [7] and [8], which was not directly used in [14].   By instantiating the above framework with different algorithms in different settings, the authors obtain tradeoffs also in a stochastic setting. They also obtain nnz(A) algorithms with a mixed spectral and Frobenius guarantee though I think there is an issue here - see below.  ","I mentioned a comment in the summary above regarding intuition. I hope the authors can clarify that.   Also, of importance, is for the authors to state what is the relationship with the following work which seems to use similar techniques for a similar problem: http://arxiv.org/abs/1607.02925  I didn't find the nnz(A) results very compelling. The main reason is that the authors seem to miss the earlier work of Cohen et al. ""Dimensionality Reduction for k-Means Clustering and Low Rank Approximation"" http://arxiv.org/pdf/1410.6801v3.pdf Applying Theorem 27 in that paper, with say, the composition of an OSNAP with an SRHT then a Gaussian, one can in nnz(A) + O~(k^2 d/eps^2) time reduce the dimension to k/eps^2 x d, at which point one can find a good low rank-k approximation of the resulting matrix using any method (this is the definition of a projection-cost preserving sketch). Importantly, the same mixed-Frobenius and spectral error guarantee is obtained. Actually the result in the Cohen et al. paper could be stronger because there is an eps/k factor in front of the Frobenius norm term, and importantly it doesn't have a sigma/sigma_{k+1} term in it, as this submission does, which can be arbitrarily large. The overall comparison of the two works depends on which algorithm one uses to solve the k/eps^2 x d problem.   Just a few presentation comments:  (1) the table in the stochastic setting was a bit confusing, it suggests  kn^{3/4} d/(sigma_k^{1/2} eps^{1/2}) <= kd/(sigma_k^2 eps^2).  which can't be true, so I guess the authors instead mean their bound is never worse than what is in parantheses.   (2) in general I feel the writing style could be toned down a bit. ",2-Confident (read it all; understood it all reasonably well),"The paper proposes a new way of computing a truncated SVD, which is based on recent work (Shift-and-inverse routine). The main result concerns a gap-free convergence result, where the gap is somehow the distance between the two lasts singular values divided by their magnitude.","The results presented in the paper seem very impressive, and the experiments are quite convincing.  I'm not able to judge the technical aspects of the paper, but, at my level, it seems to me that it's a very good paper.",1-Less confident (might not have understood significant parts),Provides a new iterative K-SVD algorithm with tighter accuracy/runtime guarantees,"This paper highlights several very interesting observations.  1) pointing out that many iterative K-SVD algorithms are dependent on spectral gap, and giving a method with provable guarantees to avoid this issue.  2) proposing a vector-by-vector method that estimates each singular vector without using stochastic sampling, giving more reliable answers  3) carefully setting up the matrix inversion so that there is strong convexity, and can therefore leverage the accelerated gradient descent method's fast convergence guarantees.  However, several limitations should be noted.  1) The paper claims to solve the K-SVD problem, but in fact the matrix input must be symmetric positive semidefinite with eigenvalues between 0 and 1. This is K-eig, not K-SVD.   2) After implementing the method myself, I found its results very sensitive to gap. For my naive experiment, anything with a gap > 0.01 was fine, but with a gap < 0.005 will have a very small chance of recovering the correct eigenvectors.   3) I don't think this algorithm preserves sparsity, which makes me question the scalability of it. (This is the regime where approximate K-SVD is useful, after all. Also note that both Krylov methods and subspace power methods exploit sparsity, which is one of their main selling points.)  4) The stopping condition given for the matrix inversion steps also use a matrix inverse, which is clearly impractical.  5) It would be nice if the writers provided code online. As it stands, my naive implementation runs much slower than Krylov or subspace power method.  6) A minor detail, though the writing in the main body is fine (though a bit notation dense), the title and abstract read very strangely. (For the title, the D in SVD is decomposition.)",1-Less confident (might not have understood significant parts),"The author(s) focus on the k-SVD problem in this paper, which aims to obtain the first k singular vectors of a matrix. Specifically, they improve the recent studies by presenting a new framework which can be characterized in three aspects, i.e., a faster gap-free convergence speed, the first accelerated and stochastic method, and better parameter regimes without using alternating minimization.","The paper studies the popular k-SVD problem by proposing a simple framework to find the singular vectors for k iterations. The paper presents Table 1 to clearly illustrate the performance comparison among the proposed algorithm and other baselines. Overall, it is easy to capture the contributions of the paper, and the proposed framework seems effective and interesting.   Here are some questions and suggestions. 1. The paper should be re-organized by providing more detailed verification of the theorems, e.g., Theorem 3.1, Theorem 4.1, Corollary 4.4, Theorem 5.1. It is a little difficult to understand the theorem in the current version, although the ideas of their proofs are presented.  2. It is confused to me that in the beginning of experiments, the author(s) mention that Lanczos method is adopted as a replacement of the proposed method AppxPCA because of the faster speed of Lanczos method. In previous section, AppxPCA is set to be an important component of IterSVD. So this replacement seems contrary to the purpose of the experiments, which aims to verify the practicality of the proposed framework.  3. More analysis and reasoning should be given about the observations of the results at the end of the experiments.",2-Confident (read it all; understood it all reasonably well),"The authors propose a algorithm called ItrSVD and show an improved bounds of solving classic SVD problem. The algorithm use the recent advance [7] as the building block to solve each eigenvector one by one, which makes this work less novel. Also, the author actually implement Lanczos as the building block in the experiment. So I treat the whole algorithm as the Lanczos algorithm for solving k-SVD but with better theoretical analysis. However, the main contribution of this work is to answer how to decide the precision of solving each eigenvector, which is related to the convergence behavior and show an improved bound. The experimental results seem promising but there are some missing parts.","My main concern would be the experiment results. Although the theoretical analysis is based on [7], the authors used Lanczos algorithm  as the building block in the experiments. I had some experience about using alternative minimization (ALS) an Lanczos algorithm on some computer vision applications. My previous experience show ALS is faster than Lanczos in practice. Therefore, I would like to see the comparison on these two algorithm during the rebuttal and more implementation details (some code if possible).    Minor comments: 1. For a fair comparison, the authors use single thread, which is acceptable. However, for the practical use, I would like to know which algorithm is faster if we are allowed to use more threads. Will power method still be the fastest one?   2. The authors mention the constant and the polynomial dependency can be further improved. It would be appreciated if the authors provide some scratches or ideas.  ",2-Confident (read it all; understood it all reasonably well),"The paper describes a new algorithmic framework to compute k-SVD of a matrix A via computing singular vectors sequentially rather than together. The authors prove that their computations are time-independent of the relative differences between the singular values of A (gap free) and that their framework outperforms the the current state of the art in many cases. In particular, they obtain faster accelerated as well as accelerated and stochastic algorithms than previously documented in literature.","It would improve readability if theorems (theorems 2.1, 3.1, corollaries 4.4, 4.5 for example) are preceded with some brief exposition summarizing or highlighting their content and relation to the later technical developments in the paper.",1-Less confident (might not have understood significant parts),,,,,,
Causal meets Submodular: Subset Selection with Directed Information,"Yuxun Zhou, Costas J. Spanos",https://proceedings.neurips.cc/paper/2016/hash/81ca0262c82e712e50c580c032d99b60-Abstract.html,This paper discusses the problem of causal subset selection using directed information.  The problem becomes one of maximizing a submodular (or approximately submodular) function  of choosing which features to use for prediction. The most interesting aspect is a novel definition of approximate submodularity and the obtained results on approximation that it yields.  ,"There are really two issues with this paper. The first one is the discussion on how to use directed information for causality. Unfortunately, the word causality means two different things and these should be separated (and this discussion should be mentioned in this manuscript I think).  The first concept of causality is really *prediction* of a time series using other time series. This prediction respects time and hence people call that causal. Given a bunch of time series we can ask which one can be used to better predict another (in the next time-step) and use this as a measure of causality. Then we can talk about finding the k-best predictors and get into 'causal' feature selection etc, as the paper does. The connection of this Granger-type of causality with directed information is interesting and was recently explored, e.g. in [20] as cited in this paper. This paper builds on this type of causality that perhaps should be called generalized Granger causality, or prediction causality or something like that.   The second type of causality (that is in my opinion, and in the opinion of most people working in this area the correct one, as far as I understand) relates to counter-factuals (or the related structural equations). In this world, the key issue is what WOULD happen if I changed X, would Y change? This relates to understanding the functions or mechanisms that generate the data, rather than predicting.  It is important to state in this paper that the best time-series prediction tells us nothing about how modifying one time-series will actually change the other.   In short, I think this paper has quite interesting results but on prediction of processes and not really about causality. More specifically, the most interesting results are on submodularity and perhaps the authors should modify the manuscript to emphasize on the approximation of near-submodular functions (with Granger causality as an application). For the non-Granger causality literature, the authors should be perhaps mentioning the work by e.g. Pearl, Imbens and Rubin and Richardson.     Theorem 2 is interesting and the proof seems correct.   Lemma 1 is the key in showing how submodularity is relaxed in this paper (even though its called lemma 2 in the appendix ) and the proof is clear.   Theorem 3 and its corollaries are the most interesting results in the paper I think, and they are really about a new relaxation of submodularity that is independent of causality (or any information metrics really) so that should be made clear in the paper.   The second issue I'd like to raise is the following:  Das and Kempe in [3] define another relaxation of submodularity called the submodularity ratio. It is incorrectly stated in this paper that [3] defined the concept of submodularity ratio for R^2 score only. In fact, the submodularity ratio is actually defined in [3],Def 2.3 for an arbitrary set function and then specialized for the special case of R^2.  Furthermore, the performance guarantee that generalizes the 1-1/e result (Theorem 3.2 in [3]) actually holds for any set function with bounded submodularity ratio. This is stated in [3] before theorem 3.2 but admittedly the paper does not emphasize this general result enough in the abstract/intro.    Now my main question is if (some of) the obtained results in this paper are also direct corollaries of Theorem 3.2 of [3]: This is because (if I understand correctly)  the Submodularity index defined in equation (6) of this paper is the difference of  adding elements one at a time vs adding them all at once.  The submodularity ratio of [3] on the other hand seems to be exactly the ratio of the same quantities.  Submodularity means the difference is greater than zero (lemma 1 of this paper ) which is equivalent to ratio >=1 (as stated in [3]).   Submodularty ratio: sum fx(A) / f_S ( A) >=1   SMI defined in this paper sum fx(A) - f_S ( A)  >=0   Also the relaxation of [3] is Submodularity ratio >= constant gamma  and in this paper SMI difference negative but close to zero.   Since the obtained results seem to be normalizing SMI/ f(S_greedy) it is possible there is a direct mapping from the results of [3] to this paper for general set functions.   This is a non-trivial analysis for the authors to perform, so I would simply recommend that the authors discuss a possible connection to the ratio of [3]. The analysis is even more complicated since [3] only yields results for monotone functions while this paper obtains results for both monotone and non-monotone, so it would seem that the authors could be generalizing [3] beyond monotonicity which is very interesting.   Minor Comments: [3] the author order should be reversed.   Proofs of Proposition 2 and 3 where not clear (or i did not find them).   The paper has many typos/ grammatical errors and there are some missing ?? in the Appendix.   Examples of typos: Consider two random process X^n (processes) directed information maximization-> maximization problems (page 3) address in details-> in detail.  any polynomial algorithm-> polynomial-time algorithm  Throughout the paper: 'Monotonic' submodular functions are called *Monotone* we mainly analysis-> analyze  we can justify its submodularity, (confusing sentence, please rephrase) that in literature-> in the literature (page 4)  several effort-> efforts   Overall I recommend that this paper is accepted after these issues have been addressed since the combinatorial optimization results are quite interesting.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors study two related problems which they refer to as source detection (OPT1) and causal covariate selection (OPT2).  OPT1 is related to a body of work on sensor selection, but whereas past work typically use mutual information this work uses directed information.  Likewise problems like OPT2 have been studied for decades, though often with strong modeling assumptions.  The authors suggest using directed information for that too (it is a non-parametric method).  The authors show that OPT1 is submodular but not monotone (thus greedy search has performance guarantees), while OPT2 is monotone but generally not submodular.    The authors introduce a novel measure of how ""submodular"" a functions is (which they refer to as submodularity index SmI) and obtain performance bounds for maximization in OPT1 and OPT2 with cardinality constratints, where the bounds are a function of SmI.   They also analyze both simulated and real world data, showing performance of the algorithm and computing some empirically observed SmI values. "," Overall, I find the work promising and it contains a number of results which will appeal to the NIPS community.  However, I think a significant revision for OPT2 is needed for it to appear at NIPS.  They are also missing citations for work closely related to their work on OPT2.  There are multiple minor issues that should also be addressed, listed further below.  Major comments (What I like):  -- Both problems are interesting and relevant to NIPS. Especially OPT2, broadly speaking, has been of great concern and generated decades of research in various domains.  To approach it with a non-parametric method is quite appealing because then one needs not worry as much about strong modeling assumptions.     -- SmI is an intriguing way of assessing submodularity, though it makes sense.  It is highly context dependent (function of A and k), but that seems necessary given your later work and data analysis.  I like how the results are not limited to specific functions but could potentially be applied to a broad range of combinatorial problems.    -- The performance bounds are good.    -- It was applied to both simulated and real world data.  Aside from empirical values for SmI, it does give credence at least to employing greedy methods (especially when exact is computationally prohibitive).    Major comments (What I think needs to be modified to improve the work):  (A) My major concern is with regards to the usefulness of the bounds for OPT2.  While results for OPT1 are clearly valid (since is submodular), the usefulness of the bounds for OPT2 is unknown.  They are useful only when OPT2 exhibits ""quasi-submodularity,"" but when does that happen?  Statements such as       (line 44) ""is “nearly” submodular in particular cases""   (line 216) ""For quasi-submodular functions having negative SmI, although a degraded guarantee is produced, the bound is only slightly deteriorated when SmI is close to zero.""   (line 227) ""Similarly, for quasi-submodular functions, the guarantee is degraded but not too much if SmD is close to 0. Note that the objective function of OPT2 fits into this category."" are made throughout the paper but not justified or made precise.    ""quasi-submodular functions"" is defined as (line 163) ""We call a function quasi-submodular if it has a negative but close to zero SmI.""  The authors never give a precise definition of what consistutes ""close.""  They also never give examples of when OPT2 is in fact quasi-submodular except in the extreme case of Proposition 2 (line 131) where you have independence *conditioned on Y_t*.  [Please give an example that satisfies Prop 2.  Even when causal covariates are marginally independent (which is already restrictive), say Y_{t+1} is a linear combination of marginally independent X_{it}'s,  then conditional independence (prop 2) won't hold.]    This is not to say that the formulas obtained are not useful for monotonic non-submodular functions (especially since they are applicable to potentially a wide range of combinatorial optimization problems), but some non-trivially ""quasi-submodular"" optimization problems need to be identified, and I don't think OPT2 is a good candidate.  Perhaps for some certain classes of distributions it could be shown to be (and if the authors can do this, I would be happy with them keeping OPT2 in), but in general not.  The graphical model literature on structure learning has examples where there is pairwise independence (similar to I(X-->Y)=0, I(Z-->Y)=0)) but the joint relationship (so I(X,Z-->Y) can be made arbitrarily large when the alphabet is not restricted (eg using the XOR function and converting vector valued binary variables (with Y(i) = X(i) XOR Z(i), then convert the vector to a larger-alphabet variable)).       Also, note that while I am happy to see they applied their work to data, and that the randomized greedy did well, the fact that randomized greedy did well does not imply quasi-submodularity holds.  [Greedy algorithms (though not randomized greedy) have been widely used in graphical model structure learning literature, for random variables and processes.]  The experiments do provide evidence for using randomized greedy, which is good.  I was furthermore happy that they included some empirical values of SmI index.  And for row 2 of Table 1, since all the values are larger than -0.6321 the bound in corollary 1 (line 224-225) would be nontrivial (approximation ratio bigger than 0).  This gives a little evidence of quasi-submodularity, and it is interesting that k=2 is the worst, whereas others have positive or ""close"" to 0 values.  In addressing this major concern, if proving when quasi-submodularity holds (maybe you can define ""close"" to be relative to E[f(S^g)], such that close means bound in 224-225 is nontrivial) is too difficult, empirical evidence is fine, though it would need to be more extensive than for a single stock in one dataset.   (B) The paper is missing a reference that appears to be closely related, especially for OPT2.  Searching ""Submodularity directed information"" on arxiv yielded two works     * [highly relevant] ""Bounded Degree Approximations of Stochastic Networks"" http://arxiv.org/abs/1506.04767 by authors Quinn et al (you cite some of their other works) which appears related for the OPT2 problem.  Especially Section 5 of the arxiv version, as they do cardinality constrained causal covariate selection (OPT2; it looks like they are doing ""approximation"", though the objective functions look similar) and propose a approximate submodularity measure and obtain bounds on the deterministic greedy search.  Their approximate submodularity measure is not the same as in this submission (SmI) and I can't tell whether the one in Quinn et al would be useful for studying OPT1.  Also, wrt my comment above about ""quasi-submodularity"" being unjustified, this work by Quinn et al also does not theoretically justify when their approximate submodularity is useful (when their alpha is close to 1).      They mention the arxiv version (I don't see it published yet) is based on a conference paper ""Optimal bounded-degree approximations of joint distributions of networks of stochastic processes"" ISIT 2013, which looks to have the same results (regarding approximate submodularity), though the causal covariate selection looks a little different.      * [less relevant] a very recent paper (just a few months out before the NIPS 2016 initial deadline) ""Submodular Variational Inference for Network Reconstruction"" http://arxiv.org/abs/1603.08616 that reconstructs the network, but does so assuming a diffusion model.           Minor comments   -- mentioned in Major comment (A), please provide examples when Prop 2 holds, esp if you cannot more generally identify when OPT2 is quasi-submodular.    -- Question: This work employs the CTW algorithm.  But the available code http://web.stanford.edu/~tsachy/DIcode/download.htm and the original paper [9] are for pairs of processes.  In the submission, you are dealing with sets.  Did you modify the code to handle DI for sets?  If so, are you sure the theoretical guarantees extend?  Or did you do a mapping, such as converting three binary processes into one process of alphabet 8? This might be straightforward but I wonder if there would be performance hit (for finite length data). I'm assuming you did not just find pairwise relations for the original processes and then combine individual parents, as that could lead to significant errors.      -- Can you provide any guidance to the reader on selecting $k$?  I think this is important given that not only there are computation tradeoffs but the approximation coefficient is sensitive to $k$ (especially for OPT2)    -- Line 185 with Algorithm 1.  You should provide a sentance or two explaining how this random greedy relates to deterministic.  It is not obvious at first glance of the notation, even though the idea is simple.  ""At each step, instead of picking the best, we pick one of the top k best"" or something like that.      -- line 131 ""any"" can have multiple meanings.  I assume you want it to mean ""every pair"" though it can also mean ""at least one pair"".  I'd suggest using ""every"" to avoid confusion.    -- Provide references for quantities in Section 2.  At first I thought you were introducing them but they are in some works you cite.    -- Double check your Reference section; right now it is a little sloppy (eg line 313 ""gaussian"" should be capitalized, journals in 340 and 346 should be capitalized, conference title in 358 should be capitalized, you use anacronyms for some conference proceedings but then write out names for others)    -- Lemma 2 line 197 please clarify the notation.    Are these lower case y's processes?  If so why lower case?  And is Y={y_1, .. y_M} or a strict subset or a strict superset?    -- in line 201 you make a comment about being able to lower bound SmI for order 2, yet wasn't that the worst case you found empirically? (thus suggesting it would be challenging to do so)    -- fix grammer line 205 ""with each element appears in""    -- formula below line 201, what is ""p_1""    -- line 238, in formula for \gamma, if S^{g_k} means |S^{g_k}|=k, then just use k in the formula, and you might consider putting a table or graph to show behavior    -- line 249 the data you use is old -- if it appeared in another paper you should cite the source.      -- Section 5  for the data, I'm sure the stock data is not binary, perhaps you designed the Bayesian network data to be so; but describe how your discretize the data (especially since to just binary, not even quantizing)    -- line 257 you should not say ""is sufficient"" for real world data unless you have ground truth knowledge you are basing that off of.  Even convergence is not sufficient (maybe for length 5 it converges to one value, for length 6 another)     -- line 261 include an equation reference; ""the 1/e bound"" is vague","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The basic idea of the paper is to formulate the two problems, source detection and causal covariate selection, into cardinality constrained directed information (DI) maximization problems. It is then shown that the objective function of source detection which is precisely the DI from selected set to its complement is submodular, although not monotonic. It is also shown that the problem of causal covariates selection which is the DI from selected set to some target process is not submodular, however, the authors define a new notion called ""near-submodularity"" which holds in this case (at least in some problem instances).  A novel quantity, namely submodularity index (SmI), for general set functions is introduced to measure how ""submodular"" a set function is. for a non-submodular functions SmI is negative. But if SmI is not so small, then the algorithm random-greedy will still perform near-optimally (even for non-monotone functions).  As a separate contribution, the authors derive near-optimality bounds of the performance of the random-greedy algorithm based on SmI. Such bounds are then incorporated into analysing the behaviour of random greedy on the two main problems considered in the paper (i.e. source detection and causal covariate selection).  In general, computing SmI may be intractable (exponential in size of the ground set). Hence, the authors provide a simple lower bound for SmI for the two problems considered. These theoretical results are then verified by numerical experiments. ","The paper is clearly written in most parts and I enjoyed reading it. Apart from proposing efficient algorithms for the two problems considered (source detection and causal covariate selection), I believe that a separate contribution of the paper is to show  the near-optimality of the random-greedy algorithm in terms of the so-called submodularity index (SmI). As a result, for functions that are ""approximately"" submodular (i.e. have a small SmI), the greedy algorithm may work well. Computing the value of SmI is exponential in terms of the size of the ground set. I suggest that the authors: (i) provide computable bounds on this index (as done in lemma 2) for other classes of se functions, (ii) provide some more applications where SmI is small.  I should also mention that the bound provided in Lemma 2, becomes useless as n growrs as the mutual information terms will most probably grow by n. As a result, I suggest that the authors investigate the performance of random greedy on problems (lets say source detection) with the ground set size |V| being large (100 or so). It would be interesting to see how the proposed methodology compares to other algorithms in this setting. Also, in the experimental results, can the authors explain why they have not made any comparison with the other methods (e.g [21-22])?  some minor comments: Line 60: ""i"" should be ""t"" or vice versa. Line 101, you never formally define what S^n is. Line 161: perhaps strictly-submodular? Line 280: should be ""reduced"".   ",2-Confident (read it all; understood it all reasonably well),"The authors attempt at two submodular optimization problems motivated by directed information maximization. The directed information is used as a measure of causality. Although this is in general not accepted as a correct causality measure, especially when more than two variables are involved (such as in the example application of learning causal graphs at the end), it is still widely used and preserves its relevance in certain problems. Then authors move to consider a generic non-monotone submodular function, and define a measure of ""submodularness"" based on the the collective diminishing value of the derivative of the function, which they call submodularity index, SmI. This metric allows them to obtain potentially tighter bounds on the optimum value of the submodular maximization problems, although the bounds have a complicated relation with the result of the randomized greedy algorithm, and the gain is not immediately clear.  The problem statements are not well justified, though the problems are theoretically interesting. The submodularity sections are interesting on their own without a causal interpretation. Authors are not careful in using the notion of ""causality"". Directed information is not a generally accepted notion of causality and in general it cannot be used to predict the effect of an action (intervention) on the system.","I would suggest authors to pose the paper as a submodular optimization paper, with applications in directed information maximization. This is because the main technical contribution of the paper is not in the causality literature, but in the submodular optimization. Also, this way, authors will avoid a discussion on whether directed information is the correct metric for causality or not.  In the second paragraph of introduction, authors completely ignore a big part of causality literature, conditional independence-based, score-based learning methods, and methods based on assumptions on the structural equations. Please cite Pearl's or Glymour's books as a relevant reference, and justify why directed information is an alternative to these in dynamic systems.  The problem formulation is not well justified and some claims are even incorrect:  They state ""The optimal locations can be obtained by maximizing the causality from selected location set S to its complement \bar{S}"".  1)  Please avoid using ""causality"" as a measure. Use ""directed information"". This is especially important since the causality measure of directed information, as used here, is not well accepted by all the researchers in the field. 2) By optimal, authors refer to the ""source sites"" of the pollution. This is even used in a case study only provided in the Supplementary material. This claim is incorrect. If the objective is to find the sources of the pollution, i.e., the set of nodes that cause the remaining set of nodes in a causal graph, using directed information will yield incorrect results. Consider three nodes that form a collider, basically X->Z<-Y. There is no reason to assume that choosing X and Y will maximize the directed information to Z. Hence this approach would not reveal the source nodes of the graph. I recommend authors to rephrase their objective.  The second problem is to maximize the directed information from a set of nodes to another set of nodes, which is a generalization of first problem, which is termed as ""causal covariate selection"". However this formulation has the same problem described above.  These are not the mere result of authors using a different notion of causality. In the first paragraph, authors give the example of advertisers choosing opinion leaders to maximize the influence of their ads. Using a node for advertising is a type of ""intervention"". And if the directed information does not yield the causal sources of the network, intervening on a child will not have any impact on its parents. Hence authors should be very careful not to make any claims about the result of a possible intervention, using directed information as their metric.  From the motivation of the problem, we know that the set S contains variables that ""influence"" or ""summarize"" other variables. From this motivation, I expect the underlying causal graph to have subgraphs of the form s1->y<-s2. This is the type of causal relation that violates the condition in Proposition 2. Hence, given the setup of the problem, it is very unlikely that s1 and s2 will be independent given Y.  (7) defines the submodularity index (SmI), which is the main novel metric in the paper for measuring the ""submodularness"" of a set function. The minimization problem to find SmI requires searching through two disjoint subsets S and A. Proposition (3) claims (no proof given) that the minimized term is a supermodular function. And concludes that SmI can be computed using tools for minimizing a supermodular function with cardinality constraints. However there is one problem: The definition of SmI requires a search over both S and A. Hence, solving for SmI does not directly become equivalent to a supermodular minimization but exponentially many supermodular minimization problems.  I suggest authors explain why Algorithm 1 runs in time O(nk): Simply say calculating M_i does not require a search, since objective is a modular function of elements of M_i.  In the simulations: Causal Subset Selection Results: I believe ""the 1/e reference bound"" refers to e\times Random Greedy whereas Upper bound by SI refers to Random Greedy/(1/e+Delta) where Delta is the additional term in Theorem 3. Please clearly state this in the paper.  In the causal structure learning application: Authors do not define the causal graph in the main paper (only in the appendix). It is important to define what causal graph means in this context since there is no universally accepted notion of a causal graph. From the appendix, it is clear that this framework only works in the ""directed information graphs"". From a Pearlian perspective, it is expected that an approach based on directed information would not be sufficient for learning  (even the parts of) the causal graph. Hence, please clearly state that the application is only for learning directed information graphs in the paper.  In general, the theorem/lemma labelings in the paper does not match with the ones in the supplementary material: The labeling of Theorems 1 is not correct. All theorems lemmas are numbered together in the supplementary file starting from 1, whereas labeling in the paper has a consistent numbering. Please fix this for easier cross referencing.  Notation used in the proof of Theorem 2 is slightly confusing and inconsistent. Authors use \bar{A \cup {y}}_t to refer to the set of variables in the complement of A union {y} at time t. This format might be confused with y_t. Inconsistently, they also use A^t\cup {y}^t instead of (A\cup{y})^t In the manuscript. I recommend using extra space between variable set and t, or using |_t to clarify the set.  Also I(X,Y|Z) should be I(X;Y|Z).  The proof of Theorem 2 is correct.  Please use \coloneqq (or similar) to distinguish equalities from definitions. It will make paper easier to follow.  The proof of Proposition 1 is correct, although the authors skipped a few of the last steps.  The proof of proposition 2 is not clear. What is causal naive Bayesian? Please provide a complete proof.  Please provide the proofs for Proposition 3 in the supplementary material.  Lemma 1 of paper is labeled as Lemma 2 in the supplementary material. What is ""deviance"" mentioned in this proof? The proof is not complete in its current form.  \ref{.} is missing from ""lemma:proba"" in the proof of Theorem 7 (Theorem 3) in the supplementary material. Similarly in the Proof of Corollary 2. Also there is ??'s in Section 2 of supplementary material.  There are also some typos in the supplementary material (he->the), please proofread. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper addresses the problem of identifying subsets of causal sources by introducing a novel measure for quantifying the degree of submodularity. To apply greedy algorithms to this type of problem, the authors derive the submodularity index (SmI), a measure that allows them to provide bounds on a random greedy algorithm as a function of the SmI. Thus, this work generalises the well established approach of submodular function maximisation. Using these results, the authors study the problems of source detection and causal covariate selection (source/destination covariates where one is the complement of another) by the information-theoretic measure directed information (DI). The first problem is shown to be submodular but not monotonic, while the second is nearly submodular (according to the SmI). The theoretical results are evaluated using DBN's Murphy's Bayes net toolbox for generating datasets and revealing the underlying causal structure.","The paper covers several fundamental and relevant topics, and provides potentially interesting results for each topic. Unfortunately, this broad scope dilutes the impact of the paper; the contribution of the work as a whole is incoherent.  The notion of the SmI introduced in Sec. 4, and in particular, the implications of Theorem 3 are of relevance to a broad class of NP-hard problems. However, the authors restrict their application of SmI to causality analysis. Although the field of causality analysis is significant in its own right, I feel there is insufficient analysis of the fundamental properties of SmI prior to presenting this particular application. The paper reads as if its main focus is on the causal subset selection component instead.  The effect of the paper's broad scope is that it suffers in depth, and consequently is difficult to understand given only the material i the body of the paper. The first 5 pages present fundamental results on optimisation of (nearly) submodular set functions, but the proofs are not included in the body of the paper. These proofs are integral to the results of the paper and should occupy a more prominent position. It could be better to condense the analysis of the causal subset selection component to make room for this; I don't feel there is scope for both inside a single conference paper.  Minor comments/questions:  - You might also consider transfer entropy as a measure of predictability, for which I imagine you will have similar results for the SmI. DI was originally intended to be attributed to feedback structure (according to Massey). [2] is a good reference for TE over DI in certain situations.  - Need references for directed information and causally conditioned entropy in Sec. 2  - Line 6 do you substantiate the idea or quantify it?  - Line 8 ""the random"" or ""a random"" or ""we propose a random""  - Line 9 ""guarantee"" -> ""guarantees""  - Line 50 appears to be the first mention of SmI in text. It is not expanded prior to this but is expanded later (line 135-6)  - Line 70 ""a a""  - Line 140 ""several effort has"" -> ""several efforts have""  - Line 143 ""existing works"" -> ""existing work""  - Line 151 ""indexes"" -> ""indices""  - Line 120 ""detour"" should probably be ""digression""  - Line 229 should SmD be SmI?  - Perhaps a definition of monotonicity is required (at least in Supp. Mat.)  - The references appear to be below the allowed font size of \small  - References inconsistently use acronyms for some conferences (and abbreviations for some journals) and full titles in others  - Line 286 Clarify under which conditions this maximum DI finds the underlying graph structure. I assume for fully observed DBN's?   - Line 286 ""can be reduces"" -> ""can be reduced""  - Line 305 ""an novel""  - Line 313 ""gaussian"" -> ""Gaussian""  - Line 331 ""dc"" -> ""DC""  - Line 340 ""The Bayes""; Inconsistent use of ""et al.""; ""matlab"" -> ""MATLAB  - Line 350 Need periods after abbreviations, e.g., ""Comput"" -> ""Comput.""  - (Supp. Mat.) Lemma 5 ""definition (??)"" is probably Definition 2?  References: [1] A. Krause, D. Golovin. Submodular Function Maximization. Tractability: Practical Approaches to Hard Problems 3.19 (2012): 8. [2] M. Wibral, R. Vicente, and J. T. Lizier. Directed information measures in neuroscience. Heidelberg: Springer, 2014.",2-Confident (read it all; understood it all reasonably well),This paper studies causal subset selection based on directed information. Two tasks are formulated and their sub-modularity properties are explored through a so called sub-modularity index introduced by the authors.,"This paper investigates the subset selection problem with respect to directed information as the causality measure. The main contribution comes from introducing a sub-modularity index to indicate how close the function is to sub-modular. The index seems to be natural and it is a bit surprising that it is not been studied before. Overall, the methods are novel and interesting. The application of the method, learn the causal structure, is quite interesting. ",1-Less confident (might not have understood significant parts),,,,,,
Composing graphical models with neural networks for structured representations and fast inference,"Matthew J. Johnson, David K. Duvenaud, Alex Wiltschko, Ryan P. Adams, Sandeep R. Datta",https://proceedings.neurips.cc/paper/2016/hash/7d6044e95a16761171b130dcb476a43e-Abstract.html,"Arguing that graphical models and neural networks have complementary strengths, the authors introduce a family of probabilistic models that combine structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks. The goal is to combine the interpretability and efficient inference algorithms of graphical models with the representation learning power of neural nets.  The main contribution of the paper is an efficient stochastic variational inference algorithm for training such hybrid models that uses a recognition model, implemented a neural network, to deal with the non-conjugate observation model to enable efficient mean field updates for inferring the local latent variables. The experimental section is minimal, showing qualitative results on a synthetic dataset and a small dataset of low-resolution video.","The idea of using a neural recognition model to implement conjugacy in a surrogate objective used to infer the local latent variables seems like a powerful and important one. At the same time it is less direct than taking the variational autoencoder (VAE) route and using the recognition model to produce the parameters of the variational posterior directly. It would have been good to contrast the two approaches and discuss their relative strengths and weaknesses.  I found the paper to be well-written overall, though it was too dense and not detailed enough in parts. The supplementary material was essential for me to understand some details unclear from the main text.  Though the experimental section is very bare-bones and lacks quantitative results, I do not think that is a serious weakness for a paper with a substantial conceptual contribution like this one.  The related work section missed several recent papers on sequence modeling in the VAE framework. Another notable omission was the work of Titsias and Lazaro-Gredilla [1].  References [1] M. K. Titsias and M. Lazaro-Gredilla. Doubly Stochastic Variational Bayes for non-Conjugate Inference, ICML, 2014",2-Confident (read it all; understood it all reasonably well),"The proposed approach essentially introduces a finite mixture model, where each component distribution is a Gaussian latent variable model, the likelihood means and covariances of which are parameterized via neural networks.    ","The main concept of the paper is not different from existing deep generative model (DGM) formulations where the postulated likelihood is a finite mixture of Gaussians.   What differentiates this work from existing DGMs concerns the postulated latent variable posteriors: Instead of considering a Gaussian posterior (conditional on the mixture component) that is parameterized via deep neural networks, the authors consider here a conventional formulation, where a set of Normal-Wishart hyper-priors is further imposed over the Gaussian posterior mean and precision matrix.  I have to admit that I’ve failed to understand why the authors drop variational posterior amortization to opt for this more simplistic solution. Using amortized variational posteriors is now the state-of-the-art approach in the literature, and is proven to allow for better modeling and inferential performance. The authors should have motivated this selection a lot more extensively.  The presented experiments fail to provide substantial empirical evidence to support the efficacy and the usefulness of the approach.  They are both limited to some simulated datasets and not exhaustively compared to the state of the art.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper presents a general modeling and inference framework that combines the strengths of probabilistic graphical models and the flexibility of neural network models. Graphical models are used to represent structured probability distributions and variational autoencoder ideas are used to learn the nonlinear data manifold in an approach that is referred to as the structured variational autoencoder (SVAE).  A major challenge of hybrid models is that inference is often difficult. The main idea of this paper is to learn recognition networks that output conjugate graphical model potentials, allowing for the use of tractable graphical model inference algorithms. They outline an algorithm for doing inference in the SVAE model.  The SVAE model is then applied to both a synthetic dataset and a real dataset. In the synthetic data experiment, a latent linear dynamical system (LDS) SVAE is trained to predict the trajectory of a dot that is bouncing back and forth in a sequence of images. The model seems to do a very good job on this relatively simple task.  Next, they apply a LDS SVAE to model depth video recordings of mouse behavior, where the model seems to predict future frames relatively well. Finally, they apply a latent switching linear dynamical system (SLDS) SVAE model to try to learn discrete states corresponding to natural behavioral units.","Combining PGMs and deep learning/neural networks is a very active and promising area of research. This is an interesting paper, with it’s main contribution to the existing literature being that it presents a model that can account for discrete latent variables. This new capability suggests that it could be used in a variety of interesting applications, including the type of behavior representation modeling shown in one of the experiments.  Overall, the organization of the paper is excellent, and the writing is clear. The technical part is however quite dense. Algorithm 1 in Section 4 is hard to follow. Some of the symbols in the algorithm don’t seem to be defined anywhere (and if they are, they’re too hard to find).   One of the strengths of the paper is that the approach is quite general. The very general description of the method might make it hard to figure out how to apply the algorithm to a specific new model. It might be helpful to include in the appendix a detailed description of the algorithm applied to the examples in Figure 3.  My main criticism is that there is no comparison to existing baselines, except for the ""toy"" example in Figure 1. I do believe that there are advantages to ""structured"" representations (at the very least, in terms of interpretability), but it would have been nice to see the results achieved by some baseline (existing) method. E.g., in Figure 5, it’s hard for someone who is unfamiliar with the data to determine if the model is making good predictions without having something to compare to.   Section 6.3 and Figure 6 should be explained in more detail. It seems that one of the more interesting contributions of this approach is that the latent switching linear dynamical system (SLDS) model can identify discrete latent behavioral states that influence the observed dynamics. Without more detail, it’s very hard to tell what is going on. For example, Section 6.3 states that there are 30 discrete states, but Figure 6 only shows two of these - are the other states semantically meaningful as well, or were these two picked because they were the only useful ones?",2-Confident (read it all; understood it all reasonably well),"The paper presents a method for generalizing variational autoencoders (VAEs) to structured graphical models with general, non-linear observation models. The main idea of the approach is to use the VAE framework to predict potential functions rather than the variational parameters themselves to facilitate inference on structured latent representations.  ","This is a good idea, and as far as I can ascertain, all claims (bounds, etc.) hold. However the manuscript in it's present form is a little hard to follow. For example The main objective, L_{SVAE}, is never even introduced in the main body of the text, and important proofs are scattered throughout the lengthy supplementary material---I think the authors can definitely improve the exposition substantially with some effort. A tighter correspondence around the algorithm description and an expanded exposition in the appendix might be useful where appropriate. Perhaps Figure 6 could go in the appendix to make some room for further explanations.  The experimental results are proof of concept I suppose, but after getting excited about the new machinery, it was a let-down to see the model exercised only on toy data and mouse depth data, and compared to other methods only on synthetic spiral data. I hope the authors will be able to demonstrate their model on a more involved and well-known task if the paper gets accepted.  Notes: - Appendix, eq (28) constant terms left in posterior.u - Sum-product networks represent discrete sub-structures and are efficient... line 86- z_n should be z_{n+1} according to Figure 2...  ",2-Confident (read it all; understood it all reasonably well),"The authors provide an approach to learn models that combine nonlinear likelihoods from neural networks with structured latent variables. They bring together several pre-existing tools including stochastic variational inference, message passing, and backpropagation using the reparameterization trick. Efficiency of optimization is improved where possible by using conjugate exponential families and natural gradients. The paper is well written with helpful examples and comments on related work. ","The paper is clearly written and neatly combines several earlier methods. To their credit, recent work that takes a similar approach [7,19,20,21,22] is cited and briefly described in Section 5. It would be worth adding a brief comment on Belanger and McCallum ICML 2016, Structured Prediction Energy Networks. Good examples are provided demonstrating scalability, and the videos are a nice bonus.  Please could you clarify the strengths of new contributions here? My main concern is if bringing together this collection of earlier tools is a sufficient novel contribution for this conference. I do not feel strongly and am not an expert in this area.",2-Confident (read it all; understood it all reasonably well),This paper combines neural networks with graphical models. Variational inference and neural network learning are combined into a single optimization objective.,"Let me start by acknowledging that I am not an expert on this topic, and I did not understand the details of this work. That being said, I found the contribution to be slightly incremental, or at least vague. I also feel like the carity of the presentation is lacking. The paper assumes that the reader is an expert on variational methods, and lots of arcane terminology is used.",1-Less confident (might not have understood significant parts),,,,,,
The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM,"Damek Davis, Brent Edmunds, Madeleine Udell",https://proceedings.neurips.cc/paper/2016/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html,"The paper presents a block coordinate optimization algorithm using asynchronous parallel processing. Specifically, the proposed method is applicable to nonconvex nonsmooth problems of cost functions obeying a specific structure (Eq 1). The algorithm is easy to implement and is accompanied with a convergence analysis. The proposed algorithm achieves near linear speedup over similar algorithms working serially. The claims are supported by empirical results on two matrix factorization problems.","The presented algorithm and its accompanied guaranteed convergence are indeed very interesting. Nevertheless, there are a few places in the paper that are not quite clear  to me:  1. Is the algorithm guaranteed to converge to a ""local minimum""? The convergence analysis in Section 3 relies on the definition in Eq(3). The latter merely guarantees states convergence to a ""stationary point"". However, throughout the paper, authors mention that the addition of the noise v_k helps the algorithm escape from saddle points (Line 116, Item 2). It seems there is an argument by the authors about reaching a local minimum, which the analysis (as currently presented) does not provide. Please clarify.  2. Nonconvex Regularization: The authors state that the proposed algorithm/analysis applies to general nonconvex regularizers. The latter may give rise to non-unique proximal operators (Line 63). Given that handling general nonconvexity is stressed as a key property of the algorithm, it would be nice if handling such non-unique scenarios were explained in greater details. Unfortunately, it seems the nonconvex experiment (Firm Norm) also limited to the special case when the proximal operator can be defined uniquely (despite nonconvexity). In general, however, how should the algorithm choose among non-unique answers? Does a random choice uniformly across the set of proximal operators work? Or it may cause a problem if the randomly chosen elements between two successive iterates happen to be far from each other?  3. It is not stated what different colors in Figure 1 represent neither in the caption of Figure 1, nor in the main text). I suppose these are the number of threads, but it should be mentioned explicitly. ",1-Less confident (might not have understood significant parts),"This paper introduces a noisy asynchronous block coordinate descent method for solving nonconvex, nonsmooth optimization problems.  The main contribution is its prove of convergence for a large class of optimization problems. ","The algorithm proposed in this paper for implementing noisy asynchronous block coordinate descent is something that comes to the mind immediately if someone is to implement such an algorithm. The main contribution of this paper is providing proofs that such a simple algorithm actually works. It proves that the algorithm matches the best known rates of convergence on this problem class.  The paper is well-written and it is very easy to follow the arguments of the paper. The authors start with a short introduction of known results and subsequently explain their algorithm and their main convergence theorem.  Through experiments on two different non convex matrix factorization problems, they showed that the algorithm attain linear speed up on the number of cpu threads.  Overall the paper is a valuable peace of work and its theoretical results can be of benefit for practitioner of large machine learning systems.",2-Confident (read it all; understood it all reasonably well),"The paper proposes an asynchronous proximal gradient descent algorithm and provides a proof of its guaranteed convergence to a local optimal solution. The main contribution is the finding that the prox function for nonconvex problems defines a set which in turn induces a measurable space and to interpret the unchecked asynchronous updates of variables as a source of noise when computing the gradient. In doing so, the distributed computation can be seen as an instance of stochastically altered gradient descent algorithm. The authors report convergence behavior under two different noise regimes resulting in constant and decreasing step sizes respectively.","I find the approach rather interesting, especially the broad and general definition of the problem makes the approach applicable to a wide range of problems. However, I was surprised by the absence of any reference to the seminal Robbins/Munro paper and also to the recent developments in stochastic gradient descent based sampling (see below). The authors do local gradient descent updates of coordinate blocks by computing partial gradients and adding noise in each asynchronous step. I was wondering, how this relates to the ""usual"" stochastic gradient descent update, i.e., given that the locally computed partial gradient will be based on delayed (noisy) variable states, a sequence of these noisy partial gradients would converge to the true partial gradient as well.  Further, recent SGD based sampling has shown that adding noise to the variable states obtained by noisy gradient updates (as the authors do as well) provides good samples of the distribution underlying the optimal variable setting also in a non-convex setting.  That being said, the work handed in remains valid, but it would have been interesting to compare the proposed approach to well established stochastic gradient methods. The overall procedure is laid out well and comprehensible. The chosen examples in the experiments section are well suited to demonstrate the scalability benefits of the algorithm.  Minor to that, I have a few remarks on style and the overall rationale: - line 60: ""each"" is unnecessary here when m = 1 - line 69: k is not yet defined as are g, and \nu - line 106: the notation using y is probably wrong, shouldn't it read argmin_{x_{j_k}^k} r_{j_k} (x_{j_k}^k) + ... ? - Algorithms 1 and 2 lack a break condition and output - Table 1: I assume the timing is in seconds?  Literature: Robbins, H., & Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical Statistics, 400–407. Welling, M., & Teh, Y.-W. (2011). Bayesian learning via stochastic gradient Langevin dynamics (pp. 681–688). Presented at the Proceedings of the 28th International Conference on Machine Learning.",2-Confident (read it all; understood it all reasonably well),"In this paper, the authors proposed NAPALM for solving nonconvex, nonsmooth optimization problems. And they claim that NAPALM is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex,nonsmooth problems. Moreover, the authors prove iteration complexity that NAPALM and demonstrate state-of-the-art performance on several matrix factorization problems. ","(1) This paper combines the two optimization techniques called ""Asynchronous parallel"" and PALM or BGCD for nonconvex nonsmooth optimization problems. In fact, the main proof techniques are standard. Hence, I do not find the results very exciting. (2) The authors claim that their method NAPALM mathches the best known convergence rates. In fact, this optimal rate only holds for the summable error case. The authors should make it clear. (3) This paper also covers the asynchronous stochastic block gradient descent.However, the convergence analysis holds only for the nonconvex, smooth optimization.  ",2-Confident (read it all; understood it all reasonably well),"The authors consider asynchronous coordinate descent with noise and possibly nonsmooth regularizer for nonconvex optimization problems, and provide the proof for the convergence rate."," The paper proves the convergence rate for asynchronous       coordinate descent algorithm with nonsmooth regularizer and noise on       nonconvex problems. The main contribution is a generalization of the       bounds of Ji Liu and Stephen J. Wright's 2014 paper on asynchronous       coordinate descent (this paper cited their 2013 work, but to me the 2014       work is more relevant) to nonconvex optimization problems with some noise       on the gradients. This topic is very interesting since there are many       applications for SCD under nonconvex setting.        I have a few questions about this paper:        1. In line 77, the paper says another update may overwrite previous       updates, but from algorithm 2 it seems that all updates are effective.        2. In line 135, the paper says the expected violation of stationarity is       the standard measure. Is there any reference for that?        3. From theorem 2 it seems that the theorem only talks about the       convergence of increasing batch size SGD. Do you think it is easy to       derive the convergence rate of constant batch size SGD like the reference       [8] using the same technique in this paper?       ",2-Confident (read it all; understood it all reasonably well),This paper proposes a noisy asynchronous PALM algorithm to solve general nonconvex nonsmooth  optimization problems. The algorithm is  actually a block coordinate stochastic proximal gradient method. The paper gives detailed convergence analysis and can get linear speedup in experiments.,"This paper is well written and easy to read. The main contribution of this paper is adding the noise to the stochastic coordinate gradient in the asynchronous PALM framework, which is also the main difference compared with the previous work [5]. But I think [5] gives more rigorous theoretical analysis and insights. The authors get different convergence rate under different noise, this is a good point.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,
Learning Tree Structured Potential Games,"Vikas Garg, Tommi Jaakkola",https://proceedings.neurips.cc/paper/2016/hash/22ac3c5a5bf0b520d281c122d1490650-Abstract.html,"This paper investigates the problem of learning potential games that are restricted to tree structures. This is an interesting problem since it is natural to explain complex data as the result of such games. A maximum margin approach is adopted that creates a difficult optimization problem, so a dual decomposition approach is introduced instead. Promising experiments on synthetic and opinion/voting data are provided that show improved predictive accuracy (synthetic data) and sensible tree recovery (real data).","Overall, the paper is well written and motivated clearly. Game formulations are nicely incorporated into the prediction task while building upon the relevant structured prediction literature. The problem area is important, but somewhat niche.  The paper first proves that max margin learning of tree structured models is NP-hard. Though I am fairly convinced that this problem is NP-hard, the proof should be clarified. The theorem statement allows both T and \theta to be chosen, while the proof fixes \theta and shows that finding T would solve the NP-hard max degree spanning tree problem. Results for the recoverability of \theta are included at the end, but it is unclear how these prevent other choices of \theta from potentially satisfying the inequalities. Additionally, I would expect the pairwise potentials to be based on the input graph, G, since the theorem statement does not include an “input” graph in which the tree is constrained.  All of the experiments (even synthetic) are on small problems with at most n=8 variables. Especially for the synthetic experiments, how does the run time performance scale as a function of n? For the real datasets, there are many simpler methods for constructing such spanning trees. Can anything be said about how well the method improves predictions of voting similarity?",2-Confident (read it all; understood it all reasonably well),"This paper considers a tree-structured potential game its associated learning problem. They showed NP-hardness of the learning problem. On the other hand, they propose an algorithm that finds a local optimum of the objective function. In the experiments, the proposed algorithm outperforms standard structured prediction baselines on artificial and real data sets. ","This paper considers an interesting learning problem that might motivates further study. On the other hand, the theoretical analysis looks somewhat standard such as NP-hardness and lacks analysis of generalization performance.   ",1-Less confident (might not have understood significant parts),"The paper incorporates dual decomposition methods into ""potential games / graphical games"". Such games are played among many players and and their joint strategy is described by a single potential function, which is described using sparse payoffs (graphs). An equilibrium of such a game is all players do not change their play given the state of all players. This paper considers payoffs structures that form a tree. The authors show that it is NP-hard to recover tree-structured games and suggest an dual decomposition approach to learn such games. ","The suggested approach extends the reach of structured prediction beyond the standard ""find the maximal assignment"" which has been extensively studied in the last decade. It is a refreshing direction - handling Nash equilibrium using structured models may have a significant impact in the future.   The authors also prove that the problem is hard. While it is nice to know we are dealing with hard problems, I am personally more excited by the dual decomposition approach which seems to work well. The authors solve the dual program by ADMM, a method that gained popularity in the last years. Lastly, in a series of illuminating experiments, both on synthetic and real data, the authors show the benefit of their approach.  ",2-Confident (read it all; understood it all reasonably well),"The paper presents a method for learning the tree structure and parameters of potential games by constructing a max-margin loss for locally-optimal assignments. The authors show that the learning objective is NP-hard, and propose an approximate dual-decomposition optimization technique. Some experiments on (very) small graphs are presented.","The problem presented in the paper is interesting. In general the notion of using games and their results as samples is intriguing. The locally-optimal nature of equilibria  seems interesting to explore, especially since many methods (such as structured prediction) usually focus on MAP assignments.  While the general idea is interesting, the experiments are disappointing, and suggest that the method is computationally prohibitive and cannot be used in practice. The lack of guarantees on the optimization process is also discouraging.  Remarks:  1) The formulation of the locally-optimal max-margin constraints is an interesting notion. It is not clear however why the authors believe that a discriminative method should return the 'correct' tree and/or parameters, as such an approach is oriented towards optimal *predictions*.  2) Since the stochastic model generates a distribution over y given x, it is not clear how predictions are made using a learned model, if at all. Moreover, the loss punishes a 'prediction' for being different than an observed label, while in truth it is only one of many possible legitimate labels. This seems unnecessarily harsh.  3) While the dual-decomposition approach looks promising, a downside is that there are no convergence guarantees on the coordinate-descent-style optimization procedure.  4) The experiments are rather disappointing. Why are they over graphs with at most 8 (!) nodes? Why are only chain graphs used as ground truth, and not random trees? Why are 100 iterations necessary for convergence? If runtime is a drawback, this should be mentioned and theoretically analyzed. This hints that the method is far from applicable.  5) The authors mention that the accuracy of their method is better, though it is unclear what exactly is measured. On the one hand, the authors claim that the goal is to reconstruct the base tree, but in all samples share the same tree. On the other hand, if accuracy relates to predictions, it is not clear (a) how they are made, and (b) how they are measured w.r.t the the probabilistic labels in the test set.  6) It is not clear why the authors believe that the real data used is based upon a tree and not a general graph. This makes the task of finding the 'true' tree puzzling. Separating conservatives from liberals is an easy task for any clustering algorithm.  ",2-Confident (read it all; understood it all reasonably well),"This paper introduces a max margin framework for learning potential games where the underlying interactions have a tree structure. A hardness result is presented first, followed by a dual decomposition algorithm for learning the tree structure and parameters of the game. Lastly, experimental results on synthetic and real datasets show that the algorithm recovers reasonable results.","I like the underlying idea of this paper, that of applying max margin learning to potential games. This works well with viewing observed behavior as local equilibria of the potential function. However, I have a couple serious concerns about the execution and technical content.  My major concern is that the paper very quickly glosses over the issue of learning N_i, the set of neighbors of each node. This appears to be the key part of the inference: as the authors point out, finding the optimal parameters with the tree structure fixed is just a quadratic program. It is claimed that special properties like submodularity could be used, but no such cases are explored in the paper. This leaves brute force, which is clearly not sufficient to tackle realistically sized problems. Also, it is never mentioned which method is used in the experiments.   Next, it would be helpful to see more detailed experiments. The synthetic and real datasets are good for illustrating that the algorithm is on some level reasonable, but two dimensions are somewhat lacking. First, all of the datasets considered are very small (and no runtime results are shown). So, it's hard to know how the algorithm scales, or whether it gives good results outside of small, clearly structured examples. Second, there is no comparison to other techniques for learning games. I'm not able to give a comprehensive list of recommendations, but a couple of papers which tackle similar problems are  Quang Duong, Yevgeniy Vorobeychik, Satinder Singh, Michael P. Wellman. ""Learning Graphical Game Models"". IJCAI 2009.  Jean Honorio, Luis Ortiz. ""Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data"". UAI 2012.   Some comparison to alternative ways of learning graphical games would be serve to validate the overall max margin approach.  A few smaller issues:  1) It's not clear where the assumed tree structure makes the inference easier. I assume that it simplifies learning the parameters, but this should be explicitly discussed somewhere.  2) The meaning of the ""context"" is not clear. What was the context in the two real world datasets?  3) In the equation just before line 59, ""y"" is undefined.  4) In the Lagrangian (just after line 118), is delta_ij the Kronecker delta? If so, isn't the summation identically zero?",2-Confident (read it all; understood it all reasonably well),"The paper considers tree structured potential games where the payoff of each player depends only on their own action and their neighbors' actions. It shows that it is NP-hard to learn the tree structure and associated parameters of such games under a max-margin setting.  It proposes a dual decomposition method to solve the problem, and give some interesting examples.","The proposed setting seems interesting and applicable to problems in real life. The experiments are interesting, and give somewhat reasonable results. There is one question though, how can the authors be sure that the expected voting pattern is in fact correct? For example, in the Congressional Voting Records example, how can it be verified that there are no extra edges between senators from different parties? (There could be unknown connections, which the authors do not know, and not discovered by the model). It seems that the result of the paper can only be critically examined if the real relations between different agents are known.",1-Less confident (might not have understood significant parts),,,,,,
Iterative Refinement of the Approximate Posterior for Directed Belief Networks,"Devon Hjelm, Russ R. Salakhutdinov, Kyunghyun Cho, Nebojsa Jojic, Vince Calhoun, Junyoung Chung",https://proceedings.neurips.cc/paper/2016/hash/20c9f5700da1088260df60fcc5df2b53-Abstract.html,"This paper introduces an iterative refinement procedure for improving the approximate posterior of a recognition network.  It shows that training with the refined posterior is competitive with state-of-the-art  methods. This refinement is further evident in an increased effective  sample size, which implies a lower variance of gradient estimate. This paper shows that inference need not all be done via a complex recognition network: iterative refinement can be used to aid in inference with a relatively simple approximate posterior.","The paper is clear, very well written.  All the steps are provided.  We would like a discussion on the time complexity of the approach.  As shown in fig 2b, the number of refinement steps is increasing with the epochs...  Please give both experimental and formal complexities.",2-Confident (read it all; understood it all reasonably well),A Directed Belief Network is augmented by an iterative refinement procedure for improving the approximate posterior of a recognition network. Training with the refined posterior is competitive with methods that are considered SOTA in this particular field.,"I think this is basically a fine paper which contributes to DBN research. However, how relevant is this work for today's deep learning community? DBNs have been largely replaced by other deep learning methods which have produced SOTA results, outperforming DBN-based systems in many domains. Few use DBNs any more; most have switched to more successful (and often older) deep NNs, even where few training data are available. To summarize, the significance of this paper for advanced deep learning seems doubtful. ",2-Confident (read it all; understood it all reasonably well),"This paper proposes a sequential Monte Carlo algorithm to improve posterior estimation in a class of generative models.  The main motivation is to reduce variance during training of autoencoder models that use discrete random variables in their latent space, which are unsuitable for standard variational autoencoder (VAE) training as they prevent the use of the so-called ""reparametrization trick"". Backpropagation through discrete stochastic unit is possible by using Monte Carlo gradient estimation, but these techniques tend to suffer from high estimation variance and hence poor convergence speed.  The proposed approach, iterative refinement for variational inference (IRVI), applies at each training step a sequential Monte Carlo algorithm to refine the posterior in order to reduce the estimation variance. The approach depends on the choice of a refinement procedure, which in the experiments reported in the paper is adaptive importance refinement (AIR).  The paper compares the proposed methods with other method described in the literature, including SOTA methods.","The paper is very clearly written and describes technical concepts in a very comprehensible way.  The approach is sound and well motivated and the experimental comparisons with other approaches are fair, though they could have been more extensive in terms of datasets.  My greatest concern is about the execution time of the proposed approach, since this is a sequential Monte Carlo method that performs multiple refinement passes for each step of the training process. The authors report convergence curves vs epochs but not vs wall clock time, which should be provided as the main motivation of the paper is to speed up training for this class of generative methods.  The experimental section is good in terms of which methods it compares against, but a bit lacking in terms of datasets. Comparisons on larger images and color images should be used. The paper reports SOTA or near SOTA likelihood, which is good, but it has been argued in the literature that this measure may not well correlate with subjective quality of generative models, hence larger and richer color image examples would be a valuable addition to the experimental section.  In conclusion I think that this paper provides a valuable contribution, it is well written well placed in comparison with existing approaches. It does however need some discussion and empirical evaluation of wall clock time complexity and it might use some more extensive choice of datasets in the experimental section. ",2-Confident (read it all; understood it all reasonably well),"In the recent years, variational methods becomes the state-of-the-art for training directed graphical models. The reason behind the success of variational methods in approximating the posterior of directed graphical models lies in better inference and learning w.r.t. previous methods.  From the other hand, however, the limited capacity of the recognition network can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. As a remedy to this problem, the authors proposed an iterative refinement procedure aiming to improve the recognition network posterior and showed that training with the refined posterior gives state-of-the-art performance. The authors further demonstrate the utility of the refinement procedure by showing how the proposed procedure has a lower variance of gradient estimate.",I do not have any particular remark.,1-Less confident (might not have understood significant parts),"The authors address deficiencies in approximate variational methods of training probabilistic networks.  They note that an inaccurate recognition network may lead to a bad estimate of the variational lower bound. They propose to improve the output of the recognition network with importance sampling, and provide empirical results.","In Section 3.1, a more precise justification of the procedure would be helpful.  If not rigorous mathematical claims, some explanation of the logic would be helpful.  I.e. when exactly is this guaranteed to improve the bound?",1-Less confident (might not have understood significant parts),,,,,,,,,
The Multi-fidelity Multi-armed Bandit,"Kirthevasan Kandasamy, Gautam Dasarathy, Barnabas Poczos, Jeff Schneider",https://proceedings.neurips.cc/paper/2016/hash/2ba596643cbbbc20318224181fa46b28-Abstract.html,"The paper models multi-armed bandit problems where there is an 'fidelity' component to be chosen by the learner (from among a finite set of fidelities) for the arm that is being pulled. An arm pulled at a certain fidelity yields a stochastic iid reward with mean within zeta_m of the true mean, where the zeta_m parameters (errors) are known a priori, and where the highest fidelity level corresponds to zeta = 0. In addition, smaller fidelity pulls are cheaper than higher fidelity pulls (i.e., cost less). The authors frame the problem of sequentially pulling arms with fidelities so as to maximize a notion of net reward, give a new algorithm for the problem, and argue that it can outperform UCB operating at only high fidelity levels. The paper also gives a lower bound for the regret of the proposed algorithm. ","The paper in my opinion studies an interesting and relevant problem - one of modelling the tradeoff between information, cost and reward (whether to choose low information that is cheap or high information that is expensive) - in online learning , specifically stochastic bandits. In this sense it may be useful as a benchmark to improve upon.   Though the paper seems technically solid, a key shortcoming is the lack of adequate explanation about its results and assumptions. The regret definition adopted seems unnatural at least from one angle - why not penalize resource consumption (or 'cost') additively instead of multiplicatively as done here? The authors' example of ad-display motivates their definition, but may not be the most general. What happens when the cost paid for suitable fidelity observations is additively factored in total reward (or regret) is perhaps a more natural question, and would be good to know.   The introduction of Assumption 1 seems rather artificial/opaque, involving constraints on the fidelity parameters given to the learner. Why would the authors expect this to hold in a general learning problem? More importantly, if the zeta_m's don't decay as fast as required by Assumption 1, is there a general way of selecting a subsequence of fidelities to satisfy the assumption and hence the results (this would significantly help strengthen the paper's message)? Though the authors mention that the assumption is not crucial, it seems to be used in the proofs, and my main concern is whether the assumption influences the algorithm's design and performance in a critical fashion. It appears that the algorithm's design is tailored towards performing well for some kinds of zeta configurations, but the justification of why these are the 'right' zeta sequences is missing. Again, the gap in the lower bound also demands more investigation and perhaps a re-design of the proposed strategy.   Interpreting the main result (Thm. 2, the regret bound for MF-UCB) and contrasting with UCB (at highest fidelity? (this is not explicitly stated in the discussion)) is not easy to grasp due to the highly technical nature of the presentation. The relative gain/loss of MF-UCB seems to depend on Delta and [[k]] which in turn depend on the rather complicated sequence of sets \cal{K}^(m). A more explicit explanation would greatly help the authors' case.   Overall, I feel the paper gets off to a good start in terms of considering an interesting learning model, but rapidly loses clarity when it comes to presenting the results.   Minor typos:   l 56: ""near-optimal"" l 126: period unnecessary before ""for all m < M"" l 179: set notation (curly braces) would be better to define [[k]] ",2-Confident (read it all; understood it all reasonably well),"This work introduces a new multi arm bandit setting where each arm can be drawn with different accuracies (or fidelities). The cost of pulling an arm increases with the desired accuracy, and the regret is defined as the product of the cost by the usual pseudo regret. The authors propose an algorithm which address this setting, MF-UCB, and provide an upper bound and a lower bound on the cumulative pseudo-regret of this algorithm."," The main paper is clear and well written. However this is not the case of the supplementary materials, where most of the proofs are. they contain many typos and small, easily correctable  errors which can confuse the reader (see below for a non exhaustive list). The correction of those errors would greatly increase the readability of the paper.   Additionally, I think the up-to-a-multiplicative-constant signs used in the paper can be a bit misleading, as constants are not really constants : for instance in Theorem 2, there is a factor \rho (a parameter of the algorithm) which is hidden in the constant. Using the exact inequalities (which are written in the supplementary materials) could improve the clarity of the results.  From a practical point of view, it is not clear to me that this algorithm can easily be applied to real life problems. For instance the strategy of MF-UCB (using low accurate pulls to eliminate greatly suboptimal arms) could suffer from inaccurate estimation of the \zeta (the fidelity accuracy). I think that a discussion on the practical limitation, and how to overcome them,  would benefit this work   From a theoretical perspective, this paper provides a nice insight into the problem, as well as interesting theorems and solid proofs.   Overall I think that this work is a significant and interesting contribution.     List of typos/errors in the appendix :  - page 10 equation (8) ; k should be k* - Section A1 : the H^(m) should be K^(m) - page 10 line 328 : \nu /(rho-2)  -- >  \nu/2 - page 11 equation after line 343 : s -- > \mu_* - page 11 equation (10) :  \gamma -- > \gamma^(m) - page 11 equation (11) : there is an extra ',' - page 12 line 375 : Lemma -- > Theorem - page 12 last line and page 13 first equation  : parentheses are missing - page 13 : in the definition of x_n, K^2 -- > K - page 13 : in the definition of y_n, ^{1/2} -- > ^{1/(\rho-2)} - page 13 line 388-389: The sequence of logical implications seems false to me, I think it simply is a consequence of \rho > 4 - page 13 in equation (15) : x -- > x_{n,\delta}. Same for y and  \delta - page 14 line 406 : N -- > log(N) - page 14 equation after line 406 : N < 2n --> N > 2n - page 14 equation before line 408 : \mu_* should be removed. The additive constants +1 and   \kappa_k are missing - page 15 : p and m are interchanged several times. - page 15 , line 432 : the opposite inequality should also be verified - page 15 equation after line 445 : The \tilde are missing - page 15 line 448 : \cup -- > \cap - page 15 equation after line 448 : the ^(l) were dropped - page 16 equation after line 451 : the P() is missing the the rightmost term - page 17 line 454 : theorem -- > lemma - section C : distribtuions -- > distributions         ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper introduces a ""multi-fidelity"" version of the multi-armed bandit problem in which there are K arms, each of which can be played at M different levels of ""fidelity"". Lower fidelity is less costly, but leads to potentially greater bias in the estimate of the arm's reward. (Pulling an arm at fidelity m draws a sample from a distribution whose expected value deviates from the arm's true expected reward by an amount whose upper bound depends on the fidelity.)   The paper analyzes the natural UCB-based algorithm for this problem, where the width of the confidential is increased to compensate for the uncertainty stemming from the bias of sampling at low fidelity. The analysis of the algorithm is summarized by theorems containing messy regret bounds that are hard to interpret; the paper argues that this regret bound will often be significantly better than the regret bound attained by always playing arms at the highest fidelity and using the standard UCB algorithm to select which arm to play. A brief section near the end of the paper presents simulation results to substantiate this claim.","I was initially puzzled as to why the paper chooses to model lower fidelity as introducing more bias, rather than introducing more variance. (The latter interpretation of ""low fidelity"" also seems well motivated to me.) The motivating examples given in the paper (e.g. algorithm selection for machine learning problems) convinced me that modeling low fidelity as greater bias is well motivated, although I'm still not convinced by one of the motivations (namely, the contention that displaying an ad for a shorter time interval yields a biased estimate of the ad's effectiveness over longer intervals) and in the example of algorithm selection for machine learning, it's not clear why minimizing regret would be the goal of an algorithm designer in that application. So to sum up, I thought that there was a disappointingly weak connection between the model in this paper and the motivating applications, but not so weak as to constitute a disqualifying flaw.  The main innovation in the paper seems to be the formulation of the multi-fidelity multi-armed bandit problem itself. The algorithm is a fairly typical application of the ""optimism in the face of uncertainty"" principle. The performance guarantee, as noted earlier in this review, strikes me as disappointingly hard to interpret. The simulation results on synthetic problems are hard for me to assess: Appendix C spells out in greater detail the set of parameter values that were used in the simulations, but does not attempt to justify these choices of parameter values. The regret of MF-UCB seems to improve that of UCB by a factor of 3 to 4 in many of the simulations; should we expect this to be typical of the relative performance of the two algorithms in practice, or is it an artifact of the way parameters (e.g. values of \zeta^{(m)}) were chosen?",2-Confident (read it all; understood it all reasonably well),"The author provide a new framework called Multi-fidelity Multi-armed bandit. In this problem, arms can be sampled with different level of fidelity and a cost is associated to each one. While having a lower cost, low fidelity samples are drawn from a distribution approximating the highest fidelity distribution.  They provide an algorithm adapting to the sequences of available approximations and cost, its regret analysis and a lower bound of the problem. They provide numerical simulation on synthetical problems where the proposed algorithm outperform UCB.","The proposed framework is interesting and if more developed can be impactful. However in this state, I found the paper unfit for publication.  1/ Introductions of the notations are quite confusing, they are very dispersed and makes their definitions hard to find. Per example, I did not find what is $s$ (after the rebuttal, it appears that $s$ is defined 1 page after its first use and is then barely used in the rest of the paper).  2/ The use of the advertising as example is misleading at some points. At many places, correlations with the display time of an ads and non stationarity are made. Non stationarity is out of context in this paper.  It may be a good idea to use another example (see 4/).  3/ I find hard to justify the regret analysis in this setting. A regret analysis involves to maximize a gain in the long run and thus to find the arm with the best ratio reward/cost. Here the optimal policy is always the arm with the highest mean, whatever the associated cost is. Thus, the optimal policy is weakened by the obligation of sampling the best arm at lower fidelity level. This may hide the cost of sampling low levels when it is not needed.  4/ A sample complexity analysis (here more a cost complexity) may be useful in the context of Multi-fidelity Multi-armed bandits. It can be motivated by the testing phase of prototypes for the conception of new products. The goal is to find with high probability the best product while minimizing the cumulated cost of the test phase before mass production. As companies are bounded by regulations, contracts and existing facilities they are obligated to use low-fidelity approximations before accessing to advanced and costly tests.  5/ I also found some parts of the text unwieldy. It may be worth to make a pass on the paper to make it more streamlined.",2-Confident (read it all; understood it all reasonably well),This paper aims a variant of the classical stochastic K-armed bandit problem where the cheap approximations to each arm is available. The authors formalise this problem as a multi-fidelity bandit problem. The theoretical results on regret bound are provided. Simulations are performed to demonstrate the effect of multi-fidelity.,"The presentation is easy to understand. Simulations demonstrate the effect of MF bandit method.  There are sufficient theoretical analysis on the lower bound of the proposed method, which shows the theoretical improvement.",1-Less confident (might not have understood significant parts)," This paper studies a new multi-armed bandit setting. The setting is that there are $M$ fidelities at which the learner can choose to play an arm. Each successive fidelity provides a better approximation to the rewards but expends larger cost.  If a strategy can use the lower and cheaper fidelities to eliminate several sub-optimal arms, while reserve higher and more expensive fidelities for a small subset of good arms, then it may outperform the standard UCB algorithm that neglects these characteristics of the fidelities. The setting could be applied to online advertising, where one may like to display ads for a few minutes to approximate the clicks in long run, for cost concern. For this problem, the authors consider a new regret definition, provide an algorithm with theoretical analysis (including regret lower bound), and conduct simulation.  The analysis is novel.       ","As each \varepsilon^{(m)}, the deviation of the m_th fidelity mean from the one of the highest fidelity, is assumed to be known in advance, how do we know the values in practice (such as the application in online advertising)? Is it possible to relax the assumption?  Minors: 1) line 326, 357, and 386:  ""... arm k \in H^{(m)}"",  'H' should be 'K'.  2) line 343:  For the last indicator term in the inequality, the argument ""\Beta_{k^\ast,t} \leq s"",  's' should be \mu_{\ast}.  3) line 345:  \gamma ---> \gamma^{(m)}  4) line 346~347: ""... and the last step uses \psi( \Delta_k^{(m)} - \gamma^{(m)} ) \leq \psi( \gamma^{(m)} ) when \psi( \Delta_k^{(m)}  ) \leq 2 \gamma^{(m)}"",  the two '\leq's should be '\geq's.  5) line 384: For the inner summand on the left side of the first inequality, the summation over all k \in those ""above"" K^{(m)} should be over all k \in those ""below"" K^{(m)}.  6) line 407: On the right side of the last inequality, \delta^{(m)} should be \delta^{(M)}, and the last term \mu_{\ast} \kappa \lambda^{(m) } should be  \kappa \lambda^{(m)}.  7) line 431:  ""for l \ge m"" ---> ""for l \ge p""  8) line 440:  in eq(17), ""l \geq m"" --->  ""l \geq p""   9) line 450: P(A_{n,3}) should be P(A_{n,2}).   ",2-Confident (read it all; understood it all reasonably well),,,,,,
Interpretable Nonlinear Dynamic Modeling of Neural Trajectories,"Yuan Zhao, Il Memming Park",https://proceedings.neurips.cc/paper/2016/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html,"The authors present a novel subclass of nonlinear autoregressive time­series model, consisting of an assumption (that the nonlinear dynamics are slow enough to be approximated using a locally linear model) and a “global leak” term (such that the state approaches the origin with an estimated timescale). The purpose of this leak term is to regularize fits to small training sets by incorporating an assumption that trajectories within the true system will occupy a finite region of the phase­space. This model is then estimated for 5 low­dimensional nonlinear dynamical systems with relevance in neuroscience and tested on held­out data, and compared (for 2 of the 5 systems) to models fit without the global leak parameterization. In general this approach seems to outperform models without the global leak term, and overall does a reasonable job of estimating fixed and ghost points of these systems. However, in several cases predicted trajectories generated by this new model seem to have qualitatively poor performance. ","Overall the paper is technically sound, but missing some content necessary to support all the authors’ claims. The paper is most in need of more rigorous comparisons (both to ground truth, and to other state­of­the­art estimation techniques), as well as more context for interpreting the significance of the results. For instance, when examining the 1­200 step prediction of the Lorenz system, it would be useful to show whether the divergence of the estimated model’s trajectories is less than or greater than the divergence of the true system in responses to small perturbations of the initial conditions over the 200 step window.  In all cases, the paper could have benefitted substantially from a more thorough comparison to other approaches for dynamical state estimation beyond just the strawman of a linear dynamical system.  ​It’s unclear from reading this paper how novel the “global leak” parameterization is, and how it differs or compares to other techniques for regularized estimation of dynamical systems.  Ultimately I think this parameterization might prove to be a useful advance, but at the moment it’s unclear the magnitude of that advantage.  The paper was generally clearly written. The inconsistency of whether this approach was being compared to both the true trajectories and those generated by a locally linear model without the leak term, or just the true trajectories made the flow of the paper a little less cohesive.  ",2-Confident (read it all; understood it all reasonably well),"This is an interesting study on nonlinear time series fitting with the aim to reconstruct the underlying dynamics and to identify fixed points and ghost points which have more interpretable meaning in neural computations. The authors fit time series from sampled nonlinear trajectories using a model with separable, linear input interaction, and radial basis functions as a nonlinear function approximator. They assume slow continuous dynamics, allowing for locally linear approximations and a model that learns the velocity field. For regularization, the model includes a global leak that pulls the model towards the origin and helps prevents blow-up. The authors demonstrate the performance of their method in a variety of applications as well as the shortcomings of their current model. As the authors argue, their method appears to provide more intuition about the underlying dynamical systems than other methods for dynamical systems analysis (e.g. fitting neural networks, recurrence plots, etc.)."," The writing is clear and the methods are straightforward. The results look promising, though it would be extremely interesting to see the differences with other nonlinear fitting models. Indeed, the main concern I have is that the authors do not adequately demonstrate the reasons their model succeeds better than other models/methods. There is some discussion of this but not many results. Instead, the authors only compare the performance of their model only with that of linear dynamical systems and to a “locally linear model” without regularization. How does it compare to nonlinear models, and what sets this model apart from other existing nonlinear models, that sometimes “produce wild extrapolations.” Is it mainly the regularization?  I commend the authors for their candid description of how their approach works and fails at the ring attractor, which is an interesting finding.  Minor: Ghost points are not defined. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors propose a method of fitting nonlinear dynamical systems from trajectories. They use a parametrization with radial basis functions, and linear interactions between them. They demonstrate the method on several example tasks, showing a match of both trajectories and overall phase space flow.","This is an important topic, and the authors present a novel approach to it. The manuscript is clearly written, and the examples are well motivated. The detection of fixed points in test data is particularly impressive. One weakness is that all examples are very low dimensional, and it is not clear whether and how the method will generalize to this regime. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","They propose a class of dynamical systems models for neural data, such as firing rates, single neuron biophysics, etc. This allows the visualization of a velocity field, where one can see features such as fixed points and basins of attraction. These are tested on simulations of models and compared to the ground truth.",This looked technically correct and well written. Some analysis of how well it works with different sampling rates might be useful.,1-Less confident (might not have understood significant parts),"Fitting models of dynamical systems from data has a long tradition in Nonlinear Dynamics. If successful, it holds promise to provide important insights into neuronal systems. The authors propose an ansatz, which goes only slightly beyond the many previous approaches using radial basis functions in that it includes a decay to origin, which appears to represent a useful regularization. ","The authors seem to mostly aim at phase portraits in 2 dimensions, which I consider of restricted utility. The method is shown to work for some 2-dimensional examples but gives partially wrong results for the ring model. An application is shown, where it the method predicts experimental data better than linear models, which is not really a surprise, and which I suspect to depend on the particular choice of parameters. Unfortunately, this most interesting last part of the paper lacks the details necessary to judge it.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors demonstrate a fitting approach using basis functions and a regularization term to robustly fit nonlinear dynamical systems. The inclusion of the global regularization term is their main contribution, and they clearly illustrate how it makes their procedure robust against test cases outside of the training range (Table 1 & Fig 6 in supplementary material). They demonstrate their approach for various types of dynamics: a fixed-point attractor, FitzHugh-Nagumo, a ring attractor and Lorentz chaotic dynamics. They also very briefly discusses how the approach can be applied to neural recordings.","Overall I found the paper to be solid and rather enjoyable, and I would qualify it as a strong candidate for a poster. The authors' method of plotting velocity fields by decomposing the velocity into direction and speed, which they've apparently introduced, is especially effective. It made their arguments and conclusions much easier to follow, and will hopefully be picked up by others.  In my opinion stating that this approach leads to “interpretable models” might be somewhat overselling the results – the interpretability of the results is still hampered by the fact that models are composed by 10-100 more or less arbitrary basis functions. That being said, their capacity to reproduce salient features of the phase diagram certainly makes them more interpretable than, say, recurrent neural networks.  *** Update following authors' response *** ""Interpretable model"" can mean many things. For instance, it can mean that the value of each c_i, σ_i,… is interpretable – the meaning I originally had in mind – or that the model results in an interpretable phase diagram – the meaning the authors had in mind. It should suffice to define the desired meaning the first time this expression is used.  Discussions are for the most part appropriately detailed, although sometimes they feel somewhat condensed, perhaps due to the large number of examples the authors discuss within their allowed 8 pages. Specific issues are noted below.  *** Section 2 — Model - The linearization in eq. (2) seems incorrect to me. One typically linearizes an update function (F here) around some other point (say x*) in the neighborhood of x_t, so that we have something of the form x_{t+1} = x_t + A(x*)(x_t - x*) + B(x*)(x_t - x*)u_t + ε_t We can then collect the constant terms as b(x_t) = -A(x*)x* - B(x*)x* Note that it is critical that x* and x_t be different in general, otherwise the linear terms are always zero. This distinction seems to be made in the given ref. [9], but it is not done here. Thus eq. (2) needs to be corrected, or if it is correct, a more detailed derivation needs to be given. These issues are further illustrated by the comment that A is not unique: if A(x_t) is already defined as the Jacobian of F_x at x_t, how can it not be unique ? Fixing the derivation of (2) should help clarify this comment.  *** Update following authors' response *** I think the issue boils down to clarifying that b(x_t) and B(x_t) are not linearizations around x_t, but rather around some other point h(x_t). It would then be clear that they are not unique, as they depend on the function h. One should probably write b(x)=Fx(h(x)) on line 38.  - line 39: B should be a function from ℝ^d → ℝ^d x ℝ^{di}.  - The previous issue might also be addressed by simply removing the first part of section 2 up to “redundant and over parametrizes the dynamics.” Indeed, this whole section is never used for the rest of the paper. I found eq. (3) a clearer starting point for the arguments than eq. (2).  - The authors should state that W_g, W_B and τ all depend on the step size Δ.  - Eq. (4). I feel that the regularization term should be exp(-τ² x_t). If it is correct as is, the authors should comment on the benefit of writing the regularization constant in this way (rather than just using a constant β ≡ exp(-τ²) ).  - line 61: Unless B is constant, the Jacobian should also include a term ∂B/∂x.  - line 63: As far as I am aware, “ghost point” is not a frequent term in studies of dynamical systems. Perhaps the authors could provide a reference, or a short definition ? Same comment regarding “slow point”, which first appears on line 92.   *** Section 4.2 — Nonlinear oscillator model - Since the text refers to figures 2a-2d, the subfigures should show alphabetic labels.  *** Section 4.3 — Ring attractor dynamics To me this is the most interesting of the presented examples. The fact that only a subset of qualitative features are reproduced makes this a good test case to explore what this fitting procedure can and cannot do. Unfortunately the authors limit their discussion to a description, simply stating that the fit and true models show qualitatively different behavior. I think it would be worth trying to distill a what parts of the dynamics are difficult to fit and why. For instance, why is it that the radial dynamics seem to completely dominate Fig 3a, even along the ring (where dr/dt should be almost zero) ?  - Fig 3b : As far as I can tell, there are in fact two fit and two true trajectories in this figure (distinguished by dashed and full lines). The legend says there is one of each.  *** Section 4.4 — Chaotic system - Fig 4c : The true trajectories are almost impossible to see. Also the line styles in the legend should match those in the figure.  *** Section 5 — Learning V1 neural dynamics This section feels incomplete. The LDS model used is claimed to be the “best”, but no justification or reference is given to support this claim. In fact it is not even stated which LDS model is used – this could be included in the supplementary material, similar to what was done for section 4.1. The example studied in this section is by far the paper's most complex, and yet its explanation is one of the shortest. Consequently, it is difficult to picture what it is exactly that the model is fitting. Ideally a figure would be added to illustrate the inputs, but at the very least the second paragraph should be lengthened to include a more precise description of the inputs. As it stands, it is not even specified what time of recording the data stems from (only that it is “large scale”).  *** Supplementary material - As far as I understand, the interest in giving eq. (15) in terms of the x_i is to relate these dynamics to physical quantities. Since the authors never do this in the text, I would remove the middle expression in eq. (15). Similarly, I think defining x_2 in eq. (16) is superfluous; moreover, x_1 is never properly defined. The expressions in terms of s_1 and s_2 already appear as-is in ref. [20]. Also, the parameters J_{A,ext} and c' which appear in eq. (17) are never defined. - In fig 7, green and red dots should be above the blue lines rather than below.  *** Grammar  Some minor grammatical typos were noted, which in no way affect the quality of the paper. Possible corrections are listed here for the author's reference. 4: Our model incorporates a prior assumption 7: bifurcations 17: dynamical systems [5,6] 27-28: These features encourage the model 36: of an autoregressive 40: in the multivariate RBF 60: dynamics are locally 61: via the Hartman-Grobman 82-83: where the tasks typically involve 150: The initial state of each trajectory is picked from a standard normal 151: We use 19 trajectories 152: and the last one for testing. 166: The input was 3 dimensional 173: This end of sentence could be better formulated: “robustly predicts well.” ",2-Confident (read it all; understood it all reasonably well),,,,,,
Stochastic Variational Deep Kernel Learning,"Andrew G. Wilson, Zhiting Hu, Russ R. Salakhutdinov, Eric P. Xing",https://proceedings.neurips.cc/paper/2016/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html,"This paper extend the ""Deep Kernel Learning"" idea for classification problems.","1. In line 39, you mention that your proposed method enables features like ""multi-task learning"", but I do not see any single experiment/discussion about it. Similarly, I do not follow what do you mean by ""deep architectures with many output features"".  2. I wonder how does your model perform if you only train the GP architecture of the networks, i.e. keep the pre-trained weights fixed. This way, the depth part of your model just provide better representation for GP in the back-end and you can make fair comparisons between GP, simple logistic regression and other classification methods.  3. How do you compare your proposed loss function in Eq 3 and 4 with cross entropy loss? and do you expect this loss integrated with your deep kernel model is a better loss function than cross entropy for classification tasks? If yes, why? by ""better"", I mean it leads better classification accuracy.  3. Significance of results: I wonder if the gains reported in Table 3 are significant?  4. Since you are proposing kind of a new classification method, why not reporting results on the state-of-the-are classification tasks? MNIST is too simple example in my opinion and there have been higher numbers in this task compare to the accuracies reported in Table 3.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",Authors proposed a new deep kernel learning model for classification by embedding an additive GP layer into the neural network (before the output layer). Authors also proposed stochastic variational inference procedures to jointly optimize all parameters of the new model.,"This paper is well written. It is reasonable and meaningful to build kernel learning models based on neural networks. The proposed deep kernel learning model and inference procedure are novel. However, the experiments are not enough to verify the significance of the proposed methods.  1) Authors claimed that the proposed model also enables multi-task learning, but experiments only involved classification.  2) Experiments were not conducted on state-of-the-art neural networks. The proposed model is actually based on the success of neural networks, and it is expected to perform better than traditional GPs. However, it is necessary to verify its usefulness in practice when building based on state-of-the-art neural networks, rather than only a simple neural network. E.g., I wonder how the proposed model performs if the ResNet-20, ResNet-56, or ResNet-110 model was adopted as the DNN model in the CIFAR10 dataset.   3) The proposed model can be considered to embed a GP layer to a neural network. I would like to know the performance of another DNN model with the GP layer is replaced by a DNN layer. This can verify whether the performance improvement of SV-DKL over DNN is from the GP layer or just increasing the ""depth"" of the DNN. ",1-Less confident (might not have understood significant parts),"The author produces an architecture containing a deep neural network with a final layer of Gaussian Processes. An approach is proposed to jointly train the full architecture with stochastic gradients, where GP gradients are obtained from a variational inference bound. Although this is not novel in itself, the authors propose a sampling scheme to provide scalable approximation to the posterior expectation and competitive results are established on a number of standard classification benchmarks (Airline, UCI, MNIST, CIFAR). ","I think the paper is well written and provides substantial improvements in the intersection of kernel-based machine learning and deep learning. Although these architectures are complex and slow in comparison to vanilla DNNs, the paper's sampling scheme is not too complex and we see significant speedups over KLSP-GP which is encouraging. I feel confident this architecture will be used by researchers and practitioners, as deep kernel learning matures and believe it should be admitted to NIPS.",1-Less confident (might not have understood significant parts),This paper presents a model (SV-DKL) that use deep neural networks as feature extractors and apply additive base kernels to the output features. The deep neural networks are pre-trained followed by a training procedure of the additive base kernel and neural network parameters in cohesion. The parameters are optimized by variational inference and the learning has intriguing runtime properties. The model shows promising results compared to SVM and deep neural networks with and without GP.,"This is a well-written paper with only a small number of grammatical and presentation errors, e.g. there is no numbering (a, b, c, d) in figure 2.   The theoretical basis for this work is sound and explained well. However, the experimental section lacks some detail on the training of DNNs and the pre-training. From the level of detail given it will be hard to reproduce the results.   Furthermore the results are not too convincing. The authors claim that ""flexible deep kernel GPs achieve superior performance"" to DNNs and SVMs. The difference in the classification results doesn't seem as convincing. It would be nice to have a more detailed discussion on whether the comparison is fair. Currently the DNNs in the stand-alone model and the DNN+GP/SV-DKL are all parameterized equally, meaning that the GP and SV-DKL additions both have more parameters. Furthermore the DNN+GP addition is trained independently, thus it will not have the same flexibility as the training of SV-DKL. Would the DNN produce better results if it were just defined with more parameters? As stated earlier it is not clear how the DNN is trained, but it would be beneficial to the training of the DNN with regularization (e.g. dropout); hence the KL term in the ELBO will work as a regularizer for the SV-DKL.  In conclusion, this is a well-written paper, but it needs to argue a better case in order to be a convincing novel contribution.",2-Confident (read it all; understood it all reasonably well),"The paper reads very well with clear motivation of incorporating kernel learning to deep networks. The authors do a really good job of explaining deep kernel learning for multi-task classification with the example of figure 1. Kernel methods and SVMs dominated various learning tasks and bringing in the kernel methods to deep learning is an interesting direction.  While I do not know the motivation behind the two experiments chosen to compare the methods the empirical results show improvements in accuracy and training time. The only doubt is the time taken to train the new framework over vanilla DNNs as compared to the accuracy improvements. From the table 1, it seems it's 8x slower to train compared to DNNs.","It would be great to get a comment on speed of training and utility of it. Also, CIFAR 10 has saturated in numbers. It would be great to see the value of this model on tiny imagenet if not the whole imagenet to get a good sense of how much the non parametric aspect of the model is helping.",1-Less confident (might not have understood significant parts),,,,,,,,,
Fast and Flexible Monotonic Functions with Ensembles of Lattices,"Mahdi Milani Fard, Kevin Canini, Andrew Cotter, Jan Pfeifer, Maya Gupta",https://proceedings.neurips.cc/paper/2016/hash/c913303f392ffc643f7240b180602652-Abstract.html,"The paper deals with learning monotonic functions. This can be useful for the data, for which there are some inputs that are known to be positively (or negatively) related to the output, and in such cases training the model to respect that monotonic relationship can provide regularization, and makes the model more interpretable. The main problem which is dealt in the paper is that flexible monotonic functions are computationally challenging to learn beyond a few features. The paper proposes to learn ensembles of monotonic calibrated interpolated look-up tables (lattices). A key contribution is an automated algorithm for selecting feature subsets for the ensemble base models. The algorithm is experimentally compared against random forests, and is shown to produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation.","The paper is heavily based on the previous work on lattice (and monotone lattice) regression, and is thus not self-contained. It is not even explained in the main part of the paper what is the actual function which a lattice represents (two examples are found in the supplementary materials, but I still found the explanation two brief). I think this should definitely be a part of the main paper, otherwise the reader is forced to check the previous work on this topic to understand what kind model is actually being considered.   The paper barely touches upon the computational complexity issues of optimizing (3), (7), and unnamed eq. in Step 3. While (7) is convex given calibrators being fixed, it is not clear whether it is convex when jointly optimized over calibration and lattice parameters (I doubt it is). Moreover, (3) seems highly complex and combinatorial optimization criterion. The optimization complexity of eq. is Step 3 is unclear to me (note that Step 3 requires knowing ensemble weights \alpha, which are only computed in Step 5; how is this issue solved?). The authors should clearly state which problems are easy to optimize, and which are only solved heuristically, likely returning suboptimal solution.  Given a complex training of lattice ensemble I missed the computational times of training. Since the authors report faster evaluation of their method over random forests at the testing phase, they should also report training time comparing to the competitors.  The experimental results are not very convincing: only around 1% of accuracy improvement is reported comparing to the random forests; and it is not clear if random forest is a good fit for these data sets. It also does not help that all data sets except the first one are proprietary (hard to find other experimental studies on these data sets). It would be beneficial to check at least one more state-of-the-art flexible regression method, such as, e.g., gradient boosting, on these data sets (the unconstrained lattice, as another competitor, is not enough, as it is not particularly known algorithm). For instance, xgboost (an implementation of gradient boosting) easily achieves over 86% accuracy on Adult, see https://jessesw.com/XG-Boost/. RuleFit algorithm of Friedman & Popescu (http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf) gives 13.6% test error (86.4% accuracy), etc. Apparently, they all beat both random forests and monotone lattices Adding more methods to the comparison would help to resolve the question, whether exploiting monotonicity actually gives an advantage in predictive performance.  Please explain what is novel in the paper comparing to Ref. [2]. Both papers share monotonic lattice regression, feature calibration and some of the optimization criteria (e.g., for joint optimization of calibration and lattice parameters). It looks as if the novel part is the ensemble learning of lattices, together with crystals algorithm to select features for lattice construction. And the parts related to incorporating monotonicity are not.",2-Confident (read it all; understood it all reasonably well),"The article shows a way to build ensembles of lattice look-up tables that code monotonic relations with respect to the output.  The creation of an ensembles of simpler lattices, instead of a more complex single lattice, is what makes the proposed algorithm computationally feasible. The article is tested on 4 datasets (1 public and 3 undisclosed) showing a slight improvement over Random Forest in accuracy and large improvement in computational time required to train and to test the models.    ","The article is interesting and promising. However, I think the experimental section is weak and in my opinion the article puts too much information in the supplemental material.  The main concerns I have with this article are:  * There is a very related article that is not cited nor discussed. This is:  Wojciech Kotlowski and Roman Slowinski, Rule Learning with Monotonicity Constraints, ICML 2009. ""The algorithm  first monotonizes the data using a nonparametric classification procedure and then generates a rule ensemble consistent with the training set""  The authors should discuss this reference and stress in what way the current proposal improves over Kotlowski and Slowinski's. In my opinion this should be done by also including this algorithm in the experimental comparison.  * It is not clear how many realizations are carried out for each experiment. In experiment 1 nothing is said but results are given with standard deviation. In the temporal experiment it would be better, instead of reporting a single experiment result, to do a sliding window experiment, to have more results. The experiment 4 it is not clear if the four subsets (train, validation, test 1 and 2) are disjoin or not. Is the same train+val used to train the models that are tested in 1 and 2? The following sentence is not clear either: ""After optimizing the hyperparameters in the validation set we independently trained 10 RF and 10 crystal models..."" Did you use one single validation set and 10 different training set? was it done with cross validation?   * Every experiment optimizes a different set of parameters and in a different range of values. The same parameter grid search should be used for all experiments.  * In addition, Random Forest (RF) does not really need any optimization to get to good results. Better results should be obtained using both train+val for training using default values. In any case, out-of-bag optimization could be used with RF to obtain the best parameter set without reducing the data for training.  * This is not very important but in pg 7 it is said ""Because RF ...chose ...minimum leaf size 1, the size of RF....scaled linearly with the dataset size..."". This could be true in this case but this is not necessary true for all classification tasks as 1 is only the minimum. A log size increase could also be observed or even no increase at all.  * Finally, it is claimed that the learned models are more interpretable. This might be true for single lattices. However, the article does not explain how this interpretation could be done when building ensembles of hundreds of lattices. ",2-Confident (read it all; understood it all reasonably well),"This paper builds on recent work on monotonic calibrated interpolated look-up tables and introduces ensembles of these tables. Much like the random forests, these ensembles do not change the expressiveness of the base model class (except if feature calibration is done separately on each model). Still, they provide a powerful regularisation, because each individual interpolated look-up table has only a few features. In the experiments conducted on 1 public and 3 private industrial datasets the proposed method is faster and more accurate than random forests.","This is very, very interesting work and enjoyed reading this very much. Of course, partly my admiration is due to my first encounter with monotonic calibrated interpolated look-up tables, presented in the soon-to-appear paper [2] in JMLR, currently available from Arxiv. Still, the step to building ensembles is a non-trivial and significant improvement, resulting in better and more interpretable models than random forests. Importantly, the models are more reliable as well, because their performance in sparsely populated feature space regions is very intuitive and meaningful.  Certainly it would have been even better if the authors could have presented experiments on more public data, as well as make the code available. My other few criticisms are very minor, just to improve presentation of this paper.  I first got the impression that the paper does not present enough details about the method in paper [2], forcing the reader to read [2] first. Actually, the supplementary provides almost enough detail about [2]. The only thing missing is a pointer to section A of the supplementary material early on in the Section 2. But in addition to that, I think that the very start of Section 2, lines 43 to 54 would benefit from a bit more explanation. Especially, the first mentioning of linearly-interpolated look-up table begs some explanation. In Section 7.1 I think it is worth emphasizing once (maybe in the caption of Table 2?) more that the values of parameters are given in the supplementary.  I found typos on lines 77 (interpolaion), 110 (define->defined), 111 (torsion->torsion is), 211 (the weather->weather). ",2-Confident (read it all; understood it all reasonably well),"The authors introduce a new learning ensembles of monotonic functions. They propose an automatic way of selecting features for the lattices of the ensemble. The key point is to ensure monoticity in predictors in order to ease (and consolidate) the interpretation of the results. The authors state the method and describe the feature selection procedure and the way they trained them, before leading experiments on 4 datasets.","The paper is well written but it is sometimes difficult to follow (especially section 2 and 3). This may be due by the fact that several notations are introduced a few lines after their first use, which renders difficult the understanding. Furthermore, it may seem obvious to the authors (and lots of other people I admit), but the term ""look-up table"" is never defined, and it is not clear to me what this is. Reference [2] is very often pointed out, hence the paper does not appear self-contained enough (at least I have great trouble to understand the current paper).  About the current method, I did not understand how S is chosen, even if it appears to be a very important parameter of the algorithm.  ",1-Less confident (might not have understood significant parts),The authors have proposed an fast and flexiable montic function learning method by ensemble of lattice. I guess it is an extension of the previous work shown in reference [2].,"The authors proposd to ensemble several lattices to learn monotonic functions. I am not an expert in this field and the paper seems interesting.  (1) There are plenty of theoretical and experimental results in this paper. Nevertheless, the paper is very hard to follow, although I have read this paper for several times and tried to catch the main idea. I cannot totally understand how to learn monotonic functions with ensembles of lattices.  (2) I think the most related work is monotic lattice regression. What is the essencial different between your work and the previous one? Adding felxiablity and computational efficiency?  (3) For clarity, it is preferable to listing an algorithmic table.",1-Less confident (might not have understood significant parts),"The paper describes a method for monotonic regression. The main idea is to represent the regression function as a sum of ""tiny"" lattices, each depending on a small number of the variables. The authors discuss several methods for selecting the variables in each lattice, including random selection, and the ""crystals"" algorithm. ",This is a very clear paper addressing a moderately interesting problem.,1-Less confident (might not have understood significant parts),This paper introduces an algorithm that learns a flexible monotonic ensemble for classification and regression problems. Monotonicity helps the user to understand the prediction model better and increases the trust in the model. Previous algorithms did not scale well with an increasing number of features. The proposed ensemble algorithm can be learned efficiently and performs well compared to random forest on large classification an regression problems.  A second contribution of this paper is an algorithm to select feature subsets which show substantial nonlinear interactions.  ,"This paper is well written and the results are promising. The introduced method is the first algorithm with monotonicity guarantees that scales well with more than a few features. The ensemble of lattices performs as good or even better than random forest. The crystal algorithm seems to be an effective way to find features subsets with substantial non-linear interactions.  Is the crystal method itself a meaningful contribution that can inform other ensemble methods? For instance, could the size of a random forest be reduced by selecting feature subsets before training?  I am wondering why each experiment is only conducted on one of the data sets. Do the results of the experiments hold for all data sets? In general, I believe that a bigger test-bed of public data sets would strengthen this paper.  All experiments are based on fairly large data sets. I am wondering how the method performs on small or medium sized standard UCI data sets.   How sensitive is the method for the choice of the hyperparameters  (step size, features per lattice)? We know for instance, that random forests often do very well with the default parameter values so that a validation set is not even necessary.      ",1-Less confident (might not have understood significant parts),,,
On Explore-Then-Commit strategies,"Aurelien Garivier, Tor Lattimore, Emilie Kaufmann",https://proceedings.neurips.cc/paper/2016/hash/ef575e8837d065a1683c022d2077d342-Abstract.html,"This paper studies a two arm bandit problem with Gaussian reward distributions. This simple setting is used to illustrate the sub-optimality of policies that use two separate phases: an exploration phase during which the algorithm experiments, and an exploitation phase during which it sticks to the with best performance during the exploration phase. They focus on two settings -- with known and unknown gap between the Gaussian distributions -- and three approaches -- an explore-then-commit policy with non-adaptive stopping rule, and ETC policy with adaptive stopping rule, and a fully adaptive allocation rule. The main lesson is that more adaptive rules have better performance.   ","This paper highlights the benefit of adaptive algorithms in a transparent manner, and I think it will be valuable for the NIPS community. The advantage of adaptivity in this paper shows up only in constant multiplicative factors, but these are important in practice, and would be missed by anything but a very careful analysis. I gave the paper scores of 3, but I don't view it as borderline. I would accept this paper.    Given that the goal illustrating the benefits of adaptivity, I'd suggest providing more intuition for the results. Really, it's quite natural that if you commit to playing arm 1 for times t > tau, but subsequent observations suggest arm 1 is bad, you'd wish you could switch back to arm 2. The track and stop rule in section 6 comes out of an optimization problem that aims to minimize the number of samples required to accumulate a given amount of information. It's pretty intuitive for there to be a gap between this, and the optimization problem solved by an optimal bandit algorithm, which essentially minimizes the regret incurred in accumulating a given amount of information. You don't need to include comments of this form, but they might extend the impact of the paper.    As the paper acknowledges, a great deal is known about each of the settings considered due to classic work in sequential statistics by Wald, Robbins, Katehakis, Lai, and likely others. Most importantly, it's known from Wald that adaptive stopping rules offer equivalent statistical power with a smaller expected sample size than any non-adaptive rule. This work is cited on page 4, but anyone who simply read the summary of results might not realize that this important insight should really be attributed to Wald. I think best practice would be to highlight this in the intro. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper analyzes a two armed bandit problem with normally distributed rewards. The objective of the paper is to show the sub optimality of fixed design or explore and then commit (ETC) strategies compared to fully adaptive strategies. To that end, they establish lower and upper bounds in the three settings to show the improvements from one to the other. ","I found the paper interesting. While I do not find surprising that fully adaptive strategies necessarily improve on fixed design or ETC strategies, I find it very elegant that the authors are able to crisply quantify the improvement through the constant that precedes the logarithmic factor in the regret. While the appeal may not be as broad for an oral level presentation, I certainly support the submission for a poster presentation.",2-Confident (read it all; understood it all reasonably well),"The paper is concerned with obtaining very precise upper and lower bounds on the regret of algorithms for the two-armed bandit problem with normally distributed rewards. The paper considers two settings: one in which the gap between the mean payoffs of the two arms is known, and another in which the gap is unknown. For both settings, the main result compares the asymptotically optimal regret bound for ""explore-then-commit"" (ETC) strategies --- those which engage in uniform exploration until a stopping time is reached and then commit to the empirically superior arm at that time --- against the asymptotically optimal bound for fully sequential policies. The latter class of policies is found to improve the regret of ETC policies by a factor of 2, asymptotically, in both the fixed-gap and unknown-gap settings. In the fixed-gap setting, this involves proving new upper and lower bounds both for ETC strategies and for fully sequential policies. In the unknown-gap setting, it involves proving new upper and lower bounds for ETC strategies, and a mildly novel result about fully sequential strategies whose novelty lies in simultaneously attaining asymptotically optimal regret and order-optimal minimax regret. ","I really like this paper! It delivers a crisp, clear message --- fully sequential policies are better than ETC by a factor of 2 --- which is important because of the widespread use of ETC policies in A/B testing. I didn't read all of the proofs in detail, but the technical work required to obtain this result appears pretty substantial. ",2-Confident (read it all; understood it all reasonably well),"This theoretical paper investigates the three main classes of algorithm to address the 2-armed bandit problem when the time horizon T is known:  1 the fixed-design algorithms, where the stopping time of the exploration phase is fixed in advance, 2 the explore then commit algorithms, where the stopping time of the exploration phase is a random variable, 3 the fully sequential algorithms, where exploration and exploitation are done simultaneously.  This paper provides new analytical results on lower bounds of the three classes of algorithms. In particular when the gap is unknown, the authors show that explore then commit algorithms suffer from an asymptotic regret two times higher than the one of fully sequential algorithms. Algorithms and fine analysis of regret upper bounds are given for each class when the gap is known and unknown.  ","The main analytical result is significant: fully sequential algorithms are better for regret minimization than explore then commit algorithms. This result was empirically known, while the lower bounds were roughly the same. This paper provides the proof in the case of 2-armed bandits, that explore then commit algorithms suffer from an asymptotic regret two times higher than the one of fully sequential algorithms. It should interest the community.  My concern is the presentation.  On the form, the abstract claims that ""empirical evidence that theory also holds in practice"", while empirical results are not given in the paper, but in the appendix. The paper has to respect the format.   On the substance, the discussion suggests that the results hold also for K-armed bandits, and then that explore-then-commit algorithms are sub-optimal for the regret minimization problem. This should be true.  Usually explore-then-commit and fixed-design algorithms are used for the best arm identification problem. Median Elimination (a fixed-design algorithm) is optimal, while Successive Elimination (an explore-then-commit algorithm) is not optimal, while fully sequential algorithms do not output the best arm.  In practice Median Elimination does not work well for regret minimization while Successive Elimination works well. So explore-then-commit algorithm such as Successive Elimination is a reasonable suboptimal choice for regret minimization and best arm identification problems, while fixed-design and fully sequential algorithms are optimal on one problem and inefficient on the other. This seems somewhat contradictory with your claims ?    Ok. I moved my notation. Thank you to take into account my concern on presentation.",2-Confident (read it all; understood it all reasonably well),"This paper aims at providing theoretical evidence to support the fact that Explore-Then-Commit strategies are necessarily sub-optimal in terms of regret in a multi-armed bandit setting. In this regard the paper considers a simple two armed bandit problem with Gaussian rewards with different means but same variance. In this setting, the authors show lower bounds for ETC strategies. This shows that ETC strategies are necessarily sub-optimal compared to some strategies that follow fully sequential exploration. The paper also shows that a strategy similar to UCB* is asymptotically optimal and order optimal in the minimax sense at the same time. ","1. One interesting part of the paper to me is the strategy in Algorithm 5 that is asymptotically optimal and optimal in the minimax sense. Although the strategy is not very original (a small modification of a strategy suggested by Lai), it is a new observation for a strategy to be asymptotically optimal and minimax optimal. However, it should be noted that it is optimal only in the case of two arms, which is a major drawback. The authors are encouraged to improve on this result.   2. I like the idea of using the hypothesis test result of Wald to come up with the ETC strategy in Algorithm 2 that achieves the lower bound for ETC strategy. This idea is new to the best of my knowledge. However, again it is challenging to extend this to multi-armed case.   3. I feel that Theorem 5 is rather straight forward to see from the results of the optimal BAI algorithm and there is not a lot of novelty in this result.   4. Although presented as the main result of this paper, I do not see very novel ideas in the proof of the lower bounds for these strategies. Hypothesis testing inequalities stemming from a particular inequality in Tsybakov's book "" Introduction to Nonparametric Estimation"" , has been used before in the bandit literature for proving lower bounds "". In fact the main step in the proof is Garivier's inequality. The parts of the proof that follows is fairly straight forward. Moreover the results have been presented only for the two arm setting.  5. Overall I like the flavor of the results presented in the paper. I like the Algorithm 5 and corresponding proof, as to the best of my knowledge this is original in the literature. The lower bounds for ETC are new results but may seem straight forward. The major drawback of the paper is the simple two arm setting and that there is not much practical significance of the paper. I would encourage the authors to generalize these results in future.   6. The presentation of the paper is very clear. ",2-Confident (read it all; understood it all reasonably well),"This paper study the explore-then-commit strategies in the multi-armed bandit setting.  The purpose of this article is to show that the explore-then-commit strategy based exportation and exploration is necessarily suboptimal.   As the authors mentioned recent progress on optimal exploration strategies have suggested that well-tuned variants of two-phase strategies might be near-optimal. This paper  On the contrary, show that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation.   This paper proved the lower bound for the Explore-Then-Commit strategy.  And showed that a fully sequential strategy can be better, which proved that ECT are necessarily sub-optimal.  The main contribution of this paper is to fully characterize the achievable asymptotic regret when the gap is either known or unknown and the strategies are either fixed-design, ETC or fully sequential. ",The argument made about the explore-then-commit strategy in multi-armed bandit is very interesting and novel.  And the authors also proved that this strategy is necessarily suboptimal by providing lower bounds for ETC strategy and general strategy with both known and unknown gap. Also empirical results also provide evidence for the conclusion to hold in practice. One important limitation of this work is that   the main proof in this paper is focusing on the two-armed bandit case. The extension to multi-armed bandit cases is not straightforward and is not clearly addressed. ,2-Confident (read it all; understood it all reasonably well),,,,,,
Verification Based Solution for Structured MAB Problems,Zohar S. Karnin,https://proceedings.neurips.cc/paper/2016/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract.html,"The paper considers the best arm identification (BAI) problem, with fixed confidence, in several structured multi-armed bandit settings.  Intuitively, in these settings, verification (of an arm being the best) is an easier problem than the identification of the best arm. Starting from this observation, a new ""explore-verify"" framework is introduced in the paper. Based on tools from previous on BAI, explore-verify algorithms are proposed for BAI in linear, dueling, unimodal, and graphical bandits settings. The algorithms are analyzed and compared to the state-of-the-art results.   ","The paper proposes a novel explore-verify framework for the well-studied best-arm identification problem and applies it to several structured bandit settings previously studied in the literature. The paper is nicely written and the proposed explore-verify algorithms are clearly presented.   A merit of the paper is that it shows how this framework can be adapted to a wide range of setups, by adapting it to four types of structure in multi-armed bandits. The proposed algorithms start from some of the techniques employed by previous algorithms in the literature, and design specific Exploration and Verification rules for each problem. The analysis is sound, presented in detail, and allows to obtain an improvement in the four settings.   On the other hand, no experiment is included in the paper. It would have been nice to see for at least one of the settings how the performance improves in practice. For instance, an example where there is a large enough \delta such that the Explore-Verify Condorcet Bandit algorithm behaves better than previous algorithms.     Typos: Abstract: Mutli-armed --> Multi-armed Lines 197,374,422:  it's --> its  ",2-Confident (read it all; understood it all reasonably well),"The authors consider the MAB problem with i.i.d. rewards, but where the reward function has a structure. They propose a general algorithmic idea for this problem, and apply it to several structured bandit problems known earlier in the litterature.    Their algorithmic idea is roughly as follows: exploration is split in two phases, in the first phase one looks for a candidate arm (an arm which is possibly optimal), and in the second phase one determines whether or not the candidate arm is indeed optimal. If the outcome is positive the algorithm has identified the optimal arm, otherwise one returns to the first phase. The authors argue that, while the second phase is the one responsible for most of the regret (for small confidence parameter delta), it is usually easier to design for some structured problems (for instance the unimodal structure where one must sample neighbours of the optimal arm).","The paper is overall interesting and clearly written. While the idea of verification is indeed simple, it seems strong enough to bring improvement to known results for several popular types of structures.   It should be made clear that what the authors propose is more of an algorithmic idea which needs to be tailored for each structured bandit problem. For instance, in order to design the verification phase efficiently, one needs to know precisely what is the structure at hand.   It would also be good if the paper included numerical experiments in order to show how the proposed algorithms behave in practice, in particular how sensitive they can be to the exploration parameter \kappa (given \delta fixed).      Small remark: the Table 1 which presents the relative improvement with respect to previous solutions is a bit unclear at places, due to the fact that it is not always clear which term dominates. It could be better if two columns are added (one for delta close to 1 and another one for delta close to 0). For instance, as far as I understand, for Unimodal Bandits, the improvement can only be \Omega(K) in settings where K is much larger than \log(1/delta). Indeed, the algorithm of [6] is asymtotically optimal, so that if the O( \log(1/delta) ) term dominates, there is no possible improvement. Also, I might have missed something, but in the linear bandit case, isnt the lower bound O( d log(1/delta) / \Delta^2 ) ?  Therefore I do not understand how one can obtain a d-fold improvement over the result [19]. This should be clarified.",1-Less confident (might not have understood significant parts),"This paper proposes a verification-based framework for solving a range of bandit problems, including condorcet dueling bandits, copeland dueling bandits, linear bandits, unimodal bandits, and graphical bandits.  The setting considered is PAC-style guarantees for pure exploration, rather than online regret minimization.  The authors show an iterative approach whereby a verification algorithm is used to verify a proposed solution by an exploration algorithm.  Since the verification algorithm has cheaper sample complexity, one can construct a meta-algorithm that iteratively calls the two algorithms in order to achieve rigorous PAC-style sample complexity guarantees.  These guarantees are compared against existing results, and the authors demonstrate improvements in some regimes.  In general, I think the technical results are interesting and mostly sound, although I can't help but feel that the presentation could be significantly improved.  Some of the results seem a bit incomplete, although I think a more complete theoretical account is straightforward.    My larger concern is the significance of the results in the paper.  Part of this could be resolved by greater clarity in the writing.  More details are covered in the sections below.  As it stands, my review is borderline.  I can see this paper as suitable for NIPS or a bit below the accept threshold, depending on the author response.","*** Significance Part 1 *** I wonder if the authors have looked into the details of the proofs of some of the previous work.  One common trick that people do in these types of theoretical analyses is to simplify the results because one cares only about the simpler bounds that merges together a more complicated summation of terms into a single term.  For example, the result for Linear Bandits shows an improvement that could be non-existent with a more refined analysis of the result in [19].  This would be somewhat analogous to what the authors noted in footnote 3.  *** Significance Part 2 *** The authors argue that these results are meaningful improvements upon previous work.  However, it would be easier to accept this argument if the authors can construct explicit cases that satisfy the results.  For instance, I'm particularly interested in the linear case.   Numerical experiments could also help and/or complement the theoretical results.  *** Clarity *** The paper feels a bit incomplete in its writing.  For instance, Copeland dueling bandits isn't substantially discussed at all in the main paper.  And related to the above point on significance, the reader feels a bit disappointed by a lack of substantial discussion on the implications of the theoretical results.  Furthermore, the paper doesn't actually formally define any problem settings.  I realize this is very difficult due to the generality of the framework being proposed, but this lack of specificity is somehow dissatisfying.  *** Proofs *** I'm curious about the equations after Line 135. It seems like there is an extra kappa*H_explore term -- can the authors explain this?  Perhaps related to the above question, the authors never explicitly state a sample complexity of the verification oracle when the findbestarm subroutine fails.  For instance, Lemma 14 doesn't have a sample complexity guarantee.  I think this should be a straightforward fix, but the theoretical results are incomplete without them.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors provide a framework for the best arm identification problem, in the fixed confidence setting. The idea is to find an empirical best arm with a small number of observations (and with a lower probability of success than the targeted confidence) and then to verify if indeed this arm is optimal, this time with targeted probability. This process is repeated until a positive verification. The authors argue that the verification problem allows to match (or even outperform in some case) the sample complexity of existing algorithms on various best arm identification problems (dueling bandits/linear bandits/...).","The proposed approach is quite interesting and is, at my knowledge, innovative. Even, if the idea itself is good, they are some flaw in the results and in the presentation of the paper.      1/ In think that the comparisons between sample complexities of the new methods and the state of the art could be unfair. Results from this paper are given in expectation (with respect to probability of the 'find then verify iterative process') whereas state of the art sample complexity results are given in high probability and therefore are stronger.  2/ Even if an high probability result is maybe hard to achieve, some experiments may help to convince of the effectiveness of the approach. With some indicators like the mean time needed to find the best arm, the number of times when the new method finds the best arm before the state of the art (for me, the more useful indicator with regard to the discrepancy between the theoretical results, the expectation may involve a lot of variance in practice)  3/ The applications of the generic framework to the Dueling Bandits/Linear Bandits/... are at the same time too much detailed and not enough. Right now, half of the paper describe these applications while being not self contained and too much dependant from the appendix. Maybe choosing one and detailing it could clarify the second part of the paper. You could then notice the reader that the same process can be applied to other setting. With the current presentation, a major part of the appendix could have their place in the paper.",2-Confident (read it all; understood it all reasonably well),"The paper proposes a meta-algorithm for finding the best arm in structured MAB problems.  The algorithm consists of two components. One is ""Find_Best_Arm"" that outputs a candidate best arm with some additional information, which could be an existing algorithm for best arm identification. The other is ""Verify_Best_Arm"" that uses the output of Find_Best_Arm and determines whether the candidate arm is the best arm or not. It provides some analysis to show why the interleaving of the two components may lead to better query complexity. The meta-algorithm is applied to Dueling bandit, Linear Bandit, Unimodal Bandit etc, which improves the regret upper bounds of existing works. ","  Regards to the second bullet and its proof in Lemma 11, what does \min_y p_{ x y(x) } means? As y(x) = \argmin u_{xy}, the \min_y operator causes confusion. For its proof, last line in page 12 in the supplementary states ""p_{ x y(x) } \geq l_{ x y(x) } \geq 2 u_{ x y(x) } \geq 2 u_{ x y' } \geq 2 p_{ x y'}"", why does ""2 u_{ x y(x) } \geq 2 u_{ x y' } "" holds? It seems to contradicts the definition of  y(x).  For Lemma 13, it assumes p_{ x y(x) } \leq 2 min_y p_{xy}. When does the assumption holds? Which parts of the proof in Lemma 13 use the assumption?  This paper would be better if some simulation results would be provided to support the analysis.  Minors:  In the supplementary  line 419:   ""let \Delta_x = \max_y p_{xy}"" should be ""let \min_y p_{xy}""             ""let t_x = c \Delta_x^{-2} \log( K / delta Delta_x )"" should be ""let t_x = c \Delta_x^{-2} \log( K / delta Delta_x^2 )""      ""we have that after t_x rounds it must be the case that u_xy \le 0""  ---> u_{x y(x)} \le 0  line 510:  ""\Delta_x \leq \hat{ \Delta_x } \leq 1.5 \Delta_x for all suboptimal x ..."" should be ""0.5 \Delta_x \leq \hat{ \Delta_x } \leq 1.5 \Delta_x""  ",2-Confident (read it all; understood it all reasonably well),"This paper introduces a method called ""Explore-Verify"" to solve pure exploration bandit problem. The author uses this framework to improve algorithms in various of pure-exploration bandit problems. ","The idea of ""Explore-Verify Frame"" is very good. But the improvement ratio of all settings in this paper needs some special settings like ""\delta is small"" or ""\delta is large"". So a question is whether there is some other solutions that do not need to use this framework given these special cases.   Some questions about the proofs: 1) In the proof of Lemma 8: why using Marcov Inequality can have the probability is at most 2^{r_0-r}? I think using Marcov Inequality the probability upper bound is (T_1\log(r^2/2k)+T_0)/2^r. But the numerator is not upper bounded by 2^r_0.  2) In the proof of Lemma 12, whythe dominate part is p_{xy}^{-2} when p_{xy} > 0? Why we can ignore the p_{xy}^{-2} in the logarithm term?",2-Confident (read it all; understood it all reasonably well),,,,,,
Multistage Campaigning in Social Networks,"Mehrdad Farajtabar, Xiaojing Ye, Sahar Harati, Le Song, Hongyuan Zha",https://proceedings.neurips.cc/paper/2016/hash/b090409688550f3cc93f4ed88ec6cafb-Abstract.html,The authors consider a model of electoral campaigning in social networks. They formulate and solve a multi-stage optimal stochastic control problem which aims at maximizing either the minimum or total exposure to the campaign. ,"The problem solved is interesting and there is some interesting modelling and math involved as well. It's not entirely clear how applicable these results are in a real situation but this is acknowledged in the paper. The presentation and the language is also pretty good, although a careful read would not be a waste of time.   In theorem 3 I am not sure what is meant by piecewise absolutely continuous. ",2-Confident (read it all; understood it all reasonably well),This paper uses Hawkes process to model user activities in social network campaigns. It further considers the problem of implementing interventions in multiple stages to shape exposure. A convex dynamic programming framework is developed to determine the optimal intervention policy to achieve desired campaigning results. ,"The design of the experiments, especially the evaluation on the real data, may not be sufficiently relevant. According to the output of Algorithm 1, the goal is supposed to find out the set of optimal interventions for u_0, ..., u_m. It is understandable that carrying out real intervention in a social media platform would be very challenging. Nevertheless, it is not clear why predicting which cascade will reach the objective function is relevant to assess the effectiveness of the proposed algorithm.   Moreover, the experimental results are not discussed sufficiently. For example, the three subfigures on the left column of Figure 2 show fluctuations of prediction accuracy. However, the results are simply summarized as ""the performance drops slightly with the network size"". More discussion is expected to understand the reasons behind the performance fluctuations, for example, does it relate to instability of the algorithm? ",1-Less confident (might not have understood significant parts),"Hawkes process has been proven to be useful in financial applications. Limit theoretic results for them have been obtained in probability theory in the past few years. In particular, this process is able to capture jumps in a way that wasn't possible in systems such as Brownian motion. The paper models the process of activity of a social network user by a multivariate Hawkes process, with links to typical cost functions.   Experiments on synthetic and real networks are provided.","The paper provides strong connections in novel areas. However, the only experiments have been carried out on very small scale problems compared to one would ideally desire. This brings concerns about scalability which can be an issue that has to be resolved.  It would be great to see both more theoretical justification for scaling, and practical observations, if possible.",1-Less confident (might not have understood significant parts),"This paper introduced a very interesting topic, i.e. multistage campaigning in social network by using social network, optimization, and closed-loop dynamic programming. This is a difficult problem because multiple factors have to be considering. For example, the user relation network, the temporal information, the uncertaining of the user behavior, etc. This paper utilized the temporal point process to model the problem, by using techniques like optimation and dynamic programming. It also provide experiments on both synthetic and real-world datasets to demonstrate the application of the proposed model. ","This paper is really interesting and with great contribution if it could be applied to real world social networks like Twitter or Facebook. The contribution is clearly demonstrated and the idea is interesting and innovative. However, the experiment is not sufficient enough. First, the baselines that are compared to should be described briefly in stead of only giving the abbrivation of the baseline names. Secondly, does the vertical line on top of the bar represent standard deviation or 95% confidence interval in Figure 1? If two algorithms have their confidence interval overlap, it means that they are not significantly different from each other. According to the graph, proposed algorithm has its confidence interval overlap with other algorithms. In terms of this issue, more explanation are expected. Thirdly, if the vertical line represents standard error, CLL has larger standard error than RND and OPL according to Figure 1 (a). It would be better to explain why the standard error of proposed algorithm is not as good as other algorithms. In addition, although a section of Conclusion is not necessary, it would be easier for future readers to have a brief understanding of what is achieved in this paper. ",1-Less confident (might not have understood significant parts),"The paper builds on existing work in shaping social activity on social networks modeled as multivariate Hawkes processes (developed in citation [8] for constant-in-time control schemes). The paper characterizes a relationship between time-dependent exogenous activity and average activity rate in the network for exponential kernels, and uses this to derive an approximate dynamic programming control algorithm for driving activity using piecewise-constant control signals. The paper compares the performance of their control algorithm to baselines on synthetic networks with simulations, as well as real-data (using a custom performance metric) and shows promising although somewhat difficult to interpret results.","The theoretical contributions (relationship between time-dependent exogenous intensity to average activity) appear significant, and the use of this result to derive a closed-form control algorithm appears to be a contribution to the field of shaping activities in social (and other) networks modeled as multivariate Hawkes processes.  Regarding the clarity of this paper:  The paper does not frame itself relative to prior work clearly. Clearly indicating where and how this work is a generalization of prior work [8] would improve clarity of the paper, and highlight the core contribution of handling *time-dependent* exogenous events. Stating that the paper ""establishes theoretical foundations of optimal campaigning over social networks where user activities are modeled as multivariate Hawkes processes"" does not properly localize the work relative to [8]. This could be improved in part by mentioning [8] in the introduction, and specifically stating that prior work has done optimal control with constant exogenous control, and that this paper addresses multi-stage exogenous intensity. Similarly, in section 3 “From Intensity to Average Activity”, it should be made clear that this is an extension of a previous analogous result for constant-time activity. Finally, the three objective functions of section 4 appear to mirror the three objective functions from [8], but the text does not reveal this when it states “In what follows, we introduce several instances of the objective function...”. These changes may help the reader focus on the core and cohesive technical contribution.  The description of the closed-loop approximate dynamic programming control algorithm requires more precise language. For example, Lines 241-252 are unclear: The excerpt “By observing the counting process in previous stages summarized in a sequence of x_m) and taking future uncertainty into account, the control problem is…” needs to be more explicit (using mathematical variables) about what specific data is given to the control algorithm, and what are the specific random variables causing the ""future uncertainty"". For example, in Line 257: “However, when control policy \pi_m is to be implemented, only x_m is observed and there are still uncertainties in future..” Why is this a problem? Assuming “implemented” means that the policy was already determined, then isn’t implementing the policy a matter of evaluating the function \pi_m on input x_m?. Being more clear about the *computation* represented by Equation 19 and the *computation* representing Equation 18 would make it more clear how Algorithm 1 is derived in a stage-wise process (e.g. on Line 268: “Solving for .. is analytically intractable [due to ...]"").  How significant is the choice of exponential kernel? It seems it may be critical to the control algorithm. If this is indeed a critical requirement, it deserves some more discussion besides just calling it “standard”.  Based on the text, in Figure 1 the error bars seem to represent standard deviation and not a form of confidence intervals, but this could be made more explicit in the figure caption.  Only the acronyms of the baselines used in Figure 1 and Figure 2 were given in the main text. The identity of these baselines could be briefly stated in the caption of Figure 1 (with citations).  Regarding the potential impact of the work:  The experimental results on synthetic data did not seem as significant of an improvement over open-loop one-time optimization over all stages as might be expected given that the closed-loop algorithm has access to significantly greater information. A demonstration for network parameter settings that better highlight the novel algorithm's strengths relative to baselines, paired with a demonstration of network parameter settings for which the distinction is less striking, would make the potential impact of the technique more clear. A discussion of in which contexts the proposed algorithm is likely to significantly improve performance would also strengthen the clarity as well as make it easier to assess impact.  The experimental results on real-world data are based on a performance metric of “predicting which cascade will reach the objective function better. They should be able to answer this by measuring how similar their prescription is to the real exogenous intensity...” This metric needs a clearer and more mathematically precise definition---the results of Figure 2 seem promising, but are hard to interpret based on the lack of clarity in the prose description of this key metric.",1-Less confident (might not have understood significant parts),This paper studies the problem of multi-stage campaigning optimization in social network. The activities of users are modelled as a multivariate Hawkes process (MHP). The intensity of exogenous events and commonly used objective functions follow a time-dependent linear relation. A convex dynamic programming framework is used to compute the optimal intervention policy which prescribes the required level of external drive to obtain a desired campaigning exposure profile.,"The peer influence and external exposure in social network is an interesting topic. The MHP model adopted in the paper has been applied to this problem by varied social network papers. The contribution of this paper is primarily the dynamic programming framework to the multi-stage optimization problem. The derivation seems to be correct and the experiments are reasonable. The major issue is that the paper is not self-contained. The related work and conclusion section have been moved to the supplement material. There are no discussions on a few citations in the paper since it only appeared in the related work. I do think related work is necessary for the main body since the problem studied in this paper is a popular topic in social network research.  Other comments include:  1. Approximate dynamic programming has been applied to solve (18). There is no theoretic guarantee on the performance of this approximation algorithm. Approximation ratio should be discussed. Otherwise, it is hard to evaluate the contribution of this algorithm.  2. Scalability of the algorithm is very important in social network analysis due to the scale of the network.  The computation complexity of the proposed algorithm is unclear and should be discussed.",2-Confident (read it all; understood it all reasonably well),,,,,,
Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization,"Alexander Kirillov, Alexander Shekhovtsov, Carsten Rother, Bogdan Savchynskyy",https://proceedings.neurips.cc/paper/2016/hash/979d472a84804b9f647bc185a877a8b5-Abstract.html,The paper studies the problem of jointly inferring several diverse labelings for a binary submodular energy in a graphical model (the M-best-diverse problem). The paper establishes an interesting relationship between the joint M-best-diverse problem and the parametric submodular minimization problem; the latter has been studied in the work of Fleischer and Iwata for general submodular functions. The authors empirically evaluate the proposed algorithm and show that it achieves very good performance compared to the prior approaches for the M-best-diverse problem.,The contributions of this paper are novel and interesting both from a theoretical and experimental point of view. The theoretical result shows an interesting connection between the joint M-best-diverse problem and parametric submodular minimization. The experimental results show very good empirical performance of the proposed algorithm compared to the previous two methods both in terms of quality and running time. The contributions are novel and will likely lead to future work. The paper is generally well written.  == post-rebuttal answer==  I have read the entire rebuttal.,2-Confident (read it all; understood it all reasonably well),"This paper proposes a new algorithm to jointly compute M diverse solutions with the objective function being their total energy minus the total diversity (measured in pairwise Hamming distance). The main theoretical result (Theorem 2) shows that for binary submodular energy, the M solutions can be solved using a parametric energy function with M given parameter values. In practice, this implies a sequential algorithm using the parametric max-flow-min-cut algorithm and a parallel algorithm which solves M min-cut problems in parallel. These algorithms are asymptotically faster than previous method for the same problem [22] which needs to solve an energy optimization problem on a graph of much larger size (quadratic to M). Experiments on interactive segmentation and VOC 12 segmentation datasets shows that the proposed methods are much faster than previous ones.","This paper is technically very sound. The main theoretical result is surprising to me and is not trivial to prove.    The result also makes the connection between diverse solution problem and parametric min-cut, both of which are very interesting and popular problem/method in vision-inspired structured prediction problem.   If I understand correctly, the sequential and parallel algorithms are asymptotically the same. However, according to Fig 2(b), the gap between the sequential and the parallel algorithms does grow linearly to M. This is kind of surprising to me and is not well explained.   I also found it odd that this paper does not have the complexity explicitly stated. ",2-Confident (read it all; understood it all reasonably well),"This manuscript identifies a relationship between diversity with binary submodular energies and the parametric submodular minimization. Based on this, the joint M-best diverse labelings for the permutation invariant node-wise diversity measures can be obtained by running a non-parametric submodular minimization solver for M different values of parameter $\gamma$ in parallel. Importantly, the values for $\gamma$ can be computed in a closed form prior to any optimization. These lead two simple but efficient algorithms for the joint M-best diverse problem, which lead to better performance over those considered competitors in terms of runtime and result quality.","- In the title, it would be better to make clear the scope of the paper, which is M-best diverse labelings for binary submodular energies. The current title can be a bit misleading for future readers.  - Also in the abstract and the introduction, it would be better to specify that the proposed stuff is valid for the class of the permutation invariant node-wise diversity measures.  - The inclusion of representative qualitative results can be interesting for a part of readers.",2-Confident (read it all; understood it all reasonably well),The authors describe an improved sequential and a parallel algorithm for the case joint diverse best M solutions problem for binary submodular energy functions and nodewise diversity functions that are permutation invariant and concave.,"This work builds primarily on the work of Kirillov but feels a bit incremental in some ways as the main theoretical result is very reminiscent of previous work.  The fact that it leads to a better algorthimic approach is important, I think that the comparison with previous work could be stated more clearly in the draft.  I would have liked to see an actual complexity analysis of the algorithms.  That is, the authors claim that it leads to an improvement that scales with M, but that doesn't seem to be presented theoretically.  What is the computational complexity of the algorithms involved?  Does this improve the theoretical asymptotic complexity, or is it just a constant time improvement?  In the worst case I think that they may be the samThis kind of analysis should be possible to state for the simple pairwise binary models discussed in this work.  The same goes for the parallel algorithm.    The draft is difficult to read because of a number of typos.  This can certainly be corrected with some effort.  Also, it appears that the main theoretical contribution is imprecisely stated.  This should be corrected as it makes it difficult to read/check the proofs.  Miscellaneous typos/comments:  -  line 61, ""and proposes a solver to it"" -> ""and proposes a solver for it"" -  line 66, ""e.g. Hamming distance"" -> ""e.g., Hamming distance"" -  line 104, ""i.e. each variable may take only"" -> ""i.e., each variable may take only"", this typo occurs in many places throughout the draft. -  line 135-136, ""In this paragraph we briefly summarize existing efficient minimization approaches for (2)"" ->  this is the end of the paragraph so it doesn't make any sense. -  line 142, ""this algorithm requires to sequentially solve M"" ->  ""this algorithm requires sequentially solving M"" -  line 161, ""which plays a crucial role for obtaining the main results of this work"" -> ""which plays a crucial role in obtaining the main results of this work"" -  line 176, I'm not sure that the definition of \Gamma here makes sense given that \Gamma should be a subset of R^{|V|}.  In fact, I think that there is a problem with the definition of the parametric problem in line 165 as ""i"" does not appear anywhere in the definition. -  line 204, ""monotonous"" -> ""monotone"" ",2-Confident (read it all; understood it all reasonably well),"This paper relates the problem of optimizing the Joint-Diverse-M-Best Problem to minimizing a parametric submodular function. Based on this relation two new algorithms are presented: (i) solving potentially up to M problems in parallel, and (ii) by exploiting the fact that the solutions to the problem form a nested tuple, the problem can be solved efficiently in a sequential manner. Experiments on two datasets show an improvement in computational time on the baseline algorithm Joint-DivMBest while getting the same exact solution. While both proposed methods return the exact solutions the runtime is even faster than the greedy approximation algorithm DivMBest.","- The paper is well written and comprehensible.   - The relationship between the joint M best diverse and parametric submodular minimization is an interesting theoretical contribution. This leads to new algorithms that scale well. Especially for concave node wise diversity measures the M problems become independent and can thus be efficiently be implemented in parallel. The original problem has proven to be useful in computer vision, therefore reducing the computational burden is great.  - Node-wise permutation invariant diversity measures are considered. Requiring permutation invariance is reasonable because wrong solutions may exist which are just permutations of the best solutions. This does, however, implicitly assume that good solutions are centered around the best solutions.  - For interactive segmentation only the best out of M solutions is evaluated, all other M-1 solutions are neglected. It would be interesting to see how accurate these other solutions are.  Further Details:  *In the main paper tuples are denoted with (y^1,y^2,...) -- in the supplementary material (e.g. line 412,480) tuples are expressed by {y} which can easily be confused with sets.  *line 515: diveristy -> diversity  *Definition 1: The nested property for binary labels is easier to grasp with additional explanations that are given in line 230-231.",2-Confident (read it all; understood it all reasonably well),,,,,,,,,
Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods,"Lev Bogolubsky, Pavel Dvurechenskii, Alexander Gasnikov, Gleb Gusev, Yurii Nesterov, Andrei M. Raigorodskii, Aleksey Tikhonov, Maksim Zhukovskii",https://proceedings.neurips.cc/paper/2016/hash/1f34004ebcb05f9acda6016d5cc52d5e-Abstract.html,"This paper analyzes the convergence of a non-convex loss-minimization problem for learning the parameters of a general graph-based ranking model, that is defined by a random walk conducted by weights of nodes and edges, which are in turn defined by random walks defined by nodes’ and edge’s features. The optimization problem can not be solved by existing optimization methods which require exact values of the objective function. The proposed approach hence operates in two level. At the first level, a linearly convergent method is used to estimate an approximation to the stationary distribution of Markov random walk. This approach is validated among others and the authors show the value of the loss function can be approximated with any given precision. They also develop a gradient method for general constrained non-convex optimization problems using an inexact oracle, and prove its convergence to the stationary point of the problem. The contribution is in the adaptation of the approach to the case of constrained optimization problems when the value of the function can be calculated with some known precision. They prove the convergence of this method and exploit it on the second level of the proposed algorithm.  The paper is well written and proofs seem to be correct though I did not go through all of them. My concern is that the main optimization points made here are adaptations of those proposed in “Yurii. Nesterov and Vladimir Spokoiny, Random Gradient-Free Minimization of Convex Functions, Foundations of Computational Mathematics, 2015, pp. 1–40”, and the supervised pagerank algorithm has been also proposed previously. On a minor level, a conclusion is expected especially that there is place to write one.  ","I would suggest to present the optimization problem in a more general context, showing the studied algorithm as to be a specific case. This would allow to better place the contribution with respect to the state-of-the-art.",2-Confident (read it all; understood it all reasonably well),"This paper addresses formulate the Supervised PageRank models as a constrained non-convex optimization problem by incorporating properties of nodes and edges. The authors propose two approaches to solve the optimization problem. The first one is a gradient-free method with inexact zero-order oracle. The second one is a gradient-based method with inexact first-order oracle. My major complaint is that none of the experiment results are in the main text, which makes the paper incomplete.","Detailed comments.  1. The authors put experiment results in the supplement, which makes the main text incomplete.  2. Convexity is assumed in Theorem 1, while the function in (2.4) is non-convex. How to choose the set $\Phi$ to guarantee the convexity of the objective function?  3. Theorem 3 provides a bound for $M_K(x_K - x_{K+1})$. Could this bound ensure that the sequence $x_k$ converges to a stationary point?  4. By Figure 1 in the supplement, GFNM converges slowly. The authors may also want to report the efficiency of the algorithms.",2-Confident (read it all; understood it all reasonably well),"In this paper, the authors proposed two optimization methods to solve the supervised PageRank problem. Unlike other existing methods, this paper proposed a gradient-based method with theoretically convergence rate guarantee and can achieve a given accuracy. The proposed gradient-free method has a guaranteed loss function decrease value. These two methods do not require exact value of the objective function and have a estimate of the convergence rate, also the performance is better than state-of-the-art in terms of the ranking quality.  The hyper-parameters used in both methods are provided. The data sets are not publicly available but the description is clear so it may not be hard to reproduce their results. ","The authors proposed two two-level optimization methods, namely gradient-based (GBN) and gradient-free (GFN) to solve the supervised PageRank problem. The lower level and upper level optimization is based on Nesterov and Nemirovski's work (ref:17).  The proposed method does not require an exact value of the objective function and has proven estimate of the convergence rate given an accuracy. Therefore the proposed method can avoid computing the large matrix while still have a good result.  From Section 2 to Section 5, the authors explained the details of their methods and included all the proofs in the supplementary. However, it may help a lot if the authors can provide a symbol table since there are too many symbols in their equations and it is really easy to lose track of them. For example in Line 139 and equation 3.1 the authors used $N$ and they did not mention the meaning of it (number of steps) until Line 193.  Also, in the experiment section, the authors only compared their work with GBP regarding to the loss function instead of the ranking quality. Moreover, since this work is a generalization of ref:17, I think it would be good if the authors can compare with it directly so we can see if the performance is better.",1-Less confident (might not have understood significant parts),This paper propose gradient-based and random-gradient-free methods to solve a non-convex loss-minimization problem of learning supervised PageRank models.,It is better to report the time of the proposed optimization method.,1-Less confident (might not have understood significant parts),"The authors motivate pagerank and then bring up a framework which has node and edge weights from a feature model. They pointed out that a previous algorithm for learning the parameters of the model had no performance guarantees. They plan to solve the methods by combining a previous method for approximating the stationary distribution of a Markov random walk with an adaptation of a gradient free method used for optimization when the loss function can only be estimated. They reproduce a framework under a different name and notation from [21]. They discuss a number of technical assumptions and a loss function and explain why the problem is hard to solve. Some lemma bounding the approximate gradient are stated. A general gradient free method is discussed, using a biased gradient-free oracle, and state a theorem on its performance. They make some more assumptions and construct an algorithm for the learning problem and adapt the gradient free methods to their problem, stating some more bounds. They outline some details of experimental results on a web graph used in related work, but detailed reporting of results and analysis are relegated to the appendix. Some parameters are not reported, and the novel algorithm is proclaimed to have outperformed other algorithms, but there is no discussion of the extent or implication of the improvement. It is unclear whether a diverse set of graphs were considered for testing.  ","The writing is littered with grammatical errors. Citations are sloppy, the problem is called Supervised PageRank with a cite to [21], but only semi-supervised page rank appears there. I am not convinced that the problem solved is impactful.    Your bounds are strong, but I have doubts about whether the technical assumptions stated will hold in emprical data. Some assumptions seem limiting, e.g. your constriction of the search space of phi to an R-ball around some chosen phi_0may greatly restrict the efficacy/applicability of your algorithm, which should be discussed.   My biggest issue with the paper is that it all the meat appears in the appendix. You neither provide proof outlines nor full empirical results. How did you choose phi_0 and R in the empirical results section? Did you run tests on more than one graph? You do not report any tables or specific results in the main body of the paper. What are the practical consequences of your improvements?  Your bounds and analysis and algorithm may be powerful, but the content in the paper itself fails to convince me. ",1-Less confident (might not have understood significant parts),,,,,,,,,
Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,"Tejas D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, Josh Tenenbaum",https://proceedings.neurips.cc/paper/2016/hash/f442d33fa06832082290ad8544a8da27-Abstract.html,"This paper considers a hierarchical approach to reinforcement learning, where a top-level controller selects subgoals for a low-level controller. The low-level controller is trained by DQN to select actions that maximise subgoal attainment for the specified subgoal. The high-level controller is trained by DQN to select subgoals as actions, which are then ""executed"" until completion, that maximise true reward. Promising results are demonstrated in the challenging Atari game Montezuma's Revenge, when using a mostly handcrafted set of subgoals. ","The ideas in this paper are interesting and worth pursuing. It's a very clear and sensible example of combining hierarchy with deep RL, the combination of which is of high current interest. The initial experiment on the ""six state"" MDP is so trivial it is uninteresting. The Montezuma's Revenge example is much nicer, demonstrating impact (albeit with a little bit of handcrafting) on a problem known to be challenging for the current state-of-the-art and would be worth seeing at NIPS. The paper is technically a little sloppy in places. As a result I have mixed feelings about the paper.  ISSUES  1. I'm afraid that the first experimental example is really quite lame. There's nothing wrong with small examples if that small size is used to provide insight, but comparing learning curves as if it is a meaningful domain does not provide us with very much. Also, it would be clearer to describe this example as an 11 state MDP with bidirectional connections between states 1-5 and 7-11, but only one way connections between 5->6 and 6->7. Dressing it up as an MDP with ""history"" makes it seem more complex than it is.   2. There are issues with reward accumulation and discounting in the paper. I believe that equation 2 is missing discounts, both on the sum over extrinsic rewards f, which should be discounted, and the max Q term, which should be discounted by gamma^N not just gamma. The stored experience for Q2 should be the discounted sum of extrinsic rewards f, not just the instantaneous reward f_t. I'm concerned that the loss function L2 used in the experiments - which is not actually given in the text - is similarly based on incorrect discounting and reward accumulation.  3. The literature review is not very comprehensive and misses many related papers, e.g. other RL papers that also have a high-level controller picking subgoals for low-level controllers, e.g. Dayan's Feudal RL, Bakker & Schmidhuber's HASSLE, etc. - this is not a new idea by any means - the novelty comes rather in the combination with deep learning.  4. The use of the term ""intrinsic motivation"" is non-standard - typically the usage in this paper would be considered more about subgoal selection than modifying the reward function used by the agent, although I can understand the authors' perspective on this too.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper addresses deep hierarchical RL with a two-levels approach. Low-level controllers are used to reach goals, and a higher-level controller chooses the current goal. Goal specification is not clearly described. The model is tested first in a toy problem and then on Montezuma's revenge, a hard ATARI game. Results are preliminary, but quite promising.","The paper addresses the timely question of building a hierarchy of behaviours out of recent deep reinforcement learning methods. This is one of the hot challenges at the moment and several teams are on that (actually, the arXiv version of the paper is already cited 10 times). The paper proposes a promising contribution to the domain, but my general feeling is that the paper and the corresponding work are not mature enough for publication at NIPS: several points need to be clarified and the empirical evaluation is too preliminary.  Globally, the major issue in the field is the automatic extraction of a hierarchy from data. Here, a good deal of the hierarchy building process is left to the engineer through the identification of a set of goals. In itself, this is not a major flaw of the paper, but the limitations of the work should be discussed under this light.  About the literature review section, two points:   1°) ""Our approach does not use separate Q-functions for each option, but instead treats the option as part of the input, similar to [18]. This has two advantages: (1) there is shared learning between different options, and (2) the model is potentially scalable to a large number of options.""  => The statement about these two advantages should be supported either by a reference or an empirical study, where the authors would compare both approaches  2)° In the ""Intrinsically motivated RL"" and the ""Cognitive Science and Neuroscience"" subsections, nothing is said to relate the described works to the submitted paper. In particular, for the latter, the content looks quite far away. Also, the ""Deep Reinforcement Learning"" subsection does not bring much. So my general feeling is that this section should be more targeted to recent deep hierarchical RL work, and should save space for additional studies such as the one described in point 1 or improving the model description (see below).  By the way, given that the arXiv version of the paper was published a while before NIPS, the authors should refresh their literature review section the many recent works in the domain. E.g. Model-free episodic control, several works about macro-actions, using A3C instead of DQN (fine if that's left for future work ;)), etc.  About the model description:  The model section could be much clearer. Although the right-hand side of Fig.1 shows it, it is not obvious from the notations in the text that the controller networks take s and g as input and a as output, and that the ""meta-controller"" takes s as input and g as output. It would be clear if you write : Q1(A|s,g) and Q2(g|s), for instance.  Also, there should be a module (inside the meta-controller?) checking that a goal is reached. This is not described clearly. The language to specify goals is not described either (despite the connection to Duik's work about object-oriented MDPs).  The replay buffer initialization is not described.  The exploration strategy deserves its own subsection. Using a separate epsilon for each goal should be discussed. Also, the adaptive annealing method for \epsilon_1,g is not described, though this is not standard.  It should be specified that a set of goals is given as input to the algorithm. Also, the method for checking that a goal is reached should be present in the algorithm. This part is poorly specified and looks quite ad hoc (see below about Experiment 2).  About the study and results:  In Experiment 1, the structure of the controllers and meta-controller are not described (and should be). It is only specified that it is not a deep network.  In Experiment 2, the lack of a clear specification of how the goals are managed is a major issue. How many entities do you have in Montezuma's revenge? If you have two instances of the same entity, do they correspond to two different goals? Is reaching an entity with the agent the only kind of goal? What about avoiding monsters at pacMan, for instance? Even if what the authors did is completely ad hoc to a specific game, it should at least be described accurately.  The parametrization of the architecture and learning process looks quite arbitrary and should be discussed.  Finally, the small paragraph about missing ingredients convinces the reader that this work, although an interesting starting point, is not mature enough for publication in a major conference, particularly one that is biased towards clean theoretically-grounded work.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper proposes a setup for doing hierarchical deep reinforcement learning when task-specific subgoals are given. The architecture is akin to the one proposed by Lin in 1993, except for the deeper networks. The paper is well-written and the experiments are interesting but rather preliminary."," (1) The paper is severely handicapped by its lack of historical perspective. In fact closely related HRL approaches (without the “deep”) have been proposed and investigated since over 20 years, starting with Long-Ji Lin’s pioneering work (1993) as well as variations like W-learning by Humphreys (1995) and HQ-learning by Wiering (1997)...  (2) The issue of option termination/interruption is not explained clearly: it sounds as if an option is executed until the corresponding subgoal is reached -- which may be an extremely long time initially (when the subgoal policy is not competent enough) or even infinite (e.g. if the key subgoal is invoked but there no longer is a key)?  (3) I find the terminology of “intrinsic” motivation highly misleading -- this is not what is usually meant by this term (e.g. drives, novelty, curiosity, empowerment, etc.). The terms of (sub)goals or sub-tasks would be more appropriate for the proposed approach.  (4) The first experiment is mis-specified, because the authors confuse the notion of (Markovian) state and (partial) observation: s2 after having been in s6 is not the same state than s2 without having been in s6. Please clear this up. Also, the results (0.13) look far from the achievable optimal policy for this task?  (5) There is a lot of detail missing for the experiments to be reproducible (generally, the paper feels a bit rushed), e.g. how long is the “first phase” (L228)?, how long is an epoch (L187)?, what is a “step” (Fig 3, left)?, how are the Atari subgoals computed, from pixels or RAM state?, etc.  (6) On one of the main results (Fig 4, bottom right), it is unclear to me how this distribution of goals is “appropriate”? Why would it not invoke the ladder goals more often than the key? Also, why would it initially pick goals uniformly even though the doors are useless because the key is never picked up yet?  (7) To end on a positive note, I’m intrigued and hopeful for the author’s proposed exploration heuristic that takes into account the empirical success rate per subgoal -- I’d love to see this properly defined and its effect studied empirically.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper proposes a hierarchical architecture for Deep Q-Learning to aid exploration via intrinsic motivation and also learn goal-oriented options via temporal abstraction. It includes a meta-controller that selects which goal the agent should aim to achieve and rewards the low-level controller if it manages to achieve it (intrinsic motivation). The low-level controller uses this goal to learn policies over states and goals. The paper evaluates this approach on a stochastic decision process and a game from the Atari Learning Environment (Montezuma's Revenge), notorious for not  being efficiently explorable using dithering techniques. They do not address how goals chosen by the meta-controller are generated, leaving that for future work.","The authors have chosen to tackle a very important problem in Deep RL, how to induce exploration and also how to effectively chunk policies to learn more efficiently. The scope and goals are outlined clearly and the experiments are designed appropriately to showcase this.  While the paper does not solve all the possible problems that come up in a standalone system using Hierarchical RL, this paper is definitely a step forward in building hierarchical models that are scalable. The experiments chosen are mostly sufficient to show that the hierarchical model works at object based Q-learning, it would be interesting to see its application to some of the problems DQN is good at to get a more complete understanding of this model's capabilities.  For example, comparing on a game like Pong or Breakout that DQN learns trivially would give readers an idea of the speed of learning as compared to vanilla DQN.  Another point of discussion is what would happen when the Agent enters an environment that has new or different objects and relations. With a new space of objects to learn over, does the meta-controller and therefore controller transfer any previous knowledge or does it need to relearn over the space of the new objects all over. While outside the scope of the paper, this might be a potential drawback that I feel should be discussed.  There are many more open questions on how the meta-controller would handle different kinds of objects, and the issue of how to learn what objects are and how to utilize them to maximize reward.  It seems to me a lot of details are yet to be worked out in this work. Considering the author rebuttal and other reviews, I am revising my opinion of this submission. It is an important step towards hierarchical models, but the paper as it stands is not a complete solution. And that that takes away from its overall value.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","While the paper overstates some claims, and does not reference some of the older literature on the topic, it is mostly well-written and offers an interesting perspective on a hard problem in Reinforcement Learning worth publishing.","The idea of employing sub-goal learning for solving RL problems with sparse rewards is a good one, as the results presented in the paper show. Additionally, it is well written and easy to follow. However, some aspects, mostly in presentation, could be improved.  Specific positive aspects: - I specifically like the small MDP shown in section 3 that illustrates the usefulness of subgoals on a toy MDP. Since this domain is so simple, it allows for a very intuitive and non-technical understanding of the idea presented.  Specific negative aspects, in decreasing relevance: - every mention and discussion of 'goals' seems purposefully opaque. Instead of admitting openly that goals could have been manually defined, without any learning, (as e.g. for the smaller MDP example, where they are manually defined as ""visit a new state if you can"") the authors try to make the goals seem learned by some means of object detection approach. Considering that the relational structure they then develop is manually defined as well, not going into the detail of how the objects are determined is superfluous and misleading. Since the hierarchy of goals is still learned, this would not diminish the quality of the paper. - the paper repeatedly claims that this approach is a 'temporal decomposition', which I find misleading. In my understanding, temporal decomposition consists of learning partial policies by analyzing the structure of the (pure) MDP. This approach instead enriches the MDP representation by adding (useful) structure to the reward signal. - the idea to define sub-goals in a way very similar to the one introduced has been introduces 24 years ago, but has not discussed. See e.g. Connell and Mahadevan: 'Automatic programming of behavior-based robots using reinforcement learning', 1992, where sub-goals are used to avoid dead-end states. - defining sub-goals as done here additionally relates to some forms of learning abstractions for planning, which could have been discussed. See Konidaris, Pack Kaelbling, Lozano-Perez: 'Symbol Acquisition for Probabilistic High-Level Planning', 2015. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The papers proposes an approach based on hierarchical reinforcement learning and intrinsically motivated goals to address the known shortcomings of many current RL algorithms when applied in a sparse reward setting. Their approach is based on a two level hierarchy where a controller learns a policy for achieving a set of goals, and a meta controller learns to set an appropriate sequence of goals. The algorithm is applied to two sample domains, including the reknownly difficult Montezuma Revenge game.","The paper is clear and addressed an important issue: how to learn in sparse reward settings. One limitation in the paper is that empirical validation is limited, Montezuma Revenge is certainly a challenging benchmark, however it remains unclear how well the method generalizes to other domains. Experiments on wider range of problems would have made the results more compelling.  Another limitation is that more details should be given on the details of experiments (hyperparameters, details of the custom object detector, etc ...). Overall, I believe that the topics addressed are very important for RL, the proposed solution is novel and deserves to be known, so I would recommend acceptance. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,
Optimistic Gittins Indices,"Eli Gutin, Vivek Farias",https://proceedings.neurips.cc/paper/2016/hash/452bf208bf901322968557227b8f6efe-Abstract.html,"This paper investigates the retirement value formula defining Gittins' indices, and proposes replacing this optimal stopping problem with a computationally simpler 'optimistic' retirement problem. In the optimistic alternative to the problem, the true mean-reward of the arm is perfectly revealed after a single sample. This always yields an upper bound on the Gittins index of an arm, since the decision-maker is presented with more information. Moreover, it should yield an close approximation for problems with large discount factors, where the quality of an arm can be accurately identified within a negligible portion the effective horizon.   The paper's main theoretical result builds on an analysis of Bayes UCB to provide a frequentist regret bound matching the lower bound of Lai and Robbins. In experiments, the algorithm appears to offer state-of-the-art performance for a very widely studied problem. ","I greatly enjoyed this paper.  I struggled to select between 3 and 4 for some of the criteria, because there is a big gap between top 30% of submissions and top 3%.  I am confident that this paper will have longer lasting impact than the average bandit paper accepted to NIPS. But because the MAB with independent arms has been studied so heavily, it's difficult to make an enormous improvement to the state of the art.   It's disappointing that the algorithm depends on a tuning parameter alpha. The algorithm seems robust to choices of alpha, but only in problems where the horizon dwarfs the differences in alpha. Little ways of tuning some of the competing algorithms can make a big difference in performance.   It should be made clear that the main strength of IDS, Bayes UCB, and Thompson sampling is that they can be naturally extended to problems with more complicated priors and observation structures. Relative to approximate Gittins' indices, these methods are very flexible. A reasonable narrative for this paper is that we know more about the structure of optimal policies for problems with independent arms, and should exploit that.    ------ Notes----- These are provided in case they are helpful. You don't need to address them in the rebuttal.   - In the simulation experiments, how does the algorithm perform for shorter horizons? How does it perform with tuning parameter alpha=0?   -It's worth pointing out that there is a close conceptual connection between optimistic approximation in your paper, and the expected-improvement algorithm, which is a popular approach in the field of 'Bayesian optimization'. The EI algorithm measures the arm that leads to the largest expected improvement over the current largest posterior mean, assuming the quality of the arm were observed perfectly. This is computed using the expression for the mean of the truncated Gaussian distribution, similar to example 3.2. Your paper merges this into the retirement problem in an interesting way.    - Even when you can't calculate the expectation on the right hand side of (4), can you solve this equation using a gradient method? You should be able to bring the derivative inside the expectation, in which case perhaps the derivative can be expressed in terms of the CDF of R(y). I'm not sure.   -In lines 102-106, [14] is cited twice, when you mean to cite [13].  - In line 122, just following the definition of the retirement problem, make it clear that the supremum is over stopping rules tau, and not fixed times tau.   - In line 159, ""Let R(y) be a random variable drawn from the prior..."" This is important enough that you I'd consider writing this in math. Also, you should consider changing notation; R(y) suggests strongly that you are drawing a reward, and not a mean, and such an algorithm might perform terribly.   -In Prop 3.1, you never explicitly noted that the optimal Bayesian regret is O(log(T)^2) due to Lai,  - I've never seen the calculation in example 3.1. Can you provide a reference, or a derivation in the appendix? ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper considers the stochastic multi-armed problem and proposes a new algorithm grounded in Bayesian approach. The algorithm is based on novel ""optimistic"" approximations to the Gittins index and is shown to match the Lai-Robbins asymptotical lower bound (including the constants) for the frequentist regret. It is also shown in computational simulations that it outperforms the best existing algorithms.","This is an excellent, well-balanced paper with a very interesting simple idea of approximating the Gittins index, which works suprisingly well in both theory and practice.  I have only a couple of minor suggestions: - the idea of an optimistic approximation to the Gittins index relies on Bayesian setting, so I would suggest to make the title more informative, e.g. ""Optimistic Gittins Indices for the Bayesian Multi-armed Bandit Problem"" - there can be some misleading with the word ""optimistic"", as there are already ""optimistic approach"" to describe the inflation term of UCB and alike. It would be desirable to clarify the difference in the introduction. - in Abstract (and elsewhere) it is not clear what ""over all sufficiently large horizons"" means. Could you give a mathematical definition of this or reformulate it? Are you assuming the discount factor < 1? - it is not clear how the ""Bayesian regret"" differs from ""Bayes-optimality"" - line 23: I would say that ""index scheme"" is not appropriately used in the paper. It would be good to keep the term exclusively for policies which play the arm with highest current value of an index. I.e., randomized policies (TS, UCB variants) are not index rules/schemes/policies/strategies. - ""Gittin's"" (several times) -> ""Gittins"" - line 39: ""it is unknown whether a Gittins-like index strategy is optimal for a fixed, finite-horizon"": Actually, it is known that an index strategy is not optimal for finite horizon (in general). It is easy to obtain the optimal policy using dynamic programming. Inspecting the structure quickly reveals that it is not an index policy. - line 102: citing [14] is probably incorrect. - line 113: Note that while Gittins index is suboptimal, its generalization proposed by Whittle (and described in [16]) is virtually optimal. See also Villar et al 2015 (Statistical Science) - line 118: using q_{i,s} as state is not correct, because it doesn't uniquelly correspond to y_{i,s} - Please add a mathematical proof of Lemma 3.1. It is not bovious why there is a ""positive probability that the superior arm is never explored sufficiently..."" (line 134) - line 153: ""is equivalent"" suggest a two-way equivalence (i.e. that also every optimal stopping problem can be solved by computing a Gittins index). This is true (established in Nino-Mora 2007), but not clear from (2). - line 182: is there any difference between \phi and \Phi used in the formula? Please specify what these are - line 185: ""and makes"" -> ""which makes"" - line 214: ""bounds holds"" -> ""bound holds"" - line 242: specify what possible values the tuning parameter \alpha can take (e.g. >0) - is the Regret reported in Tabels 1 and 2 Bayesian or frequentist? Could you add SD in Table 2? Is there any reason why not to add for comparison also the best non-Bayesian policy? - Figures 1,2,3: these are unreadable when printed in b&w. Please add arrows to indicate which curve corresponds to which policy. Also, instead of just different colors, use different line types. Also use the same format for the same policies in all figures. - many references should have capitalized words (thompson, kl-ucb, ramanuja, ucb, gittins). Also [21] is single-authored.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors propose a new anytime bandit algorithm based on an efficient approximation of the discounted Gittins index. There are two main ideas. The first is to increase the discount rate as a function of time, which saves the strategy from linear regret. The second is to approximate the Gittins index via a truncation after which the signal noise drops to zero.   The primary technical contribution is a theoretical analysis showing the frequentist asymptotic optimality of the strategy when the lookahead is just 1. ","*************************************************** POST REBUTTAL.  Thanks for the correction. I agree this makes things OK again. Nice paper. ***************************************************   Overall the paper is reasonably well written. I get the feeling the authors removed a lot of intuition to make the paper fit. I'm sympathetic to this plight, but still wish there was a little more there. Even simple things like noting that the effective horizon for discount gamma = 1 - 1/t is O(t) would be helpful. Also, in Examples 3.1/3.2 it would be nice to see some approximate solutions that reveal the strategy has a form approximately the same as some existing strategies (see minor comments).  Technically the paper is generally OK, but I do have some possibly serious issues described below. The authors should focus on these in their response to this review  to increase my score.   * It is claimed at the start of Section 4 that because the strategy is asymptotically optimal in the Lai & Robbins sense that it is also asymptotically Bayesian optimal. Please justify this claim. As far as I know the UCB strategy is not asymptotically Bayesian optimal, but it is optimal in the Lai & Robbins sense. The first index-strategy that is Bayesian optimal is by Lai in '87 (you cite it). I do believe your strategy gets both, but I don't understand the argument.  * In L397 in the supplementary material. I don't understand the > in the display. What if q = 1/10 and p = 1/2. Then the first logarithmic term is negative. I tried a little to fix this result and could not. This lemma seems quite critical and I wonder if this is where the improvement is coming from that allows some log factors to be shaved in the confidence interval essentially (I am talking about the discussion in L223).  Besides the theoretical results there are also some experiments. I found these to be a bit of a tease because (a) the performance is excellent and (b) we only get to see examples from a two (random) experiments. One Gaussian and one Bernoulli. It's nice to see the quantiles of the distribution, but I would have preferred to see the expected regret over a larger range of problems. Eg., the worst-case regime. Instead the focus is on a very easy problem with few arms and a distribution for which most of the arms are well separated. That said, the results are impressive.   Overall I find the paper very interesting. The technical issues force me to lower my score and I wish the execution were better (crisp intuition/experiments). See below for some minor comments.    Minor comments:  * It might be worth discussing the approximations given in Examples 3.1 and 3.2 in a little more depth. I did not investigate the former, but the latter turns out to be quite interesting. Solving the inequality and doing some naive approximation suggests that the optimal lambda has approximately the form  lambda = sqrt(2sigma^2 log(t sigma^2)   (here I have chosen gamma = 1 - 1/t as recommended). If the prior is the flat improper Gaussian prior, then in round t we have  sigma^2 = 1/N_i(t-1) and one recovers a well-known index, which at least for the Gaussian case has now extremely tight, but not quite optimal (in finite time),  regret guarantees. There are also asymptotic guarantees, see [1]. Note that this is basically the algorithm that Lai showed was Bayesian optimal (replace t by n).  * L39. I think Berry & Fristedt showed already that the Bayesian strategy is index-based only if geometric discounting and infinite horizon are assumed.  * L76. This reference is wrong, although that paper does study the case where the horizon is not known, so may also be relevant. I guess you mean [2]  * L216. Not everything that decays faster than 1/t is summable (eg., 1/(t log(t))  * The regret analysis included in the main body is all totally standard, while the interesting stuff is deferred to the appendix. An odd choice?  * L208: s_t, s_t' are introduced. Are they ever used?  * A bit petty, but I find some of the notation a bit displeasing. For example, T is used for the non-random horizon while k_i(t) is used for the  random number of times arm i has been chosen.  * Is it written which priors were used by Thompson sampling/Bayes UCB in the experiments?  * For what prior is Theorem 1 proven? (looks like flat beta prior, but is it written?). Can you get asymptotic optimality for any prior? Also see [1] for a more general results on BayesUCB   References:  [1] On Bayesian index policies for sequential resource allocation. Kaufmann, 2015 [2] Regret analysis of the finite-horizon Gittins index strategy for multi-armed bandits. Lattimore, 2016","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper considers the multi-armed bandit problem over all large time horizons, rather than an infinite time horizon. In the latter setting, a famous result is the Gittins index, which is an efficient, optimal solution. However, in this paper's setting, the Gittins index is not proved to work and is computationally inefficient. The authors propose an approximation to the Gittins index that assumes an increasing discount factor. They prove that it matches a known lower bound on regret. Additionally, they empirically demonstrate its advantage over competitors.","The core contribution of this paper is a notable improvement over past results. The ""lookahead"" nature of Optimistic Gittins is interesting and innovative. However, there are doubts as to whether the increasing discount factor is appropriate. If I understand correctly, an important assumption, constant discount factor, has been changed, and the paper could be greatly improved with discussion and justification. Since the problem is now different, it is hard to see why it is fair to compare to past results without some argument in the paper. If the increasing discount factor is justified, then this paper could be viewed as quite valuable.   There were many small issues with the presentation of this paper. Additionally, the paper could  benefit from including results for general K and the exponential family of distributions.     L21: Around here might be a good place to discuss some real-world applications of MAB.  L36: What is the significance of optimizing over an infinite horizon vs all large time horizons?   L92: These two equations are confusing because it is not stated which variables the expectations are taken over. Also, is this ""reward"" or ""expected reward""?  L99: Is the lim inf here the reason this problem is considering all large time horizons instead of an infinite horizon? It could benefit the reader to formally state the difference between the two settings.  L102: ""Another"" paper is cited, but the citation number is still [14], the same as the previous paper?  L137: Assuming an increasing discount factor could be a significant change to the problem. Could it be discussed and justified more thoroughly? Also, are the results dependent on the rate being asymptotic to 1/t? The assumption seems strong.  L139: If the discount factor changes at each step, it seems odd to use Equation 2, which appears to use a constant discount factor. Or is Equation 2 being modified for this strategy?  L150: It would help to move the sentences at the very start of 3.1 to here, since ""computational burden"" is introduced here but explained there.   L152: Feels strange to only have one subsection. Perhaps call everything before 3.1 and change this to 3.2  L195: ""In the sequel"" is an outdated phrase. A better alternative would be ""in the following.""  L201: Has \theta^* been defined yet?  L202: This line is redundant.  L211: This equation needs an explanation. At least the interpretation of L(T) should be described. Also, in the last line, how does d(\theta_2,\theta_1) gain the (\theta_1-\theta_2) factor in (5)?  L241: This note would be better placed at the introduction of the increasing discount factor.   L255: It may be informative to mention the specifications of the CPU.   Table 1 and 2: Please boldface the best result in each row. Also, ""Algorithm"" should be something like ""Metric"" because its row is for column headers, but ""Algorithm"" is a row header.  L283: If this section is to be included, it seems necessary to make space for the graphs from A.6. Otherwise the paragraph feels like it has little impact.  All these experiments should show hypothesis tests or confidence intervals.  The lack of conclusion is jarring.",2-Confident (read it all; understood it all reasonably well),"This paper proposed approximations to Gittins index with lower computational burden, with a competitive performance on regret lower bound.","1. Please refine the paper. For example, in lines 96 and 102, reference 14 was mentioned, but one of them should be the other paper. 2. An table to show the pros and cons of the mentioned index schemes in different situations (MABP on iid or Markov RV, for example) should be added. 3. For figure 1, error bars should be added.",2-Confident (read it all; understood it all reasonably well),,,,,,,,,
Efficient Second Order Online Learning by Sketching,"Haipeng Luo, Alekh Agarwal, Nicolò Cesa-Bianchi, John Langford",https://proceedings.neurips.cc/paper/2016/hash/15de21c670ae7c3f6f3f1f37029303c9-Abstract.html,"The authors strive to make second-order online learning efficient by approximating the scaling matrix A_t by a low-rank sketched version based on S_t^T S_t. They prove scale-invariant regret guarantees for this approach when the desired matrix A_t is well approximated in this way, and show that the algorithm can be implemented efficiently for sparse data.","The quadratic update time and space requirements of second-order online methods like Online Newton Step make such algorithms unsuitable for most practical problems. The present work takes a significant step in addressing this. The primary contribution of the paper are variations of Online Newton Step that remove this drawback using a sketching approximation to the scaling matrix and a clever implementation of sparse updates.  The primary theoretical contributions are the analysis of the RP and FD versions of the algorithm. For RP they show a regret bound which holds when the matrix G_T (the matrix of observed gradients) is actually low-rank. Given the structure of the loss functions assumed,  f_t(w) = \ell(< w, x_t >), gradients will always be in the direction of the examples x_t, and so I think this theorem only holds when the data is actually low-rank. But if that was the case you could always simply project the data onto a basis and run ONS on the lower dimensional space. Hence, this result feels somewhat limited. The FD result Thm 3 is thus stronger (and is the one presented in detail in the body of the paper) since it depends instead on the spectral decay of G_T^T G_T. This point should be clarified in the paper.  The authors emphasize the fact their results are scale-invariant, but this also comes at some cost. Most results in online convex optimization apply to arbitrary convex functions, possibly satisfying additional conditions like strong convexity or smoothness. This work assumes a much more restrictive class of functions where f_t(w) = \ell(< w, x_t >), essentially a generalized linear model. This appears necessary to introduce the concept of ""invariant learning"", but the importance of this approach in practice isn't clear to me. One can choose the fixed norm bound on the feasible set after scaling the features, and in practice one can often avoid projecting onto a bounded feasible set at all as long as the learning rate is set reasonably. Further, while the author's claim the bound in Theorem 1 is ""qualitatively similar"" to the standard setting, the explicit dependence of the dimension d is a significant difference in my mind.  The experimental section presents results on both synthetic and real-world datasets. The synthetic data nicely highlights the potential for improvements from this approach; only some of this is demonstrated on real-world datasets. The author's only present experiments on the Oja-SON variant, which is unfortunate since it lacks theoretical guarantees. The importance of the theory is somewhat called into question by this, since it implies the assumptions on A_t necessary for RP-SON and FD-SON to work well may not actually hold in practice (as discussed above, this seems quite likely for RP-SON). Further, in order to outperform AdaGrad (which is 10x faster and substantially easier to implement), a significant trick was needed (lines 254-259). Given that this approach was necessary to get good performance in practice, a more thorough discussion is warranted.",2-Confident (read it all; understood it all reasonably well),"Despite the attractive properties of second-order online methods (e.g., Online Newton Step (ONS)) such as being invariant to linear transformations of the data, but these family of algorithms have a quadratic memory and time dependency to the number of dimensions which limits their practical applicability. This paper aims to improve second-order online methods by integrating random projection and sketching methods into the updating and proposes Sketched version of ONS algorithm ((though with different projections). In particular, this paper achieve regret guarantees nearly as good as the standard regret bounds for ONS, while keeping computation as good as  first order methods. The authors prove scale-invariant regret guarantees for their approach and introduce nice tricks for practical implementation of their algorithm which enjoys  a running time linear in the sparsity of the examples. ","The problem being studied is interesting and the solution proposed in this paper bridges the gap between nice theoretical properties of ONS and its practical value. The presentation of the paper was mostly clear. The claimed contributions are discussed in the light of existing results and the paper does survey related work appropriately. The paper is technically sound and the proofs seem to be correct as far as I checked.   From a theoretical standpoint,  the paper presents regret analysis for ONS when Random Projection (RP) and Frequent Directions (FD) are used to sketch the matrix G_T (the T by d matrix of sequence of T gradients). The results holds when the sketching dimension m  is roughly \Omega (r + log T), where r is assumed to be rank of the G_T which is equivalent to rank of data in their setting. This means when data points lie in a low-rank manifold, then ONS with random projection can be utilized to improve the running time. I think the statement of theorems needs to  be stated clearly at least as a Remark in terms of rank of actual data points rather than the rank of G_T.  Empirically, three sketching approaches as well as a sparse implementation are evaluated on both synthetic and real world datasets. While the experiments on synthetic data looks promising, but I think the experiments on real-world datasets in the current status does not fully complement the theoretical achievements of the paper and needs to be strengthened (or at least needs through discussion to convince the reader). First of all, my first impression from experiments is that the neither the RP-SON nor FD-SON which come with strong theoretical guarantees can outperform Oja-SON which unfortunately does not come with theoretical analysis. Also, the Oja-SON was outperformed by AdaGrad, however, using a simple diagonal re-scaling results in much better results which need through discussions.    Overall I liked the idea of sketching in second order online optimization and its analysis, and I lean towards acceptance (assuming the paper will honestly discuss the issues mentioned earlier). ",2-Confident (read it all; understood it all reasonably well),This paper proposed an improved ONS method with better regret guarantees and computation cost. The overall contribution contains three part: 1. relax the assumption about fixed norm to bounded predictions. 2. tackle 3 matrix sketching techniques to reduce the computation cost  and prove regret bounds which is similar with the full second order update. 3. Develop sparse versions of these updates with a running time linear in the sparsity of the examples . Some empirical analysis has been performed to demonstrate superiority of the proposed method.   ,"Specifically, this work first relaxed the assumption about fixed norm in ONS method and prove a similar regret bound, then replaced the full matrix update with sketching matrix (RP-sketch, FD-sketch and Oja’s sketch) to reduce the computation cost, they also provide similar regret bound.  Finally, they proposed sparse version of these 3 algorithms.  Quilaty:  From a theoretical perspective,  this paper is really good. They provide  a similar logarithmic regret bound with relaxed assumption. To reduce the computation cost about the second order information, they adopt 3 matrix sketching method to approximate the full second order matrix and prove similar regret bound. This analysis can be generalized to other matrix sketching method.  The weakest point of this paper is the experiments. In my opinion, full matrix update can get a little better improvement than diagonal matrix update  but take much more time in many real-world datasets. This paper claimed the effectiveness of employing matrix sketching techniques, but there is no experiments about computation cost analysis.  Clarity: This paper is easy to follow.  Significance: Adagrad is widely used in different machine learning community.  This paper is a step toward replacing the full matrix update with sketched matrix update to reduce the computation cost.  Other comments: 1 I am confused why the authors proposed a lower bound in theorem 1. 2 line 341 in appendix,  can you give more details about this inequality that uses Khintchine inequlity. 3 line 373 in appendix, Assumption 1 -> Assumption 2 Should \|w\|^2_{A_0} be \|w – w_1 \|^2_{A_0} ?   ",2-Confident (read it all; understood it all reasonably well),"This paper introduces a sketched version of the Online Newton algorithm which enjoys runtime respectively linear, O(md), in the data dimension (d) and in the sketch size (m). The proposed algorithm (SON) enjoys improved regret gaurantees (bounded predictions instead of solutions) for ill-conditioned matrices. Three sketching approaches as well as a sparse implementation are defined and tested on both synthetic and real world datasets. Experiments with an AdaGrad-flavored SON show strong empirical performance although this algorithm is unfounded theoretically. A truly condition-invariant algorithm which uses the Moore-Penrose pseudoinverse as opposed to the true inverse is defined and proven in the Appendix.","Using sketching, the authors provide what looks like the first linear-time, second-order, online learning algorithm with an invariant regret guarantee. They also provide a sparse implementation with these properties along with 3 different sketching techniques. This work significantly improves upon Online Newton Step in a non-trivial way. The strength of this paper is in the algorithm design and proofs, however some emiprical results are exemplary; the Oja version of SON was outperformed by AdaGrad, however, incorporating a simple modification gave superior results. Overall, SON is the first to reach the linear time benchmark for second order online learning which merits recognition by itself. The sparse implementation, sketching variants, and AdaGrad-flavored Oja-SON put the paper above and beyond expectations.       Comments:       1) Comparison to Online Frank-Wolfe would be appreciated",2-Confident (read it all; understood it all reasonably well)," This paper considers an online algorithm similar to online newton step proposed by (Hazan et al. 2007). The proposed algorithm is invariant under linear transformation of the data. The definition of regret in this paper differs for the usual definition. Instead of requiring that predictor vector belong to a fixed compact convex set, they require the output of the linear predictor applied to each data point be in a bounded interval [-C, C]. This definition is taken from (Ross et al. 2013) which also consider scale invariant online algorithms. They provide optimal regret bounds under different assumptions on the loss functions.    To reduce the per-iteration cost of their algorithm and required memory for storage of the A_t matrix in their algorithm, the authors consider three sketching methods that maintain a low-rank approximation of that matrix. They provide regret bounds for two of the three sketching methods that depend on the rank in the low-rank approximation of A_t. They also show how to implement these algorithms such that the run-time depend on the sparsity of example data vectors rather than their dimension. ","According to the numerical experiments section, the third sketching algorithm has the best performance. However, this method does not have a regret bound. The first two sketching methods with regret bounds have been skipped over in the numerical experiments. This reviewer believes that the experiments section should include the first two sketching algorithms since these are the ones for which regret bound guarantees are presented in the paper.  The authors have not commented on how much using a different definition of regret makes the proof of regret bounds different for their algorithm. Since for the classical definition of regret the online newton step has been analyzed for exp-concave loss functions, it is important to explicitly compare the proof of theorem 1 in their paper to the regret bound proofs in the literature (e.g. in Hazan et al. 2007). For example, a question that should be answered is: Can the proof be extended for any convex compact set K_t in the definition of regret? ",2-Confident (read it all; understood it all reasonably well),This paper studies sketching techniques in the context of online Newton and applies Oja's algorithm to online Newton. ,"The paper has two different components. One is an extensive theoretical analysis of random projections and forward directions applied to the online Newton, with regret bounds, a description of their sparse implementations and their running times. However, the authors do not show any sort of experimental results for either of these two sketching methods.   On the other thand, the authors implemented Oja's sketch method for online Newton and compared it to AdaGrad. There is an improvement in a handful of the datasets tested. Unlike the previous two methods, this method comes with little theoretical justification.   These two parts are not joined very well, and feels like a 'theory half' and a 'experimental half' that are only related by the topic of sketching in online learning and not in the specifics. It would be much more cohesive if, say, the authors included experimental results for the first two methods (FD and RP) even if they were not competitive, for comparison's sake. Especially since the sparse implementations of the theoretically justified methods are covered in such detail.   Minor comment:  Table 2 of the appendix should have the best result in each row bolded or emphasized.   The equation after line 409 in the appendix should be an inequality in the second line. ",2-Confident (read it all; understood it all reasonably well),,,,,,
Unifying Count-Based Exploration and Intrinsic Motivation,"Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos",https://proceedings.neurips.cc/paper/2016/hash/afda332245e2af431fb7b672a68b659d-Abstract.html,"Count based exploration bonus is incorporated in  several RL algorithms. When the state space is large, generalization over neighboring states is required to get a meaningful count measure. This paper proposes a transformation from density estimation to a count measure. The transformation is simple and local, and the resulting pseudo-count enjoys several attractive properties. The approach is demonstrated by adding exploration to some  Atari2600 games.",I find the proposed scheme to be both elegant and effective. The presentation is quite clear.  Adding some details on the density estimation scheme used in the application should be useful.,2-Confident (read it all; understood it all reasonably well)," This paper presents how we can extend the notion of state-action visitation counts to reinforcement learning domains with large state spaces, where the traditional notion of visitation count becomes uninformatively sparse to be useful. The main idea comes harnessing density estimation to derive pseudo-counts that can be seen as equivalent to counts. Asymptotic analysis is given to show that they are indeed equivalent.  The effectiveness of the pseudo-count derived from density estimation (CTS) is demonstrated by extending two RL algorithms (DQN and A3C) to accomodate count-based exploration, and running on a range of games in ALE.  "," This is a very nice paper that could be useful for many (deep) RL algorithms with large state spaces. I have a few comments regarding the pseudo-count:   (1) In ICML this year, a number of exploration algorithms were presented that used model uncertainty for exploration. It seems to me that using model uncertainty is simpler and more natural, rather than using an external, density model. This issue is briefly mentioned in future directions section, but how does pseudo-count exploration compare to these model uncertainty exploration methods? When is it advantageous to use pseudo-counts?   (2) How does the density-based counting scheme to a really simple counting scheme, e.g. partition the screen into big chunks and use color indicators?    (3) In eqn (5), N(x) -> N_a(x) since we want the bonus to be dependent on states *and* actions?  ",2-Confident (read it all; understood it all reasonably well),"This paper proposes a new ""exploration bonus"" approach based on the notion of ""pseudo count"". It demonstrates several desirable properties of pseudo count in two computer games: FREEWAY and PITFALL (Section 4); analyzes the asymptotic relationship between pseudo count and count (Section 5); discusses the connection to intrinsic motivation and information gain (Section 6); and shows some experiment results of using pseudo count for exploration (Section 7).","This paper is very interesting in general. To the best of my knowledge, the notion of ""pseudo count"", how to use this notion for exploration, and its connection to information gain, are novel. The experiment results in Section 4 and 7 are convincing. Overall, I think this paper has met the standard of NIPS poster, though the theoretical part of the paper is a little bit weak.  My major concern of the paper is that it is not well-written in general. In particular, the transitions between sections are not very smooth. Moreover, the title of the paper indicates that this paper will focus on the relationships between pseudo count and intrinsic motivation (information gain). However, when reading the paper, I feel that this is only the focus of Section 6. Other sections seem to have different (even unrelated) foci. Please polish the final version of the paper.  Minor comments:  1) figures in Figure 1 and 2 are too small.  2) what is the difference between C_n(x,a) in Section 2.1 and R^+_n(x,a) in Equation 5? I think they are the same (up to a factor of \beta), right?",2-Confident (read it all; understood it all reasonably well),"This paper proposes an exploration strategy for deep RL. In particular, the authors derive a 'pseudo-count' from a sequential density model of the state space in ALE and claim to make explicit a relationship between information gain, prediction gain, and their proposed pseudo-count quantity. The authors present breakthroughs on ALE, making significant progress in Montezuma's Revenge.","Exploration is a key problem in RL; the authors revisit old ideas and derive new insights to improve on the state of the art. This is a good paper, with strong experimental results, but I still would like some additional explanations:  - The pseudo-count quantity is derived from the so-called recoding probability of a density model, which is shown to be consistent with the empirical count. Say you have a specific state element x, which corresponds to some pseudocount N(x). Updating the density model with this element would lead to an updated pseudocount N'(x), which is consistent with the empirical count. But what happens to N(x) if you update the density model with a different element y? I would guess that N(x) can go down (which is obviously not consistent with the empirical count). This seems to relate to Fig. 1 (right),in which the pseudo-count has up/downward spikes. - Somehow I have the feeling that the notion of pseudo-count is somewhat redundant to the whole story. It seems that the authors can derive prediction gain (PG) and immediately derive information gain (IG), with no additional data needed. IG is well-established as a notion of intrinsic motivation. My question is, why work with pseudo-count? It seems to be quite a loose bound on PG/IG, which makes me think of it to be inherently less stable. If the whole pseudo-count measure would be removed from this paper, we basically have: measure the size in update to the density model and use this as an intrinsic reward. - It seems to me that the authors' definition of IG is different from what is used in literature (e.g., Planning to Be Surprised by Sun2011). In particular, it seems that an agent can gain infinite information over time by following a predefined policy, without actually exploring. This seems invalid. If the authors' IG quantity is different from what is generally established in literature, how could the authors relate it to it? If the authors' IG is a new quantity, what is the added value of of the authors' contribution 'a relation between IG and pseudo-counts'?  Minor comments: - Corollary 1 reads quite dense. - In Fig 1 (left) the pseudo-count goes up in periods without salient events, is this due to generalization across states?",2-Confident (read it all; understood it all reasonably well),This paper has multiple contributions: 1. an extension to counting-based exploration based on sequential density model; 2. a simple but effective modification to DQN; 3. impressive empirical results incl. progress towards solving Montezuma's Revenge; 4. a connection to intrinsic motivation (more comments on this topic in detailed review below),"Overall this paper is a good paper that should be accepted. In particular, the formulation of pseudo-count seems novel, and Montezuma's Revenge experiment demonstrated convincingly the method can be effectively applied.  I have however 2 concerns that I'd like the authors to address/respond to:  1. As stressed in title and in line 206, the claimed main result is the relationship between pseudo-count and information gain (IG), which is a classic concept in intrinsic motivation literature. The stated relationship, though technically sound, doesn't seem to relate to existing intrinsic motivation literature. I'd be happy to be corrected but as far as I can tell, the term ""Information gain"" here is used to define a quantity that's both technically and philosophically different from the usual Information gain.  Specifically IG in this paper is defined over information gain over a mixture sequential density models of state visitations whereas IG is usually defined as information gain of ""model of the environment"" [1,2]. Philosophically, IG is introduced to create a ""knowledge-seeking agent ... to gain as much information about its environment as possible"" [2] and this is a characteristic that the ""IG"" definition in this paper fundamentally lacks. The reason is that if the behavior policy changes, there will always be information gain of state density model because state visitation frequencies change as the policy changes yet this specific ""IG"" doesn't need to reveal anything new about the environment, hence making the ""IG"" used in this paper distinctly different from the classic notion.  The connection between ""IG"", PG, and pseudo-count is an interesting one but I'm wondering if the authors can justify how it ""connects to intrinsic motivation"", and ""unifies count-based exploration and intrinsic motivation""?  2. Can the authors discuss more why pseudo-count is preferable to PG? From figure 2 in appendix it seems PG performs very competitively without extra tweaking on selecting what function to use to transform it but the inverse pseudo-count performs poorly with the /sqrt transformation. I suspect that if similar PG can be tried with different transformations it's going to outperform the tweaked pseudo-count bonus. Proof of Theorem 2 also indicates that 1/pseudo-count >= e^PG - 1. Since 1/psudo-count is exponentially larger than PG, it'd not be surprising that PG is better-behaved than 1/pseudo-count. I'm wondering if the authors can clarify if pseudo-count has distinctive advantages over PG or they are more of less interchangeable?   [1]: Sun, Yi, Faustino Gomez, and Jürgen Schmidhuber. ""Planning to be surprised: Optimal bayesian exploration in dynamic environments."" International Conference on Artificial General Intelligence. Springer Berlin Heidelberg, 2011. [2]: Orseau, Laurent, Tor Lattimore, and Marcus Hutter. ""Universal knowledge-seeking agents for stochastic environments."" International Conference on Algorithmic Learning Theory. Springer Berlin Heidelberg, 2013.",2-Confident (read it all; understood it all reasonably well),The paper describes the use of intrinsic motivation for exploration in reinforcement learning. The measure of uncertainty used here is a new quantity called pseudo-count. It is able to work on large state spaces which is also shown in the experiment section with several hard games. The authors also show tight relations to other intrinsic motivation measures such as Bayesian information gain.,"It would benefit the paper to include more connections to the cited reference ""Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"" as they also consider mutual information or channel capacity to be some form of path-counting.  Technical quality: Good experiments with reasonably complex environments.  Novelty/originality: Novel form of measuring uncertainty through counts. Relations to similar methods incomplete (c.f. section A in ""Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"").  Potential impact or usefulness: Useful since it works with large state spaces.  Clarity and presentation: Clear structure.",2-Confident (read it all; understood it all reasonably well),,,,,,
Satisfying Real-world Goals with Dataset Constraints,"Gabriel Goh, Andrew Cotter, Maya Gupta, Michael P. Friedlander",https://proceedings.neurips.cc/paper/2016/hash/dc4c44f624d600aa568390f1f1104aa0-Abstract.html,"The paper tackles the problem of linear classification with realistic constraints of various types (churn, fairness, stability, recall, …) that can be modelled as dataset constraints of a particular structure. The focus of the paper is on the modelling part; the main optimization model, (2), is discontinuous, involves an L2 regularizer, a loss which is a positive linear combination prediction rates for the datasets considered, and a (small) number of constraints of a similar structure. The prediction rates are replaced by expected prediction rates (the classifier is made into a randomized one), which transform the main model into a continuous (but nonconvex) optimization problem. This problem is then solved using a Lagrangian approach (removing the constraints and transfering them to the objective with dual multipliers), coupled with an iterative majorization approach where the  non-convex functions are replaced by convex upper bounds tight at the current iterate. It is suggested that a cutting plane approach be used for this. Experiments with fairness and churn problems show that the proposed approach works well in practice. ","The paper is very well written, with a minimum number of typos and issues. The main problem is well motivated through considerations arising in practice, and hence there is very good potential for pick up by practitioners. It is interesting to see that a large number of practical goals can be modelled via dataset constraints of the type the authors consider. In some sense, the proposed methodology is straightforward given the basic idea – but seems to be rather of a tour-de-force nature, involving many elements (introduction of the ramp function to make the problem continuous, introducing Lagrange multipliers, iterative majorization by convex functions, cutting-plane technique + SDCA, …) . It is interesting to see that the authors have an open-source Julia implementation of the method. There is prior work which motivates the developments here, but nearly all of this work seems to be rather dated, and I think it is a good time to introduce similar problems to the attention of NIPS again. The main body of work does not contain any theory and is purely of a modelling / expository nature. A sizeable supplementary material is included which I have not used in this review (the authors deemed it not important enough to include it in the main text).   Small issue: Figure 1 seems to be missing labels.  -------- post-rebuttal remarks --------  I have read the rebuttal and keep my scores unchanged. ",2-Confident (read it all; understood it all reasonably well),The paper proposes a model and algorithm to solve linear classification problems on datasets with some common side-constraints that are often ignored.,"I really like the basic premise of the paper. I am no expert on deployed systems, but it seems like these authors know about real-world systems (and have access to some proprietary real datasets) and have thought carefully about actual issues that come up when making a classifier.  The paper proposes (1) a new mindset, (2) a new model, and (3) an algorithm to solve the model. At times, I think any one of these components is a bit thin, and it’s not always clear why certain decisions (especially for the algorithm) were made — in particular, the numerical examples are provided to validate the new mindset but do not justify the algorithmic decisions. But overall, I think this is sufficiently novel that I am supportive.  A few specific points:  - I found the descriptions of the issues on page 1 and 2 (e.g., the churn example) quite interesting  - Page 3 and further in the paper, referring to “Problem 2” is a bit confusing when it is only labeled by an equation number (most authors define a problem environment, like a theorem). I also didn’t see r_p and r_n defined (line 103) — are these s_p and s_n, or a (un)normalized version?  - The randomized prediction (line 105) didn’t seem like it was fully explored. Does this affect training or only the final prediction? After vaguely introducing this concept, I felt like it was ignored until the numerical experiment plots, and I do not understand exactly what you are doing.  - Fig 1 was designed to make the convex bounds intuitive, but this means that for z >= 1/2, at the next step, there is no actual dependence on this component of z? With the hinge loss, at least there is still dependence. Intuitively this makes your scheme sound like a bad idea.    — Furthermore, discussing Davenport (line 178-180), you say that ramp-loss is similar or slightly better in accuracy than hinge-loss, but much faster. This doesn’t make sense to me, since to solve the ramp-loss you must solve a sequence of hinge-loss problems, so how can it be faster? Could you elaborate?  - Section 2.2 uses SVM solvers inside a cutting-plane routine. This may be a good idea, but you could describe why you didn’t pursue alternatives. Everything is piecewise linear, except the regularization which is quadratic, so you could solve everything with an off-the-shelf QP solver, which seems much simpler. Perhaps you prefer existing SVM solvers since you know good ones, and many of them naturally take in datasets in specified formats (such as libsvm format)?  A bit of discussion on your design decisions would be useful.",2-Confident (read it all; understood it all reasonably well),"This paper proposes handling multiple goals on multiple datasets by training with dataset constraints, using the eamp penalty to accurately quantify costs, and present an efficient algorithm to approximately solve the resulting non-convex optimization problem. Experiments demonstrate the effectiveness of the proposed approach.","In my opinion, the novelty of the paper lies in integrating multiple objectives on multiple datasets, while traditional formulations are mainly one objective on one dataset. So conceptually, the paper is novel. However, only a simple linear classifier was learnt and the whole solution method was customized towards the goals that the authors listed in Introduction, making the proposed algorithm unuseful if one is interested in other goals.  I would encourage the authors to discuss:  1. Is there a principled general solver that might work for general classifiers and general goals? 2. Multiobjective optimization can also work for multiple goals, but it does not combine the objectives by linear weighting. How is the proposed approach related to multiobjective optimization? 3. How to choose the weights for different goals?  ****************** Comments after reading the authors' feedback: Although the author feedback is OK to me, I still feel that the paper is a bit strange. It is not in a traditional formulation. I would lean towards rejection. The reason is that: as I commented, the novelty is in integrating multiple objective across multiple datasets. So in this regard, the authors should elaborate more on why and how to integrate multiple objective across multiple datasets (e.g., the general solver and general classfier issues that I raised), and then give some examples, including the one shown in the paper, to illustrate the idea. However, the authors simply sell their sole example from the beginning. The whole paper and supplementary matrial is on defining the particular example and how to solve it in detail, which is not interesting to me at all.",2-Confident (read it all; understood it all reasonably well),"In this paper, the authors try to combine misclassification error rate as well as other real-world goals, such as Churn, Coverage, Fairness, etc., into the classification problem by converting them as dataset constraints. The authors formulate the problem as an optimization problem. By replacing the indicator function with ramp function in the original optimization problem, the authors finally come up with an iterative algorithm to solve the problem. ","1. The paper should be self-contained. Since part of the contribution in this paper is converting the real-world goals into dataset constraints, it is better to include the explanations of how to convert the goals into the constraints of the optimization problem in the paper instead of the supplemental material.  2. The complexity of the algorithm should be included in the paper as well so that the readers can have an idea about the scalability of the algorithm by just reading the paper.  3. There are some typos that need to be fixed.  ",2-Confident (read it all; understood it all reasonably well),"This work aims at solving multiple goals, namely, coverage, churn, recall & precision, and egregious examples, using various datasets specific to these goals. This is achieved by introducing dataset constraints and proposing an algorithm to approximately optimize the non-convex problem.","I acknowledge the work done in tackling various possibilities in the goals and creating formulation for those types.  In terms of organization of paper, it can be improved to make it more readable. I also recommend having a conclusion section in the end.  ",1-Less confident (might not have understood significant parts),"This paper considers the problem of fitting a linear classifier to data, so that it minimizes/satisfies multiple objectives/constraints; the scenario in mind here is choosing a classifier for release to a real-world production system, where multiple (possibly competing) performance criteria must be achieved/satisfied.  This is a useful goal, and one that has not received a ton of attention in the literature so far.  The authors cast this as an optimization problem, present a relaxation of this problem, and then derive a (majorization/minimization) algorithm for optimizing the relaxation.  Two numerical studies are presented.","In addition to my above comments re: empirical evaluation, here are some further remarks.  Approach: - Why not simply use the hinge (or squared) loss relaxation(s) of the 0/1 loss (which are convex) instead of using the ramp loss?  This would make for a much simpler and possibly faster optimization approach, since it appears that scalability may be an issue you are concerned with. - As you mention (L183), it looks like Joachims (2005) previously considered using the hinge loss as a relaxation (at least for a subset of the performance criteria you care about).  I think, to demonstrate the efficacy of your method, you need to compare your approach to his (at least on some criteria like, say, scalability); otherwise it's not clear what the value-add of your approach is. - L144-150: I'm a little confused, I thought most SVM solvers return the primal and dual variables ... why do you need an outer cutting plane method to compute the dual variables? - How many iterations does it typically take for your majorization/minimization (MM) approach take to converge?  Do you have any timing results?  It sounds like your approach may be costly.",2-Confident (read it all; understood it all reasonably well),,,,,,
Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images,"Junhua Mao, Jiajing Xu, Kevin Jing, Alan L. Yuille",https://proceedings.neurips.cc/paper/2016/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,"This paper examines training and evaluating multi-modal word embeddings with large data sets. It contributes a new data set of images with captions derived from Pinterest, and a new data set of phrase similarity judgments automatically derived from click through data. A subset of these judgments were checked in a user study. A number of RNN-CNN models are trained on the Pinterest data, and evaluated on the newly create data sets. It is found that a multi-modal RNN with a transposed weight sharing scheme between the input word embeddings and the output layer fed to the softmax word prediction layer achieves the best performance on the evaluations. ","The paper was clear and well written. The data set and the evaluation that was conducted could be useful to the community. However, the paper unfairly characterizes or omits some previous work, and was not clear enough about the limitations and biases of their evaluation strategy. These points detract from a paper that otherwise makes an interesting contribution.  First, there is an implied criticism of WordSim-353 and MEN at the bottom of page 2 that they only contain similarity judgments at the word level. However, there is a large amount of work on learning phrase and sentence-level embeddings in the recently literature that overcome these issues (see representative work by Mirella Lapata, Marco Baroni, Stephen Clarke, Richard Socher, among many others), which the paper does not mention. Thus, learning 2- or 3-word embeddings is already well investigated, rather than a source of new challenges.  The criticism of the analogical reasoning task on page 3 is also misplaced. The paper criticizes this task for not covering all the semantic relationships of millions of words in the dictionary. In my view, relatedness judgments are much worse than analogical reasoning, because they reduce all semantic relations down to a single scalar.  The paper should be up front about its limitations and biases. The data collection process the resulting evaluation are clearly biased towards multi-modal methods, because an image is displayed in the interface along with the text. This is not a problem, but the fact that multi-modal representations outperform pure text ones is then less meaningful, and by no means spells the end of models trained purely on text.  Also, the paper should discuss possible confounds with the construction of the click-through evaluation data. A link can be clicked on for reasons other than relevance or similarity between query and the phrase that is presented. It seems that other factors are involved as well (e.g., catchiness of the phrase or the perceived informativeness of the linked article).  The paper would be stronger if the multi-modal methods were evaluated on the WS 353 and MEN data sets as well. This would give an indication of whether they might outperform pure text models on tasks that were originally conceived for text models, at least on words related to visual imagery.  Finally, the decision to omit cases involving word overlap is understandable, but it does not come without a cost. Accounting for homonymy or polysemy would not be tested by their approach. ",2-Confident (read it all; understood it all reasonably well),"This paper presents a new multi-million image-caption style dataset and use it for training multimodal embedding models. They also present a new user click based similarity dataset for evaluation. Finally, they try 2-3 CNN-RNN style models for multimodal phrase embedding similarity. The datasets are decent contributions (but with some issues). However, the evaluation has multiple issues in its current form and the models are also borrowed from previous work. ","After author response: Thanks for answering some of my questions -- I updated some of my scores accordingly. I still encourage the authors to answer the rest of the questions, esp. eval on better downstream tasks like captioning and visual zero-shot learning.  --------------------- Evaluation issues:   -- No existing datasets have been used to evaluate, e.g., WordSim353, SimLex-999 or the new visual similarity datasets VisSim and SemSim from Lazaridou et al., 2015. The authors have not compared to any existing paper/model on these datasets to show the advantage of either their larger/better training dataset or their models, hence leaving no takeaways.  -- Why only work on word similarity? Why not show other image-language tasks in addition, e.g., captioning, visual zero-shot learning? This will better demonstrate the advantages of the training dataset.  -- For word embedding baseline, please use stronger latest models such as GloVe and Skip-thought and paragram.  Evaluation dataset issues:  -- The dataset mixes multiple types of similarity, e.g., related versus paraphrase/synonym, and this has been discussed to be a big issue for evaluating embedding models.  -- the data uses triplets instead of the standard ranking+correlation method, and then they also choose random negative phrases for these triplets (which will be easy to detect), which makes the task much easier.  -- In the CrowdFlower experiment, the turkers will assume they need to choose at least one of the two phrases most of the time, and even if they choose the correct phrase, it might be only because it is more related to the query phrase as compared to the other random negative phrase, but overall the positive phrase might still be only very slightly related to the query phrase (because of the initial recommendation system based retrieval). To verify this, the authors should get turkers to also rate how much these 'positive' phrases are related to the query phrase in absolute terms/ratings.  -- they use 3-5 annotators and then choose phrases where >50% of the annotators agree, but this will mean agreement between just two people in many cases. For such a noisy initial dataset, the filtering should be more strict than 50%, or the #annotators should be higher.  Training dataset:  -- Since the data is from Pinterest and the 'captions' are just user comments, I am worried that some of these captions might not be standard description style sentences but instead might contain some non-image/visual story or information -- the authors should investigate and present this, and also verify if this corpus can be used for training captioning systems.  Model:  -- main model figure is very unclear and should be expanded and maybe separated.  -- all models are from previous work; would have been good to also suggest some new model variants for this task/datasets.  -- why use average pooling at test time for a phrase representation and not run the trained RNN model?  Other:   -- Lots of typo's throughout the paper, e.g.,  Line15: ""crutial"" --> ""crucial"" Line34: ""commemted"" --> ""commented"" Table 3: ""weigh"" --> ""weight"" Line 232: ""to Model A, We"" --> ""to Model A, we"" ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",This paper introduces a wonderful new image-sentence dataset. It should be a great resource for multimodal research and training. I hope it will come with a good license.  The model isn't that interesting and NIPS cares historically more about that and hence misses out on some impactful dataset papers. ,I'd like to see some more analysis of the dataset. number and distribution of unique words. problems like personal comments (vs visual descriptions) and ungrammaticality etc. ,"3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper introduces a new large scale dataset of annotated images from the web. More precisely, the authors crawled approximately 40 millions images from the website interest, along with descriptions submitted by users. There is an average of 12 sentences per image, and many images are described by multiple users.  The authors also introduce a new dataset for evaluating word representations. This dataset is made of triplets of short phrase, first two phrases being semantically closer than the first phrase and the third phrase. The positive phrase pairs were obtained using click data, while the negative pairs were randomly sampled. This dataset contains approximately 9.8 millions triplets. The authors also manually cleaned 10,000 triplets, using a crowdsourcing platform.  Finally, the authors propose different baselines to learn word vector representations using visual information, based on this dataset. More precisely, they describe three RNN-CNN models, inspired by models for caption generation. They apply these models on the proposed dataset, showing that using multi-modal data is helpful for this evaluation dataset. In particular, they show that on the proposed evaluation set, the proposed models outperforms the pre-trained word2vec vectors (which were trained on approx. 300 billion words).","This paper is very clearly written. It introduces two large scale datasets, which could have a big impact for researchers working on learning models from multi-modal data. I believe that collecting and sharing high quality dataset is important for the machine learning community, and it seems to me that the Pinterest 40M images could be such a dataset.  However, I have a couple of concerns regarding this paper. First, the paper does not mention the Yahoo Flickr Creative Commons (YFCC) dataset, which contains approximately 100 millions images from Yahoo Flickr. This dataset also contains images with descriptions provided by users. It also contains other metadata, such as tags, location or time. While I believe the two datasets are different, I think the authors should discuss the difference between the two (and not claim that they propose a dataset ""200 times larger than the current multimodal datasets"").  Second, I think that baselines simpler than RNN-CNN should be considered in the paper.  Examples of such baselines are: - use the skipgram or cbow models from word2vec on the descriptions (pure text baseline) ; - use the multimodal skipgram described in [Lazaridou et al.].  Overall, I enjoyed reading this paper and I am looking forward the release of this dataset. However, I believe that this paper would be stronger with better discussion of existing datasets and baselines for multimodal data.  == Additional comments == A classical evaluation methodology in for multimodal data is retrieval: given a description, is the model able to retrieve the corresponding image. Have the authors considered this tasks?  [Thomee et al.] YFCC100M: The New Data in Multimedia Research (http://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67) [Lazaridou et al.] Combining Language and Vision with a Multimodal Skip-gram Model",2-Confident (read it all; understood it all reasonably well),The paper provides a method to fuse the visual information in the word embeddings and it tries to prove that the visual information is able to improve the performance of the word embeddings by semantical similarity. ,"1. The conclusion in line 281-282 which is not fair, since T.Mikolov's method and the author's model are trained by different datasets. It is not able to see if the performance is improved by the author's model or just the dataset. Is it possible to train the T.Mikolov's model with your dataset?   2. Could you describe a little more about your baseline (Model A without visual) in line 259-260. Otherwise, it is not clear to see if the performance improvement comes from the visual information of the image or just the relationship between the descriptions and images.",2-Confident (read it all; understood it all reasonably well),"Pinterest is crawled to generate a sentence/image aligned corpus of 300M sentences/40M images. This data is used to train joint image-language models in the spirit of image caption training, with the intent of learning word embeddings with visual information. Another corpus is collected which contains 10M semantically similar phrases of the form base phrase, positive phrase, negative phrase. 22k randomly selected samples from  this corpus is cleaned up resulting in 10k triplets used for evaluation of the multi-modal trained word embeddings, given three phrases 1,2,3 the distance in word embedding space, d(1,2) and d(1,3) where it is known 1 is semantically closer to 2 than 3, system must declare d(1,2) < d(1,3) to be correct. Three multi-modal trained word embedding models are investigated, with all 3 using VGG based image features, and GRU to model the language. Two models augment the base loss of maximizing the  sentence probability with MSE losses from (1) embedded GRU hidden state to embedded image and (2) embedded words to embedded image.  The best model uses only the base loss of sentence probability and a weight sharing strategy between word embedding and softmax output. This model shows a strong advantage to using multimodal trained word embeddings vs. text only trained word embeddings.","The collection of multi-modal data from a new source Pinterest and the large semantic relatedness dataset is very important to the community. Comparisons are made based on size of corpus to MSCOCO/Flickr30,8 and SBU Im2Text, but no discussion on quality of data in the comparison. MSCOCO and Flickr data are carefully labelled for tight alignment with what is in the image and the sentences, whereas Im2Text is not, using the Flickr sentences rawly from the people who posted the image. Because of this we may train and evaluate caption and bidirectional retrieval systems reliably on MSCOCO and Flickr, but this is not the case with Im2text, often what is in that data are sentences not talking about the image. The sample images of figure 1 in the paper show that Pinterest sentences may include a lot of information outside of the context the sentence, so it is not clear how generally useful this new set is. Certainly you can train better word embeddings using the visual information to measure semantic relatedness, so there is some alignment, but this might be more forgiving than the difficult tasks of captioning and multi-modal retrieval? More should be said about the image extraction pipeline, since VGG is used at level before softmax, you must have used a single crop of multiple crops and averaged? Also the inclusion in model B and C of MSE objectives, this seems reasonable forcing sentence embeddings either from hidden state or word embeddings to match embedded image, but it is not working, this probably requires more investigation, and also more details of the RNN/GRU used. Was this a single layer or two layer GRU? If single does it make sense that the resulting sentence embedding being responsible for generating the words should also be able to create a sentence embedding that converges to the sentence embedding. ",2-Confident (read it all; understood it all reasonably well),,,,,,
Deep Neural Networks with Inexact Matching for Person Re-Identification,"Arulkumar Subramaniam, Moitreya Chatterjee, Anurag Mittal",https://proceedings.neurips.cc/paper/2016/hash/e56b06c51e1049195d7b26d043c478a0-Abstract.html,"The paper addressed the re-identification problems, I.e. identifying two images of the same person as belonging to the same man/woman. A Siamese CNN architecture is suggested, with novel dedicated layer performing normalized correlation of each location with a large set of neighboring locations. The new layer is embedded in the middle of a standard CNN. The empirical results how a significant advantage of the new architecture over previous ones, especially when combined in a fusion with a more standard approach. ","The pros and Cons of the paper are as follows: + the empirical results are strong, showing consistent advantage of the proposed method over previous art (as far as I can judge – I am not familiar with re-identification literature) + the new normalized correlation layer is a sensible architecture for the re-identification task. Each input map is compared alone using normalized correlation to a large array of might-ne-relevant positions, providing reach output (1500 output neurons per location – that’s a lot) for further processing. - The domain of re-identification is a relatively small one, of limited interest (hence significance of the paper is low, as it is relevant to a relatively small community) - clarity of presentation is low, with English levels which should be considerably improved.  More detailed comments:   Page 1: Lines 50-53: I did not understand the argument in these lines. It is stated that due to a large search area and inexact matching object parts in image 1 may be matched to background parts in image 2, and this is stated as a remedy to the partial occlusion problem. However, if this happens without significant penalty, irrelevant matches contribute to the score and may come to dominate it if there are many of them. - This argument seems to repeat in lines 155-158, and again I do not understand it  Page 3: Line 77: “…learning the best representation” – this sentence is a too-bold claim. CNNs are not guaranteed, and are not, learning the best representation, and it is not even well defined what the ‘best representation’ is.   Page 4: Line 134: - Why does ‘a’ run in 1..12 and not in 3…10? I mean, a represents the center of a 5*5 neighborhood and the map size is 12*37, so only y values of 3..10 correspond to patches inside the image. Is padding used? - Why is y treated different than x in this formalism, that is: y is searched exhaustively , while x is searched only locally. Is it because the typical long-but-narrow aspect ratio of standing humans? Page 5: Line 168: why does the output needs two units when the output is a single binary variable? A simple sigmoid can be used here instead of the exploss.  Section 3.2: I checked the derivative formula given and found it correct. Page 6, table 1: it is not stated clearly enough what the measurement score is. I assume it is the percentage of correct retreivals up to rank r, right? This should eb explicitly explained.  Page 7-8: the experimental results show a significant advantage of the suggested method.   The English level is not high enough, where sometimes the words choice make reading harder than it should be. Some examples Abstract: I believe ‘upto’ is not a word  Line 30: ‘entails’ is probably not the right word here. Maybe ‘is’ Line 35: “Have’ -> ‘contain’ Line 58” “show Normalized X-Corr to hold promise” -> “show Normalized X-Corr to fulfill the promise” Line 102-103: “Besides, Normalized X-Corr stands out by retaining the matching outcome, corresponding to every candidate in the search space of a pixel rather than choosing only the maximum match” – I did not understand this sentence. ",2-Confident (read it all; understood it all reasonably well),This paper proposes two CNN models for person re-identification. The novel aspects of the first model are normalized cross correlation (Normalized X-Corr) layer and wider search area for patch matching. The second model is the combination of the Ahmed’s model [1] and the first model. The experimental results show the substantial performance gain over the state-of-the-art deep learning based methods on three benchmark datasets. ,"I think the proposed method seems to be somewhat novel and the performance is impressive. However, the evaluation and the writing are clearly rushed and incomplete. Detailed comments are bellows:   Pros ) The performance of the proposed method is impressive. Even with only Normalized X-Corr model, the proposed method achieves substantially higher identification rates on CUHK01 and CUHK03 datasets than state-of-the arts.   The Normalized X-Corr layer is interesting and seems to reasonable if the performance is really gained by this layer (see Cons. comment).   This paper shows that combination of the different deep architectures improves the performance. This is simple idea, but as far as I know, this is not explored except a work [A]. I consider that the work [A] is published after the submission of this paper.   [A] F.Wang, W.Zuo, L.Lin, D.Zhang, L.Zhang, Joint Learning of Single-image and Cross-image Representations for Person Re-identification, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.1288-1296, 2016.  Cons ) The main drawback of this paper is the incomplete baseline comparison. The proposed method has two differences from the Ahmed et al [1]. These are; (1) Normalized X-Corr layer and (2) wider area search. However, this paper does not report the baseline results except the reported results by Ahmed et al. [1]. Therefore, contributions of each part ((1) and (2)) are not understand. For example, the high performance of the proposed method might be comes from the wider area search and the Normalized X-Corr layer might be not contributed to the performance, and vice versa. The authors could compare the performance with the Cross Input Neighborhood layer of Ahmed’s model with wider search area, and Normalized X-Corr layer with smaller area search of Ahmed’s model, for example.   One advantage claimed in this paper “the parameters of the Normalized X-Corr model is about half number compared to the Ahmed’s model (eg. line170-171)” is incorrect. In Fig.3, the size of Normalized X-Corr layer is 12 x 37 x 1500 while the Cross Input Neighborhood layers of Ahmed’s model is 12 x 37 x 1250. Clearly, the parameters of Ahmed’s model is smaller. This paper doesn’t explain how the number of parameters comes from and why it is smaller than that of Ahmed’s model. In my understanding, the depth of Cross Input Neighborhood is comes from 5x5 (search area) x 2 (asymmetric distance) x 25 (feature maps) = 1250. While the depth of Normalized X-Corr is comes from 12x5 (search area) x 1 (symmetric distance) x 25 (feature maps) = 1500. Due to the large search area, the parameters of the proposed model is larger than that of Ahmed’s model.   The VIPeR dataset is the most commonly used dataset for person re-identification. However, the experimental results on this dataset is not reported. It would be better to report the results on VIPeR dataset.   Writing quality of the paper is low:   - Line 21. and 59: What is jumped and which datasets?  - Line 29: “1.1 Problem Definition” can be removed.  - Line 51, 158: What do you mean “images are regular” ? - Line 154: It would be better to explain how did Ahmed et al. set search space.  - Line 183 “they are fused in a 1000 node fully connected layer”: Is it two separate 500 node fully connected layers (Fig. 3) ?  - Line 250: How much did you get speed up by multi-GPU training?  - Line 255, “Normalized X-Corr model give a 3% gain”: It should be “1.63% gain”. (64.73% vs 62.1%).  - Line 275, “a gain of upto 16% over the existing state-of-the art”: Please specify which setting on the CUHK01.  - Some capitalization should be collected or unified. eg.,   “Person Re-Identification” → “person re-identification”   “Dataset” → “dataset”   “Underground Re-identification (GRID)” →“underGround Re-IDenditifcaiton (GRID)”  Supplementary material: - How did you obtain Eq.(1) ? More explanations are need.  - Please use \eqnarray (if you use latex) for Eq.(1-2) and Eq.(5-7) . - Eq.(3) should be removed.  - I do not understand your intention to show the Fig.7-8.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",The author proposes a Normalized Correlation layer based on Ahmed's work to better deal with large viewpoint variation problem. Experiments show that the proposed method can achieve much better result when combined with the original Ahmed's network. This paper simply extends Ahmed's work and the idea is straightforward.,"This paper focuses on person re-identification problem and extends a previous Ahmed's CVPR 2015 paper. The idea is rather straightforward.  In Ahmed's work, the Cross-Input Neighborhood Differences layer can only capture relatively local feature correspondences. However, in real cases, large viewpoint variation problem could not be handled well by the Cross-Input Neighborhood Differences layer. So the author proposes a Normalized Correlation layer to better deal with large viewpoint variation problem as well as illumination and occlusion. The idea is rather simple and straightforward, i.e. search exhaustively along the x axis (width), and search in a small range [-2,2] along y axis (height).  There are much to be done in the task of person re-identification. How to align different parts of two persons in different viewpoints. Will pose estimation help a lot? Personally, I feel that human make decisions by looking at different local parts such as face, gender, pattern on clothes, color of clothes, accessories and so on.  If this is true, carefully design that involving detection, alignment, spatial transformer net and so on may further improve person re-identification result by a large margin. So my suggestion is that the authors should try to solve the problem to make it practical instead of doing  some easy improvement. And as far as I know, one paper in CVPR 2016[1] and several unpublished papers[2] can achieve similar results compared to this paper.   Overall, the paper implements a new layer, and improves person Re-ID result by collecting far-away feature correspondences in x-axis. But the contribution relatively limited.   [1] Learning Deep Feature Representations With Domain Guided Dropout for Person Re-Identification, Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang. CVPR 2016. [2] End-to-End Comparative Attention Networks for Person Re-identification, Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang and Shuicheng Yan.  http://arxiv.org/pdf/1606.04404.pdf",2-Confident (read it all; understood it all reasonably well),The paper proposes two new deep networks for person re-identification. The first one has a layer of normalized correlation which captures the similarity of two input images. The second one is a variation of the first one. This network has one more branch containing a new layer of which incorporating information from the local neighborhood of a pixel.,"Person re-identification is very useful in application. The proposed solution in this paper is reasonable. The two new additional layers are helpful. And the computation details of the normalized correlation are given in the supplementary.    From the experiment results, we can see that the Nom X-corr model is worse than previous methods on the dataset of QMUL GRID. By contrast, the fuse model gains the best performance. However, in the section of training algorithm, only the gradient is presented. The fused model has a new branch which is different from [1].  Therefore, I don’t think the training method of [1] can be directly used. How to jointly training the two branches? The paper should give proper description. This is critical point for a mixed network. Moreover, a discussion on complexity of the method should be included in the experiment.  ",2-Confident (read it all; understood it all reasonably well),The authors propose two DCNN architectures for person re-identification that allow inexact matching to handle variations in data. Experiments on multiple benchmark datasets are done to evaluate the proposed algorithms and compare them to published appraoches.,"This paper presents a CNN architecture for person re-identification with improvements of marginal novelty on [1] by 1) using a symmetric normalized cross correlation (versus differences) to match intermediate representations of two input images, 2) searching a larger area and feeding all matching scores to next level CNN processing (versus  keeping only the best scores), and 3) using more convolution layers. In a second architecture the proposed CNN is fused with the architecture in [1] to further improve the performance. What is the most impressive is that there is significant performance gain using the fused architecture on three benchmark datasets, especially for the CUHK03 dataset.  As the paper is inspired by [1], I suggest to give a more detailed description of [1] is Sec. 2.2 and highlight what is in common and what is different with [1] in Sec. 3. Other than this the paper is well organized with sufficient references and reads well. ",2-Confident (read it all; understood it all reasonably well),"This paper proposes a deep end-to-end network using a “siamese” type architecture by extending the work by Ahmed et. Al. [1] to address multi-camera person re-identification. Specifically, it replaces the cross input neighborhood difference layer in [1] with a Normalized Correlation layer to exploit the strength of correlation analysis which is known to handle illumination variation well in other areas of computer vision including person re-identification. A clever use of wider search space (compared to [1]) to find neighborhood patches from other view enabled the method to address large viewpoint variations which in turn made the proposed approach more resistant to occlusion. In addition, the two stream network which is a result of fusing [1] with the proposed method shows remarkable boost in benchmark datasets. The proposed method is evaluated on 3 benchmark datasets.","Strong points: 1. The use of normalized correlation in place of cross input neighborhood difference layer as in [1] facilitates illumination invariance 2. The use of wider search space for neighborhood patches makes the model fit for occlusion robustness 3. Experimental results on 3 benchmark datasets show impressive improvements over state-of-the-art. Confusing points: 1. I’m not sure if the “fused” network can be claimed as a separate CNN-based architecture (as said in line 9 ‘two’ CNN based architecture). It merely uses the same architecture as [1] in one stream throughout and fuses the proposed architecture at the end. 2. The paper says about using around half the parameters compared to [1]. An explanation about this difference would be helpful. On an intuitive level, is it because the number of feature maps/channels in the last few layers (between correlation layer and the final binary output layer) being half (25) compared to [1] (50)? A related question may be – how does the use of wider search space and hence computing correlation with a lot more number of neighborhood patches scale the computational load (say with [1])? 3. The experimental results are quite impressive.  However, it is not clear whether the use of wider search space or the use of normalized cross correlation features have boosted the performance so much. A comparison with an extension of [1] with similar wider search space would have given an insight towards this. However, the lack of this experiment does not affect my view towards this paper as this addition can be kept as a future work.  Finally, a suggestion to include some qualitative results in the main paper. Many a time, the re-identification papers don’t provide qualitative results and this does not help in supporting some broad claims (like occlusion robustness, illumination invariance etc.). However, the supplementary material has some impressive results on showing occlusion robustness or viewpoint variation. Inclusion of some of these (if space permits) would greatly benefit the paper and may encourage the community to include them subsequently.  So, even though the paper’s count towards ‘novelty’ poses some question in my mind and the design of the new layer seems a little ad-hoc, I’d recommend it as a poster (at this moment). The experimental results seem to be a big plus point for this paper and this would encourage the community to look further into its strength and weaknesses and try to use it in other areas of computer vision and machine learning as well.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,
Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than $O(1/\epsilon)$,"Yi Xu, Yan Yan, Qihang Lin, Tianbao Yang",https://proceedings.neurips.cc/paper/2016/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html,"The submission considers algorithms for solving a specific class of optimization problems, namely min_{x in Omega_1} F(x), where F(x) = max_{u in Omega_2} \langle Ax, u \rangle - phi(u) + g(x). Here, g is convex, Omega_1 is closed and convex, Omega_2 is closed, convex, and bounded, and the set of optimal solutions Omega_* \subset Omega_1 is convex, compact, and non-empty. The submission also assumes a proximal mapping for g can be computed efficiently.   The above framework is apparently general enough to capture a number of applications, including various natural regularized empirical loss minimization problems that arise in machine learning.   Classic work of Nesterov combined a smooth approximation technique with accelerated proximal gradient descent to converge to a solution with epsilon of optimal in O(1/epsilon) iterations. Roughly, the smoothing parameter mu controls both the quality of the smooth approximation to the original problem (smaller mu means a better approximation), and the speed of convergence (smaller mu means slower convergence). Choosing mu to balance these costs yields an eps-accurate solution in O(1/eps) iterations.  The submission shows that if F satisfies a ""Local Error Bound"" (LEB), then one can obtain faster convergence. Roughly, an LEB guarantees that the distance of any x from an optimal solution falls like a constant power of the distance of the value F(x) from the optimal value F(x^*). The key idea is to use Nesterov's approach, but the start with a large mu, and gradually decrease it over the course of the execution. The point is that one does not need a good approximation to the original problem at the start (since one is far from an optimal solution then anyway). But as a good solution is approached, one can afford to drop mu to get a better smooth approximation, without killing the convergence rate. A primal-dual version of the algorithm that does not require hand-tuning of parameters is also described (mostly in the supplement).  Applications to specific classes of problems are discussed, and some experimental results from 3 different application domains are provided. The experiments show significantly improved iteration complexity for small values of epsilon, compared to basic Accelerated Proximal Gradient Descent, and a first-order primal-dual method. ","This seems like a nice contribution, both in terms of the analysis (which is clean, intuitive, and apparently the first for this kind of smoothing algorithm), and the concrete efficiency as demonstrated in the experiments. (However, I am not knowledgeable enough about the area to know for certain that the algorithms experimentally compared against truly state of the art).   The class of problems considered capture a variety of important applications, and the theoretical results do seem to generalize or asymptotically improve on several directly comparable prior works.   For applications where theta=1, such as regularized empirical loss minimization with L_1-norm or L_{infty}-norm as the regularizer and a non-smooth loss function like hinge or absolute loss, the submission's algorithm will converge in O(log(epsilon_0/epsilon)) iterations, while the submission suggests that prior work would require O(1/epsilon) iterations to converge. Is this accurate? If so, then this is an exponential improvement on prior work, which seems rather impressive, and perhaps the submission should emphasize this point more.   It would be nice to see how the proposed algorithm fares against prior art for larger values of epsilon. My understanding from the experiments is that the advantage of the present algorithm is seen only at small values of epsilon. But if this is the case, why not use the other methods to quickly find a reasonably good solution, and then run the new algorithm starting from said solution?  Smaller questions: *The submission states that if L_mu is difficult to compute, one can use the ""backtracking trick"". Was this necessary in any of the applications considered in the experiments?   *Since the proposed primal-dual method requires operating on the dual as well as the primal, it is perhaps surprising that the experimental wall time for this version of the algorithm was best overall. Did the reported wall time account for both the time to update the primal solution and the dual?",2-Confident (read it all; understood it all reasonably well),"Review of ""Homotopy smoothing for non-convex problems with lower complexity than O(1/e)""  The paper explains a new homotopy smoothing algorithm for solving non-smooth optimization problems. The introduction and related work sections explain the problem setting and specific contribution. Whereas the Nestorov method uses a fixed smoothing parameter for all iterations, the proposed HOPS method changes the smoothing parameter to achieve speedups in early iterations, and better approximations of the non-smooth objective at later iterations. The paper provides the first theoretical guarantee of the complexity of the homotopy smoothing method.  Sections 3 and 4 explain the homotopy smoothing method. The text in Section 4.2 is especially helpful for understanding the main idea of the homotopy method. Figure 1 is also helpful for understanding the local sharpness theta parameter which appears in the local error bounds. It is nice to see that in Section 4.4 the duality gap is used to monitor the accuracy of the solution. Section 5 gives some experimental results, and Section 6 is a brief conclusion. ","In general the paper is a nice contribution to the optimization literature, and gives insight into why the homotopy smoothing method works.  The main weak point of the paper is the experimental section. Without some measure of error (standard deviation, confidence interval), it is difficult to believe statements about significant differences between the speed of the algorithms. Also, I was expecting to see some experiment/figure that shows how the speed of convergence is affected by the choice of homotopy smoothing parameter (b). One end with b=1 (no updates to the smoothing parameter) it should be relatively slow (same as APG), and then it should speed up as b increases. Right?  Minor comments:  - In the experiments, what did you use for the b parameter of the HOPS   algorithm? Does it matter?  - what does ""the optimal value is obtained by running PD with a   sufficiently large number of iterations such that the duality gap is   very small"" mean? Why do you need to know the optimal value? You   should be able to compute the duality gap without having to know the   optimal value. ",2-Confident (read it all; understood it all reasonably well),"The paper addresses the composite optimization problem: minimize F = f + g, where neither g nor f need be smooth. The current paper assumes that g has an efficient prox operator, and focuses on f with the rather general structure:    f(x) = max_u(< Ax,u > - \phi(u))  The paper proposes to repeatedly approximately minimize versions of f smoothed by amounts that decrease exponentially. The main result Thm 5 shows that when F fulfills a \theta Local Error Bound (LEB) condition (for theta in (0,1]), the number of iterations needed to optimize smoothed versions is O(eps^(theta-1)). For theta = 1 (absolute value, hinge loss...), this leads to convergence linear in total iterations; for theta = 0.5 (e.g., strongly convex functions), this leads to 1/t^2 rates.  The paper discusses some different conditions that result in LEB conditions, including KL property, and hinge loss ERM as in SVM. The paper shows some experimental results comparing to other first order methods.","Technical quality: - The experiments are interesting and well done, except for a significant point reported below. - The proof Thm 5 is explained clearly in the supp. mat., and the aspects in the paper itself (Thm, definitions, examples) are sound well written.  Novelty: I am not sufficiently familiar with the exact literature, so am assuming the authors claims hold. In this case, the new rates under the proposed setting are new and non-trivial. It is worth noting that there exist previous work on gradual smoothing for faster rates, such as ""Optimal Black-Box Reductions Between Optimization Objectives"" despite it being quite different in assumptions and results.  Impact: The paper shows how LEB and KL can enable faster than typical rates for problems where f,g are both non-smooth, which has potential to spur further research into methods adaptive to such data dependent parameters. It would be even stronger:  - If it considered in detail the computational complexity for ERM problems, e.g., by replacing APG by an SVRG variant. - If it focused on representative performance. The experiments report the best HOPS performance among several arbitrary seeming settings of t, but this is not a practical way to use the algorithm. If PD-HOPS fully avoids this, it should be the focus, and we should see its downsides as well (in the paper, not just the supp. mat.).  Clarity and presentation: The sketch of Thm 5 is not clear enough to be useful. The most salient point is the use of x^dagger_s-1,eps and its interaction with Lemma 1, but the induction is neither clear there nor that interesting.  l.231: Typo: min_{x\in\RR^}F(x) is a number, does not have an epigraph, remove the min. ",2-Confident (read it all; understood it all reasonably well),"This paper shows how to run Nesterov’s smooth minimization of non-smooth functions technique in phases, with increasingly better strongly convex regularizers for the dual, in order to obtain faster convergence for a class of optimization problems. Improvement follows from the fact that the iterate obtained at the end of a phase can be used to warm start the next phase. This is guaranteed via a simple lemma bounding norm distance to sublevel sets in terms of value distance.  Together with a local error bound condition, better convergence follows for a set of problems.  ","I find this idea really cute. It is a very intuitive thing to do; however, for it seems that we lacked rigorous analyses for such a “homotopy” method. The paper is a pleasure to read, and it introduces the reader very gently to the topic, by first presenting Nesterov’s smoothing trick.  It would be interesting to write down what is the precise iteration bound for optimizing Hoffman’s bound and for cone programming (linear convergence for these problems sounds very exciting; I don’t know if it is new or not, and it would be nice to mention whether anything similar results via first order methods were previously known).  ### update ### I still like the paper, so I maintain my scores. A quick Google search shows that this is not quite the first claim of a first order method for LP with linear convergence. It's not clear to me how various parameters of the problem affect the iteration count, so I suggest carefully analyzing those instances before claiming something in that direction. ",2-Confident (read it all; understood it all reasonably well),"The paper considers a non-smooth optimization problem with a composite objective, the first part of which is given by an explicit max structure. By combining Nesterov's smoothing and acceleration with Yang and Lin's recent result [23], the authors propose a homotopy method which converges at a rate potentially faster than O(1/\epsilon). The actual rate depends on the error bound parameter \theta, and the paper gives some examples of functions for which the error bound holds.","The paper is well written and easy to read. It uses familiar ideas and proof techniques from convex optimization, but presents them in an elegant way, resulting in what seems to be the first analysis of Nesterov smoothing and acceleration that gives a faster rate than O(1/\epsilon) for non-smooth functions. The result relies on the local error bound property. The authors make a connection between this class and other properties of functions commonly studied in convex optimization, such as local strong convexity, and the Kurdyka-Lojasiewicz property. I would have liked to see these and other connections further discussed, and I encourage the authors to compare their work to some other related works such as [1,2].  Another concern is how knowledge of the constants c and theta affects the performance of the algorithm. Prior knowledge of these constants seems essential for applying the algorithm, and the authors should discuss how they can be estimated, or in which applications it is reasonable to assume knowledge of these constants. Section 4.4 hints that the primal-dual version of the algorithm can solve this problem, but this is not convincing and the discussion should be significantly expanded.   Below are additional comments that I encourage the authors to consider: - The comment on line 75 (it can be made sharp by some transformation) is ambiguous. To clarify, you should refer to Definition 4 and the following remark. - The proofs seem to be heavily inspired from [23]. This should be acknowledged. - The notation for the dual norm is not standard, is there a particular reason for this? - Equation (11) can be improved by moving the quantifier to the beginning. - In applications, discuss whether the constant c is typically dimension independent. - Line 231, min_x F(x) has a polyhedral epigraph: this statement does not make sense to me, since min_x F(x) is a scalar, and has no epigraph. - The example Line 240-242: this function is not necessarily locally strongly convex, if x^* - x \in Null(A) then ||x - x^*|| > 0 but F(x) - F(x^*) = 0. Perhaps the authors meant that this function satisfies condition (10) - Define what o-minimal structure means (line 252)  [1] Zhi-Quan Luo and Paul Tseng, Error bounds and convergence analysis of feasible descent methods: a general approach. Annals of Operations Research, 1993. [2] B. T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 3(4):643–653, 1963","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors propose a new optimization algorithm based on a homotopy smoothing, which requires O(1/eps^(1-\theta)\log(1/\eps)) iterations to achieve an eps gap towards the optimal objective value.","Based on my understanding of existing literature, most of the results in this paper are known or straightforward generalization of known results.  For example:   (1) Zhang et al. (2012) has shown that for sigma-strongly convex function, an iteration complexity of O(eps^{-1/2}sigma^{-1}). The analysis in Zhang et al. (2012) can be further extended to locally strongly convex optimization, which is identical to the result in this paper on Line 237 of Page 7.  Zhang et al. (2012) Smoothing Multivariate Performance Measures. JMLR --http://www.jmlr.org/papers/volume13/zhang12d/zhang12d.pdf  (2) The KL condition on Line 243 of Page 7 is also known as the Polyak-Lojasiewicz, or gradient dominated condition. There have been quite a few papers, which used the PL condition to relax the strong convexity.  I suggest the authors to give more concrete justifications of the significance of the proposed algorithm.",2-Confident (read it all; understood it all reasonably well),,,,,,
