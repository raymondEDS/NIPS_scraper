Title,Author,Link,Reviewer 1 Summary,Reviewer 1 Qualitative Assessment,Reviewer 1 Confidence in this Review,Reviewer 2 Summary,Reviewer 2 Qualitative Assessment,Reviewer 2 Confidence in this Review,Reviewer 3 Summary,Reviewer 3 Qualitative Assessment,Reviewer 3 Confidence in this Review,Reviewer 4 Summary,Reviewer 4 Qualitative Assessment,Reviewer 4 Confidence in this Review,Reviewer 5 Summary,Reviewer 5 Qualitative Assessment,Reviewer 5 Confidence in this Review,Reviewer 6 Summary,Reviewer 6 Qualitative Assessment,Reviewer 6 Confidence in this Review,Reviewer 7 Summary,Reviewer 7 Qualitative Assessment,Reviewer 7 Confidence in this Review,Reviewer 8 Summary,Reviewer 8 Qualitative Assessment,Reviewer 8 Confidence in this Review
Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach,"Remi Lam, Karen Willcox, David H. Wolpert",https://proceedings.neurips.cc/paper/2016/hash/5ea1649a31336092c05438df996a3e59-Abstract.html,"Bayesian Optimization algorithms for deterministic functions (possibly observed in noise) with continuous input domain have been intensively adressed in recent decades within the framework of Gaussian Process modelling. Here the authors focus on an important issue with most state-of-the-art strategies, namely that the acquisition function is often relying on one-step-lookahead considerations, and does not account for the information brought by candidate points regarding susceptible pay-offs in future iterations. This is known as the finite budget / time horizon set-up, and has inspired several of the papers referenced in the bibliography. The focus here is on transferring know-how and formalism from the world of Dynamic Programming (DP) to Bayesian Optimization (BO). In particular, it is recalled and formalized how the finite time BO problem can be cast as a DP one, and more centrally how an Approximate DP approach called Rollout can be adapted to it.  A finite time BO algorithm based on Rollout is proposed and tested on Gaussian Process realizations and four classical test functions. Empirical results suggest that the Rollout outperforms the considered state-of-the-art one-step-look-ahead criteria on GPs with fixed covariance parameters, while on test functions results are more mitigated and further highlight the importance of adequately choosing the Rollout horizon and discount factor. ","I enjoyed this paper where an effort has been made to transfer a relevant formalism and an appropriate technique from the world of Operations Research and Dynamic Programming to Bayesian Optimization. While this was partly done in previous work here it it seems to go one step further, and I am not aware of publications where the Rollout was adapted to this precise problem. The results look promising, especially on the GP realizations, but I really felt the absence of comparison to other strategies recently proposed to adress this very issue; GLASSES of [5] seems a natural competitor here (and maybe also the MCTS of [13]). Also I was wondering if further improvements could be reachable at reasonable research investment regarding the (currently rather simple) base policies. As for the empirical comparisons on functions, the way the models are set appears a bit contrived, and the conclusions would have more weight with some experiments in more realistic conditions.  Overall, I assess this work as good and promising, but I feel that it should be seriously improved before publication. Beyond the points mentioned above here are a few additional (essentially minor) remarks along the lines: * abstract, line 5: about `uncountable', isn't it the case already in most usual sequential settings (without the finite budget issue)? * lines 23-24 after the colon: this formulation sounds unclear to me.  * line 28: `optimal' with respect to which cost? * Equation 1 (and later, e.g. in Equation 5): why should there be unicity of the maximizer? Besides, what guarantees its existence unless assumptions such as continuous f and compact X are made?  * In Equations 3-4: noise appears at a sudden. But it was not really mentioned earlier (unless I oversaw it), nor the consequences for Bayesian optimization.  * Lines 99 and 103: the set Wk first depends on zk and uk then not anymore. As formalisation is an important part of this paper, I guess it should be all the more taken care of.  * In Equation 7: shouldn't wk be upper-cased here as it is a random variable with respect to which expectation is taken? Similar questions for further quantities at stake might be relevant.  * In equation (9): where are u{k+1} and w{k+1} now?  * The beginning of the sentence in line 118 seems somehow redundant.  * In line 212 (and/or around): is h typically assumed much smaller than N?  * In 6.1: how are the GP simulated? What is the discretization? (+The software used?)","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper proposes a new Bayesian optimisation framework in which the remaining evaluation budget is taken into account to determine the location of the next evaluation of the objective. Since the problem is intractable, an approximation to the optimal policy is computed by dynamic programming using rollout. Several experiments show interesting results achieved by this method compared to standard Bayesian optimization approaches.  ","I think that this is a good paper that tackles, in my opinion, one of the most interesting open problems in Bayesian optimization. The proper use of the available resources to define good policies, and how this can be adapted to the exiting acquisition acquisition functions, is crucial to push the field forward.   The paper itself is a nice piece of work, well structured and easy to read. The idea of using dynamic programming to integrate the effect of the new evaluations in future decisions is a natural one and it is nicely described and executed.  My main concern about this paper are the experimental results. Although the authors show some improvements with respect to standard acquisitions, comparisons with existing not myopic methods of with non-greedy acquisitions, such us Entropy search, are missing in the paper. These comparisons would make this paper much stronger but even without those I think that this work is an incremental contribution to the Bayesian optimization literature.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper addresses Gaussian process-based optimization. The question of interest is to find new points where to sample a function f, in order to minimize it over a bounded Euclidean domain. The new points are selected by maximizing the 'potential reward' brought by an evaluation of f at this point.   In the literature, the authors mention two kinds of choices for the 'potential rewards' to maximize. The 'potential rewards' of the first kind are simple to calculate, but potentially sub-optimal (expected improvement, probability of improvement, upper confidence bound). Those of the second kind have Bayesian optimality properties, but are close to intractable to use in practice.  The authors propose an approximation of the optimal reward functions, which lead to a new Gaussian process-based optimization method. They compare their proposed methods to those based on simple reward functions (expected improvement, probability of improvement, upper confidence bound), on simulated Gaussian process trajectories and analytical functions.  ","The paper starts by presenting the optimization problem, its practical importance, and the use of Gaussian processes in this context. I found this first part of the paper clear and pleasant to read.  Then, the paper presents dynamic programming in general, its application to Gaussian-process  based optimization, and the approximate dynamic programming approach they propose. This second part is thus at the core of the innovation brought by the paper. My important concern is that, here, I find the exposition unclear and difficult to follow (even though I am not an expert in dynamic programming). In my opinion, the exposition should be improved significantly. In particular, I think that it would be valuable that the authors summarize in an algorithm how exactly they select the new point where to evaluate the function. I would like to see clearly what exactly is optimized, which mean values are approximated numerically, etc... I think the exposition in Figure 1 should be improved.  In a last part, the authors compare their proposed algorithm with standard greedy procedures for selecting the evaluation points (expected improvement, probability of improvement, upper confidence bound). After reading this part, I am not sure that the improvement brought by the paper is significant. First of all, the standard greedy methods have the advantage that they are simple to use, and that the time to select the new function evaluation point is small. Unfortunately, the authors do not provide information about the time taken by their algorithm to select an evaluation point. I find it essential to provide such an information, at least for the settings of Table 1 and 2, because the reader should be convinced that the algorithm proposed by the authors is numerically tractable. This issue is even more important in higher dimensional cases than those addressed in the paper (1 and 2).   My other concern is that, setting the algorithmic cost issue aside, I do not find the improvement brought by the paper very significant compared to the standard greedy methods. In particular in table 2, addressing the more interesting two dimensional cases, there are 3 standard methods performing best 3 times while 4 new methods of the paper perform best 4 times. (actually these 4 new methods are 4 different configurations of the algorithm proposed in the paper). This seems relatively even to me. Also, since there are different configurations of their algorithm, perhaps the authors could give advices on how to configure their algorithm in practice.         ",1-Less confident (might not have understood significant parts),The paper presents a Bayesian optimization approach with finite budget. In particular it presents a heuristic to solve dynamic programming using a rollout strategy.,"I think the idea is not totally new. In the context of model predictive control and look-ahead strategies this approach is used very often. However, this technique can be useful in Bayesian Optimization. In the present version of the paper it is missing an analysis of the computational load of the various algorithms.",2-Confident (read it all; understood it all reasonably well),"The paper shows how solving a global optimization problem under a finite objective evaluation budget can be posed as a dynamic programming (DP) problem. The ""dynamic state"" of the problem represented by the x-y data collected so far, the ""control"" is the design point to evaluate next, and the ""stochastic disturbance"" arises from the uncertainty in the predictive distribution of the design-objective map. The key lies in choosing the reward function of the DP to be the improvement in the solution to the optimization problem. With the exception of the choice of the reward function, the formulation of an optimization of a DP problem is not new. Similar ideas, can be found in W. B. Powell's book named ""Optimal learning"" (see in particular Chapter 7). I have to mention, however, that I am not familiar with something that is identical to the present work. The full DP problem is extremely complex (especially due to the fact that the state space is growing) and cannot be solved computationally. The authors employ the well known roll out technique. Roll-out approximates the value-to-go function by simulation assuming that in the future one would follow sub-optimal heuristic strategy. The authors demonstrate that their approach performs better than the examined greedy heuristics (probability of improvement, expected improvement, and upper confidence bound).","There are some issues/concerns that the authors should address:  1. It would be nice to see a more challenging example (beyond 2D). There you would start seeing the importance of going beyond the simple greedy approximations.   2. Why do you pick the last suboptimal heuristic policy to be the one that minimizes the posterior mean? Please explain in the text.  3. How would the result change if you finished all strategies (PI, EI, UCB, and R-?-?) with a final step that just minimizes the mean of the GP?  4. Your method is outperformed by EI and UCB for two of the test functions. Why do you think that happens? Is there anything peculiar about these functions?","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper proposes a new acquisition function for Bayesian optimization taking into account remaining evaluations. The main idea is to formulate the problem as finite horizon dynamic programming problem.  The solution is approximated by not computing the optimal value function but the value function of a heuristic policy, using Gauss-Hermite quadrature and restricting the horizon.","Technical quality: 3/5 The derivation of the acquisition function is well motivated.  The final experiments should contain a direct comparison to the closest competitor (GLASSES). Since the code is freely available this shouldn't be too much to ask. After that I'd be happy to rate a 4.  Novelty: 4/5 I am not aware of other literature that uses DP to obtain an acquisition function for BO. The rolling horizon approximation introduces back a certain myopia but a BO strategy that is aware that there are at least 5 function evaluations to go is still a great step.  Impact: 3/5 The results are not overwhelming but it is a promising approach.  Clarity: 4/5 Excellent in every respect. Please explicitly state the computational effort of the acquisition function (per evaluation).  Minor issues: * Reference [5]: replace arxiv citation with AISTATS * Please add a reference to Gauss-Hermite quadrature (l. 213) * Line 151: It seems intuitively clear that E[...] is Expected Improvement. However, a little formal proof in the appendix would complete the picture. * For the sake of completeness mention Entropy Search as acquisition function. * Please augment every book reference with page numbers, especially [1] and [15].","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,
Deep Learning without Poor Local Minima,Kenji Kawaguchi,https://proceedings.neurips.cc/paper/2016/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html,"This paper proves several important properties for the standard loss function L (as a function of connection weights) of essentially any linear or rectifier deep neural network with training data set (X,Y) such that XX^T and XY^T have full ranks: (i) L is non-convex and non-concave, (ii) Every local minimum is a global minimum, (iii) Every critical point that is not a global minimum is a saddle point (i.e., there are no local maxima), (iv) under some reasonable assumption, the Hessian at any saddle point has a strictly negative eigenvalue. Item (i) excludes the simple cases, while items (ii)-(iii) show that there are no poor local minima and no local maxima. Item (iv) implies that for a network with just one hidden layer the Hessian at any saddle point has always a strictly negative eigenvalue while for deeper networks there are saddle points with no strictly negative eigenvalue. It is shown that these ""bad saddle points"" can be eliminated by perturbation.  While these results are simple to state, the proofs provided are quite long and dense and they involve the use of several lemmas and consider different cases of the rank of the product of the weight matrices. As far as I could see the proofs are correct though some clarification is required (see below). ","The results in the paper are theoretical, yet quite significant. They surely expand our knowledge about deep learning even though they may not have any immediate applications as the paper itself acknowledges.   Here are my comments and questions about the paper:   (i) Line 97: I suggest you include a summary of the reasons provided in previous work (by Baldi and Hornik 1989) why the assumptions underlying your results (that XX^T and XY^T have full ranks) are realistic. This would definitely strengthen the paper and make it more attractive to the reader.   (ii) Line 139: It is not clear why \Psi_j the total number of paths from the inputs to the j-th output depends on j. Given the linear structure I would have thought that \Psi_j is independent of j.  (iii) Line 146: I think the product in the end of this line should go from k=1 to k=H+1.  (iv) Line 466: I think the proof in footnote 6 should be provided in the body of the document rather than in the footnote.  (v) Lines 468-472: This is where I think the proof becomes unclear. In line 185, {\cal R}(M) is defined as the range space of the matrix M. So what do you mean by {\cal R}(\overline{\cal L}}) and by {\cal R}(f) given that \overline{\cal L}} and f are not matrices.   In addition, since r'=(W'X-Y)T and r=(W_{H+1}....W_1X-Y)T with W'=W_{H+1}...W_1, it follows that r=r'. So what do you mean by {\cal R}(r)\subseteq  {\cal R}(r')?  Response to Author's feedback: Your remark on this item is unsatisfactory and does not clarify the issue. You say ""{\cal R}(f) means the range of a map f (as a function of W)"". This does not make any sense. The range of a map is a SET and cannot be considered as a function. In particular the range of f is simply a subset and not a function of W. As sets  {\cal R}(\overline{\cal L}}) and{\cal R}(f) are identical. And {\cal R}(r) and {\cal R}(r') are the same sets.   (vi) Line 563 W --> We",2-Confident (read it all; understood it all reasonably well)," The paper proves two theorems: one unconditional result about the loss landscape of deep linear networks (a chain of matrices) and one assumption-based calculation for deep relu networks.  Both answer questions raised in previous work: the first is a strengthening of a conjecture and the second uses fewer assumptions than a previous result.  The results provide excellent intuition towards the robustness of neural network optimization.  Although the proofs are a fairly technical calculation, the theorems themselves are extremely intuitive.  The paper is likely to benefit researchers trying to understand why simple algorithms (SGD) suffices for technically NP-hard optimization problems, and may lead to further strengthenings or similar results. "," The paper makes a significant dent in the central mystery of deep learning: why are neural networks so easy to optimize?  The precision of the linear result is quite impressive!  The nonlinear result still makes a implausible assumption, but is stronger than previous work and the implausibility is carefully discussed.  The paper itself was a delight to read, though I will admit that the full proofs were a bit of a slog.  My main question is whether the proofs could be simplified.  Two approaches come to mind: use random perturbation tricks to avoid some of the length devoted to singularity, and constructing a monotonic descent path from any point to the global minimum.  Here's a sketch of how to construct a descent path.  First, I claim that the global minimum is    min_{rank W = p} 1/2 || Y - W X ||^2  This is clearly at least as good as the global minimum, and can be achieved by decomposing the above optimal W as     W = U D V' = Ue 1 ... 1 De 1 ... 1 Ve' = W_{H+1} ... W_1  where U D V' is the SVD of W, Ue/De/Ve are zero extended versions of U/D/V, and the 1's are rectangular extended identity matrices.  Now consider an arbitrary point W = W_{H+1} ... W_1, and decompose each factor via SVD.  Given a matrix decomposition D U V (resp. U V D) for diagonal D and orthogonal U/V, there is a smooth path to a decomposition of the form D 1 V (resp. U 1 D) that does not change the value of the product.  Similarly, D U E for diagonal D/E can be transformed into the form 1 U E without changing the product.  Using a series of these transforms chained together (which will be continuous if not smooth), we can find a path ending in the form    Ue 1 ... 1 De 1 ... 1 Ve'  A final path changing Ue/De/Ve' can be constructed to reach the global minimum, where the loss decreases monotonically.  Note that the path can also be made to increase after staying constant.  If correct, this is a much simpler proof of 2.3(i) and (ii).  The above shows that every point W can be monotonically decreased to a global minimum.  However, I believe the above proof can be extended to show that every nonglobal minimum is a saddle point, essentially by rearranging the path construction to change singular values immediately.  This would prove 2.3(iii).  I am not sure if there is a similarly easy proof of 2.3(iv).  Additional comments:  1. Does Theorem 2.3 need a size > 0 condition to ensure (i)?  2. ""scaler"" to ""scalar"" in the proof of Lemma 4.6.  3. It's good that A.7 avoids discussing the other direction, since it is false. For example, if $A = 0 \kronecker 0$ then all matrices are $A^-$, but only some matrices are Kronecker products. ",2-Confident (read it all; understood it all reasonably well)," This paper settles a couple of conjectures from the machine learning community about neural networks. They prove in the linear model case (e.g., Y=W_H * W_{H-1} … W_1 *X) that every local minimum of the least squares regression problem actually attains the global minimum value.  However they show that there exist (for deep networks with > 3 layers) that there can be “bad” saddle points where the Hessian might not have a negative eigenvalue.  The authors show that small perturbations make these bad saddle points go away (but I  was not able to understand this part very well).  They also extend this to handle nonlinear networks with ReLU nonlinearities (which are much closer to networks that are used in practice).  This result extends a recent set of results from Choromanska et al to work with weaker assumptions, however still relies on a few somewhat questionable assumptions. "," This was a long and highly technical paper and I will say that unfortunately I was not able to give the proofs the time that they deserve (and so my confidence rating is low).  My sense is that the contribution is quite  important, since a theoretical understanding of “why neural networks work” still does not exist yet.  There are  several critical questions relating to overfitting and optimization.  This paper addresses the optimization problem: why can we usually find good local optima for neural net optimization problems?   Dauphin et al conjectured that (1) most local optima are “good” and (2) almost all critical points are saddle points.  This paper addresses this first conjecture (and to a lesser extent the second one) by showing that all local optima are global optima. More concretely, the authors really nail the linear neural network case, and improving a recent result by Choromanska et al showing the same thing for nonlinear neural networks.  In the nonlinear case, there are still several assumptions that must be discarded before this problem will be considered to be “solved”, such as assumption “A5u”, which says that whether a “path” is active or not is independent of the inputs to the network.  However even here, the result is a big step forward and relies on much more elementary results than the Choromanska paper, which appealing.  ",1-Less confident (might not have understood significant parts),"Some major claims are made: (i) If the output dimensionality is less than the input dimensionality, all local minima for the loss surface of a deep linear network (with any depth and width values) will be globally optimal; and (ii) under a set of assumptions from Choromanska et al 2015, an analogous result also holds for nonlinear deep models.","- This is potentially an important paper, if all the proofs of the assertions are verified to be correct.  - Without any conditions on p, Theorem 2.3 (iv) is vacuous  - since the assertion implies that that claim holds for any p.  - The presentation needs to be significantly improved. The author should make some effort to make the paper self-contained.   - The proof sketch for Theorem 3.2 is missing and the proof is entirely delegated to supplementary material, making verification in a conference submission somewhat hard. Insights are not provided on what assumptions make the loss surface well behaved in terms of not having poor local minima.  - Overall, pretty hard to read but could be promising line of work.   ",1-Less confident (might not have understood significant parts),"This paper characterizes the critical points of linear neural network and non-linear neural network. For linear neural network, it shows that there is no poor local minima, and for shallow network all saddle points are strict saddle, while for deep network there could be bad saddle points. For non-linear neural network, with two additional assumptions, the authors show that it can be reduced to the linear neural network case, thus the same claim holds.   This can be seen as one possible explanation for the success of training deep neural networks, because all local minima are global minima, and most saddle points have negative eigenvalues to escape. (Although assumption A5u-m which says the activation of any path is independent of the input data, is not realistic)","In terms of technical quality: The whole proof is very intricate, and I didn't follow A.6 and B.1. But other parts seem correct to me.  The lemmas provided in section 4 are intuitive and helpful, which characterizes the first and second order information of the critical points. The proof sketch provided in the main paper seems correct to me (except the one for Theorem 2.3(ii) which I didn't follow).  Since I'm not sure about the correctness of the whole proof, but the parts that I checked seem correct, I give 3 points for technical quality.   In terms of novelty:  The paper is about proving a conjecture and (partially) solving an open problem. Both problems are important problems about training deep neural networks, and the paper gives general descriptions about the loss surface of linear network, as well as non-linear network (with additional assumptions). The loss surface of linear network, especially the properties of the saddle points and local minima, were not known previously. So I think the novelty is good.   In terms of potential impact:  If the proof and the claim are correct, I think this paper could be potentially very important.  Although the results on non-linear neural network are based on some unrealistic assumptions, they are fewer compared with the prior work. Moreover, the results on linear neural networks do not rely on any assumptions. Thus, it could help people understand the loss surface of neural network, and understand why simple first order optimization methods like SGD works for training neural networks.    In terms of clarity:  I feel the paper is well written. It would be better if the authors could write down all the discarded assumptions instead of just referring to their names A1p, A2p, A3p, etc. The proof sketch for Theorem 2.3 (ii) is still too abstract, a little bit more intuition might be more helpful.  MISC:  Line 59: ""which with"" -> ""with which"" Line 114: ""corolalry 2.4"" -> ""corollary 2.4""  Line 604: ""theorem 2.3 (i) ""-> ""theorem 2.3 (iii)"" Line 563: ""proof w"" -> ""proof we"" ",1-Less confident (might not have understood significant parts),"The main contribution of this paper is to prove that for deep ""linear"" networks, under some assumption on the training data and for the squared error loss function, every local minima is a global minima. They further show that picking a subset of assumptions in (Choromanska et al. 2015b) would reduce the model to a linear network.","Understanding the optimization in deep learning is very challenging and any progress in this area even with several assumptions is valuable. This paper addresses the optimization under 3 assumptions:  1- There is no activation function (deep linear networks) 2- Assumption on training data (XX^T and XY^T are full rank) 3- The loss function is squared error loss.  This is a significant contribution and almost all of the technical body of the paper are proofs that lead to the theorem 2.3 which is using the above assumptions. However, my main problem with the paper is with the claim about deep NON-LINEAR networks. I find it a bit misleading to claim something about non-linear networks but pick a set of assumptions that makes the network linear! How can someone call a network non-linear if the setting is such that the output of the network is linear in the input and the parameters? In particular, based on the assumptions on the non-linear setting, the loss of the network is EXACTLY EQUAL to the loss of the linear network and one can only understand this by looking at the appendix and the discussion in the main text is rather vague.  Also, authors claim that their statement is tighter than what is suggested in (Choromanska et al. 2015b). However, as far as I understand the assumptions 2 and 3 in this work are different than the ones in (Choromanska et al. 2015b). I would appreciate some explanation about that. I do agree that the assumptions seem more realistic but the problem is with general claims when such conclusions cannot be easily made.  Suggestions:  I think the paper will be much better if it is clear from the beginning that the contribution of the paper is on deep linear networks with above assumptions. The abstract is currently very misleading because the current paper is not proving anything about a really non-linear setting. I suggest removing the section on deep non-linear networks and instead briefly discuss that picking a subset of assumptions from (Choromanska et al. 2015b) would reduce their setting to linear networks which is analyzed in the paper. If you insist on having a separate theorem for deep non-linear networks, I think you should bring the discussions about the theorem from the appendix to the main text and make it clear that the network is in fact linear after adopting those assumptions.  Finally, I think section 4 needs more text that explains intuitions behind lemmas or otherwise it is not really helpful to have the lemmas in the main text.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,
On Robustness of Kernel Clustering,"Bowei Yan, Purnamrita Sarkar",https://proceedings.neurips.cc/paper/2016/hash/b5a1fc2085986034e448d2ccc5bb9703-Abstract.html,"This paper shows that in the context of kernel clustering, an SDP relaxation provides strong consistency and better performance with respect to outliers when compared to an SVD relaxation.  ","Although the main findings and analysis look promising and it is interesting for the community, I'm concerned about the following:  Assumptions:  * It is very unrealistic to assume that all clusters have the same size.  What happens if this assumption is lifted ?   * The number of clusters k is known.  This is also unrealistic since in most real-life problems, k is not known.    * The centroids are well-separated.  Again this casts doubts on whether the analysis is valid in real-life problems.  Experiments:  * All experiments are performed using toy problems and in some cases, the parameters seem to be set arbitrarily.    *  Same \sigma, same number of clusters k.  These parameters are critical in kernel clustering and the current state of the experiments fails to convince the reader.   ",2-Confident (read it all; understood it all reasonably well),"This paper introduces a semidefinite programming relaxation for the kernel clustering problem, then proves that under a suitable model specification, both the Kernel-SVD and Kernel-SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent. Experimental results on toy datasets are presented.","Here are few comments.   1. The analysis assumes that all inlier clusters are of equal size and uses this fact for analysis. However, this seems to be a very restrictive assumption. In real life, it is very unlikely that all clusters will be of equal size. It will be nice to have a discussion describing how much additional work is necessary to remove this unrealistic assumption.  2.  Using the current notation both the algorithms takes only inliers as input (Y-1,... Y_n are inliers). It will be nice to change the notation so that input to both algorithms can be both inliers as well as outliers.  3. Spectral clustering is based on normalized/unnormalized ""Laplacian matrix"" and not kernel matrix. It will be nice to have a discussion how they are related. Note that in case of spectral clustering using Laplacian matrix, eigenvectors corresponding to the lowest eigenvalues of Graph laplacian is used for embedding and k-means clustering, where as in case of kernel-PCA eigenvectors corresponding to the largest eigenvalues of the Gram matrix are used for k-means clustering.  4. Theorem 1 is based on Gaussian kernel (line 135) so is Lemma 1. Therefore, notation wise it will be easier to use K, \tilde{K} and exponential in place of function f etc.  5. The theoretical results of this paper are "" high probability"" results based on theorem 1. Theorem 1 uses an union bound and the failure probability is n^2p^{-c^2\rho}, where c and \rho are constants. Note that c is controllable depending on how much deviation one wants but \rho is just an existential constant. Therefore, for fixed data dimension p, as n tends to infinity the failure probability tends to one. For any fixed p, this limits the maximum size of inliers that the algorithm can handle. In particular if we want a failure probability of \delta, that implies n can be at most \sqrt{\delta p^{c^2\rho}}$.   6. Statement of Lemma 1 is a little unclear. In particular, the inequality in the proof is not clear. Note that only a lower bound of the eigengap is proved. No upper bound Big-Oh is proved and therefore using \Theta notation may not be correct.  7. In the supplementary material (proof of Lemma 1), no justification is not provided for the inequality between line 67 and 68. It is not clear how this inequality is obtained.  8. In proof of theorem 3, first only inliers are considered when no outliers are present and Davis-Kahan theorem is applied. However, in this case the corresponding expression, equation 4, involves $m$ which is the number of ouliers. This sounds strange because this analysis does not consider any outlier and since presence of ouliers are handled right after equation 4 and is given in equation 5.  9. Proof of theorem 3 utilizes results of [22]  which requires that centroids are well separated. It guaranteed in proof of theorem 3 that the centroids are well separated  so that results of [22] can be applied.   10. The accuracy of the clustering process is measured based on the criteria that inliers are assigned to the correct respective clusters. However, in presence of outliers, if an outlier is assigned to a cluster (consists of inliers) that is also an error.  How is this error taken into consideration in the analysis?   11. All experimenal results are on toy datasets involving same \sigma for all clusters. Experimental results can be improved by considering, (a) different sigma values  for different clusters in case of toy (synthetic) datasets, (b) varying dimensionality p for synthetic datasets (choce of specific p=160 seems arbitrary),  (c) applying the proposed method on realworld datasets.",2-Confident (read it all; understood it all reasonably well),In the paper the robustness of kernel K-means is analyzed and performance guarantees are given for sub-Gaussian distributions  for sparse cluster means and in presence of outliers. The results are supported by proofs.,"some details are not made explicit in the paper such as the mutual convergence of p/n. If p/n->0 as is often assumed in classical statistics, [11] does not apply here since it requires p/n --> c != 0 which is another regime not fulfilling some basic convergence theorems of classical statistics.   The authors declare in which asymptotic regime they are operating to make clear for which kinds of parameters their theorems hold. At several points the authors should be more explicit about the variables and the terminology used. I had to look up the meaning of some variables in the additional material. Instead of writing: T(n) is O(n) iff... it should be something like: A function T(n) is O(n) iff... I was confused and started looking for the definition of T(n) first :)  I don't see why the affinity matrix in Assumption 1 is also a Gram matrix. A Gram matrix is defined as G := X^TX. Hence, this would apply to the inner product kernel only.  The notation is introduced at two places in 2 (at the very beginning and the very end).  You should make it appear at only one place so that a reader nows where to look it up.   The discussion of the asymptotic regime should definitely be fixed before publication. Apart from that it seems to be a very favourable and mathematically rigorous work with comprehensible proofs. The language is very clear and free of mistakes. ",2-Confident (read it all; understood it all reasonably well),"As claimed, semidefinite programming relaxation is, at the first time, used to do kernel clustering. It is shown that proposed algorithm achieves better consistency and robustness in some sense than kernel SVD algorithms. ","Overall looks good. The authors did shed lights on the robustness of kernel clustering algorithms. The following are a few questions I have: Line 138: wouldn't it make more sense to define $\tilde{K}_f as f(\sigma_k^2+\sigma_l^2) if i=j$? Line 156: more justification for the positiveness of $r_{min}$ Line 161: the concentration of $K$ from Theorem 1 is for a submatrix of $K$ (corresponding to the index set $I$), how to extend the results for $K$? Figure 1: I saw some potential contradictions between (b) and (c). If we focus on the performace of SDP, in (b) it shows the accuracy is roughly 0.9 when m=60 and d^2=0.08, while in (c) it shows the accuracy is roughly 0.45 with the same parameters. So what changes here? Plus, there should be more explanations about the parameters. For example, what is $d^2$ (I assume it is min distance between centers)? Moreover, wouldn't it be more persuasive if more experiments are conducted on some real data sets?",1-Less confident (might not have understood significant parts),"The robustness of kernel clustering has not been sufficiently explored yet. The paper proposed two methods for kernel clustering, i.e. the semidefinite programming relaxation for the kernel clustering problem (SDP) and kernel singular value decomposition (K-SVD), and investigated the consistency and robustness of two kernel-based clustering algorithms.","1. K-PCA is a technique for dimensionality reduction, it is ambiguous that the paper considers it as a kernel clustering algorithm without any extra stating (line 101).   2. It seems that the paper applies a new SDP relaxation to the kernel clustering for the first time according to the author(s), but just reminds there exist the same relaxation in previous work (line 107), lacking of explanation for why it is reasonable and effective to do so.   3. The paper does not give analysis on robustness of other kernel clustering methods based on semidefinite relaxation (if there exist such method), thus lacking of comparision to show the essentialness for proposing the SDP relaxation for kernel clustering.  4. The experimental results on synthetic data set seem not to be convincing, tests on real data sets are needed.  5. Some terms and notations may be ambiguous without special explanation, e.g. ""population matrix"" (line 136), ""[n+m]"" (line 63).   ",2-Confident (read it all; understood it all reasonably well),"The authors provide a robustness analysis of two popular forms of kernel k-means clustering:  a semi-definite programming relaxation and a kernel singular value decomposition.  The goal is to understand behavior in the presence of (a possibly growing number of) outliers, in particular, consistency. They adopt a problem set-up that is well-matched to k-means (e.g. equal-sized clusters) and partition the data into groups of inliers and outliers.  The framework is also well suited for scenarios where the data lie on a lower-dimensional manifold that may be obscured.  Their theoretical analysis and results are accompanied by a very nice summary, one that will serve audiences not as familiar with the technical details of the proofs.  In short, SDP is the winner, showing strong consistency when there are no outliers and also better performance when the number of outliers grows slowly as compared to the number of observations.  The numeric analyses show general agreement with the theoretical results. ","In general, I thought the paper was well-written and made some nice theoretical contributions to this area.  What I valued most was the authors' ability to explain the theoretical results conceptually along side the technical details.  This is hard to do and not that common.  The paper's audience and influence will be larger for it.    I would have liked to see some more discussion about the assumption that the noise is sub-gaussian and isotropic.  Something along the lines of how limiting those assumptions might be in practice and if some of their results might still apply (via some sort of approximation).  In the experiments, the authors use an extra cluster for outliers, but it seems odd to assume that the outliers would have a similar distribution or structure as the actual clusters.  In mixture model clustering, it is common to assume something like a uniformly distributed cluster to pick up the noise/outliers or to use a contaminated distribution to pick up outliers.  Treating all the outliers like they're a group of the same size and distribution as the rest of the inliers sounds unrealistic.   p.2, line 76, should be ""where the data lie on a low-dimensional manifold""  p.3, line 111, should be ""One first does SVD on …, then does k-means on the first k eigenvectors""  p.5, lines 182/183 - inconsistency in algorithm notation  p.5, line 185 - K-means should be capitalized (it is on p.7)  p.8, line 275, should be ""K-SVD is doing almost as well""  p.8, lines 285-286, should be ""as the number of observations grows and the outliers are a small fraction of the inliers"" ",2-Confident (read it all; understood it all reasonably well),,,,,,
An Online Sequence-to-Sequence Model Using Partial Conditioning,"Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, David Sussillo, Samy Bengio",https://proceedings.neurips.cc/paper/2016/hash/312351bff07989769097660a56395065-Abstract.html,"The paper proposes a ""neural transducer"" model for sequence-to-sequence tasks that operates in a left-to-right and on-line fashion.  In other words, the model produces output as the input is received instead of waiting until the full input is received like most sequence-to-sequence models do.  Key ideas used to make the model work include a recurrent attention mechanism, the use of an end-of-block symbol in the output alphabet to indicate when the transducer should move to the next input block, and approximate algorithms based on dynamic programming and beam search for training and inference with the transducer model.  Experiments on the TIMIT speech task show that the model works well and explore some of the design parameters of the model.","This is a well-done paper.  It attacks a problem that is worthwhile:  how to construct and train a sequence-to-sequence model that can operate on-line instead of waiting for an entire input to be received.  It clearly describes an architecture for solving the problem, and walks the reader through the issues in the design of each component in the architecture:  next-step prediction, the attention mechanism, and modeling the ends of blocks.  It clearly explains the challenges that need to be overcome train the model and perform inference with it, and proposes reasonable approximate algorithms for training and inference.  The speech recognition experiments used to demonstrate the utility of the transducer model and to explore design issues such as maintenance of recurrent state across block boundaries, block size, design of the attention mechanism, and depth of the model are reasonable.  There are a few issues that should be addressed to improve the paper.  Page 3, line 93:  ""We first compute the probability of l compute the probability of seeing output sequence""  -- editing problem.  Section 3.4:  I was disappointed that there was no discussion of how well or poorly the three different approaches to end-of-block modeling worked.  I assume that the use of the < e > symbol was best, but how much worse were the other two methods?  Section 3.5:  Computing alignments less frequently than model updates is also quite reminiscent of the use of lattices in sequence-discriminative training of acoustic models in speech recognition, as in D. Povey and P. C. Woodland, ""Large scale discriminative training for speech recognition,"" in Proc. ASR Workshop, 2000, http://www.danielpovey.com/files/asr00.pdf, and B. Kingsbury, ""Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling,"" in Proc. ICASSP, 2009, https://www.semanticscholar.org/paper/Lattice-based-optimization-of-sequence-Kingsbury/2443dc59cf3d6cc1deba6d3220d61664b1a7eada/pdf  In future work on the transducer model, it might be of interest to consider using lattices to represent alignment information. Methods developed for rescoring lattices using RNN language models might be useful for dealing with the fact that the neural transducer is conditioned on all of the input up to the present moment and all of the output labels generated up to the present moment.  See, for example, X. Liu, Y. Wang, X. Chen, M. J. F. Gales, and P. C. Woodland, ""Efficient lattice rescoring using recurrent neural network language models,"" in Proc. ICASSP, 2014, http://mi.eng.cam.ac.uk/projects/cued-rnnlm/papers/RNNLM_latrescore.pdf  Page 7, lines 202-203:  ""Log Mel filterbanks"" -> ""Log Mel spectra""  Section 4.2:  It appears that the TIMIT experiments are not entirely standard.  It is typical to collapse the TIMIT phones more than was done in this paper, and to specifically exclude certain data from the training set and to report results only on the core test set.  See, for example T. N. Sainath, B. Ramabhadran, M. Picheny, D. Nahamoo, and D. Kanevsky, ""Exemplar-Based Sparse Representation Features: From TIMIT to LVCSR,"" IEEE Transactions on Speech and Audio Processing, 19(8):2598--2613, Nov. 2011, https://36e9b848-a-62cb3a1a-s-sites.googlegroups.com/site/tsainath/tsainath_tsap2010_submission_2column_final.pdf or K. F. Lee and H. W. Hon, “Speaker-independent Phone Recognition Using Hidden Markov Models,” IEEE Transacations on Acoustics, Speech and Signal Processing, 37:1641–1648, 1989, http://repository.cmu.edu/cgi/viewcontent.cgi?article=2768&context=compsci The TIMIT experiments would be more easily compared to others if the standard framework were followed and this was clearly stated in the paper.  Section 4.2:  You should have tried initializing the transducer by training on the HMM-GMM alignments, and then continuing training with aligments inferred using the transducer.  This might have led to even better TIMIT performance. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors propose a neural network method for learning to map input sequences to output sequences that is able to operate in an online fashion. Like similar models of this type, the input is processed by an encoder and a decoder produces an output sequence using the information provided by the encoder and conditioned on its own previous predictions. The method is evaluated on a toy problem and the TIMIT phoneme recognition task. The authors also propose some smaller ideas like two different attention mechanism variations.","I find the subject matter of the paper interesting as sequence/structured prediction with neural networks is still an open problem and I agree with the authors that there's a need for methods that can do online sequence processing and prediction.   The biggest issue I have with the paper is that the term 'transducer' has been used multiple times before in the context of neural sequence prediction and the authors don't cite this work or discuss how it relates to their own method. This is somewhat surprising, because the authors do cite a paper in which both CTC and the transducer from Graves (2012) are evaluated and the transducer actually outperforms CTC. The transducer by Graves also takes into account previous predictions while still allowing for dynamic programming style inference as in CTC. This method differs from the method proposed by the authors and doesn't include the feature to process the input data in blocks but should be an important baseline to compare with. The existence of this prior work (and for example the work by Boulanger-Lewandowski et al., 2013) limits the novelty of the ideas presented in the paper and the extend to which the current title of the paper is appropriate. Perhaps something like ""An Online Neural Transducer"" or ""An Incremental Neural Transducer"" would shift the emphasis in the right direction.  I find the empirical work somewhat limited. The results on TIMIT are decent, but don't really show the benefits of the new method in comparison to existing algorithms. I don't find the argument that it was beyond the scope of the paper to use regularization methods to get the best results possible convincing when the related work they compare with also uses an attention mechanism and windowing. That said, I did like the comparison with a model in which the RNN state is reset between different blocks and the plot about the relation between the window size and the use of attention mechanisms. All in all, it would have been interesting to see results on a dataset with longer sequences (like Wall Street Journal or Switchboard), where the new method may actually have a significant advantage.    ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",The authors present a new sequence-to-sequence model architecture that processes fixed-size blocks of inputs using an encoder RNN and generates outputs block-by-block using a transducer RNN; the latter receives its inputs from the encoder RNN and maintains its state across blocks using recurrent connections. This class of models is aimed at online tasks like speech recognition that require predictions to be made incrementally as more data comes in.,"This paper is well written and the idea of using a blocked transducer is novel. On the TIMIT core test set, the proposed online model performs quite comparably (19.8% PER) to the best-reported offline sequence-to-sequence model (17.6% PER), without using very carefully trained models.    Since this model is designed to alleviate attention issues that particularly affect long utterances in sequence-to-sequence models, it would be interesting to see how the performance of the proposed model varies as a function of utterance length. One suggestion to make room for this experiment in the paper would be to drop the addition toy task which can be omitted without losing the flow of the narrative.   There are a couple of missing implementation-specific details that’ll be useful for the reader to know:  * In the comparison model (with a PER of 18.7%) that used a bidirectional encoder, were bidirectional features computed after seeing the end of an input block or after seeing the entire utterance? The latter would mean that the model is no longer offline. This should be clarified in the write-up.   * What value of M was used in the experiments? (M appears in the DP algorithm used to compute alignments during training.) What beam search width was used during inference?   As the authors have mentioned, the results reported in Table 2 could be improved by using better regularization techniques. Another way to potentially boost the performance of these models is to use better acoustic features (with speaker-dependent transforms). Lu et al., “Segmental Recurrent Neural Networks for End-to-end Speech Recognition”, Interspeech 2016 shows significant PER improvements on TIMIT obtained by using better acoustic features.  When the authors say that the alignments from the proposed model are similar to the GMM-HMM alignments, do they mean this in terms of per-frame phone error rates? Clearly, the models benefited from using the GMM-HMM alignments as evidenced by the final PER numbers (19.8% vs 20.8%). What could this be attributed to, if the alignments were very similar?   Some minor edits: — pg.3, typo in “compute the probability of 1 compute” — pg.4, “is in computed in” —> “is computed in” — pg.8, “showed that is can” —> “showed that it can” — argmax should be a single operator and not “arg max” in Eqn 12. — use \mathrm for the softmax function in Eqn 8.",2-Confident (read it all; understood it all reasonably well),This paper describes an online sequence-to-sequence model that emits output incrementally as it processes blocks of an input sequence. The map from block input to output is governed by a standard sequence-to-sequence model with additional state carried over from the previous block. Alignment of the two sequences is approximated by a dynamic program using a greedy local search heuristic. Experimental results are presented for phone recognition on TIMIT.,"An incremental transducer is a natural extension of sequence-to-sequence models. Experimental results on TIMIT indicate that this model can be effectively optimized. I would guess that this model, or future iterations of it, will find lots of applications to data streams.  A minor nitpick: the two tables in section 4.2 with two rows each seem a little tacky. Is there a better way to present this data? Also, column 1 of table 2 is empty.",2-Confident (read it all; understood it all reasonably well),"The authors introduced an encoder-decoder architecture with attention over blocks of input and variable length decoder structure. The encoder is a multi-layer LSTM RNN. The decoder is an RNN model conditioned on weighted sums of the last layer of the encoder and it's previous output. The weighting schemes (attention) varies and can be conditioned on the hidden states or also previous attention vectors. The decoder model produces a sequence of symbols, until it outputs a special end character ""e"" and is moved to the next block (other mechanisms where explored as well (no end-of-block-symbol and separately predicting the end of a block given the attention vector). It is then fed the weighted sum of the next block of encoder states. The resulting sequence of symbols determines an alignment of the target symbols over the blocks of inputs, where each block may be assigned a variable number of characters. The system is trained by fixing an alignment, that approximately resembles the best alignment. Finding this approximately best alignment is akin to a beam-search with a beam size of M (line 169), but a restricted set of symbols conditional on the last symbol in a particular hypothesis (since the target sequence is known). Alignments are computed less frequently than model updates (typically every 100 to 300 sequences). For inference, an unconstrained beam-search procedure is performed with a threshold on sequence length and beam size.","First of all, I think the paper is well written and clear.  Judging by the title the main contribution of this model is a novel model architecture. Looking at the references, there have previously been speech recognition systems, that use attention ([2] and [5]). Sequence transduction has also been introduced by Graves in ""Sequence Transduction with Recurrent Neural Networks"" in 2012. The follow up paper is cited as [7] uses this transduction method. Grave's describes his transducer to ""extend CTC by defining a distribution over output sequences of all lengths, and by jointly modeling both input-output and output-output dependencies."". To me the novelty of this paper then is the idea of applying the transducer on a block-by-block basis and defining a RNN conditional on it's last definite output. Other methods integrate over alignments using the output of an RNN, whereas here the authors define a separate complex network structure to perform the transduction. However, Graves and Hinton also use a separate prediction network to feed into the transduction graph. To me, the disadvantage of the method presented here is the complicated training procedure, since exact constrained inference becomes intractable. I'm am also not sure how to judge the novelty of this approach, because I might be missing literature, since there has been a lot of developments in this area. It seems very similar to Graves and Hinton [7], especially since they also use the separate prediction network, except for the attention mechanism (but in later experiments we see, that the attention mechanism doesn't necessarily help that much (unless larger window sizes have advantages (one of which might be speed))).  The experimental evaluation of this method is performed on an addition toy task and TIMIT. For the addition task the authors constrain themselves to adding two three-digit numbers. It would be interesting to see how the model generalizes to longer numbers. That is, if trained on additions of three-digit numbers, how well does it add four-digit numbers?  For TIMIT the authors first compare having recurrent state across blocks, vs. not having a recurrent state. I think, if you don't have recurrent state in the transducer, you might be able to perform more efficient inference using Graves' transduction method. I'm not too sure on this point. Figure 3 shows that for some block sizes no attention performs equally well. I think no-attention means, that the encoder hidden states are averaged per block and then fed to the transducer. This is then akin to a convolution of size W and stride W. Maybe it is interesting to see how the encoder would perform, if W states are combined and fed into a simple RNN, that produces a softmax over symbols, which is then trained with CTC or Graves' Transducer. The best performance of Graves' Transducer on a unidirectional LSTM RNN has been reported at 19.6%, which is close to the best 19.8% reported here, albeit I understand, that the authors argue, that this model needs to be tuned more, which can be in itself a lot of work. Their best network uses a 3 layer LSTM RNN transducer. I think four layer transducers might vary much in accuracy, because they start overfitting a lot on TIMIT. It would be interesting to see how much these models overfit.   Overall, given the complexity of the model and the performance, plus similarities to other methods, I'm not very sure how interesting this method is (I might lack the breath in literature to point out exactly how similar this model is). I would be interested in understanding how exactly it differs from Graves' and Hinton's Transducer network. I am not very sure on my review of the novelty of this method. The performance in itself is competitive on TIMIT, but other, simpler methods perform equally well. I am not sure if the extra capacity and context is necessary in comparison to other methods or for this dataset. Perhaps, if the model was trained on more data, it would become more apparent, but I'm not sure how well it scales. I wonder how long it takes to fully train a model? ",1-Less confident (might not have understood significant parts),,,,,,,,,
Bayesian Intermittent Demand Forecasting for Large Inventories,"Matthias W. Seeger, David Salinas, Valentin Flunkert",https://proceedings.neurips.cc/paper/2016/hash/03255088ed63354a54e0e5ed957e9008-Abstract.html,"       The paper proposes to use Bayesian inference i.e. the Laplace approximation in a generalised linear model with a multi-stage likelihood to perform demand forecasting for intermittent and bursty time-series data based on temporal features such as holidays and seasonality. The authors suggest the ""twice logistic link function"" as a replacement of the exponential or logistic link function. Newton steps for mode finding are reduced to Kalman smoothing to make the method scalable. A set of experiments (partly on public data) are conducted to illustrate and benchmark the approach.    ","       I like about the paper that it describes many important aspects needed to make a Bayesian inference useful in a production system. Hence it is a very practical contribution.        Unfortunately, the exposition of the technical parts is very condensed which makes the material hard to assess. A more pictorial description/illustration of the model in particular the multiple stage and the latent state aspects would make the paper much stronger.        All items are dealt with in isolation. There is likely a lot of similarity among items of similar type. This aspect is not covered by the model.         There are unclear points in the experiments:       a) How does the ""LS-pure"" method work without features?       b) Why is the third row in table 1 left i.e. ""LS-feats"" for the ""Parts"" dataset missing?       c) How many features are used in the end? Can one interpret the w vectors?    ",2-Confident (read it all; understood it all reasonably well),"The paper proposes a Bayesian model combining generalized linear models with time series smoothing, tailored for probabilistic forecasting of consumer good demand in retail. It provides many details on the practical implementation and shows comparison with state-of-the-art methods on real world data sets.","The potential impact is high, since this has become an important application domain for machine learning methods. The technical quality appears solid; also on a positive note, the paper provides details on the implementation which may be of interest to practitioners in this domain. I see a couple of shortcomings: - The originality is limited. The authors need almost an entire page to differentiate their work from Chapados (ICML 2014), which indicates that the improvements might be somewhat incremental. - The authors emphasize at several places that their methodology is part of a production system, running on Apache Spark. To make those claims relevant to a scientific publication, more information would be required, e.g., about the number of cores, running time, number of models, actual data volumes etc. - The experimental evaluation could be improved. Figure 2 suggests that the model might be prone to overfitting. Figure 3 (a) suggest that the main benefit of the proposed model is that it better captures seasonal effects, which could be achieved using much simpler models (e.g., generalized additive models). I am confused that, on the other hand, for the weekly risk forecasts in Figure 3 (b) and (c), the competitor methods do capture a seasonal effect. Maybe I am missing something here? - A minor comment: in the outlook (line 302-305), the authors mention the importance to model the dependency between different items in future work, however, it appears that some work in this direction has already been done by Chapados (ICML 2014). ",1-Less confident (might not have understood significant parts),This paper deals with the temporal problem of demand forecasting at-scale.   The approach is to combine Gaussian smoothing (temporal) with Generalized Linear Models (uncorrelated).  A key part of this paper are the numerical methods employed to enable training and prediction at scale.     ,Nice paper Clearly explained and well written Deals with important problem and uses real-world data Generous description of key details needed to get the approach to work robustly at scale (Section 3.2) Very nice and honest discussion of related work especially Lines 183-219. solid empirical evaluation    ,2-Confident (read it all; understood it all reasonably well),The paper proposes a demand forcasting model that combine the merit of generalized linear models and exponential smoothing. It also considers the multi-stage likelihood case. The experiment is solid. My main concern is the novelty of the model. ,"Since generalized linear models do not include any temporal dependencies, the authors then borrow ideas from exponential smoothing to include the latent states. The linear dependencies among latent states then carries information from the past to the future. There are some technical problems for maximum likelihood estimation. For example, the non-Gaussianity of p(z|y) and the overall optimization for theta. It seems that the authors properly solve these technical problems while maintaing the efficienty of the algorithm. The experiments show that when more features are available, LS-feats outperforms other methods. While NegBin might be better when no features are available. The methods this paper propose can scale to very large datasets.   However, the contribution of the paper seems incremental, since it barely combine several existing ideas without introducing new modules to solve their problems. But considering the effectiveness of the algorithm shown in this paper, I would give a borderline score.",2-Confident (read it all; understood it all reasonably well),"This paper describes a method for intermittent demand forecasting that combines GLM with time series smoothing. Approximate inference method is used to enable non-Gaussian likelihood, and Newton-Raphson method is used to improve convergence.","This paper builds upon previous work and combines GLM with time series smoothing for demand forecast in short and long term scale. The author claimed empirical evidence for the usefulness of the deterministic linear part is provided, which is a critical difference from [6]. However I don't see relevant analysis on the deterministic linear part in the experiment section.  I found the paper very hard to follow: 1) a lot of footnotes are used which are distracting and fail to provide clear explanation in many places; 2) a lot of technical/mathematical terms without proper explanation, e.g., what's the intuition behind multi-stage likelihood at Line.68? The author only gives the mathematical formulations which cannot be straightforwardly understood. What's (7X,7Y) at Line.268? dL/dS at Line.267 is misleading.   In Figure.2, the author claimed demand becomes uncertain in out of stock region. But even in in-stock region the demand can also be uncertain, such as the period between Mar.2015 to Sep.2015 in the right-most plot.  The empirical results show the proposed method brings only a little improvement. In Fig.3(a) the new method cannot predict the peak around Christmas holiday, which is claimed to be a strength in the paper. Also, in Tables.1-2 the improvement looks not significant.  The paper claims the proposed method is scalable. However, no empirical results on time cost of the models are shown. The only related ""result"" is that the authors apply their models on some large datasets. However without results on time cost w.r.t data size, it's not enough to validate the scalability.",2-Confident (read it all; understood it all reasonably well),"The paper aims to forecast accurate probability distributions of demand, particularly when the data is count or bursty as in an e-commerce setting. To do this, state space models are extended as in the case of GLMs, with a double logistic link function, and algorithms are proposed for maximum likelihood learning of parameters.","The authors should familarize themselves with the work on State Space Models in statistics: a useful starting point would be Durbin, James, and Siem Jan Koopman. Time series analysis by state space methods. No. 38. Oxford University Press, 2012. There is a big section of the book devoted to non gaussian models, and they would also find discussion on adding regression effects, referred to as ""modeling of deterministic part"" in the paper, line 27-28. In particular, closely related papers would be:        Fahrmeir, Ludwig, and Stefan Wagenpfeil. ""Penalized likelihood estimation and iterative Kalman smoothing for non-Gaussian dynamic regression models."" Computational Statistics and Data Analysis 24.3 (1997): 295-320.    Durbin, James, and Siem Jan Koopman. ""Time series analysis of non‐Gaussian observations based on state space models from both classical and Bayesian perspectives."" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 62.1 (2000): 3-56.    The above papers discuss how finding the mode reduces to kalman Smoothing, and simplifying the non-gaussian likelihood by linearising it around its mode.     Its hard for me to judge the paper without any discussion on relation with very closely related papers. However, based on my understanding a lot of the technical contributions of the paper are well known. More detailed comments follow below.     1.) How is the proposed method bayesian ?    2.) The abstract says you pay special attention to ""intermittent and burst"" items, but then at the end, it is mentioned that improvements are for fast and medium moving items. Isnt this contradictory ? How can an item be both intermittent and fast moving at the same time ?     3.)12-13. ""Classical forecasting methods produce gaussian distrbutions only"": not true. See the references I cited.    4.)18-19. very crucial point. should have been emphasized and elaborated on more.    5.) line 28: again not true representation of related work.    6.)Discussion in 3.1 and 3.2, are very hard to follow. Since this is a core and crucial technical contribution of the paper, its important to be presented in a clear way.    7.) Eq. (3) is non-standard, and most people with Math/Statistics background would find it puzzling since measurement error is completely skipped. I know this notation is from one of the references, but it would be better to follow standard notation which is more widely followed.    8.)Section 5.1 is fairly obvious and can be skipped.   ",2-Confident (read it all; understood it all reasonably well),,,,,,
Unified Methods for Exploiting Piecewise Linear Structure in Convex Optimization,"Tyler B. Johnson, Carlos Guestrin",https://proceedings.neurips.cc/paper/2016/hash/cb2c2041d9763d84d7d655e81178f444-Abstract.html,"The authors propose new algorithms for minimizing composite objectives comprised of strongly convex and piecewise (?) components. Apart from numerous claims about importance of the paper contribution, the authors propose a “working set” algorithm and a piecewise screening test, along with an experimental study which seems to show good performance in some application scenarios. ","The authors seem to be unaware of the literature about bundle methods (see [Hiriart-Urruty, Lemaréchal, 1993] for the references). The proposed “working set” algorithm looks like a bundle family method. Note that the “basic” bundle algorithm – the Kelley method – also often shows fast convergence. Nevertheless, it minimax complexity is disastrous (although in the strongly convex situation it exhibits dimension-independent convergence rates, which are suboptimal). Same as a majority of work on bundle algorithms, the authors provide some bounds for the model gap, but do not seem being interested in providing any guaranties. My feeling is that to legitimate this type of methods (which “often” exhibit “fast convergence”), the minimax analysis is necessary (cf. methods of bundle-level family, [Lemarechal et al, 1995, Lan 2015].  Hiriart-Urruty, J. B., & Lemaréchal, C. (1993). Convex analysis and minimization algorithms II: Advanced theory and bundle methods, vol. 306 of Grundlehren der mathematischen Wissenschaften. Lemaréchal, C., Nemirovskii, A., & Nesterov, Y. (1995). New variants of bundle methods. Mathematical programming, 69(1-3), 111-147. Lan, G. (2015). Bundle-level type methods uniformly optimal for smooth and nonsmooth convex optimization. Mathematical Programming, 149(1-2), 1-45. ",1-Less confident (might not have understood significant parts),"The paper studied the problem of selecting important components to solve some particular convex optimization problems faster.  Two selection methods are considered to exploit the structure.  Two methods for exploiting structures are considered. One is screening methods, the other is working set algorithms. The theoretical analysis of the two methods are unified to solving the particular problem, i.e., minimization of a sum of piecewise functions.  The algorithms based on working set technique and screening test are proposed and experimentally tested.   ","Overall, this paper is well rewritten.  The problems and contributions have been clearly identified.  The theoretical and experimental results have been clearly presented.   1. The paper proposed the piecewise optimization framework which enable the unified theoretical analysis of working set and screening tests possible.  2.  Theoretical results have been established for the proposed working set algorithms. ",2-Confident (read it all; understood it all reasonably well),"This papers proposes a working set algorithm for solving a sum of piecewise functions. Based on it, the author introduces a piecewise (in optimization) screening test.  ","Algorithm 1 is a generalization of BLITZ (working set algorithm for sparse optimization). One advance is that it can handle more formulations e.g. the objective of SVM ( l2 regularization hinge loss ).  But I have a question about the per-iter backtracking line search for selecting y. For certain problems, this step may have closed form solution, but in general the overhead could be large. Besides, as the author mentioned, testing the conditions C1 - C3 can be time consuming (I think C2 & C3 may even be feasible only when the problem has good structure. Please clarity this point.). Selecting the radius tau in every iteration may be challenging, too. This might limit the set of problem Algorithm 1 can apply to.   From the experiments, we do see the working set idea improves the results, as the author acknowledged, the effectiveness of piecewise screening is not clear. Algorithms performs roughly the same without or without piecewise screening, especially for larger m. Besides, the group lasso experiment is in a low dimensional setup. It’s more interesting to see compare the performance in high dimensional setting, p >> n.",2-Confident (read it all; understood it all reasonably well),This paper provides a unified framework to exploit the piecewise structure of a class of optimization problems. Extensive numerical studies are provided to backup the methods. ,"This paper provides a unified framework to exploit the piecewise structure of a class of optimization problems. Extensive numerical studies are provided to backup the methods. The concept is somewhat interesting and novel. However, I do think that there are some issues that I hope the authors should address:  1. The presentation is not very user-friendly. The authors assume a ""piecewise"" structure of $\phi_i$'s. What does piecewise mean here? My understanding is that the authors assume that these functions are linear on the boundary while more complicated in the interior. Can you find a better name to describe it? Could you provide some geometric illustration? This will help authors to understand it.  2. All the theorems in the paper are like Lemmas to me. This is because these theoretical results are more like building blocks towards a main theorem. What I am expecting here is some thing like f(xt) - f^* \le O(1/t). Am I missing something here? Also, the screening results are not intuitive to me. Can the authors present them in a more user-friendly way?   ",2-Confident (read it all; understood it all reasonably well),"This paper presents an unified framework for solving a class of piecewise convex functions. Two specific algorithms based on working set and screening test are proposed. The proposed methods are validated on group Lasso and linear SVM. This paper is clear written and solid. However, since some structure output prediction problems are special case of the proposed problem formulation, it seems necessary to empirically and theoretically investigate the relationship between the proposed method and existing results of working set methods along those lines, e.g., see ``Cutting-Plane Training of Structural SVMs”.","In the section of experiments, it would be nice to compare with other related methods which suit for these problems, like cutting plane methods, sub-gradient projection etc. It is not clear why authors only compare the proposed methods using dual coordinate descent on SVM.  As author mentioned, the chosen of \beta_{t} is important for practical problems, it would be good to investigate how the performance varies with different updating scheme and values.",2-Confident (read it all; understood it all reasonably well),"This paper presents a meta optimization algorithm that exploits piecewise structures in the cost function. For large scale problems, handling piecewise linear functions can be costly, which can be significantly reduced by identifying the structure. The paper provides a bound on the relative decrease in the gap between the lower bound and the current estimate, which can be useful for analysis. Based on this bound, the authors propose two ideas for the proposed algorithm: piecewise working set and piecewise screening. The paper provide empirical results on two examples; group lasso and SVM, and shows that the piecewise working set algorithm can achieve faster convergence.","Overall, I think the issue that the paper is tackling is quite interesting. Most important result in the paper is Lemma 3.1, which can be a good guideline for designing a similar algorithm. Parts of the approach seems unclear, though. Since it is a meta algorithm, the choice of beta and tau will depend on the problem, which has to be tuned by user or designed separately for each problem.  There is a doubt regarding Algorithm 1. In line 120, C3 is about the ""upper bound on \phi' near x_{t-1},"" but the inequality is on the entire space, i.e., x \in R^n. Is this possible? If \phi_i^j and \phi'_{i,t-1}=\phi_i^k are the linear functions about different pieces (j \not k), this seems impossible. I'd like to see an explanation in the rebuttal.  It seems that the proposed algorithm shares many similar ideas with active set or trust-region approaches. An explanation about the relationship between them would be nice.  ",1-Less confident (might not have understood significant parts),,,,,,
Wasserstein Training of Restricted Boltzmann Machines,"Grégoire Montavon, Klaus-Robert Müller, Marco Cuturi",https://proceedings.neurips.cc/paper/2016/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html,"The paper proposes training RBMs using the gamma-smoothed Wasserstein distance W_{gamma}(\hat{p}, p_{theta}) (eqs 3 and 4), where  \hat{p} is the data distribution and p_{theta} is the model.  They  obtain derivatives of W_{gamma}(\hat{p}, p_{theta}) and train  with gradient descent.  However, the shrinkage of the Gaussian example in sec 4.3 (and the results in sec 4.2) gives serious concern wrt the consistency of Wasserstein training and this issue MUST be investigated more carefully before the paper is suitable for publication.","The paper refers to [2] and says that those authors proved statistical consistency. However, I am then surprised to see in section 4.3 that non-zero shrinkage is obtained (including for gamma=0) for the  very simple case of modelling a N(0,I) distribution with  N(0, sigma^2 I). What is going on here?? A failure of consistency  would be a serious flaw in the formulation of a statistical learning criterion.  Also in sec 3 (Stability and KL regularization) the authors say that at least for learning based on samples (\hat{p}_{theta}) that some regularization wrt the KL divergence is required. This clearly weakens the ""purity"" of the smoothed Wasserstein objective fn.  Experiments (sec 4) are carried out on MNIST-small (the 0s from MNIST), a subset of the UCI PLANTS dataset, and 28-d binary codes (MNIST code). The results (as reported in lines 168-172 and Figs 3 and 4) seem to produce ""compact and contiguous regions that are prototypical  of real spread, but are less diverse than the data"". This reinforces  my belief above (relating to the Gaussian example insec 4.3) that  minimizing the (smoothed) Wasserstein does not lead to a consistent density estimator.  In sec 4.4 the authors try to ""spin"" these weakness into strenghts wrt  data completion or denoising.   Overall: The authors are to be congratulated on investigating an interesting alternative to standard KL-based training of RBMs. However, the shrinkage of the Gaussian example in sec 4.3 gives serious concern wrt the consistency of Wasserstein training and this issue MUST be investigated more carefully before the paper is suitable for  publication.  Other points:  Title: the title over claims. The paper only discusses training *restricted* BMs.  Intro l 7: RBMs do not need to be restricted to binary x's, see e.g. Exponential Family Harmoniums  https://papers.nips.cc/paper/2672-exponential-family-harmoniums-with-an-application-to-information-retrieval.pdf  Fig 5 would be much more readable using different colors for the different gammas. Also last entry should be gamma=0 (not OpenCV). You can explain in the text that you used OpenCV to compute this.   === COMMENTS POST REBUTTAL =====  I have read the other reviewers comments and the rebuttal.  I will remove my charge of a ""fatal flaw"" based on the point that the sample size in the experiments in sec 4.3 is small. However, the KL version of the experiment (where we fit a Gaussian with cov matrix theta^2 I to n = 100 samples drawn from a 10-d Gaussian) the values of theta I get from the max likelihood estimator are all pretty much in the range 0.95-1.05 (for about 10 repetitions). This should be compared  with something around theta = 0.65 (estimated by eye from Fig 5(left))  for the Wasserstein (gamma=0) estimator.  The convergence rate given by the authors of O(n^-1/(10+1)) for  this simple problem is truly appalling, and presumably is  in general O(n^-1/(D+1)) for D dimensions. This gives rise to  serious concerns about the practicality of such a method.   It would be helpful to see a study in this simple Gaussian case, showing e.g. how the theta that minimizes W_0 varies with n, although the rate given above suggests convergence will be  painfully slow. Can you understand in such a case why there is  a shrinkage bias?   I note the response to R5 wrt lines 177-178 i.e.  ""the entropy regularizer contributes to the clustering effect [...]. It also appears for the true W distance in Fig 5.""  One could use say a KL matching criterion KL(\hat{p}||p_theta) along with an entropy penalty (no Wasserstein), and my understanding from the quote above is that this will cause clumping, whose virtues are extolled in sec 4.4 wrt data completion and denoising.  (Note that due to the KL regularization in section 3 this is partly happening anyway in many of the experiments).  I also agree with other reviwers who found the clarity of technical explanation poor, and suggested use of supp mat to help with this.  Overall, I am not really in favour of accepting this paper (although I could live with acceptance). There is quite a lot of interesting work, but the issue where the clumping comes from (Wasserstein or entropic prior) needs to be better articulated and dissected. I believe some of claims wrt Wasserstein training are actually to do with the entropic prior, and this should be teased out by comparing against a KL + entropic prior construction.     ",2-Confident (read it all; understood it all reasonably well),"This paper studies using Wasserstein Loss as the objective of learning generative models, with a focus on learning restricted Boltzmann Machines. It explored the effects of varying the strength of entropy regularization to the Wasserstein objective and its impact on image completion and denoising. ","I like using Wasserstein loss as an alternative to minimizing KL and agree that it can be more robust in some settings. One limitation of this approach is the computational cost. Even with the recently developed optimization techniques, this approached was only tried on small toy datasets by the authors.   It is interesting to see the effects of increasing the strength of entropy regularization (Figure 4). Can you give some intuition for why increasing lambda leads to models that are more concentrated in the image space?   Why is E_{\hat{p}}[\alpha^*] = 0 but in Eqn 5, E_{p}[\alpha^*] not 0?",2-Confident (read it all; understood it all reasonably well),"This paper proposes to train Boltzmann machines using a Wasserstein distance between data and model, rather than log likelihood. This is a neat idea, and is expressed clearly. Novel unsupervised training objectives are of potentially very high importance. I have concerns that, since the approach is sample based, it may scale particularly poorly with dimensionality and sample size. All experiments were on unusually small data, were heavily regularized with standard KL divergence, and only showed improvement in terms of Wasserstein-like measures of performance.","eq 4: sum over ""x, x'"", rather than ""xx'""  eq 4: I struggled a lot with this equation. Especially, it seems that it is only defined for gamma > 0, and it's very difficult to understand how the last term induces the correct dependencies. More explanatory text, and a change to stated gamma bound, would be good.  67: alpha* hasn't been defined  92-93: I suspect this approach, of measuring the distance between two distributions using samples from the two distributions, will suffer from exactly the same problems as Parzen window estimates (ie, sec 3.5 in L. Theis, A. van den Oord, and M. Bethge, A note on the evaluation of generative models, 2016 ). Specifically -- you need a number of samples which is exponential in the dimensionality of the space for the estimate to be reasonable, and when there are too few samples there will be a strong bias of the model towards the mean of the data distribution.  115: So this method would not be suitable for minibatch-training?  121-122: Why are local minima more of a problem for the Wasserstein case than the KL case? This didn't make sense to me.  137-139: All of these datasets are unusually small. Can you talk about the scaling properties of this algorithm, both in terms of number of samples and problem dimensionality?  177-178: It seems that the clustering is a result of the entropy regularizer, as opposed to the Wasserstein distance itself.  sec 4.3: OK -- but is this shrinkage a desirable property? Will this be much worse for higher dimensional problems? I suspect it will.  sec 4.4: All the performance measures are with respect to Wasserstein or Hamming distance.  =========================== post rebuttal ===========================  Thank you for your response! It clarified a lot. I would encourage you to include discussion of scaling in the final paper.  (re ""When γ=0, we recover the usual OT dual constraints."", I think this should rather be in the limit as γ goes to 0?)",2-Confident (read it all; understood it all reasonably well),"This paper introduced a new objective function for restricted Boltzmann machine by replacing KL divergence with smoothed Wasserstien distance, resulting in a generative model suitable for problems where the use of metric is important. The paper provided theoretical proof for the derivation of the gradient for the new objective function, quantitative comparisons for analyzing the characteristics of the smoothed Wasserstein distance, and applications in data completion and desnoising to demonstrate the usefulness of this new objective function. Generally speaking, this paper is well written in respect of language expression and internal logic structure. ","This is paper provided solid prove, well designed experiments and potential application scenarios. The description of the idea is easy to understand and the motivation is clearly demonstrated, i.e. the KL divergence is not good enough to act as a metric for problems where performance is measured in terms of distance. However, the comparison of the results only conducted among the kernal density estimation, standard RBM and Waserstein RBM, where the previous two both use non-distance metrics. It would be interesting to include the results of Euclidean distance and Hamming distance for comparison. ",1-Less confident (might not have understood significant parts),This paper proposes to use Wasserstein distance instead of the commonly-used KL divergence as the cost function of RBMs. It presents advantages of Wasserstein distance through toy examples and promising experimental results.,"I am convinced on the good direction this paper perused to apply Wasserstein distance to machine learning, and on the advantages of Wasserstein RBM which considers the metric structure of {0,1}^d while classical RBM does not.  My main critism is regarding the writing. It is not self-closed with many outgoing links and requires one without prior knowledge of Wasserstein distance to read the refernce [4][5]. It seems that the author(s) didn't put much efforts on making it easier to read and broadening the potential audience. For example, it is useful to introduce, in the RBM setting, the optimal transport problem after equation 3, and then the dual problem. then the optimal dual alpha^\star. In its current form, it is not clear how alpha^\star is obtained. Similarly, in equation 6, it is not useful to point the reader to [5] without explaining the expression of alpha^\star(\tilde{x}_n) and its meaning. The author(s) are suggested to rewrite section 2 to be more self-closed and to be more intuitive. To save space the author(s) can put the proofs into supplementary and streamline section 3.  As an unsupervised learning method, the proposed Wasserstein RBM has three additional hyper-parameters: gamma in equation 3, and lambda and eta in the end of page 4. The requirement of tuning these hyper-parameters puts a limit on the usefulness of the proposed method. As the author(s) said, ""the proposed procedures can be seen as fine-tuning a standard RBM"". Is lambda and eta necessary because they can give an improved energy surface that is easier to optimize, or because that without them the minima of the cost function is essentially different and gives trivial solutions?  The Wasserstein distance is well defined on the parameter manifold including its boundaries, while KL divergence is not. This has a meaning in de-noising problems. If certain input dimension is mostly 0's with only a few 1's , a Wasserstein RBM can remove the noise and make the dimension deterministic, while KL-based RBM cannot. Do similar observations appear in the experiments? This might be a potential advantage of the proposed method.  Minor comments:  The title could be misunderstood as improving the training of the classical RBM. However the method proposes a different cost function and essentially is a different learning method.  Equation 4. It is easier to read if the sum is over (x, x') instead of (xx').  Line 91: why the dual potential is centered? This is not straightforward and need more words.  Section 3: the beginning has some overlapping contents with section 1 and can be streamlined.",2-Confident (read it all; understood it all reasonably well),"The paper introduces the idea of augmenting the standard MLE objective for training an RBM with a smooth Wasserstein distance. The Wassterstein distance is a function of a chosen metric on the data space, and as such encourages the model to put mass in areas not far from the data according to the metric. This results in more plausible samples from the model at the possible expense of coverage.","The paper is clear and well-written.   The clustering or shrinkage property observed in the experiments is compelling and could be very useful if the method can be scaled to problems of practical interest. On this note I think it would be useful to include details on how long experiments took to run, and more generally some information about complexity of the steps involved would be nice.  Objective functions that induce more realistic samples is currently a hot topic in the variational autoencoder / generative adversarial network community, and I think that this work makes a useful contribution along related lines for the RBM. If there were possible applications to those methods as well it would be well-worth exploring. ",2-Confident (read it all; understood it all reasonably well),,,,,,
"Examples are not enough, learn to criticize! Criticism for Interpretability","Been Kim, Rajiv Khanna, Oluwasanmi O. Koyejo",https://proceedings.neurips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html,"This submission presents a model criticism approach for machine learning methods, inspired by the Bayesian Model Criticism.","Nice and simple model criticism method, derived the right way.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",The paper proposes a method for identifying examples of where a model performs badly to help to understand what it does.,"       The paper argues that examples of where a model performs well are insufficient to understand it completely and that, in addition, examples of where it does not perform well should be given. The authors propose a methodology for this, leveraging existing techniques. Providing means to understand machine learning-generated models is an important research area with a large potential impact.        The proposed approach is justified and explained well. It would help understanding to present the overall algorithm used rather than just the part presented in Algorithm 1.        One aspect I particularly like about this paper is the thorough evaluation that involves user studies. The figures illustrate the approach nicely and demonstrate the usefulness of the returned prototypes and criticisms.        The presentation of the paper could be improved. For example, the notation is sometimes used inconsistently (e.g. page 3, line 105, R is not using the font used in the rest of the paper. Page 4, line 119.5 is using u and v, which are not defined. Page 5, lines 188 and 190, k-medoids is misspelled. Page 9, lines 341, 342 the citation uses a different style than the rest. In general, the paper is well-written though.        In summary, I feel that this paper would make a valuable addition to the NIPS programme.       ",2-Confident (read it all; understood it all reasonably well),"The paper presents an approach for selecting prototypes which best describe a dataset, as well as criticism examples, which describe which aspects of the dataset are not captured by the prototypes. The  goal is to improve the intepretability of the data. The selection of the prototypes and critisisms is based on the maximum mean discrepancy (MMD) criterion and a greedy algorithm. This is facilitated by the submodularity of the cost function, which is proven for a larger class of problems. Experimental results show that the method performs comparably to related nearest prototype in terms of classification performance, and is considered interpretable in a small user study. ","The paper tackles an underexplored but very important problem of interpretable machine learning, and has many potential applications. Although I am not able to understand the details of the proofs in Section 3, using MMD to select prototypes and criticisms seems to be an intuitive and well-motivated approach.    The experiments are appropriate and it is refreshing to see a user study included. The explanation of the procedure, and the presentation of the results for the user study could be a bit clearer (ideally with an example of the question inside the user interface, and a table for the results).   Minor concerns: Line 225, The earliest references to “nearest prototype classifier” are from the 90’s, with among others work by Kuncheva et al.   Line 234, local kernel – isn’t it required to know the labels of the test data in order to compute this? It might be good to mention that this is an oracle / upper limit ",2-Confident (read it all; understood it all reasonably well),"Given a set of examples, the objective of this paper is to select a subset of examples that are both representative (called prototypes) of the distribution and the outliers (called critics). To achieve this objective, the authors propose MMD-critic algorithm which is based on the recently proposed maximum mean discrepancy(MMD) measure. The authors propose optimization problems for prototype and critic selection and show that for RBF kernels they are both submodular and allow greedy selection algorithms. Due to the difficulties associated with direct evaluation of the algorithm, the experiment used two indirect methods: nearest prototype classification and human subject evaluation. The results indicate that the algorithms serves the original purpose.","Positives: + the authors give an elegant principled approach for selection of representative examples + the experiments on human subjects are inventive + the results are promising + the paper is well-written Negatives: - the proposed algorithm is limited to RBF kernels and the prototype selection (described in Section 3.1) does not qualitatively differ from many previously proposed distance-based prototype selection algorithms - the critic part of the algorithm (Section 3.2) seems to be of an outlier selection algorithm. This paper does not discuss its connections with outlier selection and no comparisons are made with any existing outlier selection algorithm. Since all the baselines used in the experiments are prototype-based algorithms and not outlier selection algorithms, the baselines can be considered as straw-men. ",2-Confident (read it all; understood it all reasonably well),"The authors give a presentation of the mmd-critic algorithm, which selects prototypes and criticism for human interpretable machine learning by optimizing a statistic called the maximum mean discrepancy. This is a novel application of a method that was previously applied to model comparison and comparing models to input data. They describe the model and show a proof for submodularity that ensures the method is scalable. They tested the method against other prototype-based machine learning baseline methods from Bien and Tibshirani (2011) for different numbers of prototypes on the USPS handwritten digit database. The local version of MMD-critic showed superior performance to the baselines, mainly for less than 1000 prototypes. A qualitative test was done on images of dogs. Finally, they ran a pilot study to collect measures of interpretability for MMD-critic. They concluded that people performed best when given both prototypes and criticisms.","The authors explore the compelling question of how to develop interpretable machine learning methods using prototypes and criticisms. The paper was well written and clear, even for a non-expert in the field like myself. The mathematical results appear to be sound.  It is hard for me to assess the originality of the work in the field of machine learning, but I imagine that there is work on training with both positive and negative examples. At the very least, within the human category learning literature the issue of learning a concept through examples of the concept and non-examples has been explored. Given that the authors are interested in human interpretable machine learning methods, they should investigate this work in psychology.  Relatedly, the methods they compare mmd-criticism to do not have criticisms (negative examples), which makes direct comparison between the algorithms difficult to assess. This is also the case for the human experiments: participants may have done well if they were simply given randomly chosen criticisms and not those chosen by mmd-criticism. In short, the authors have certainly shown that computer and human performance is better given prototypes and criticisms rather than only prototypes. It would have been interesting to see how mmd-critic performs against other algorithms that also provide both types of data.  Smaller points: - Eq 4 seems to be missing ""k("" - In figure 1, does the value on the x axis include both prototypes and criticisms for the mmd-critic model?",1-Less confident (might not have understood significant parts),"The Maximum Mean Discrepancy (MMD) is a measure of the difference between two probability distributions. Rather than use this measure for a pairwise statistical testing of two models or to compare a model to a given input, the authors propose a scheme for using MMD to select subsets of a dataset that in some sense best summarize or explain the full dataset, as well  'criticism' examples that are not well explained by the prototypes. They demonstrate the utility of the method both by using it to produce a nearest-prototype classifier and comparing performance against other baseline models, and by appealing to a pilot human subject study in which people were asked to perform classification tasks with and without protoypes and criticism examples. In the former case, the method of the authors performs as well or better than benchmark methods at classification; in the latter, people found classification significantly easier (though slower) with both prototype and criticism examples.","Technical quality The authors provide proofs of the consistency and computational tractability of the proposed method in a wide range of practically salient conditions. The tests described are relevant and appropriate for assessing performance.  Novelty/originality So far as I am aware, this is the first time that systematic use of 'criticism' examples has been proposed.  Potential impact or usefulness There are many contexts in which interpretability of classification models is important. In these cases, the method is likely to be adopted.  Clarity and presentation The paper is well-written and clearly organized.",2-Confident (read it all; understood it all reasonably well),,,,,,
Unsupervised Learning of 3D Structure from Images,"Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, Nicolas Heess",https://proceedings.neurips.cc/paper/2016/hash/1d94108e907bb8311d8802b48fd54b4a-Abstract.html,"This is an interesting paper combining autoencoders, 3D representation, and LSTMs for learning neural networks with 3D representation. Learning 3D representation is important, which makes this paper interesting.  More experiments into the reason for selecting some of the architectures could be useful.    ","This is an interesting paper combining autoencoders, 3D representation, and LSTMs for learning neural networks with 3D representation. Learning 3D representation is important, which makes this paper interesting.   some 'tricks' used by this paper is useful and interesting. For example. using a 3D+2D convolutional network instead of a render makes the decoder differentiable. modeling only the distance of 162 vertices from the centroid of the object makes predicting degenerate meshes impossible.  The videos are interesting and showcases the capability of 3D representation in a neural network.  Some clarifications would be nice:  - How important is it to have sequential generative model? What happens if the 3d volume h is generated by a 3D convolutional network instead of a read/write LSTM?  - From what distributions do you sample the 2D meshes before using OpenGL to render into an image x?  - In the mesh rendering method, did it take many samples per mesh for learning to be stable?  - What kind of lighting model used for generating the training data? ",2-Confident (read it all; understood it all reasonably well),"This paper demonstrates a model that possesses an understanding of the 3-dimensional world, even when the training data itself consists of 2-dimensional images of 3D objects. The approach is to train the model to learn a latent representation that captures complicated 3D properties, and from which 2D views of the world can be generated that are consistent with the training data. The model itself is a variant of a sequential generative network that is trained via a variational lower bound. The model maintains an internal hidden state that maps onto an internal 3D representation (the canvas) that can be used to generate a 2D image by way of a renderer. There are many ways to represent 3D surfaces and this paper considers a volumetric representation and a mesh representation.  The model is demonstrated on numerous applications including generation, conditional generation, imputation and inferring 3D structure from multiple 2D views.","This paper is primarily about piecing together existing techniques, utilizing the advances in generative modelling in order to properly model 3D structure. Inferring 3D structure from 2D images is a canonical goal of computer vision, but I haven't seen many attempts to solve this in the recent deep learning literature (i.e., in an end-to-end fashion). I think that this is an exciting frontier that will no doubt receive a lot of attention. I think that the main contribution of this paper toward these goals are several interesting datasets and corresponding baselines, which are essential in order to garner mainstream attention.  The paper itself is well written, albeit quite dense due to the number of different components that must be included to introduce the problem, model, and applications. This is reflected in the size of the appendix. Even still, I think that the level of technical detail is somewhat light, and a lot of details that would be required for replication are left out. These include specific parameterizations and architectures (e.g., of the VST network or rendering MLP), training parameters, etc. I'm reasonably confident that upon release of these datasets that researchers will be able to meet the baseline numbers, but it would be much easier to reproduce with more specific details.  The results themselves are impressive, however I have no baseline for just how difficult the problem is (although I assume it is difficult). That is, there is a lack of a naive baseline that does not involve a sophisticated recurrent generative model. Even a non-recurrent generative model when ground-truth information is available would be interesting.  Aside from these minor points I think it is an excellent paper.",2-Confident (read it all; understood it all reasonably well),"The paper proposes a DRAW-like (https://arxiv.org/abs/1502.04623) neural network for 3D input spaces that learns in an unsupervised fashion to generate 3D structures, in some cases purely from learning from 2D views of the data.   The propose a new dataset MNIST3D to do such experiments, as well as use the ShapeNet dataset. They show that training class-conditional models on ShapeNet recovers object structures of chairs, persons etc. in a way we expect. They say that the datasets will be available upon publication of the paper. ","Apart from the fact that the paper does not have any baseline models whatsoever, I dont see another flaw in the paper. But the fact that it has no baseline models (simple ones, or ones based on previous literature) is not acceptable.  On the section of Potential impact or usefulness, I did not see the original DRAW paper be picked up with excitement by the community since it's publication, though everyone thought it was cool. This paper is DRAW3D, hence, I only gave this a 3 and not a 4 rating.",2-Confident (read it all; understood it all reasonably well),"The paper proposes to learn 3D structure representations in an unsupervised fashion from either 2D images or 3D volumes via sequential generative models. The overall training follows the variational auto-encoder style. From a 3D structure representation learned, two methods are prosed for projecting it onto a 2D image: via 1) a learned network, 2) OpenGL renderer. The models are demonstrated on several datasets: Necker Cube, Primitives, MNIST3D and ShapeNet. The paper shows qualitative results and also quantify their method in log-likelihoods.","OVERALL: This is a fascinating paper that applies sequential generative models to learn deep representations of 3D structures in an unsupervised manner. I really enjoy reading the paper! It's also a plus that the authors are making the code available for the community.  TECHNICAL QUALITY:  - The paper proposes a set of well thought out experiments. I particularly like the choice of the datasets (especially the newly created MNIST3D), which are diverse and suitable for the 3D structure generating task. - I wonder if the authors could quantify how much variability (multi-modality) of the data is captured by the model.  - For completeness, the paper should also discuss/show the possibility (if any) of overfitting/memorizing the dataset.  NOVELTY: The paper develops upon the previous sequential generative models. However, the work is the first to learn to infer 3D representations in an unsupervised way. The proposed learning framework that supports both 3D volumes and 2D images is also novel, afaik.  IMPACT:  The work will enable unsupervised learning of 3D structures in many different settings, e.g. robots with RGB-D cameras automatically collecting and learning from real-world 3D objects; or learning in 3D simulation, etc.  CLARITY & PRESENTATION: - The paper is very well written, and a pleasure to read. I really like Fig. 2. - A minor point: the diversity of generated 3D structures is not easy for one to qualitatively evaluate in black/white figures (e.g. Fig. 7).",2-Confident (read it all; understood it all reasonably well),"This paper provides a generative model of 3D volumes and 2D images using an internal 3D latent representation. The model can be conditioned on a context, which in turn can be either a category or multiple views of the 3D volume. In a very similar way to DRAW, the model transforms a set of latent Gaussian variables into an internal 3D representation, which in turn is converted to a 2D image via a convolutional network or OpenGL renderer. Inference in this generative model is performed using variational inference with a recognition network. Five different tasks are considered.","The paper is well written, clear and contains appropriate references. Also the number of figures help understand and illustrate the text appropriately.  The technical content of the paper seems sound, but its ideas are not particularly novel. The proposed model is an extension of DRAW, which now includes a context (which is observed and used as an additional input to the recognition and generation networks) and generates 3D representations instead of 2D representations. This requires the addition of a projection step to 2D at the end of the generation process when the observations are 2D. Well-known variational inference is applied to the resulting model.  Some of the claims in the introduction are misleading. ""We show how the aforementioned models and  inference networks can be trained end-to-end directly from 2D images without any use of ground-truth 3D labels."". This make it seem as if 3D latent structure was discovered exclusively from 2D information. This is not the case, and prior knowledge about 3D structure and projection from 3D to 2D is included in every version of the model. Only the last experiment seems not to use 3D input data directly (the multi-view experiment uses depth in the provided context). But in that experiment, an OpenGL renderer is introduced in the loop, providing, if we wish, an infinite amount of 3D ground truth by sampling from its input-output pairs. Additionally, not every 3D shape can be accommodated by the model of the last experiment due to its particular parameterization, which is not pointed out in the paper.  I would like more clarity regarding training and test splits. For instance, the objects in Figures 9 and 10 were observed at all during training or not? I would like to distinguish generalization to new perspectives from generalization to entirely new shapes.  One thing I found concerning about this paper is the lack of comparison with benchmarks. The experiments comprise 5 different tasks. I would like to see the performance of a reasonable benchmark for each of these tasks, so that it is possible to judge whether the results produced by this model are particularly impressive or not.  - For instance, Section 3.5 discusses multiview contexts. The text is not clear about the nature of the views, but the corresponding figure and legend clarify that the input views are 3D. It seems like it should be relatively easy to infer a 3D representation from multiple views with depth information. The obtained results should be compared with those of a different technique for the same task to ascertain whether the model is working particularly well or not.  - Similarly Section 3.6 uses a very specific parameterization of the input space that restricts the applicability of the model. My understanding is that the same experiment could not be performed with the objects from ShapeNet, for instance. This should be pointed out. Are the objects we see in Figure 10 used to train the model? If they are as I suspect, couldn't the model be basically memorizing 3D structures for each object and then inferring pose for each test image? (as opposed to being able to get 3D understanding of a new object). Then the problem could be posed as, given a set of training images, identify same-object clusters and find its 3D representation. A baseline for this task should be included so that the quality of the proposed model could be judged.  - Section 3.3 and Figure 7: Again the generalization capabilities are unclear. Is the model generating _new_ shapes within the same category or multiple rotations of already-observed shapes? The nearest training neighbor to each generation, computed using a meaningful metric (that matches shape regardless of rotation, translation, and scaling) could be useful here.  Minor:    Section A.3: ""of the three kernels"": there are only 2 kernels in the 2D ST. Section 2.1:  ""fwrite(st,h_t-1;theta_w) = VST(g1(st),g2(st))"" This equation doesn't seem correct, isn't h_t-1 used during writing?","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",This paper proposed a generative model of 3D objects. The model is essentially a variational auto-enconder which can be learned directly from 2D images.,"This paper is the first attempt for literally realizing 'vision as inverse graphics'. A highly original and inspiring work.   However, there are several issues:  1) Since the title is 'Unsupervised Learning of 3D Structure from Images', I suggest the authors trim or remove the experiments with 3D structure as inputs. Please elaborate more on the learning with only 2D images. For example, what does the model represent when the 2D input image in ambiguous such as the one in Figure 1? 2) In Figure 2 and 3, it's better to change 'OpenGL' to 'renderer'. The proposed method is not restricted to any specific renderer such OpenGL. One could use other renderers such as DirectX. 3) For optimizing a black-box generative model, I refer the authors to  Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models, by M.U. Gutmann and J. Corander, JMLR 2015.",2-Confident (read it all; understood it all reasonably well),,,,,,
Can Active Memory Replace Attention?,"Łukasz Kaiser, Samy Bengio",https://proceedings.neurips.cc/paper/2016/hash/fb8feff253bb6c834deb61ec76baa893-Abstract.html,The authors propose to replace the notion of 'attention' in neural architectures with the notion of 'active memory' where rather than focusing on a single part of the memory one would operate on the whole of it in parallel. ,"The author build on the Neural GPU architecture to make it applicable to large scale real world tasks. They analyze the shortcoming at each phase, and propose a series of improvements. The findings are of interest for the community. Particularly interesting is the study of performance dependency on the input complexity/length where they show to achieve higher robustness and hence they conclude that this can be an indication that active memory approaches can replace active memory. A more detailed study on an artificial task whose complexity can be fully controlled would improve the paper as it would show the dependency of the network complexity wrt the task complexity.",2-Confident (read it all; understood it all reasonably well),This paper introduces an extension to neural GPUs for machine translation.  I found the experimental analysis section lacking in both comparisons to state of the art MT techniques as well as thoroughly evaluating the proposed method.,I found the model section to be very hard to follow. I would like to see a more thorough evaluation of the parameters and design decisions of this model.,2-Confident (read it all; understood it all reasonably well),"This paper proposes active memory, which is a memory mechanism that operates all the part in parallel. The active memory was compared to attention mechanism and it is shown that the active memory is more effective for long sentence translation than the attention mechanism in English-French translation.", - Figure 6 should be displayed well without color. - Is the BLEU score of the extended neural GPU statistically significant improvement against the GRU+attention model? - One language pair of translation is not strong experimental results. You need more experiments to support your claim. - Conclusion section should exist.,1-Less confident (might not have understood significant parts),"This paper proposes two new models for modeling sequential data in the sequence-to-sequence framework. The first is called the Markovian Neural GPU and the second is called the Extended Neural GPU. Both models are extensions of the Neural GPU model (Kaiser and Sutskever, 2016), but unlike the Neural GPU, the proposed models do not model the outputs independently but instead connect the output token distributions recursively. The paper provides empirical evidence on a machine translation task showing that the two proposed models perform better than the Neural GPU model and that the Extended Neural GPU performs on par with a GRU-based encoder-decoder model with attention. ","The contributions of this paper comes from the proposed Extended Neural GPU model and from the empirical results demonstrating that it performs on par with an attention mechanism. The contribution of extending the model by modeling the output sequence dependencies has not been applied to the Neural GPU specifically, but it is well-established in the literature (e.g. LSTMs and GRU RNN decoders for language modeling, machine translation, image captioning etc). On the other hand, the experimental contribution of making the Extended Neural GPU model work effectively on a machine translation task is useful, and it is especially interesting to see that such an architecture may yield the same advantages as an attention mechanism,.  The need for a variable-sized memory is partly supported by (Cho et al., 2014), who demonstrate that the performance of an encoder-decoder translation model, where the encoder is a convolutional neural network, also degrades with sentence length. This adds evidence to the paper's argument that the memory should not be restricted to a fixed-sized vector, but instead allowed to grow with the input sequence length.    If this paper is to be published, it needs to at least address the following issues:  1. The output tape tensor ""p"" should be defined formally. It is not clear to me how it is computed based on the description on lines 144-147 alone. It is also not clear to me, how the previous decoder states, e.g. d_t, affect future decoder states, e.g. d_{t+1}. This should be clarified.  2. On line 207, the paper states that the greedy decoding strategy is comparable to the decoding strategy of the Extended Neural GPU. In my opinion this is incorrect, since the Extended Neural GPU runs a separate greedy decoder for every output size in the interval [input size, 2 x input size]. In this case, the later is much better and more expensive than the former and therefore the GRU+Attention (short) is not really comparable to the Extended Neural GPU model.  In addition to these two points, it would also be good if the final paper includes results for the Extended Neural GPU model on the WMT' 14 task with long sentences, as well as a plot showing performance of the different models w.r.t. beam size in order to demonstrate that the Extended Neural GPU is not benefiting significantly from the alternative decoding procedure.    Other comments: - Line 106: is {0, ..., I} the vocabulary with size I? If so, it would be more clear to explicitly define the vocabulary with its own symbol. - The paper should include an appropriate reference to the WMT' 14 task. - Lines 239-244: The statements made in this paragraph are already well established. The paragraph should be shortened to a single sentence. - Reference section should be improved.  ------------------------------------  UPDATE: Thanks to the authors for their rebuttal. The clarification on beam search helped, and so did the improved results.",2-Confident (read it all; understood it all reasonably well),"This paper proposed extended model of active memory that matches existing attention models on neural machine translation. Also, the paper try to investigate this model and explain why previous active memory models did not succeed.","It would be informative to compare the training time & test time between proposed model and attention model, because the active memory model seems to have more computation.  In addition, Figure 6 should compare much longer source sentence length, because the long baseline GRU model with attention is trained including all sentences up to 128 tokens. The paper also didn't compare their results with previous papers.",2-Confident (read it all; understood it all reasonably well),"This paper is built on the top of Neural GPU model by making changes to the decoder to handle sequence generation tasks like machine translation better. While the original Neural GPU model was applied for algorithmic tasks, author try to use this model for machine translation task. Results show that proposed active memory model is comparable to Attention model for NMT.","After Rebuttal:  I have read the rebuttal and I see that authors have addressed most of my concerns. I am changing my rating for impact from 2 to 3.  Before Rebuttal:  This paper proposes 2 extensions to Neural GPU model. Markovian Neural GPU is an obvious extension of Neural GPU for sequence generation. Extended Neural GPU is a non-trivial extension.  Firstly the claim (in abstract and everywhere) that active memory has not improved over attention for NLP tasks is wrong. Dynamic Memory Networks and End-to-end memory networks have been used for comprehension based question answering task. For machine translation, Meng et al., 2016 (http://arxiv.org/pdf/1506.06442v4.pdf) proposes a deep memory based architecture which is an active memory model and it does perform better than the attention based NMT model. In fact their model is very similar to Neural GPU, except for the fact that they use a different transformation instead of convolution.  Input structure as described in line 106-110 is not very clear. If I understand correctly, authors use w*n*m matrix as input where only w[0] is filled with n m-dimensional word vectors and w[k] for k > 0 are all set to 0? Also if number of words are less than n, then remaining part of the w[0] matrix are also set to 0? Please clarify.  From lines 39-41 I understand that authors consider NTM as a complex attention model than an active memory model? If so, I disagree with that point of view.   Experiments:  1. I understand that the authors want to compare their model with attention based model without beam search. But I do not understand the reason for choosing two attention models – one trained with sentences upto 64 tokens and another one with sentences upto 128 tokens. Why is Extended Neural GPU trained with sentences upto 64 tokens only? Specifically, the comparison is incomplete without the following rows:  a. Attention model trained with sentences upto 64 + beam search b. Attention model trained with sentence upto 128 tokens + greedy decoder. c. Neural GPU models trained with sentences upto 128 tokens. d. Meng et al., 2016  2. Authors should also highlight the limitations of the proposed model clearly. a. Is it not possible to train Neural GPU with 128 tokens? Or the results are worse? In either case, this is a limitation. b. Neural GPU models expect the output sentence size. In lines 203-208, authors say that they consider all sizes between input size and double of it and pick the result with smallest perplexity. This is very inefficient when compared to simple decoding done with LSTM decoder. c. I see that it is very difficult to do beam search with Neural GPU, because of the proposed complex decoder. This is another limitation.  3. Just based on Figure 6 (which is approximate), authors cannot make the claim that less sensitive to sentence length than the attention baseline. Because other 2 models where trained with UNK token and few other variations.  Overall, this paper proposes an interesting extension to Neural GPU, but the application is not very convincing. It is not very clear, what is the gain in using a complex architecture like this when compared to the simple attention model if the results are only comparable and not better. I would like to see a comparison with Meng et al., 2016 to see if this complex architecture really helps to improve the performance. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,
Variational Bayes on Monte Carlo Steroids,"Aditya Grover, Stefano Ermon",https://proceedings.neurips.cc/paper/2016/hash/0c9ebb2ded806d7ffda75cd0b95eb70c-Abstract.html,"The authors describe an application of recent projection-based stochastic bounds on the partition function to improve the lower bound on the marginal likelihood used for training a latent variable model, such as a deep generative model.  Pros: elegant and potentially important application of recent theory to hot problems in learning deep latent variable models; promising experimental results Cons: paper detail is spent on minor adaptation of recent work, and too much detail is omitted from the practical application of those ideas to the learning process ","The concept seems solid and the authors' execution seems promising.  It is intuitive that an improved estimate of the likelihood should also improve training.  My main concerns are:  (1) The theory developed appears to rest very strongly on [13]; while the authors say they ""extend"" [13] to bounding the marginal likelihood of directed latent variable models, this may not itself be a significantly novel contribution.  (2) The application of the main idea to black-box learning and test likelihood for deep generative models do not seem sufficiently well explained; it is mainly a few paragraphs on page 6.  The pseudocode in the supplement helps, but should not be required to understand the paper.  Similarly, the discussion is often vague (for example Section 4.2.1).  I feel that Section 4's practical application of the theory is the main contribution of the work; if the authors agree, it needs a more detailed treatment and discussion in the draft.  ",1-Less confident (might not have understood significant parts),"This paper proposes an extension of recently proposed random-projection methods to models with latent variables using variational inference. The authors are able to derive novel estimates of marginal likelihoods, and to bound the error of those estimates.","I found the approach and the paper quite interesting—it's a very different way of doing approximate posterior inference, and it's great that some kind of theoretical analysis is possible. I'm not an expert on these new discrete random projection results, but this seems like a nice application of them.  I was less impressed with the empirical study. For one thing, it's not at all clear that there's enough information to reproduce these results. ""Many control variate techniques were employed"" is not sufficient detail. The quantitative results are suggestive, but not overwhelmingly compelling, and the qualitative results are hard to interpret.  A few more specific comments:  You should consider changing the title/algorithm name. The meaning of the colloquial phrase ""x on steroids"" as ""a stronger version of x"" is clear enough, but an academic paper title isn't the most appropriate place to make light of substance abuse. Also, steroid abuse comes with some nasty side effects that you probably don't mean to associate with your algorithm.  The bounds in section 3.2 feel very loose at first glance---their relative tightness might be clearer if they were expressed in terms of log p_theta as one traditionally does in variational inference derivations. Being off by a factor of 3200 in a 100-latent-variable model sounds pretty bad, even if that's only 8 nats.  Line 83: I wonder if this statement conflates the ability of deep networks to produce multimodal marginal distributions p(x) rather than multimodal conditional distributions p(z|x).  Line 95: [0,n] should be {0,...,n}, since k is integer.",2-Confident (read it all; understood it all reasonably well),"The authors describe a method for variational inference in latent variable models that is motivated by importance sampling and recent work on random perturbations and hashing.  Additionally, the authors provide bounds (that depend only on the number of latent variables) on their estimator that hold with high probability, in contrast to typical variational methods which can be arbitrarily bad.  Finally, the authors describe how to apply these theoretical tools to the design of variational learning algorithms in a variety of settings.","I liked the paper and the results, but I do think that some of the high level proofs could be improved.  I would also have liked the authors to comment a bit more about the hardness of mean field and how that affects practical performance.  General comments/typos:  -line 19, ""Variational approximations are particularly suitable for directed models"" -> more so than undirected models?  -line 70, ""variational inference is optimizes"" -> ""variational inference optimizes""  -line 133, ""i.e. distributions"" -> ""i.e., distributions""  -line 134, is this the same Q as on line 130?  -line 290, ""for e.g.,"" -> ""e.g.,""",2-Confident (read it all; understood it all reasonably well),"The authors suggest a new method to perform approximate inference in latent discrete variable models, by adding random projections (random parity check constraints) to the posterior and correcting for the bias introduced by the subsampling. They prove that if one can solve the randomly-projected problem, one can recover the true posterior with high accuracy.","An interesting technical paper. More space should have been devoted to (ideally geometric) intuition behind the theory, and strengthening the experimental section, which is on the weaker side right now (few experiments, weak baselines).  - The paper follows [13] quite closely - this is not necessarily a deal breaker, but it would benefit from clearly highlighting the contribution from previous similar work.  - The paper main idea is that to divide the main problem into subproblems, for which we have removed many points from the posterior (by adding random parity check constraints) and appropriately scaled the estimates to compensate for those missing points. The implication would be that if we can solve the subproblems, then we have an excellent approximation to the log-partition function of the original problem. But it is not clear to me the subproblems are in fact significantly easier to solve. One intuition would be that by removing points at random, each subproblem can be slightly less complex (less multi-modal?) and the posterior can focus on those fewer modes instead (this seems suggested in lines 31-33, 85-88). I could very well be wrong, but it's not clear to me this should be occurring - the parity check constraints are not going to remove a particular part of space, but will appear to remove points from the posterior quite uniformly. In other words, instead of removing a bunch of modes, it seems to me it should make 'swiss cheese' from the posterior, which might not be significantly easier to approximate as a result. ",2-Confident (read it all; understood it all reasonably well),This work extends hash-based inferences techniques to the learning of latent models. It bridges the gap between existent the theory for hash-based inference for discrete latent variable models and the existent practical inference techniques (sampling and variational methods). The authors provide a theoretical bound on the marginal likelihood of the latent variable models when using hash-based inference then apply it to learning sigmoid belief networks where they claim an improvement as compared to existing benchmarks.,"This paper provides theoretical bounds that are tighter than existing variational bounds for the problem of learning latent variable models. The authors extend applied existing theory of hash-based learning and amortized inference to design a black-box learning algorithm. They later applied it to learning a Sigmoid Belief Network. The main advantage to this approach seems to be the partitioning of the search space for posterior distributions into buckets/subsets that are faster to search than with a typical sampling method. The proposed inference scheme then leverages mean-field inference (used heavily in the context of variational inference) within each subset. One of the main technical contributions is the tighter bound on the likelihood using two aggregate estimators which was an extension of an existing work (specific to undirected graphical models) to the directed models setting. However, it was not intuitive how this was more advantageous to stochastic (called mini-batch in the paper) or batch variational inference schemes. While the general variational bounds on the marginal likelihood are provenly tighter, we would have appreciated a less terse explanation (in comparison to mini-batch + mean-field). Additionally, the empirical analysis seemed lacking as there weren't enough details regarding  recent developments and heuristics in the domain were discarded as complementary rather than competing. However, it is clear that the goal of the paper was to prove the superiority of this novel hash-based inference scheme to the common sampling or variational (they used stochastic variational inference for learning the ELBO of the basic SBN unless I misunderstood?) and for that their analysis might be adequate. Overall, despite a few typographical errors, this paper was written eloquently and the methodology was presented adequately and the analysis of the results was quite appropriate.",2-Confident (read it all; understood it all reasonably well),The paper presents a hybrid variational/monte carlo inference technique for improving variational lower bound. The method has theoretical guarantees for tightness. Experimental evaluations on neural nets (sigmoid belief network) is presented for image and text modeling.,"Several things to note:  1. Given the projection step and multiple iterations for collecting lower bound values, how is the running time affected?  2. Some additional plots comparing lower bound values might be useful. In particular, given the numbers in your experimental section, it seems like the base variational inference bound is not that loose either. Some additional plots making the improvement clear will make the presentation better (maybe in the supplement?).",2-Confident (read it all; understood it all reasonably well),,,,,,
Privacy Odometers and Filters: Pay-as-you-Go Composition,"Ryan M. Rogers, Aaron Roth, Jonathan Ullman, Salil Vadhan",https://proceedings.neurips.cc/paper/2016/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html,"In the adaptive composition of differentially private mechanisms, this study considers a setting that, the privacy budget \epsilon and \delta can be adaptively changed  after observing the responses of mechanisms at each round.  To achieve differential privacy in this pay-as-you-go setting, two notions are introduced, privacy filter and privacy odometer. The privacy filter is a function that determines whether or not to stop releasing responses to analysts at each round. The privacy odometer is a function that evaluates the privacy budget consumed at the timing of evaluation without knowledge of the entire privacy budget that would be consumed in the future.  In the pay-as-you-go-setting, the adaptive composition theorem is not exactly same as the regular adaptive setting, but incurs at most a constant-factor  multiplicative loss.","The problem discussed in this paper is novel and very interesting theoretically.   It was unclear to me, in what kind of situation, the adaptive parameter selection is necessary. In regular differential privacy settings, analysts issue statistical queries and the curator controls the entire privacy budget so that the entire privacy breach does not exceed a given privacy budget. In the pay-as-you-go setting, who determines the privacy budget, the curator or analysts? If the analysts can determine the privacy budget at each round, what if the analyst does not listen to the advice of the privacy filter? If the curator can determine the entire privacy budget, why does she need to run it in the pay-as-you-go setting?  It seems to be difficult to understand the structure of the paper without supplementary. For example, it is unclear why Section 4 is necessary. The readers of this paper would be familiar with the notion of differential privacy and adaptive composition. To me, it was hard to me to understand the intention of Lemma 3.3. My recommendation is to spend more space on the main contribution of this work than preliminaries on differential privacy. ",1-Less confident (might not have understood significant parts),"The composition theorem in differential privacy estimates the privacy cost resulting from running several differentially private computations, possibly adaptively. As the most common way of designing differentially private mechanisms for complex tasks is to put together basic DP building blocks, the composition theorem is the all-important glue to hold it all together. The traditional composition theorem works when we run a sequence of DP mechanisms where the privacy cost of each step, and the number of steps are fixed in advance. In some settings, it is important to be able to run adaptively: end early in some cases and go home with a smaller privacy loss, or use different privacy parameters per iteration depending on the answers so far. This is the question the authors study in this work. The authors show that the failure of previous composition theorems to apply to this setting is not just a technical omission. The adaptivity in fact comes at an asymptotic cost: if the adversary runs for a DP mechanism for a number k of steps that she chooses, the  privacy loss can be made at least as large as sqrt{k loglog k}, whereas for fixed k, this loss is sqrt{k}. The paper shows that one can still recover good bounds: the sqrt loglog k loss is the worst possible: the authors show an upper bound. Moreover, if the goal is to only allow running until some privacy budget is exhausted (but not publish a privacy cost at each step), then one only loses a constant over the standard strong composition theorem. Finally, the authors also show that the standard composition theorem does not lose anything. This is valuable as for small k, it gives better bounds than the strong composition theorem. ","The paper relates to an interesting and very important question in differential privacy. It extends the applicability of composition theorems beyond what was known earlier, and does so with essentially optimal loss in parameters. Given the importance of privacy to the NIPS community, I think this is a solid contribution that should be accepted.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper mainly considers adaptive composition theorem for differential privacy. The paper allows privacy parameters and number of rounds to be adaptive, and to solve the corresponding problem (for example, differential privacy is not defined suitably), authors propose two natural concepts, the privacy odometer and privacy filter. The privacy odometer can give an upper bound of privacy loss on every round, and the privacy filter can determine whether to continue or to halt the process, in the guarantee of global privacy budget. What’s more, authors prove the privacy filter has the same asymptotic order of bound as advanced composition theorem (C. Dwork 2010), while this cannot be achieved by the privacy odometer, except with an addition O(\sqrt{\log \log(n)}) factor.","In my personal view, the situation considered in this paper is much more general than almost all the existing work, and is more appropriate to use if differential privacy is applied in real word. Besides it, authors also give a deep understanding for the proposed primitives. These are main reasons for me to accept or to rate high scores for this paper, and I think it may open another door for differential privacy related research.  Intuitively speaking, giving an upper bound for privacy loss every round is more difficult than determining whether to continue or to halt the process, as we can easily obtain a privacy filter from a privacy odometer. Thus, I think conclusions in this paper are theoretical sound. A question I am concerned is whether we can obtain a privacy odometer from a privacy filter, for example, once we can solve equation (7) to put \epsilon_g in only one side, could we say the corresponding term in the other side can be used as a privacy odometer?   Some minor comments: 1. For the last word in line 86, I think authors want to say “Continue” not “Halt”; 2. In definition 2.6, there is a clerical error with the third case in randomized response; 3. In line 278 – 279, it will be better to explain more for the content in the brackets, as I am not clear why the left side of the inequality is an upper bound of statistical distance, and why this can conclude \epsilon_i has to be larger than \frac{1}{10n} (though I know this is a basic fact for many DP algorithms); 4. The format of paper’s references and supplementary is different with NIPS standard format. ",2-Confident (read it all; understood it all reasonably well),"The paper investigates an interesting question about the adaptive composition of differentially private procedures when the differential privacy parameters themselves are chosen adaptively. That is, the paper studies the setting where the number of rounds of interaction with the differentially private mechanism is not set beforehand, and the privacy parameters in each round is decided adaptively based on the outcomes/settings of the previous interactions. Under such setting, the standard composition theorems are not necessarily valid. The paper provides a characterization of the composition under this setting using two objects: ""privacy filter"" and ""privacy odometer"" that the authors define. The main result shows a separation between composition in the parameters-adaptive and non-adaptive (standard) settings. This separation, however, is only by a small asymptotic factor.","The paper studies a new interesting setting in differential privacy. The authors give careful analysis that answers almost completely the composition question in this setting. However, despite being non-trivial, the results are not very surprising (though this has nothing to do with the quality of the work).","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper proposes a new setting for composition in differential privacy, where the length of composition and the privacy parameters can be chosen adaptively. It proposes privacy odometer that returns the privacy loss up to certain time, and privacy filter as a stopping rule to detect if the privacy budget is exceeded. It then provides bounds for both, compares them with existing composition theorems, and shows separation between privacy odometer and privacy filter.",Composition for differential privacy is a very important topic in differential privacy. And it is very interesting to look into the setting when the privacy parameters are also chosen adaptively. The paper makes a significant contribution to the direction of this setting.  The paper is well-written. Yet it is pretty dense and it will be better if the paper can be re-organized by moving more contents to the appendix.,"3-Expert (read the paper in detail, know the area, quite certain of my opinion)",,,,,,,,,
Probabilistic Linear Multistep Methods,"Onur Teymur, Kostas Zygalakis, Ben Calderhead",https://proceedings.neurips.cc/paper/2016/hash/23c97e9cb93576e45d2feaf00d0e8502-Abstract.html,"The paper extends earlier work on probabilistic numerics for ODE solvers to linear multistep methods, providing a probabilistic counterpart of the existing algorithms. The existing deterministic methods are used to some extent, especially in problems where very high accuracy is required or f() is expensive to evaluate.","Technical quality:       The method is very clear and elegant. Experimental evaluation could be improved by introducing more context: how does the method perform relative to other alternatives (probabilistic Runge-Kutta with potentially different parameters, perhaps relevant non-probabilistic methods). This would help understand the trade-offs: what is the method good for and when another alternative would be better. The way the paper reads the proposed method seems the best for everything, yet at least Matlab recommends using Runge-Kutta-based ode45 as the default instead of Adams-Bashforth-based ode113.        Novelty:       The proposed algorithm is a novel combination of existing techniques in probabilistic numerics and the existing deterministic algorithm. There do not seem to be significant broader novel contributions besides the proposed algorithm.        Impact:       The presented experiments look very impressive, but the practical impact potential is unclear and seems marginal. Clarifying the strengths and weaknesses of the method (see above) would clearly help in making the case for its impact.        Clarity:       The paper is very clearly written and easy to follow. It would be interesting to see a version of Fig. 1 including error bars estimated from the method - it seems that currently only the estimated means are ever used. More emphasis could be put on explaining the big picture of when the method is actually useful too.        Other comments/questions:       1. In Eq. (1), why does y depend on theta but not f?        2. It would be nice to know the source of the variance seen in Fig. 1.",2-Confident (read it all; understood it all reasonably well),"%%% UPDATE %%% Thank you for your detailed response.   In discussion, several points were raised by a meta-reviewer about the connection between this work and the work of Schober et al in NIPS 2014. In particular, it was suggested that the two methods may be near identical, albeit with different derivations. I shall not repeat the discussion here as I have only a limited understanding and I do not wish to make statements that are incorrect. I do hope these points will be made available to the author(s) by the meta reviewer(s).  At the outset this paper excited me, although I am not expert in ODEs. It is clearly well written and technically proficient. However, the nature of the concerns that have been raised make it difficult to argue strongly in favour of the paper. As a result, my confidence score and corresponding novelty scores have now significantly decreased from the initial review stage.  I would like to add that it would have been much better if the issue of equivalence with Schober et al 2014 had been put to the authors during the initial review stage, so that there was an opportunity to provide a rebuttal. (A comparison was requested by one reviewer, but at that stage the argument that the two works may be near identical was not put forward.) The unfortunate situation appears to be due to the structure of the NIPS review process and I do sympathise with the author(s)' predicament.  %%% END %%%   This paper develops a probabilistic version of the classical Adams-Bashforth and Adams-Moulton numerical integrators. The authors propose a distribution over (stochastic) solutions of the ordinary differential equation (ODE) that is intended to provide a probabilistic quantification of epistemic uncertainty due to the presence of numerical error. This paper is in line with other recent research into probabilistic numerical methods.","This is an outstanding paper. The author(s) should be warmly congratulated on a timely, intelligent and thought-provoking contribution to this emerging literature.  The only substantive negative comment that I can make is that there is not a frequentist assessment of the coverage properties of the proposed distributional quantification of epistemic uncertainty (empirical or theoretical). However, that would not be easily achieved within the confines of 8 pages.  In general terms, the fundamentalist Bayesian could reject the idea because information is ""thrown away"" outside the s-length moving window. They could also object because the prior measure on the solution is not placed on an infinite dimensional function space at the outset, but is instead defined on the finite dimensional distributions in a somewhat ad-hoc manner (relative to the fundamentalist Bayesian approach). The paper presumably aims to strike a balance between being ""fully"" Bayesian and being a computational pragmatist. Perhaps there is room to raise and address this trade-off in discussion?  Some minor comments:  Please provide a reference for the Picard-Lindelof theorem (page 2).  On page 2, lines 55-56, I find the description ""marginal measure given evaluations of the vector field"" to be rather odd, since it sounds like you are in fact conditioning on evaluations of the vector field. My guess is that the equation that follows this description is not the one that was intended to be written here.  Again on page 2, rather than \mathbb{Z}^+, do you mean \mathbb{N}? As far as I can tell, zero is excluded from this statement, so \mathbb{N} = {1,2,3,...} would be more standard. (I may well be mistaken - perhaps some clarification could be made.)  I wonder how standard it is to refer to (e.g.) Eqn. 10 as a Gaussian ""process"" prior? Would not simply a ""Gaussian prior"" suffice here, given there doesn't seem to be a continuous time variable present at this point?  I believe references [2] and [3] are both now published, so these reference should be updated.  In the appendix a bold-face `f' is used, whereas in the main text it is not bold. ",1-Less confident (might not have understood significant parts),This paper makes use of the Gaussian Process framework to presents a derivation of the Adams-Bashforth and Adams-Moulton families of linear multistep ODE integrators and extends them to be probabilistic. The posterior of the resulting family of probabilistic integrators has mean (at each step) equal to the corresponding deterministic integrator and standard deviation corresponding to its truncation error.,"I can't really comment on novelty of the method presented here or the appropriateness of the experiments as it's outside my area of expertise. Still, this paper seems to present an interesting result consisting in the extension of the Adams family LMMs to the probabilistic framework given by Gaussian Process. The (theoretical) proof of their convergence rates as well as their (empirical) verification is also a nice contribution.   Minor:  - Figure 2a is missing a legend (I assume it's a simple enumeration 1-5)",1-Less confident (might not have understood significant parts),The paper presented a rigorous derivation of probabilistic linear multistep methods Adams-Bashforth and Adams-Moulton ODE integrators from Gaussian processes. A probabilistic framework has been provided to obtain numeric Adams-Bashforth and Adams-Moulton integrators up to an arbitrary order. Proof has been given that the same convergence characteristics apply to probabilistic Adams-Bashforth integrators as to their deterministic counterparts. A practical implementation of a probabilistic Adams-Bashforth integrator has empirically been verified to match convergence expectations.,"The presented paper provided a rigorous theoretical derivation of the probabilistic Adams-Bashforth and Adams-Moulton numerical integrators.   Detailed literature referencing puts the paper into context followed by a formal introduction of linear multistep methods. The authors also found a very good balance of stating what is necessary to follow theorems and concepts but also providing enough detail in the Appendix to follow proofs in full length. Certain assumptions or practices are always well motivated or sufficiently referenced when necessary. Therefore, clarity and logical flow is maintained throughout the paper providing a well chosen level of detail.   Probabilistic multistep methods have the potential to estimate the degree of certainty of approximated numerical solutions. Probabilistic numerical implementations are a significant improvement over their deterministic counterparts. They maintain the same convergence characteristics while providing additional information about the quality of the solution. Therefore, they are of great interest for the NIPS community and might have the potential to be pioneering. ",2-Confident (read it all; understood it all reasonably well),"In line with recent developments in probabilistic numerics, this paper constructs a probabilistic derivation for Adams-Moulton numerical solvers for ODE's. The development of these algorithms is important since it completes the understanding of the assumptions underlying these methods, in addition to being able to quantify uncertainty of the resultant answers.","Overall, an important contribution to an exciting field.  Comments: - The clarity of the probabilistic interpretation could be improved, which is an important factor for this work to be taken up The derivation of the method follows the numerical procedure closely, by placing distributions over quantities that would have been computed in the original algorithm. The insight gained form treating numerical procedures as an inference problem generally relies on placing well defined distributions over all quantities, and then deriving the implied distributions by marginalisation. From the presentation in this paper, it is not immediately clear to me that there even is a consistent joint distribution over all variables, something which is important for a method attempting to be Bayesian. Additionally, presenting the method in this form would make the properties of the priors in the model more explicit, and easier to critique and improve on.  A picture would also help the explanation of Adams family solvers (something along the lines of http://imgur.com/J46NIrV?). Perhaps in conjunction with a visualisation of relevant posteriors. Currently the paper lacks intuition and illustration of to how the probabilistic method behaves.  The experiments provide a nice proof of concept, together with a neat illustration of how the probabilistic features of this model can predict when small errors accumulate to the point of no return in chaotic systems.  Finally, it is unclear how the ground truth for estimating the empirical errors in figure 2 is created.  Overall, a good contribution to the exciting field of probabilistic numerics. The paper is already a useful technical contribution, but could benefit a lot from additional clarity in presenting the assumptions and behaviour of the underlying probabilistic model.  Additional points: The equation in line 56 seems odd, since it marginalises out the prior over function evaluations, rather than doing any conditioning.",2-Confident (read it all; understood it all reasonably well),This paper tries to derive a new approach to numerically solving ODEs. The idea is to construct a GP which has as its mean the deterministic Adams-Bashforth method but additionally provides variance estimates for the values of a function y(t) with given t. ,"This is a very interesting submission which however leaves me with mixed feelings. The fundamental questions that I think is unaddressed is the following: the uncertainty of the estimator does not stem from measurement error and is purely epistemic. What exactly then is the rational behind the chosen augmented bases in eq (11) and (12)? Why does this make sense, what are other possibilities for augmenting the bases (or why are there none)? In particular, a useless 'probabilistic numerical method' is constructable by taking any deterministic numerical method to define the location of a gaussian, and arbitrarily setting a standard deviation for the gaussian. The paper would gain immensely by explicitly showing how it's construction is not arbitrary in this sense. A less important question is the following: if I'm not completely mistaken, all predictive densities are gaussian - is Monte Carlo really necessary in this case or is an analytical solution possible?  === After rebuttal and reviewer discussion === My rather fundamental question about the arbitrariness of the probabilistic extension of a deterministic method was not properly addressed: so the SD of the probabilistic formulation coincides with the local truncation error - why should it? Whats the interpretation of that? Related to this: the paper conveys the impression of to not using priors and not solving a bayesian inverse problem (as it says it is not occupied with a bayesian treatment of θ). Apart from the comparison to the closely related probabilistic Runge-Kutta-Method, methods based on Monte Carlo (MC) are not even mentioned, though they now exist for quite some time [1]. Not comparing to (state of the art) MC makes the paper look a bit strange or even dishonest when taking into account MCs pervasiveness.  [1] Arnold et al (2013). Linear multistep methods, particle filtering and sequential Monte Carlo. Inverse Problems, Volume 29, Issue 8. ",1-Less confident (might not have understood significant parts),,,,,,
Constraints Based Convex Belief Propagation,"Yaniv Tenzer, Alex Schwing, Kevin Gimpel, Tamir Hazan",https://proceedings.neurips.cc/paper/2016/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html,"The basic idea is that one would like to do convex BP but including certain constraints, e.g. that b(x1)=b(x2). (Note that this is very different from including x1=x2, which is trivial). This is basically done by taking the regular formulation for convex BP where inference is phrased as an optimization over the local Polytope. Then, an extra set of Lagrange multipliers are added to impose the constraint, and the rest of the optimization details go through slightly changed to include these.","This is an interesting and plausible idea. I only have a few random comments.       - On line 63, please describe more clearly the problem with (iv) (Differnce between x1=x2 and b(x1)=b(x2))       - In Eq. 4, should be maximum also take place over v?       - I think the general discussion of speed needs more discussion. The introduction implies that it should be faster *per iteration* compared to regular convex BP with a ""c penalty"". Please formalize how much faster. In addition, the experiments often compare speed, but this is not clearly described enough to be useful. Why is it faster? Is it because of faster iterations or fewer iterations? The experiments don't discuss stopping criteria, basically making them useless as written. (Some discussion of implementation details should also be made.) How do we know the speedup isn't an illusion caused by the convergence threshold? Some plots of, e.g., accuracy vs time would be much more convincing.       - In Sections 4.2/4.3, what is the comparison? To some fixed value of c, or to just discarding the constraints?       ",2-Confident (read it all; understood it all reasonably well),"The authors propose belief propagation for discrete graphical models with certain consistency constraints, related to PN-potentials. The contribution lies in deriving closed-form solutions to the belief-propagation operations in this more general model and showing empirically that it is advantageous over naively doing belief-propagation on general factors explicitly modeling the consistency constraints. ","General comments:  (i) The authors only solve a new special kind of higher order consistency constraints, generalizing soft PN-potentials, but not a truly general class of constraints, as indicated in the title or in the abstract.  (ii) I do not agree with what the authors say in lines 72 - 78. In case of MAP-inference, which is normally desired, the goal is to obtain a single assignment which satisfies all given linear constraints. The proposed model (i.e. computing marginals) is then less desirable. The relaxed model the authors optimize is simply a byproduct of looking for marginals instead of MAP-assignments (the added entropy is responsible for this). In case of vanishing entropy one gets the same model. Hence there certainly remains the disadvantage of a parameter in the PN-potential, but now hidden in the entropy. Additionally, when a MAP-solution is wanted, the proposed algorithm is in fact disadvantageous, as inference is not done w.r.t. the energy of a single MAP-solution and rounding results in some arbitrariness of the obtained solution.  (iii) Experimental comparison: - No comparison against solving MAP-inference with PN-potentials and existing inference algorithms is given. - Experiments are very small scale, e.g. image segmentation is only done on superpixels. I do not consider such microbenchmarks very informative. One can usually just plug everything into an off-the-shelf LP-solver in such cases. - To be able to really judge the algorithm, convergence plots would be helpful, but none are given. In general, I deem experimental comparison unconvincing: while many experiments have been performed, no real comparison against any algorithm other than CBP is performed.  (iv) Related work: Many references to work on inference with higher order potentials are lacking. To name a few: - Komodakis: Beyond pairwise energies: Efficient optimization for higher-order MRFs  - Tarlow et al.: HOP-MAP: Efficient Message Passing with High Order Potentials. - Kappes: Higher-order Segmentation via Multicuts E.g., the work of Kappes shows how to very efficiently include PN-potentials in an LP-relaxation. Generally, efficient ways for PN-potentials have been explored before, which is not acknowledged in the author's text.  (v) The work is rather incremental: The main contribution is the derivation of a new updating formula for convex belief propagation and consistency constraints.  Detailed comments: - The entropy H is never defined. - Notation is scattered around: Some is defined in section 2 and some is defined in section 3, but only informally in the text, making the article harder to read. - Line 8: What is the standard approach? - Line 71: which is expect -> which is expected - Line 96: One can also derive duals when the primal is not strictly convex (duals exist even for non-convex programs). ",2-Confident (read it all; understood it all reasonably well),"This paper studies adding constraints on the values of beliefs during convexified belief propagation, as opposed to including factors in the model that penalize disagreement between random variables. This is a weaker form of constraint, but the authors argue that it is sufficient for many applications and show that it is less computationally expensive on synthetic data and benchmark problems without sacrificing significant accuracy.","The proposed method is appealing in that it provides a convenient way to bypass tuning the weights of PN-potentials for convexified BP. My opinion is that it is a straightforward derivation, but one that is worth sharing with the community.  The experimental validation is sound. It supports the argument that constrained beliefs are sufficient for enforcing the domain knowledge typically encoded as PN-potentials. If anything, I think the argument that needs to be supported more is the claim that such constraints are useful in practice. It is sometimes considered conventional wisdom, and reinforced by the papers addressing the topic, but it would be good to also evaluate in this manuscript how accurate a model is on these tasks that just includes untuned PN-potentials (so that they're probabilistic dependencies, not hard constraints), as well as some baseline that doesn't include them at all. I'm curious to know how much the inclusion of constraints improves performance.  Regarding related work, the discussion on lines 271-276 should be expanded. Some of the cited references, such as [1, 12, 13], use the alternating direction method of multipliers. For this reason, incorporating additional linear constraints that constrain beliefs (or continuous random variables in the case of [1]) is trivial. I think the distinguishing feature of this work is that it incorporates constrained beliefs into a BP algorithm that optimizes the objective using block-coordinate descent.  Also, the references [1, 12, 13] are primarily focused on MAP inference. Would there be any complication in extending CBCBP to a max-product version?  After author response:  I think the authors' plan of including empirical evidence supporting the argument for using constraints will strengthen the manuscript.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","In this paper, the authors proposed a dual belief propagation algorithm for marginal inference with a special type of constraint --- some variables' assignment have to agree. They enforce this by letting the beliefs of these variables to equal, and add to the local marginal polytope to form a new constraint set. Together with the original linear objective function (with a entropy term), it forms a LP problem, whose dual admits a belief propagation algorithm.   ","The idea is straightforward, and no surprise.   Minor issues: (1) the title is not informative. It doesn't really tell what the task is and how to do it.  (2) 'consistency structure' is not a widely accepted term, perhaps should not be used in the abstract. It would be even better if avoid. (3) The constraint that the proposed BP handles is too special, which restricts its impact.  Overall, I am leaning towards accepting it for it's a useful addition to the community.  ==Post rebuttal == I still would like to accept it.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper considers a special case of the approximate marginal inference of discrete Markov random fields, where hard equality constraints of label assignment are enforced for certain nodes. Unlike previous approach [9] who introduces additional consistency potentials, the proposed method explicitly imposes equality constraints to the variational formulation of the marginal inference and solves it with dual coordinate descent. The experiments on synthetic data, image segmentation and machine translation show its computational and statistical efficiency over the baseline [9].","                 Technical quality:         > The main contribution of this paper is the idea of writing the transitive equality constraints of consistency as simpler equality constraints of common constants. However, I am afraid the transformation (i.e., (4)) is just a relaxation of the original problem rather than an equivalent formulation. One simple argument is that v_k can take different values in [0,1] leading to different optimization problems. The optimal one among those problems gives rise to the original problem. I am also not sure if v_k can be canceled out in the dual problem. The explanation in line 115-116 is vague. Even if we cancel out the linear term, the log-sum-exp function itself could be unbounded below. The soundness of the derived sum-to-zero constraint of nu variables is thus arguable. Also, it seems nontrivial to recover (4) from the dual of the dual given in Lemma 3.1. But all of these could be the matter of details.          > Regarding experiments, did you check the beliefs numerically to verify the correctness of those consistency constraints?         > It is not 100% clear why sums are taking in (6) without including k and x_r^k, i.e., \sum_{k^\prime \in K_r \setminus k} and \sum_{x_r \setminus x_r^k}. More steps or explanations are needed.           Novelty and potential impact:         As far as I know, the formulation of label consistency constraint is novel. However, the proposed method works only for a particular type of constraint. Its usage could be limited.                Clarity and presentation:         This is a well written paper with clear presentation and sufficient experiments. It could be better if the convergence analysis and/or its variants are presented.          After rebuttal:         The ""maximizing over v_k"" issue doesn't affect the main results of the paper. However, the proof of lemma 3.1 could be majorly revised: By taking derivative of the Lagrangian w.r.t. v_k, we get the sum-to-zero constraints immediately.          Other issues have clear response in the rebuttal. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","Authors of this paper extend the previous work on convex belief propagation by including consistency constraints. Presented benefits of the proposed model include increased flexibility in constraint definition and computational speedup. Algorithm and proof of convergence is given. Model is compared with the baseline on one synthetic and two real tasks, semantic image segmentation and machine translation.   ","This paper introduces an incremental improvement over the previous work on convex belief propagation. Contribution seems minor, but results show notable improvement over the baseline.   Overall technical quality of the paper is good, but some parts of the paper could use additional clarification. For example, eq. (1) is hard to follow if the reader is not familiar with prior work. Additional explanation (even a single additional sentence) regarding the constraints would make it much more apprehensible.  ",2-Confident (read it all; understood it all reasonably well),,,,,,
Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations,"Behnam Neyshabur, Yuhuai Wu, Russ R. Salakhutdinov, Nati Srebro",https://proceedings.neurips.cc/paper/2016/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html,"The idea of path-SGD is applied to RNNs. Path SGD differs from more traditional regularisers (such as L2 weight decay), by taking a particular form which is a sum over paths from input to output nodes (see equation 2). It is shown that the regulariser remains invariant to those rescalings of the parameters which leave the function unchanged.   An approximation from a previous paper is adapted to deal with recurrence as in RNNs. Furthermore, it is shown that the regulariser is dominated (in practice) by the cheaper of two terms which need computing, hence this further approximation is recommended and justified empirically.   Experimental results are provided which, while they don’t obtain the level of LSTM etc, provide a significant improvement for vanilla RNN, with it’s associated advantages of simplicity etc. ","I enjoyed reading this paper. Employing a norm with the demonstrated invariances, is extremely interesting and has the potential for big impact in the neural net community. This is one of several papers pushing this idea, and the only argument might be that the advance here is perhaps overly incremental. Nonetheless, that we can apply this idea to RNN, and that it is so effective here, is good to know.  ",2-Confident (read it all; understood it all reasonably well),"This paper addresses issues with training RNNs with non-saturating activations functions (e.g., RELU).  They present a theoretical justification by adopting the a general framework of a feed-forward network  with shared parameters (an RNN unrolled infinitely has a single parameter  for all steps).  They present a path-SGD algorithm which allows one to  train these models w/o the vanishing or exploding graident problem.  They  show empirically that while there is an additional term to compute for the  path-norma gradient update, it does not impact training time.  They also  show that the path-norm sgd training is far superior to standard SGD  training. ","The paper is well written but the notation is tough to follow and the lack  of diagrams makes the geometric nature of the problem more difficult to  understand than necessary.  I think that a few sketches of what is going on  would help in interpreting the path algorithm.  As I understand it, the path-norm is used to control the magnitude of the  gradients.  It also appears to be a good way to speed up learning as it  renormalizes the gradients during learning. ",1-Less confident (might not have understood significant parts),"This is an interesting paper that attempts to understand the geometry of plain RNNs and proposes a path-SGD optimization method for plain RNNs with ReLU activations. The authors start by viewing RNNs as feedforward networks with shared weights, and then explore the invariances in feedforward networks with shared weights, especially for RNNs with ReLU activations. The authors further propose a path-regularizer and path-SGD optimization for feedforward networks with shared weights (RNNs). Experiments on both synthetic problems and real language modeling tasks illustrate the effectiveness of the proposed path-SGD method.  Generally, I think the paper does a good job of advocating the importance of investigating the geometry of plain RNNs and designing optimization algorithm with a geometry that suits the RNNs. The paper, however, does not convince me that similar strategies will improve more advanced RNNs such as LSTMs.","1. This may not be a fair comment to the authors, but the first impression when    I read the paper was like: why RNNs? why not LSTMs? I understand that LSTMs    are more resource-intensive, but they are the main stream in a lot of    applications due to their superior performance. So why don't the authors    investigate the geometry of LSTMs instead of plain RNNs? Sorry, but this was    my first impression...  2. Since the paper is mostly focusing on plain RNNs with ReLU activations, I    think it might make sense to conduct experiments on more practical tasks,    e.g., speech recognition tasks (toolkits such as Kaldi should be very    helpful). This way the authors can really convince the readers of the    effectiveness of the proposed path-SGD to plain RNNs with ReLU.  3. It might make sense to go beyond plain RNNs with ReLU. As a theory paper, I    think the focus on plain RNN with ReLU activation is too limited. But as a    practical paper, I'd like to see more experiments.  4. Good results on both synthetic problems and the real language modeling task!",1-Less confident (might not have understood significant parts),"In this paper, the authors attempt to extend upon a previous work on path-normalization for regular feedforward neural nets into the regular (plain) RNNs using ReLUs. In the formulation, they cast RNNs as a fully unfolded feedforward neural nets with shared weights. The motivation is straightforward. However, the formulation only works for RNNs using ReLU (not for sigmoid RNNs and so on).  It is well known that ReLUs work extremely well for feedforward NNs, such as DNNs and CNNs. Some work was reported for ReLU RNNs as well, however, we have NEVER been able to make ReLU RNNs work as well as regular sigmoid (or tanh) RNNs, not to mention LSTMs.   The theoretical formulation to extend path-norm to RNNs seems straightforward. However, it is still critical to expose both strength and weakness of your proposed approaches to readers. I have two concerns:  (1) Regarding the complexity of your path norm: you mention that the motivation to study plain RNNs rather than LSTMs is due to the simplicity. However, when you apply the path-norm to optimize RNNs, it seems you have to compute both k1 and k2 for every single data sample. This seems prohibitedly expensive, comparing with a simple SGD.   (2) Performance your path norm: in section 5.3, you study your method for two popular language modelling, PTB and text8. Oddly enough, you report bits-per-character (BPC) for these tasks, rather than the standard word-level perplexity values. These two tasks have been widely studied and the state-of-the-art performance is well known based on the word level perplexity. I don't know why you choose a non-standard metric for these. If you have nothing to hide, please report the results in terms of word PPL. In this way, readers may easily compare your models and methods with tons of work in the literature. I INSIST this as a pre-requisite to accept this paper for publication.    ", As per my comments above:  1. Address the computational complexity issues of your path-norm.  2. Report your LM performance using the standard PPL.,"3-Expert (read the paper in detail, know the area, quite certain of my opinion)","In this paper, the path-SGD optimization method for feed-forward networks is extended to ReLU RNNs. Experimental results on sequential MNIST and word-level language modeling on PTB corpus show that the RNNs trained with the proposed path-SGD outperform IRNNs.","The authors extended the path-SGD to ReLU RNNs. Although some parts (e.g. proofs) of the paper are beyond my understanding, the flow of the paper is very logical and the experimental results are convincing. As shown in Section 4.4, the proposed method is computationally cheap when the second term (k^(2)) is ignored, and authors showed by experiments that the second term is not needed in practice.  ",1-Less confident (might not have understood significant parts),"In this work the authors present an adaptation of the Path-SGD algorithm for RNNs. They characterize all node-wise invariances in RNNs and show that the update rule for their modified algorithm is invariant to all feasible node-wise rescalings. They provide experiments which indicate that their modified optimization method yields improvements for plain RNNs trained with existing methods, although they still lag behind LSTMs.","The authors frame their work as a step towards bridging the gap between plain RNNs and more complex LSTMs/GRUs by using an optimization method which accounts for the geometry of the loss surface. This seems to be a worthwhile goal (since plain RNNs are computationally cheaper and easier to analyze theoretically) and their experiments show some promising results in improving performance over plain RNNs trained with existing optimization methods.   However, it is not clear to me how the method that the authors use in practice differs significantly from regular Path-SGD introduced in previous work. The authors do present an adaptation of Path-SGD to networks with shared weights, and show that the new rescaling term applied to the gradients can be divided into two terms k1 and k2. But then, they note that the second term, which accounts for interactions between shared weights along the same path, is expensive to calculate for RNNs and show some empirical evidence that including it does not help performance. In the rest of the experiments, they ignore the second term, which to my understanding is essentially what makes the method introduced here different from regular Path-SGD. Isn't ignoring the second term the same as simply applying Path-SGD as if the weights were not shared, and combining the rescaling terms corresponding to shared weights? Also, part of the justification for this work in Section 3 is that RNNs with ReLUs can encounter the ""exploding activation"" problem due to the fact that ReLUs do not saturate, and that taking into account the interactions between shared weights along the same path can avoid this problem. I would have thought the second term would be necessary to avoid this. I would like the authors to clarify the major differences, if any, between the approximation that they use in the experiments (which ignores the second term) and the Path-SGD method introduced in other work, and its relation to the exploding activation problem in ReLU-RNNs. I would be willing to raise my score if the authors can explain how the method that they use in practice is significantly different than the previous Path-SGD algorithm.  The experiments do indicate that using path normalization offers a consistent improvement for plain RNNs on language modeling and sequential MNIST, which is encouraging, although they have yet to catch up to the LSTM.   A few remarks: -Section 5.1: it would be better to report experiments on the same tasks that are used in evaluation, i.e. character level rather than word-level experiments.   - for addition problem experiments, it seems like the authors initialize the RNN with identity transition matrix (this could be made more clear). Note that this is close to the exact solution presented in ""Recurrent Orthogonal Networks and Long Memory Tasks"", Henaff et al. 2016, and gives the RNN an advantage. Also, the experiments in that paper show that LSTMs can solve the addition task for T=750. For fair comparison with LSTMs, the RNN should be initialized randomly. -Line 217: both version -> both versions  ",1-Less confident (might not have understood significant parts),,,,,,
A Simple Practical Accelerated Method for Finite Sums,Aaron Defazio,https://proceedings.neurips.cc/paper/2016/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html,This paper presents an accelerated variant of SAGA for finite sums. Accelerated algorithms are part of an important class of optimization methods which have a much better dependency to the condition number. There is recently been a lot of renewed interest in this class of methods and many papers have been published including accelerated SDCA or the Catalyst method. The authors here describe an accelerated version for SAGA in the regime when L/mu > n. They describe their method as “significantly simpler and requires less tuning than existing accelerated methods”.,"Technical quality: To the best of my knowledge, the paper is technically sound and the proofs derived in the paper are correct and not hard to follow. There is not much intuition given as to why the modified step leads to an acceleration. I know accelerated methods are not always very intuitive but is there any intuition that could be gained from recent work on accelerated methods? e.g. the work of Allen-Zhu, Zeyuan, and Lorenzo Orecchia. ""Linear coupling: An ultimate unification of gradient and mirror descent."", where the acceleration results from a primal-dual point of view on the progress of the algorithm.  Novelty/originality: The main problem with the paper is the novelty aspect of the work. Similar convergence rates for accelerated variance reduced SGD have already been derived in the catalyst paper as well as other references. One missing reference is Nitanda, Atsushi. ""Accelerated Stochastic Gradient Descent for Minimizing Finite Sums."" arXiv preprint arXiv:1506.03016 (2015). The authors claim that their method is “significantly simpler and requires less tuning than existing accelerated methods” but I think this point is quite controversial as one could claim that other methods also only have one parameter to tune. The authors acknowledge in the experimental section that “SDCA was chosen as the inner algorithm for the catalyst scheme as it doesn’t require a step-size, making it the most practical of the variants”. Furthermore, the only parameter described for the other baselines in the experimental section is the step-size (“The step-size parameter for each method (κ for catalyst-SDCA) was chosen using a grid search of powers of 2.“). I think this point is very ambiguous in the paper and needs to be rewritten with some compelling arguments as to why their method is simpler or this claim should be toned down. One positive contribution though is that the method proposed by the authors handles non-smooth loss which is not the case for most existing variance-reduced methods (expect SDCA). Could the authors elaborate on whether or not Theorem 7 guarantees a better convergence in comparison to SDCA?  Experimental section: In the experiments with the hinge loss (as a non-smooth objective), the convergence seems linear while their Theorem 7 provides a sub-linear convergence. Could the authors comment on a potential improvement for “nearly everywhere smooth loss functions” as done in the SDCA paper?  Minor: Lines 68-9: there is a typo where the sign of g_j^k is negative.  ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)",This paper proposed an accelerated and incremental proximal point method for solving finite-sums problems. Theoretical analysis showed the convergence rate of this method has a square-root dependence on the condition number. And its numerical performance is superior to non-accelerated methods on ill-conditioned problems.,"It is good to see that this paper extends SAGA to the proximal point version and accelerates this method.   However, the motivation of studying this kind of extension is not well justified. The algorithm proposed in this paper requires that each component function has an easy proximal operator. However, this actually rules out a lot of important applications. The logistic regression does not satisfy this requirement. As the authors showed in the supplementary material, the logistic loss does not have easy proximal mapping, whose solution needs to be computed by a Newton method, which could be costly.   The experimental part is poorly written. First, I cannot even find what problem is testing in this part. Second, “epoch” was used as the x-axis in the figures, but how is an epoch defined? Since many different methods are involved, it would be important to clarify it.   The claim in line 48-49 is misleading. Your method does not fall into the lower bound of [Lan and Zhou 2015] because the proximal operator is used here, which implicitly utilizes the gradient information of the next iterate. However, in their analysis only the gradient of the current iterate is accessible.  The equation between line 68-69 is wrong. There should be a minus sign before g^k_j according to your algorithm description.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The authors propose a novel accelerated optimization scheme for finite-sum objective functions as they appear in (regularized) empirical risk minimization. The approach is closely related to existing methods, most prominently SAGA. An accelerated rate of convergence is proven. In contrast to its competitors the method is even applicable to non-smooth objectives (e.g., SVM training with the hinge loss). The only downside is the application of the prox operator, which can be a limiting factor in some cases, but actually not so for standard supervised learning. The emphasis of the paper is on establishing the theoretical analysis, which is followed by a brief but meaningful experimental analysis. ","The paper reads very well, even the proofs. Since I don't have a lot of experience with this type of analysis I did not follow every detail, but the authors guide the reader very nicely through the proofs. This does not mean that the paper is trivial to follow, but provided the depth of the material the authors do a great job.  The experimental evaluation is focused on validating the predictions of the theory. For the empirical assessment the use of machine training problems as benchmarks is not essential, other problems would have done the job. The rates are relevant to optimization in the first place, but for machine learning they are only one aspect (e.g., dependency of the training time on hyperparameter settings can be a game changer). Such aspects have been ignored. This becomes apparent, e.g., from the fact that the regularization constant was chosen so that differences between the methods show up nicely, and not to minimize the prediction (e.g., cross validation) error. The experimental evaluation is still convincing in the sense that the method seems to suitable for fast training of learning machines. ",2-Confident (read it all; understood it all reasonably well),The authors propose and analyze an accelerated incremental optimization method for finite sum. The algorithm is similar to SAGA with the difference that proximal operations are used in lieu of gradients steps.,"The authors propose an algorithm, Point-SAGA, that is an accelerated incremental minimization method based on proximal operations (i.e. implicit gradients) instead of (explicit) gradients. Convergence rates are derived in the case where the functions are strongly convex (dual property to smoothness) and linear rates are given when they are in addition smooth. The algorithm is original and the analysis globally convincing however: 1) The writing has to be improved notably for the experiments/implementation; as well as for the ""operators"" and contractions definitions. 2) The reach of the algorithm seems to be limited as functions have to be prox simple and strongly convex. Although prox can be solved by inner loops (similarly to Catalyst), the complexity can then be a burden; and popular regularizations as l1 norms or projections are out of reach of the algorithm.  Particular Remarks: * The authors mention l 38-40 that ""the sum of gradients can be cached and updated efficiently at each step, and in most cases instead of storing a full vector, only a single real value needs to be stored."" This is unclear for me as step 3 of the authors algorithm necessitates replacing only the j-th gradient which means that the j-th gradient at time k has to be accesible and not only the sum of the gradients.   * A reference should be given for Prop 1.  * The term Firm Non-Expansiveness is used in a confusing manner. In Prop. 1, named  Firm Non-Expansiveness, the verified inequality means that p_{gamma f} is (1+mu*gamma)-cocoercive or that (1+mu*gamma)*p_{gamma f} is FNE but not p_{gamma f} directly.  * Lemma 4, the conditioning with respect to the past should be clarified e.g. by writing it E[ X | \mathcal{F}_k]; same thing for Th. 5. In addition, the inequality used for proving Lemma 4 (1 in the appendix) can appear strange at first glance but is simply the Variance/Expectation usual formula; the proof could be clarified by stating it.   * Experiments. The authors seem to deal with L2 regularized logistic regression although it is not directly stated!  The comparison with catalyst is then the most significant as both Catalyst and Point-Saga have double loops for log. reg., then why not taking Newton iterations as an inner scheme for both? Also, if possible it would be nice for comparison to also have comparison with full batch methods on subsets (e.g. PPA and ADMM).  Minor remarks: * l. 112 \sum_j^n should be  \sum_{j=1}^n  * There is a equation numbering issue for Eqs. (4)-(5). They correspond to the same equality, and the ""first inner product in Eq. (5)"" actually Eq. (4)... * The term ""non-sparse updates"" l 169 is confusing; lease reformulate","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper proposes a SAGA-like extension of proximal point algorithm for minimizing finite sums. The paper shows the optimal convergence complexity for smooth strongly convex problems with a relatively simple proof. The method has only a single hyper-parameter (i.e. learning rate), so that it is more practical than other optimal methods. Experiments show competitive performance compared to some optimal methods. ","The paper is well written and technical part may be correct. Although the paper is incremental, it makes the following contributions:  - A highly practical method; it has only a single hyper-parameter. # Added: However, the method has limited applicability, e.g., needs Newton method for logistic regression. - The method achieves optimal complexity with a simple proof, although a proof of an optimal method tend to be complex in this literature.  Thus, the paper is useful to the literature.  ",2-Confident (read it all; understood it all reasonably well),"This paper presents an accelerated version of the SAGA algorithm, Point-SAGA, for minimizing finite sums. This randomized algorithm is an extension of the SAGA algorithm and achieves an accelerated convergence rate comparable to that of e.g. Catalyst-accelerated SDCA. It is based on gradient and prox oracle access and can be applied to non-smooth problems.         After motivating the desirability of accelerated algorithms and comparing their method with SAGA, the authors prove the convergence rate of Point-SAGA for strongly convex functions both in the smooth and non-smooth case. Next, they very briefly touch upon a few implementation details and conclude with an empirical comparison of their algorithm against several other commonly used optimization algorithms.       ","         This paper addresses its stated goal of providing an accelerated algorithm for finite sum optimization. The algorithm is somewhat simpler to state and analyze than other accelerated methods for finite sums that I am aware of, but it does not seem to yield substantively better guarantees or empirical results than existing algorithms (its guarantees and empirical performance are roughly matched by, e.g. Catalyst-accelerated SDCA). It represents a step forward in terms of ease of implementation (and it is indeed quite simple) but that is it as far as I am aware.          The paper is clearly communicated in general, although there are one or two steps in proofs which are slightly opaque (e.g. the proof of corollary 6 makes use of the smoothness property of the functions f_i, which is never explicitly defined in the paper, and could be confusing for some).          I believe there is a small bug in the proof of Theorem 5, in particular, the setting of c and \gamma on line 141, which do not seem to guarantee that the second coefficient in the equation on the line above is non-positive. At the very least, there must be conditions on L and \mu for the statement to be true (as a counterexample, take a spherical quadratic where L=\mu, meaning \gamma=0 and the second term is strictly positive, furthermore \alpha = 1 so there is no convergence). This is probably not a fatal flaw, in that I believe there is probably a choice of \gamma such that both coefficients are negative and \alpha is strictly smaller than 1, but it is somewhat concerning since the convergence guarantee rests on it.        ",2-Confident (read it all; understood it all reasonably well),,,,,,
Learning Transferrable Representations for Unsupervised Domain Adaptation,"Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese",https://proceedings.neurips.cc/paper/2016/hash/b59c67bf196a4758191e42f76670ceba-Abstract.html,"In this paper, the authors proposed a new approach based on deep learning by incorporating an idea of transductive learning for transfer learning. Experiments are conducted on a fully transductive setup. Though the experimental experimental results look promising, there are some major concerns on the proposed model as well as the experimental setup. Please refer to my detailed comments.","Regarding the proposed model, below are the major concerns:  1. In Abstract and Introduction, the authors highlighted several times that transfer learning or domain adaptation aims to align the mismatch between the training and testing data distributions, such that good generalization can be obtained across domains or tasks. In the problem setup, the authors further explicitly state that \hat{x}_i and x_i follow different distributions p_s and p_t, respectively. However, different from some existing methods, like [19] and [Pan etal., Domain adaptation via transfer component analysis, IEEE TNN, 2011], the proposed model indeed does not explicitly minimize the distance or align the mismatch between the training and testing distributions. There is no guarantee that based on the new representation, the mismatch issue between distributions can be addressed. Therefore, I would suggest the authors the rephrase those claims carefully. The new high-level representation learned by deep learning may be able to reduce the difference between domains, but is not able to align distributions explicitly.  2. Compared to other transfer learning methods, the new idea here is to introduce a tranductive step to make use of predicted labels on the target domain unlabeled data. This looks new to transfer learning. However, this idea is indeed borrowed from the co-training technique in semi-supervised learning (transductive setting). The adaptation step, which learns common and task-specific parameters, is quite standard in deep learning based transfer learning methods. Thus, technically, I would consider the proposed model is a combination of existing techniques.  3. The solution to labeling target domain unlabeled data, i.e., (2), is quite heuristic. The basic idea of the Reject Opinion is to introduce a kind-of confidence on the predicted labels. However, it would be more solid if some theoretical analysis is provided.  4. Discussions on convergence of the alternating optimization procedure are missing.  Regarding experiments,  1. I guess the performance of the proposed method is quite sensitive to the value of k in k-nn on different datasets. Sensitivity analysis on k is missing. In addition, in practice, how to tune the value of k is lack of discussions.   2. I guess the performance of the proposed method is also sensitive to the value of \gamma in the proposed Rejection Option. Sensitivity analysis on k is missing. Similarly, in practice, how to tune the value of \gamma is lack of discussions.  3. Experiments are only conducted on a fully transductive setup, which is not practical. Actually, after the labels of the target domain unlabled data are estimated by the proposed model, a k-nn can be applied to make predictions on out-of-sample target domain test data. It would be more interesting to show results on an out-of-sample test dataset.",2-Confident (read it all; understood it all reasonably well),"This paper propose to solve the domain adaptation problem under the setting that the label of  data in the target domain are unknown. To solve this problem, the proposed solution alternative do the transduction and adaptation. Experimental results demonstrate the effectiveness of the proposed solution. ","I have several concerns about the work.   Firstly, for the Structured Consistency term, it only enforces the instances with different labels have small similarity, it may result that instances with the same label also have small similarity, which is not desirable for the classification on the target domain. How can the proposed solution avoid this?   Secondly, the setting of hyper-parameters. the lambda in equation 1 should greatly affects the performance of the proposed solution. Please shows the performance of the propose solution with respect to different lambda. Please also show the effect of alpha.   Thirdly, since the proposed solution do the transduction and adaptation alternatively. Usually how many iterations are needed? It seems for each iteration, we need to learn the DNNs, if there are N iterations, then we need to learn the DNN N times. Therefore the whole framework should be very time consuming. Please report the training time of different methods.      ",2-Confident (read it all; understood it all reasonably well),"The paper presents an end-to-end deep framework to learn the transferable feature representation for predicting the labels of target domain data-points. The key idea in their proposed method is the iterative optimization strategy, which runs two main components for adaptation and transduction in turn. The authors evaluate their algorithm on several unsupervised domain adaptation tasks and show the good performance on classification and recognition.","The paper is well written and easy to follow. The authors contribute two interesting heuristics in their optimization strategy: cyclic consistency for domain shift and structured consistency for prediction. Based on my understanding, the former heuristic focuses on the alignment between source and target manifolds in the feature embedding space, and the latter optimizes to group target examples with the same label together. The optimization strategy looks reasonable and the experimental results outperform all state-of-the-art methods.  As the authors state in the paper, their approach has one major issue of the inaccuracy of transduction during the initial stage of the algorithm. Two solutions have been proposed to solve this problem, but I still have the following questions for authors: 1) The structured consistency only considers to pull examples with different class labels apart from each other, but it does not optimize to push target data-points from the same classes to each other. Although the cyclic consistency optimizes to penalize two neighboring points of same or different labels, it just enforces in the similarity metric between source and target domain. Why not optimize the similarity metric of two neighboring points of same labels in same domain?  2) I cannot find how the parameters of $\theta_s$ and $\theta_t$ are initialized. Are they initialized randomly? Because the $\theta_s$ and $\theta_t$ are tightly coupled, it will lead to a noisy and even bad start. The authors should provide more details to display how such defective start can convergence to the optimal result, e.g. the accurate curve of the learned metric during the iterations.  3) I think it would be helpful if the authors can provide a convergence curve of the optimization loss. The number of the ""max_iter"" in Algorithm 1 is also welcome to offer.  About spelling: 1) Line 153: the numerator $k'_y(x_i)$ should be $k_{y'}(x_i)$",2-Confident (read it all; understood it all reasonably well),This paper presents an unsupervised domain-adaptation method that jointly optimize both labels of the target data and representations of the source and target data. The proposed method is based on two types of consistency: cyclic consistency and structured consistency. The cyclic consistency is defined as a consistency between the label of the source data that is predicted based on the inferred label of the target data and its ground-truth. The structured consistency is defined as a consistency between the inferred labels of two target points that are similar to each other. Experimental results show that the proposed method performs better than some state-of-the-art methods.,"The basic idea of this paper is joint optimization of the target label and representation of each domain data. I appreciate that the idea is reasonable, and it should be effective for unsupervised domain adaptation. However, I have some major concerns about its implementation as shown below.  In Eq. (2), the first term of Eq. (1) is ignored, however, it would be unreasonable because i+ or i- in the first term of Eq. (1) depends on (y_1, ..., y_Nu).   In Eq. (2), the loss l(x,y) is newly introduced. Since it changes the original objective function and doesn't appear in Eq. (3), convergence of the alternating minimization is not already guaranteed.   In Eq. (3), the second term of Eq. (1) is ignored, however, it would be unreasonable because Phi_t in the second term of Eq. (1) depends on both theta_t and theta_c.     After rebuttal:  In the rebuttal, the authors show high stability of the proposed method, and I appreciate that the authors purposefully omit some terms of the object function in the sub-problems to tackle the inaccuracy problem in the initial iterations. However, I feel it somewhat ad-hoc, and I wonder if the optimization process actually minimizes the original object function shown in Eq. 1. I think the authors should modify the object function instead of changing sub-problems. Moreover, the authors describe ""It cross-domain distances are accurate ..."" in the rebuttal, but it may contradict the above inaccuracy problem.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper proposed a new deep model for unsupervised domain adaptation. The proposed method replaces softmax layer and loss function with specifically designed domain adaptive objective. The objective jointly models transduction and adaptation. By optimizing the objective, the proposed method achieves state of the art on digit and office benchmark dataset.","A new deep learning approach for unsupervised domain adaptation is proposed in this paper. The method achieves state-of-the-art performance on digit and office benchmark dataset. The paper is clear and well organized. Though the contribution looks incremental, the experimental performance is promising.   1. Does the alternating solver converge? How stable is the algorithm, and how stable are the results? Could you get the same result every time?  2. line 166, I cannot understand why (3) is convex when feature functions are convex. The feature function is in the minus part of (3)  3. The related work seems insufficient to me. The most recent related work mentioned is [10], which is from ICML 2015. [5][19][32] are compared in experiments without explanation. I think they also need to be discussed in related work section.  4. The writing could be improved, especially in introduction and related work. Many sentences are broken by reference in the related work section. ",2-Confident (read it all; understood it all reasonably well),,,,,,,,,
Ancestral Causal Inference,"Sara Magliacane, Tom Claassen, Joris M. Mooij",https://proceedings.neurips.cc/paper/2016/hash/f3d9de86462c28781cbe5c47ef22c3e5-Abstract.html,"The paper presents a hybrid (constraint-based and score-based) approach for causal discovery called Ancestral Causal Inference (ACI). ACI is inspired by a previous work (Hyttinen et al., 2014) which uses global optimization to learn a causal graph. The authors suggest a coarse-grained representation of the causal structure based on ancestral relations to address the scalability issue in global optimization. They present a set of ancestral reasoning rules and novel causal reasoning rules and propose a method to score the output causal predictions.  ACI takes an input a set of weighted (in)dependence statements and ancestral relations and models the task of causal discovery as an optimization problem subject to the reasoning rules. The authors evaluate their approach on synthetic and real world data and show that ACI outperforms the state-of-the-art methods.","The paper is well written with a clear presentation of the motivation, proposed method, and evaluation. However, I would like to see the following points clarified / addressed in their review:  1- In the analysis of the evaluation over synthetic data, the authors say that ACI is performing ""significantly better for nonancestral predictions and the top ancestral predictions"". However, the PR graphs of the non-ancestral predictions show that the bootstrapped FCI and CFCI perform better for a high recall subject to a minor decrease in precision. A similar observation is present in the PR graphs of ancestral prediction but the loss in precision is significant. It should be justified by the authors if the precision is more crucial than the recall.  2- In the introduction lines 44-45: ""we only represent ancestral relations"". It would be more clear to present a brief explanation (2-3 statements) of the alternative. Addressing this issue clarifies the numerical comparison made in the third section lines 125-126 about the improvement in the search space over previous related work.  3- The authors mention the Bayesian weights as part of the experimental conditions over the synthetic data, however, the results are not discussed in the paper. The results are present in the supplementary material but it is better to reference them or provide a brief summary in the paper.  4- Reference [18] is broken: ""J. Ramsey, J. Zhang, and P. Spirtes. In UAI, 2006.""",2-Confident (read it all; understood it all reasonably well),"The authors study the problem of causal discovery from limited data and propose a hybrid (both constraint and score based) approach called ACI. This approach solves a constraint optimization problem to discover the ancestral causal relationship of a system. Using the ancestral causal representation, they could reduce the computation time compared to the other approaches such as the one in [10]. The proposed method scores the causal predictions based on their confidence, which allows the predictor to rank his predictions according to their reliability. Moreover, ACI allows its user to combine observational and interventional data in order to improve the precision.  The authors prove soundness and asymptotic consistency of their method and demonstrate its performance on synthetic and real world datasets. ","Strengths: - This paper studies an interesting and practical problem. It is well written and easy to follow.  - Recovering the ancestral graph instead of DAG reduces the search space and improves the computation time.  - Scoring the causal prediction is interesting and allows the user to rank the predictions according to their reliability.  - Although the precision of the proposed method (ACI) and the method proposed in [10] are quite similar, its computation time looks drastically better, which is also advocated by the simulation result and Figure 1.   Weaknesses:  - The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data.  If otherwise, why [10] cannot use these side information?   - The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph?   - Second rule in Lemma 2, i.e., Eq (7) and the definition of minimal conditional dependence seem to be conflicting. Taking Z’ in this definition to be the empty set, we should have that x and y are independent given W, but Eq. (7) says otherwise. ",2-Confident (read it all; understood it all reasonably well),This paper introduces an algorithm that learns causal ancestral structures from conditional independence tests. The test results are weighted based on their perceived reliability and the method uses answer set programming to find the structure that minimizes the sum of the weights of violated constraints. The experiments show that the method is both accurate and relatively fast.,"This work contributes a new algorithm that tries to handle errors in conditional independence tests. The algorithm improves state-of-the-art both in terms of accuracy and speed. Thus, there is potential for a significant impact.  Overall, the paper was well-written and easy to follow.   There seems to be an error in rule (7) in Lemma 2. By the definition of minimal conditional dependence, X is conditionally independent of Z given W.  Fonts in the figures are too small. Especially, Figure 2 is very hard to comprehend because it is difficult to figure out which curve corresponds to which method. Further, yellow is a bad choice for the color of a curve.  To put things in perspective, it would be nice to know how the running time of FCI and CFCI compares to the current method.   Reference [18] is missing a title. ",2-Confident (read it all; understood it all reasonably well),"This paper formulates the causal discovery problem as an optimization problem and thereby, proposes a sound and (computationally) efficient method for reconstructing ancestral structures. To this end, the paper proposes 6 new causal reasoning rules (in lemma-2) and a sound scoring method. The methods proposed here are applicable in the presence of latent variables as well. ","Causal discovery is a hard problem and this paper proposes a reasonably efficient and seemingly easy to implement solution to this problem. Interestingly, the solution proposed here also works in the presence of latent variables.  The part dealing with combining observational and interventional data is not clear to me. What exactly do the authors mean by ""suitable"" interventional data? Noting that the authors consider this as a crucial contribution (stated in the abstract), it will be very useful if they can explicate this section in a better manner.   How do you bring down the combinatorial explosion? Although you mention this on the last paragraph in page-3, you do not talk about this further in the (theory section of the) paper. To what degree is your method helpful in reducing or containing the complexity? ",2-Confident (read it all; understood it all reasonably well),"This paper considers learning a causal structure through constraint testing, representing that structure as an 'ancestral' object rather than the more common directed acyclic graphs.  I think this is essentially equivalent to representing each DAG by its transitive closure (perhaps the authors could comment on this). ","Overall this is a nice method to an important problem.  The setup seems quite flexible, so I can imagine incorporating other kinds of information relatively easily.  The use of ancestral models rather than Bayesian networks is sensible and novel to my knowledge - though see below for a comment about recovering a BN.    The paper is well written, seems technically solid and novel, and provides a method that could be very useful.  Some questions and suggestions for improvement are given below.  ----  COMMENTS  Page 2 line 46 - could you be a bit more precise when saying 'keep the computational burden low'.  Relatedly, it would be helpful to discuss how quickly the number of ancestral objects grows asymptotically, and whether Chickering's result about learning BNs being NP-hard applies to ancestral objects as well (is this is easy to check).  Lemma 2 - it would be useful to have here a signpost to the discussion after Theorem 1.  The list of rules immediately had me wondering: ""how did they come up with this list and is it complete?""  In the simulations, I think it's a bit unreasonable to throw in the red curve (which simulates what happens if some interventional data are available), since none of the other methods claim to be able to deal with this.  This should be removed from the plot, as it is misleading.  What's going on with CFCI and FCI in the plots - they seem to have essentially no points on the plot.  Is this because it's too computationally hard to run FCI with different test sizes?  If so, perhaps using RFCI instead would help (I'm not sure if there is a CRFCI though!).  The description of the bootstrapping is a bit hard to follow - is this somewhat like stability selection?  Perhaps you could use the supplementary material to explain this better.  For the interventional setting, the method of Peters, Buhlmann and Meinshausen (2016) [invariant prediction] would seem like the most sensible comparison.  It can deal with a very flexible class of interventions, so would make a good counterpoint to the discussion at the end of Section 3.  Given the ancestral object, is it hard then just to recover a directed graph by doing some additional tests?  Perhaps this is harder than it seems because parts of the graph may not have an ordering at all, but I'd be interested to know the authors' thoughts.  ----  TYPOS ETC  Though not intentional, a naive reading of the second paragraph of the introduction gives the impression that 'hybrid methods' are a creation of this paper!  There are, of course, many other hybrid methods available for learning Bayesian networks and therefore causal structures.  p5 line 191 - ""allow to deduce"" -> ""allow one to deduce""  Figure 3 means nothing to me - what is it supposed to represent?  Could you give the ancestral structure discovered in the appendix?  It would be good to compare to the 'ground truth' and the Sachs method. ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper introduces Ancestral Causal Inference aiming to recover ancestral relations (X being ancestor of Y) instead of the full fine-grained causal structure. Providing the necessary theoretical foundations and inference rules, the authors present an algorithm to perform inference using answer set programming and discuss how to score the obtained causal predictions. The method is compared to (bootstrapped) (C)FCI and COMBINE on simulated data and a real-world protein data set.","This manuscript is written to a very high standard and the organisation and line of argument is good throughout the manuscript. It lays out interesting ideas and concepts that inspire and allow future research building upon the defined concepts. It may be fruitful to further discuss how results of ACI can be readily extended in order to obtain more fine-grained causal insights. While the authors are right in stressing the computational benefit of their methods it is important to address that one looses on detail of causal insight (it is a trade-off and not a win-win situation). This is why I view it important to further discuss this aspect (cf. g) below).  a) Please ensure all abbreviations/acronyms are introduced at first occurence (e.g. PC/FCI in Line 18, (C)FCI in Line 69, CFCI and BCCD and COMBINE in Line 104, ADMGs in Line 144, CFCI in Line 233).  b) Line 20: Please consider replacing ""significant"" by ""substantial"" or another synonymous word.  c) Line 82: Please define ""causal relationship"" at this point in the manuscript to increase accessibility to a broader audience.  d) Typo in Line 151: Missing space after ""semantics"".  e) Figure 1(b) and Figure 2: Please change the font to match the serif font of the manuscript, increase the font size, and increase the line width for clarity.  f) Line 246: ""going on"" is a little colloquial ;-)  g) Line 284: Please elaborate on this point and clarify _how_ this can be done since it is an important point to discuss to further convince readers of the practicality and use of the proposed ACI method.",2-Confident (read it all; understood it all reasonably well),,,,,,
Reconstructing Parameters of Spreading Models from Partial Observations,Andrey Lokhov,https://proceedings.neurips.cc/paper/2016/hash/404dcc91b2aeaa7caa47487d1483e48a-Abstract.html,"The paper describes a method for estimating transmission probabilities for diffusion over a network, when only partial information is available. In contrast with full information scenario (where each node's neighborhood can be processed independently), here we need to learn the parameters for the unobserved nodes using information from other nodes.   The authors propose an estimation algorithm based on message passing and show that it is better in accuracy and computational efficiency than MLE baseline. ","The paper tackles a general problem of parameter estimation in communication graphs that may apply to different domains with network diffusion. The algorithm is reasonable and follows from likelihood and mean-field approaches. The gains in computational efficiency compared to MLE baseline are impressive (authors do not cite or compare to any other method, so I am assuming that's the most reasonable baseline).   I understand that MLE approaches are slow and therefore testing on larger networks can be impractical. However, the paper evaluates their algorithm on small networks (N=30). Therefor, it is not clear how useful this algorithm will be in practice, where a sizeable number of nodes may be missing or when the missing nodes may not be at random wrt. the structure of the network. Here are some experiments that will be useful to see:  1. Instead of removing nodes uniformly at random, the nodes may be removed in inverse proportion to their importance. This is a fairly likely scenario: we expect to have data for busy airports, but less so for smaller airports.  2. It might also be useful to understand how the underestimation varies as the ""loopiness"" of the graph increases, because it is realistic to expect graphs with loops.  This could also be done by transforming edges in the available small-N networks.  3. How do the run time of the proposed algorithm scale with the size of the network in practice? In other words, what size of networks can this approach be applied empirically?  Overall, the paper is well-written and experiments are clearly described. One minor detail: I'm assuming that the authors are using independent cascades as their diffusion model, but it will be good to specify it in section 5.1. ",2-Confident (read it all; understood it all reasonably well),"The goal of this paper is to infer the parameters of a susceptible-infected (SI) model on a known graph, in the face of partial information about the status of nodes over time.  Since summing over all possible unobserved components of the network to get the actual likelihood is intractable, the MS. proposes approximating this by message-passing, borrowing from work on message-passing techniques for SI models.  My sense from the presentation in the MS. is that the material in section 3 is old work, while the new stuff comes in section 4, especially in the efficient calculation of the gradient.  The problem considered is interesting but extremely stylized: not only is SI rather a caricatue of any real epidemiological process, but it's assumed that there are many completely independent realizations of the epidemic process to average over.  (In the ""real data"" example of section 5.2, results are reported using _ten thousand_ independent cascades over a fragment of the US airport network.  But if each cascade is taken to represent, say, a flu season, the idea of having even _thirty_ independent cascades under similar conditions strains credulity.)  Moreover, it is not at all clear that the present approach really would suggest a way forward to more realistic epidemic models and/or problem situations.  Still, as an incremental contribution to this literature with reasonable numerics, it looks promising. ",This seems like a reasonable contribution to advancing the state-of-the-art on a stylized problem; whether it could be a step to something more significant is less clear to me.,2-Confident (read it all; understood it all reasonably well),"The paper deals with the problem of diffusion graph weighting, from activity data observed from the studied networks. More specifically, it focus on an adaptation of NetRate (which corresponds to an extension of the Independent Cascade model with  delays of infection, here in the discrete case, following a geometric law on these  transmission delays) to the case where only the activity of a subset of the nodes of the network can be monitored. The claim is that non observed nodes shouldn't be ignored when learning the transmission probabilities of the diffusion graph.     ","Very interesting and well written paper, which deals with an important problem when learning diffusion models: it may be impossible to observe the activity of every node of the network. For instance, if the source is Twitter, the streaming API only allows one to monitor the activity of about 4000 users over several millions.   The proposed approach is elegant and well sounded. I enjoyed to review this paper that brought me new knowledge about learning in such complex situations. The proposal is maybe not too much innovative since it mostly employs already published techniques, but from my point of view the followed methodology deserves to be published in some main machine learning venue, at least for its pedagogical aspect. Moreover, a good theoretical analysis of the algorithm is given for the case of tree network  structures.    I have two concerns about the applicability of the method in real-world scenarios:      - The model is based (as NetRate and a lot of other models) on the hypothesis that the earlier a node is infected the more likely it is to be. This is induced by the fact that a single parameter controls the activation probability and the time of this activation. In such models, the non-infected nodes are usually considered as infected at a horizon time T. My experience is that this parameter $T$ has a very great impact on the results and is very difficult to tune. However, considering asynchronous diffusion models such as Saito's ones leads to greatly more complex likelihood formulations. I do not know if the present work could be transposed in such a setting.        - The model considers a geometric law for the time delays of transmission between nodes. This is a strong assumption about the regularities on transmission dynamics that may not hold in  real life.   Three questions/coments about the experiments:      - What is the MonteCarlo method used for HTS ? From my point of view this should be an MCMC, such as Gibbs Sampling, since it has to consider activation times of observed infected nodes.     - How are generated the cascades from the real world graph? Using a IC model with a geometric law on diffusion delays as considered in the paper ? I regret that real ""real world"" data have not been used here, as this does not allow to assess the model for real life tasks...     - Only very small networks have been considered here. It would be nice to experiment it on greatly larger networks to assess how it scales  For me the paragraph ""missing information in time"" should be removed since it presents very few interest: it appears as obvious that the algorithm can be easily adapted with a different discretization of the time. At least, it should be reworked (I spent more time to understand this small paragraph than the whole rest of the paper!):    - what is the difference between $\cal K$ and k here ? For me, $k$ should be used in place of $\cal K$ everywhere    - for me $\delta_k$ should be noted $\delta_i^c$, it is the interval in which $\tau_i^c$ is included. Then, it would allow one to skip the indicator function in the following equations, by giving $\Delta_{k_i^c}$ that would be more clear     - line 215: the upperbound of $\tau_i^c$ should be $\tau^{k+1}$ rather than $\tau^{k}$   - Last but not least: I do not understand why non-infections are ignored here. It is claimed that, provided the final time T is not included in the observations, the second term of (9) can be ignored. I disagree with this: every non infected node at the end of the last observed step should be considered as not infected. Thus, a term $P_S^i$ should be given for them. Else, every probability is likely to be greatly over-estimated.. No ?     Minor comments:     - ""Using expression (11)"" line 199 is somewhat misleading (at least it confused me a little for the comprehension of the proof). From my point of view, it would be more clear without this indication.     - Formula (9): \leq T-1 rather than \leq T in the first term, no ? (or strict inequality)    - Line 75: ""the its""    - Formula 4 : I have some problems with the notation of the sum. For me the subscript notation for ""each possible set of hidden activation times"" is not correct here.     - I would add a $\cal O$ as a subscript of \Sigma in formula (9)       - Line 282: overestimated => underestimated (and if I am right the opposite operation line 280) ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper considers the problem of reconstructing parameters of spreading models from partial observations. Because calculating the exact joint probability of partial observations is intractable, it uses the mean-field-type approximations along with the dynamic message passing to get the estimations of the parameters of a propagation model. Compared with the heuristic two-stage (HTS) algorithm, the proposed algorithm has more accurate reconstruction results, lower computational costs, and more robustness over the number of hidden nodes over synthetic data. It also shows meaningful results on real world air transportation data.","My main concern of this paper is that it lacks the comparison over the other approximation approaches. I totally understand the exact calculation is intractable for high dimensional data with partial observations. However, there are several choices of approximation methods. The author needs to justify or compare the chosen mean-field approximation over the other approaches such as expectation propagation approximation and sampling methods. I noticed sampling is used in the HST, but I want to know the comparison over a whole sampling design.  In addition, in the title and introduction, the authors mentioned the proposed algorithm can apply to the spreading models. However, in the problem formulation section, they narrow down the scope to the stochastic susceptible-infected (SI) model. Does SI model represent all the other spreading models? What if the dynamics is recurrent? What about susceptible-infected-susceptible (SIS) model? I want to know if the proposed algorithm generalizes well with the other models. Also, in the real world influenza pandemics example in section 5.2, I think SIS model is more appropriate because people can recover from the influenza.   Also, there are some missing details in the experiment part. For experimental setup in section 5.1, in the synthetic data, how many different graph structures are tested? How sparse are these graphs? In figure 3(b), why HTS behaves very differently when H grows from 4 to 6? Also, in the missing time step experiments, the “every other time stamp is missing” setup is too arbitrary and it is not conclusive. In figure 4, why the parameters are constantly overestimated? Can you explain it? At the end of the paper, there is a fake URL that intends to show the data set and the code. Did you include these materials in the Supplementary Files? I did not find them. If not, I think the fake URL is not necessary here.      Detailed Comments: I think the description that the proposed algorithm is “independent on the number of nodes with unobserved information” (line 65) is not accurate as shown in Figure 2(b). I think they are not independent.  In my personal opinion, the proof of theorem 1 is not necessary because I did not see new insights in this proof. It just repeats the existing proof.  I did not find the definition of N in line 115.  In line 136, it should be “the” instead of “t he”. ",2-Confident (read it all; understood it all reasonably well),The paper provides a version of the message-passing algorithm in order to compute marginal probabilities that appear in the likelihood function marginalized over unknown activation times in the spreading process where the transmition rates are unknown parameters and a part of the cascade is hidden. The paper also provides numerical experiments both on synthetic data and on real-world data in order to show that the proposed method works better than the methods where the marginal (conditional) likelihood is evaluated exactly. ,"Although it is certainly a useful practice to apply message-passing algorithm to compute conditional likelihoods in the proposed setting of the paper, I think the paper lacks many important factors required for publication as I outline below:  The paper is very weak in expressing the basic statistical concepts used in the paper: for example only on page 3: there is no need to adapt any algorithm to see that the MLE for G_alpha is the solution to Equation (3); you cannot refer to any scheme as maximum likelihood estimator (MLE) -- MLE has a very clear meaning, and although it is true that you are finding the MLE for the conditional likelihood (4), the provided explanation is extremely confusing in the least; you should be clear what Monte Carlo method you are using for estimation of Sigma_H (I assume some type of MCMCMLE?); etc.  Although the paper claims to propose a new algorithm, it seems to me that all it does is to compute the conditional likelihood through computing the necessary marginal likelihoods by message-passing. So this seems not to be a new algorithm, but just a way to compute (relatively trivial) steps of the original algorithm faster.  I do not see how Equations (6) and (7) are related to Theorem 1. I think you need another result explaining why (6) and (7) compute the expression in (5).    The quality of English writing is very low: for example, ""can be argued"" not ""an argued""; ""consists of"" not ""consists in""; ""denoted by"" not ""denoted as""; ""mean ..."" not ""have a meaning ...""; ""respectively"" not ""correspondingly""; etc. ",2-Confident (read it all; understood it all reasonably well),"The paper introduces a dynamic message-passing algorithm for spreading parameter reconstruction in a network from incomplete spreading data. The network structure is assumed to be known and the cascades follow the dynamics of the SI-model in discrete time.  The model is evaluated on both, a synthetic- and a real-world data set. On the synthetic data set the algorithm is compared to a related algorithm (HTS). Its applicability to real-world problems is demonstrated by an experiment on the Bureau of Transportation Statistics data set.","The scope and goals of this work are clearly stated, the overall presentation is accessible. While the approach seems promising, the assumptions of knowing the entire network and the SI-model for the spreading dynamics seem (to me as a non-expert in this field) as strong limitations.  While the experiments on the synthetic data seem rather weak (only a comparison to one other algorithm on a small data set), I did find the experiment on the real-world data interesting. I look forward of being corrected in the rebuttal period.",1-Less confident (might not have understood significant parts),,,,,,
Learning User Perceived Clusters with Feature-Level Supervision,"Ting-Yu Cheng, Guiguan Lin, xinyang gong, Kang-Jun Liu, Shan-Hung (Brandon) Wu",https://proceedings.neurips.cc/paper/2016/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html,"The authors propose a solution to integrate a particular type of side information (called here perception vectors) into clustering algorithms as a new way to implement user supervision. More precisely, classical user supervision such as seeds or link/do not link constraints are supposed to be complemented by the so-called perception vectors (e.g. a vector space representation of a text justifying the constraints). Those vectors are not described in the same vector space as the original data. The authors propose to embedded the original data into the perception vector space in a way that respects the specified constraints. Classical clustering algorithms are applied to the embedded data. ","This paper is clearly interesting but it suffers from being squeezed into the page limit of NIPS. In my opinion, the paper does not stand alone (without the supplementary materials) for several reasons. Firstly, reproducing the results without the supplementary materials (SM) is probably very difficult (the algorithms and all the technical details are only given in the SM). Secondly, I had to refer to the SM to get a good grasp at the perception vector concept (more on that latter). Moreover, the experiments contained in the main paper are a bit limited. Finally, even with all that has been transfered to the SM, the paper does not contain any conclusion and real discussion on the results.   Apart from this obvious problem (it's not really a flaw of the paper but rather a consequence of the page limit of the conference), I also think that the main ""selling point"" of the paper, that is the sampling bias, is not very convincing (I'm convinced by the results, not much by the explanations) or at least incorrectly presented. Indeed, the authors only show that their method is less sensitive to the actual constraints provided by the users when they make use of perception vectors, while classical algorithms can be mislead by incorrect or inappropriate constraints. The problem is that users can be ""tricked"" into giving wrong feedback (for instance when shown ambiguous pictures without much context) rather than a sampling problem. In addition, and this is explained quite well in the paper, users base their reasoning on some external assumptions and on additional features that are not accessible to standard algorithms. So the key point here appears to be the possibility of integrating additional features rather than avoiding some possible (but not shown) sampling bias.   In order to fix those problems, I would recommend the authors to reduce the discussion on the sampling bias to a bare minimum (maybe move it to the SM) so as to bring back in the main paper the much needed technical details. There is also some redundancy in the paper as the main solution (seeded OKM + the proposed embedding) is described twice (page 3 and page 6). ",2-Confident (read it all; understood it all reasonably well),"This paper discusses a special semi-supervised clustering algorithm that considers feature-level constraints via user input. It claims it goes beyond instance-level supervision, and is using so-called perception embedding to learn the mapping.   The paper provides good motivation to the feature-level supervision, but I have a few doubts on how this can be applied in real-world applications. The users would normally give high-level supervision instead of at the feature level. What is the exact supervision the Turkers gave in the MTurk example shown in this paper? My understanding is that the users provide certain grouping of images via keywords, which is not what I'd call the feature level. It's more likely that the users provide certain grouping which can be translated into must-link or cannot-link relationships. Also I don't understand what's the ""perception features"" for the song and citation data sets that are used in experiments.   From the formulation (1), it seems the latent variable matrix F and the perception matrix P has the same number of columns (i.e. dimensions). Is this a coincidence or a requirement for the proposed approach? From what I understand the perception can be in any dimensions, which may or may not be the same as the latent dimensionality. I also don't understand why the instance-level constraints can be dropped from the formulation in (3).   What's the time and space complexity of the proposed algorithm? It seems all the 3 test data sets are fairly small so I'm not sure how large-scale can the proposed approach be used. How sensitive are the parameters (dimensions, kernel parameters, etc) to the final results? I'm asking this because there are quite a few parameters to tune so we need to make sure the results are robust.   A minor comment is that the authors put a lot of details into supplementary materials. They should have shortened much of the motivation and discussions, and put those important details in, e.g. algorithms, complexity, data summary.   ","I would suggest the authors to clarify the motivation better, maybe starting with a more clear image for figure 2. Throughout the paper I'm not very clear on what the perception features are. Also the paper can be restructured to have more key details with less lengthy discussions/details. ",2-Confident (read it all; understood it all reasonably well),This paper proposes a novel clustering algorithm which utilizes the side information to consider the user perceptions.  The basic idea is to introduce a so called perception embedded vector to identify the key features that affect the user perceptions. ,"The idea is interesting and reasonable. It can reduce the negative affect of the sample bias problem in semi-supervised clustering methods by utilizing the feature level supervision.  Experiments are performed on several datasets to compare with some baselines, and results validated the effectiveness of the proposed method.  I am curious about how would the proposed method work if the constrains come from different users. Because different users may cluster the data based on different features. And then it would be less possible to learn a unified perception embedding.   I would like to see the affection of the parameter \beta in Equation 3. An empirical study on the sensitivity of parameters can better show how the PE works.  I am not sure why overlapping clustering is a necessary? This constraint makes the baselines weak in the experiments. It is more common to perform non-overlapping clustering.   It would be interesting to also report the results of baselines with only the original features.  The datasets used is quite small with only 180 images.  Figure 2 is less readable; authors may want to replace it with a clear one. ",2-Confident (read it all; understood it all reasonably well),"The abstract describes it quite well.  In the process of semi-supervised-clustering, a user can add contextual information to each cluster seed.  A linear (&optionally nonlinear) embedding from the original features to this ""perceptual"" space is then learned.  Once embedded, clustering is done & performance improved.  ","The approach/problem seemed novel/valuable to me, and the technique sane and technically justified.    Linear/Nonlinear PE objective formulations seem reasonable.  The section on ""Avoiding Sampling Bias"" made a clear point, but the title & claim that this ""avoids sampling bias"" seems like a different claim.    The concrete claim made in that section is: the linear PE objective results in perceptual feature similarity, as well as hard-seed-constraint-similarity being accounted for.  While capturing additional notions of similarity in the objective appears to boost performance, I don't see how this is explicitly aiming at correcting sampling bias.  Sampling bias corrections usual involve some model of sampling, and some post-correction based on this model.    ",2-Confident (read it all; understood it all reasonably well),"This paper describes a semi-supervised clustering technique--an extension of spectral embedded clustering--whereby users provide low-dimensional labels called “perception embeddings” for a few seed data points in each cluster (ex: user provides a few words describing each of several seed images per cluster, in an image clustering task), and these are used to help learn a low-dimensional embedding of the data points to cluster them.  In particular, these perception embeddings do not need to be in the same feature space as that of the data points. Crucially, this method focuses on cases where there are only a small percentage of data points labeled as seeds, and where there may be sampling bias in selecting the seeds.  Noteworthy empirical gains are presented on a range of datasets.","The empirical results seem impressive and thorough.  Overall, this seems to be a simple yet powerful approach that I could see being of great practical use in many real world scenarios- the perception embeddings seem easy to describe in many different useful cases, and the core formulation of the learning objective is a simple and clear extension of popular spectral methods.  The main weakness of the paper, in my opinion, is the clarity of the technical explanation of why the method works over just selecting seeds i.e. specifying data points in feature space, specifically, the explanation of why the method is more tolerant to “misleading” seed sets.  This is explained well initially at a high level in the intro, but somewhat poorly and vaguely in the “avoiding sampling bias” section.  Theoretical guarantees, or something slightly more formal / rigorous / quantitative defining how the method avoids sampling bias / tolerates variance across seed sets, and “bad” seed sets in particular, would be very helpful here.  For example, even just defining a metric for measuring the bias of a seed set (and therefore what exactly a “bad”/”misleading” seed set is) would have been helpful to start.  Perhaps some of the material from the appendix could be helpful here as well.  One question in particular comes to mind for me: how are the perception embeddings actually more helpful than just an indicator vector of class labels for the seed points?  In Fig. 4(b), the use of lower-dimensional PEs is explored, but it doesn’t specifically answer this question.  There is a nice toy example in Appendix Fig. 3, however here the PEs are the class labels.  This seems like an important question to address?  A few minor points: - Lines 121-2: What does “skipping spoliers” and dropping “low quality” clusters mean? - Line 226: Where is Theorem 1? - Editing for grammar / typos would be useful, esp. In the “avoiding sampling bias” section (lines 174-207), and lines 272-279 for example - The point that this method is particularly applicable when there is a small amount of supervision / seeds specified would be useful to emphasize more upfront ",2-Confident (read it all; understood it all reasonably well),"The authors present a framework for Perception Embedded clustering, which combines side information from the user and traditional clustering information to create a semi-supervised clustering.  The key insight is to use information provided by the user at the feature-level to identify the data features that the user finds useful (""cares about"") in clustering.  The introduce two versions of PE, linear and non-linear PE, discuss some theoretical properties non-linear PE, and demonstrate the efficacy of the method on a number of common clustering datasets.","This work has potential to be very valuable to the field given the empirical results the authors show.  There are two main issues with the paper as far as I can tell:  1.  There is far too much time spent in sections 1 and 2, detailing the problem and perception gap.  I didn't really grasp what the authors' contribution was until section 3.  This should be better explained in the abstract and in the introduction.  That is, the authors should state the general idea of their PE approach much earlier in the text, e.g. in the abstract and/or introduction.  The authors should give some intuition as to why they take this approach, and why their approach will lead to better clustering results than the common methods to which they compare.  2.  There are seemingly important empirical results buried in the supplemental material, when these could have a strong positive impact on how the paper is received.  For example, Figure 5a shows that the run-time of the non-linear PE algorithm is roughly constant in the number of data features, which is very important.  However, this is only discussed briefly.  There are other nice results and explanations of the experiments in the supplementary materials, leaving me to wonder if NIPS is really the most appropriate destination for this work.  The paper is generally well-written.  Here is a non-exhaustive list of typos:  ""pairwise constrains""; ""if the user experience difficulty""; ""When clustering musics/videos""; ""Table 1 in supplementary.""; missing a comma in ""nonlinear correlations respectively""; ""which"" should be ""that"" in ""data-features which the user really cares about""; ",1-Less confident (might not have understood significant parts),,,,,,
Double Thompson Sampling for Dueling Bandits,"Huasen Wu, Xin Liu",https://proceedings.neurips.cc/paper/2016/hash/9de6d14fff9806d4bcd1ef555be766cd-Abstract.html,"The paper develops Thompson-sampling style algorithms for the interesting and relevant problem of dueling bandits, where the best arm has to  be learnt using only pairwise preference feedback. The key idea in the main algorithm is to sample twice, and independently, the unknown model from the posterior distribution over models, and essentially pit the best arms for both models against each other. This has the advantage of promoting exploration in early phases and converging to playing the optimal arm against itself in later phases. The authors derive detailed regret bounds for the algorithm for both the Condorcet winner and the more general Copeland winner ground truth models, and perform numerical comparisons of the proposed algorithm with existing, Upper-Confidence-Bound (UCB) style strategies. ","EDIT: I have read the author response.  The paper takes on the challenging task of adapting Thompson sampling-style Bayesian-inspired approaches to the dueling bandit problem, and thus I feel that its contributions are significant. The dueling bandit problem, being a true partial monitoring problem (not all actions give nonzero statistical information), is in a sense harder than standard bandits, and hence requires more care in designing the exploration-exploration tradeoff. Although this is not the first time Thompson sampling has been proposed for the problem (e.g., RCS is an earlier attempt), the smart idea of sampling twice and following it up by a UCB-elimination step is quite novel, and supplemented by fairly competitive analytical guarantees. I believe this would open up paths for more in-depth looks at Thompson sampling approaches to solve classes of dueling bandit problems.   Detailed comments and observations are as follows.   *    Without the Condorcet winner assumption, the algorithm is shown to give a O(K^2logT) regret bound, where as the CCB algorithm of Zoghi et al. gives O(K^2 + KlogT) regret, and SCB gives an even better regret bound of O(KlogKlogT) with certain restrictive assumptions.  Yet, in the experiments, these algorithms seem to be performing quite poorly w.r.t. the proposed algorithm which is counter intuitive, even for 500-armed case D-TS seems to be performing better than CCB/SCB (Fig. 4(d)). Could the authors suggest an explanation for this observation?  * Comparing with existing algorithms with different notions of regret for the dueling bandit problem: Are the regret definitions used in this paper directly comparable to that of some of the existing dueling bandit algorithms (e.g. RUCB, Merge-RUCB, RMED, specially those which are based on Condorcet winner assumptions)? It seems that the notion of regret used in this paper is not directly reducible to that used in these other papers with a Condorcet winner assumption.  * One wonders how the regret performance gets affected if one just uses double Thompson sampling for pulling the two arms (a^(1) and a^(2) in Algorithm 1) from the whole set of K arms, instead of selecting them from the restricted set as described in line 11 and 14 of Algorithm 1. In other words, how much significant is the restrictive sampling (that is used for pulling the pair of arms) in terms of bounding the regret?   * Experiments:                  -It might be worth comparing how the algorithm's performance scales w.r.t. other algorithms, e.g. Merge-RUCB, CCB/SCB etc. with increasing K.   -It might also be worth comparing different algorithms with certain other class of pairwise preference matrices, e.g. pairwise preferences with a Borda winner, with total ordering, or preferences generated from random utility model etc.   - It is not clear why ECW-RMED has higher standard deviations of regret in comparison to the proposed method since in the later case both the arms are sampled from some posterior distribution.      ",2-Confident (read it all; understood it all reasonably well),"The paper proposes a Thompson Sampling (TS) style algorithm for the Copeland dueling bandit problem. The practical significance of the paper arises from the fact that, unlike the MAB setting where TS and KL-UCB have comparable performance, in the dueling bandit setting, TS-style algorithms tend to perform significantly better in practice than those using confidence bounds and this paper provides the first analysis of such an algorithm.  The authors provide regret bounds for the proposed algorithm and conduct extensive experimental analysis, comparing the algorithm against every baseline published in the literature using numerous examples, carefully studying the strengths and the flaws of their algorithm. I must say I appreciate their thoroughness and honesty as far as the experimental analysis goes, which unfortunately is outside the norm for ICML/NIPS papers.  The regret bound proven for the algorithm does not match the lower bound published in reference [7], but it has the enormous advantage that it is much more easily interpretable, which one could argue to be a more desirable quality than optimality, when the lower bound takes the cryptic form that it does in [7], where it is stated as the solution to some linear program.  I do however have two specific issues with the proofs and I'd like the authors to address them in the rebuttal: 1- As stated, Lemma 6 is false because when applying Chernoff-Hoeffding, n (i.e. the number of samples being summed) needs to be fixed and not a random variable depending on t. It seems like the authors need a union bound on n, which means that in Equations (12-14), t^{2\alpha} should be changed to t^{2\alpha-1}. 2- In the case of Lemma 5, I do not understand how the proof can use only the asymmetric counts N^(1)_ij and not the symmetric N_ij when \theta is sampled from a posterior distribution that depends on the latter and not the former. Could you explain intuitively why you do not use the N_ij in there or are they used and I'm not seeing them for some reason? ","The authors seem to have expended a fair bit of effort on comparing their algorithm against the existing baselines and on providing detailed arguments for their theoretical results. Moreover, the algorithm seems to perform substantially better than the baselines on most examples arising from the applications and so it is a valuable contribution from a practical point of view.  The proof techniques are not really groundbreaking, but this is not a COLT submission, so I'm judging the paper more for its algorithmic contributions rather than theoretical ones.","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","The paper provides an algorithm for the dueling bandit problem with the Copeland regret definition. The authors provide an algorithm that uses a mixture of the confidence bound, and Thompson sampling methods. Crudely speaking, Thompson sampling is used either to solve sub instances that can be considered as a multi-armed bandit problem, or to break ties. The resulting regret bound is non-trivial yet is not as good as previous results. However, the algorithm  seems to perform better in practice based on thorough experiments.","The algorithm proposed follows the general ideas of RUCB. It picks a left arm according to its optimal score, meaning it chooses an arm maximizing the optimal Copeland score, and then the right arm as the one that has potential of beating the chosen left arm. The difference from RUCB is two-fold. First, the right arm is not chosen according to a UCB type procedure but  rather via a Thompson-sampling procedure. Standard techniques allow to bound the expected number of pulls for a  pair of arms, leading to the regret bound. The second difference is in the selection  of the left arm. Rather than choosing an arbitrary maximizer of the Copeland score, ties are broken via sampling from the posterior of the last round.  The main contribution of the paper as I see it is not in presenting novel techniques but rather in a way of combining existing techniques to obtain a more (empirically) practical algorithm. The experiments provided give a convincing  argument that the algorithm has a superior empirical behavior.    The paper is overall well written with the possible exception of not having a clear enough high level description of  the proof techniques. The above explanation needs to be extracted out and something similar should probably appear  somewhere in the paper. The proofs seem to hold and are sufficiently well written, the experiments are thorough in evaluating  the algorithm, and the review of the previous results is accurate and fair.  Minor comment:  Lemma 6 in the appendix (line 410) is not accurate. Chernoff bound can be used only when the number of trails is fixed here, the number of trials depend on the outcome of the coin tosses. However, via a union bound for all possible tries the lemma still holds but with 2\alpha-1 rather than 2\alpha. This affects only the constants in the proofs","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","Authors addressed the dueling bandit problem where the task is to pull a pair of arms in each iteration from a given set of K arms, upon which one gets to see the winner arm drawn from some underlying pairwise preferences. The objective is to identify the set of arms which are the Copeland winners or minimize regret w.r.t. the Copeland winners.","1. The major contribution of the work is not clear. Similar or even better regret guarantees are given previously for the same problem. For e.g. the RMED algorithm by Komiyama et al. gives a O(KlogT) regret bound which is optimal to constant factors of the regret lower bound with condorcet winner assumption where as the proposed algorithm gives a regret guarantee of O(KlogT + K^2loglogT), which is clearly theoretically worse than the performance of RMED, yet in Figure 1(a), D-TS/D-TS++ seems to be performing better than RMED which is not so convincing.     Without the condorcet winner assumption, the algorithm is shown to give O(K^2logT) regret bound, where as CCB algorithm of Zoghi et al. gives O(K^2 + KlogT) regret, and SCB gives an even better regret bound of O(KlogKlogT) with certain restrictive assumptions, yet again in the experimentally these algorithms seem to be performing quite poorly w.r.t. the proposed algorithm which is counter intuitive, even for 500-armed case D-TS seems to be performing better than CCB/SCB (Fig. 4(d)), what is the reason?    Even the concept of using Thompson sampling is not new, in RCS algorithm, Zoghi et al. used the similar notion to pull the first arm and the best competing arm of first arm is chosen as the second arm.  2. Comparing with existing algorithms with different notion of regret: Are the regret definition used in this paper directly comparable to that of some of the existing dueling bandit algorithms (e.g. RUCB, Merge-RUCB, RMED, specially those which are based on condorcet winner assumption), since notion of regret used in this paper is not directly reducible to that used in these other papers with condorcet winner assumption.  3. How does the regret performance get affected if one just use simple Thompson sampling for pulling the two arms (a^(1) and a^(2) in Algorithm 1) from the whole set of K arms, instead of selecting them from the restricted set as described in line 11 and 14 of Algorithm 1. In other words, how much significant is the restrictive sampling (that is used for pulling the pair of arms) in terms of bounding the regret.  4. Notation: There are few typos in the paper, for e.g. line 237: O(KlogT + K^2logT) --> O(KlogT + K^2loglogT) etc.  5. What is the practical significance of Assumption (2)?  6. Experiments: As pointed out in comment 1, some of the results/plots are not very convincing since the experimental results seem to be conflicting the theoretical guarantees.   - It might be worth comparing how the algorithm's performance scales w.r.t. other algorithms, e.g. Merge-RUCB, CCB/SCB etc. with increasing K.   - It might also be worth comparing different algorithms with certain other class of pairwise preference matrices, e.g. pairwise preferences with borda winner, with total ordering, or preferences generated from random utility model etc.   - Its not clear why ECW-RMED has higher standard deviations of regret in comparison to the proposed method since in the later case both the arms are sampled from some posterior distribution. ",2-Confident (read it all; understood it all reasonably well),"This paper proposes two variants of Thompson Sampling for the dueling bandit problem. The authors obtain a regret bound for the two variants, and present an explanation for the advantage of one variant (D-TS+) over the other (D-TS). Experimentally, the authors show that the proposed algorithms outperform previous methods for the dueling bandit problem in some regimes."," In the same section, it is mentioned that theoretical analysis of TS in MAB is difficult. However, there have been some recent work in the literature which consider analysis of TS.   Why double sampling used in both arms gives D-TS advantage over RCS, and significantly reduces the regret?   The authors mention in Section 1 that the two selected arms in dueling bandits may be correlated, and hence to address this issue the proposed D-TS algorithm draws the two arms independently. Performance-wise, is there any advantage in the two selected arms being independent? If so, why?  There are some typos/grammatical errors: in line 282, replace results with result; in line 307, replace non with not; in line 78, replace experiment with experimental.  In Section 5 line 299-300, the authors mention that the arms are randomly shuffled to prevent algorithms from exploiting special structures of the preference matrix. More elaboration on this is required.  One drawback of this paper is that the regret bounds for the two proposed algorithms are the same; so it is not easy to gauge the advantage of D-TS+ over D-TS, considering that D-TS is outperformed by ECW-RMED in the non-Condorcet regime. In addition, as mentioned by the authors the regret bounds are likely loose.  If RUCB-based elimination seldom occurs in practice, then why use it at all? As a result of using it, both the algorithm and its regret analysis become much more complicated, and the obtained bounds are loose. Also, is it possible to obtain tighter bounds if RUCB is dropped from the algorithm? Have the authors checked the difference in performance of the proposed algorithms in numerical experiments if RUCB-elimination step is dropped?  From Section 4.1, it seems that in D-TS, the second arm is obtained from comparison with the first chosen arm only. Is there any explanation for this selection?  In lines 305-309: if RMED1 is optimal in Condorcet dueling bandits, then why the proposed algorithms outperform RMED1?  Any intuitions into why the STD of the proposed algorithms is so much smaller than that of ECW-RMED in the non-Condorcet case?  The only difference between D-TS and D-TS+ is in the tie-breaking criterion. In numerical experiments, D-TS and D-TS+ perform identically in terms of regret in the Condorcet case, but D-TS+ outperforms the other in the non-Condorcet case. How is this performance gap explained by the difference between D-TS and D-TS+? ",2-Confident (read it all; understood it all reasonably well),"This paper gives an algorithm Double-Thompson Sampling solving the dueling bandits problems. From the experimental results we can see this algorithm has a good behavior, and this is also proved in the theoretical analysis. ",The idea of D-TS is good and the analysis result is an improvement in analyzing TS algorithms in MAB problems. The experimental results also shows that the D-TS algorithms behaves well in simulation. ,2-Confident (read it all; understood it all reasonably well),,,,,,
High resolution neural connectivity from incomplete tracing data using nonnegative spline regression,"Kameron D. Harris, Stefan Mihalas, Eric Shea-Brown",https://proceedings.neurips.cc/paper/2016/hash/f337d999d9ad116a7b4f3d409fcc6480-Abstract.html,"This paper presents a structured regression model for inferring neural connectivity from viral tracing data. Combining a nonnegativity constraint, a spatial smoothness regularizer, a low-rank assumption, and a mask that handles ambiguity within the injection site, the authors derive a model that can leverage observed tracer data to infer the underlying connectivity. While the individual components of this model have been explored in other structured regression problems, this paper presents a thorough and well-executed application of these ideas to an interesting problem.","The underlying model is a non-negative linear regression, y = Wx + \eta, where \eta is drawn from a spherical Gaussian model.  The weight matrix, W, is assumed to be nonnegative and, in probabilistic terms, drawn from a spatially smooth prior. Optionally, a low-rank assumption may be incorporated into the weight model, which can dramatically improve memory efficiency for large-scale problems.  While the individual components of this model (nonnegative regression, Laplacian regularized least squares, low-rank constraints) are well-studied, I think this is a nice combination and application of these techniques to a real-world, scientific problem. The presentation of the model, the synthetic examples, and the real world applications (and supplementary movies) are particularly clear.  While it is certainly valid to directly construct a objective function that captures both the reconstruction error and the domain-specific constraints and inductive biases, I think a probabilistic perspective could elucidate a number of potential extensions and connections to existing work. For example, Jonas and Kording [1] tackle a similar connectomics problem by formulating a probabilistic generative model in terms of latent variables of each neuron. While the setup is somewhat different (theirs is an unsupervised modeling problem whereas yours is framed as a regression problem), one could imagine a similar set of latent variables serving as a prior for your weight matrix.  Similarly, from a probabilistic perspective, the Gaussian noise model is not particularly realistic given that the fluorescence observations are nonnegative. Likewise, the spherical covariance seems likely to be a strong assumption. While these may justifiable and computationally advantageous simplifications, it is worth discussing. Finally, the orthogonal projector, P_{\Omega}, seems like a rather fancy way of saying that your loss function is only summing over observable voxels.  Minor comments: - The labels in Figure 3 are not very legible.  [1] Jonas, Eric, and Konrad Kording. ""Automatic discovery of cell types and microcircuitry from neural connectomics."" eLife 4 (2015): e04250.",2-Confident (read it all; understood it all reasonably well),"This paper improves the accuracy and resolution of linear models fitted to neural projection tracing experiments, by adding a smoothness prior. Additionally, low rank approximation to the connectivity matrix enables memory savings.","The paper is very easy to read. Also, the suggested method seems straightforward to implement, and significantly improves performance of previous region-based method. Therefore, to the best of my knowledge, the method should be useful to practitioners, especially if the code is released after publication.  %% After authors' rebuttal %% I thank the authors for the rebuttal and very nice paper. My only minor suggestion is that the authors change the title so that the technical jargon appears in the end, e.g.  ""High resolution neural connectivity extracted from incomplete tracing data using nonnegative spline regression""",2-Confident (read it all; understood it all reasonably well),"The authors present a novel method for identifying structural connectivity in the brain on the basis of sparse retrogade labeling experiments. The method they use combines a matrix completion loss, a smoothing penalty that enforces spatial homogeneity in the recovered structural connectivity, and a low rank approximation of the data which promises to make the method scalable to full brain maps.  The method is tested on synthetic data, and applied to publicly available data from the Allen Mouse Brain Connectivity Atlas, on which it significantly outperforms state-of-the-art approaches. ","The authors tackle a relevant and interesting problem: how best to infer anatomical connectivity at the mesoscale from brain tracing experiments, when the number of tracing experiments that can be performed is low, the number of free parameters in the connectivity matrix is a priori high, and experiments suffer from artifacts directly caused by the injection themselves.   They present an elegant method that takes into account those three difficulties by means of a well-designed optimization problem. They make an assumption of spatial smoothness for the connectivity matrix that is well justified by the retinotopic organization of the visual cortex, from which the data they use comes. They address the issue of experimental artifacts at the site of injection by using a projection operator similar to those used in matrix projection techniques. They provide an extension of their optimization procedure that allows for a low-rank representation of the connectivity matrix. This extension could be used to scale the model to larger data.   The paper is well written and easy to follow. The authors carefully demonstrate the various modeling choices they made on synthetic data for which ground truth is known. They conclude by providing results on real data, for which they significantly outperform the state of the art. ",2-Confident (read it all; understood it all reasonably well),This paper applies a regularized multivariate regression method to an interesting task of recovering the neural connectivity of the visual cortex of the mouse brain at a finer level by estimating a large spare coefficient matrix. Experiment results on real data shows the improved performance over the results at the coarser regional level.  --------------------------------------------------------------------------- I adjust the score after reading the rebuttal and comments of other reviewers. ,"The main contribution of this paper is to apply regularized regression method to an interesting application from neural science, which estimates the connectivity maps of mouse brain. The author extends the work of [17] by solving the problem at a finer Voxel level which corresponds to a larger regression coefficient (5000x5000 matrix).  However, from the viewpoint of modeling, it is just a smoothing penalized multivariate linear regression model for the least squares problem, which is very similar to many existing matrix/tensor completion methods, thus the novelty is rather limited.   For experiments, the author didn't compare with any other methods, so we don't known whether the proposed method is good enough or not. I think more algorithms and baselines should be included in the comparison.",2-Confident (read it all; understood it all reasonably well),"The paper describes a technique for estimating a weights matrix, corresponding to brain neural connectivity. The technique combines several machine learning tools.","The result may be of extreme importance to neuroscientists. This is entirely out of my area, and my review relates only to the technical contribution in terms of the novelty of the proposed algorithm.  ",1-Less confident (might not have understood significant parts),"This paper concerns a method to find the connectivity map between ""voxels of a template brain"" using several viral injections that trace the anatomy of the involved neurons. Such viral approaches to connectomics have gained popularity in the last ~10 years. Recently, the Allen Institute released one such dataset. Here, the authors refine the connectivity inference routine of that original paper.","The paper suggests a reasonable approach and proposes a method to infer the connectivity. The results show that the proposed method outperforms the baseline, which assumes that connectivity is homogeneous within brain regions. In this sense, the proposed method is a relaxation of the baseline.  I think this approach is valuable in revealing the major connectivity themes (similar to diffusion MRI). However, the smoothness constraint also somewhat defeats the purpose of the complicated neuroanatomy.  It would be helpful to discuss the linear model: it assumes that the fluorescence grows linearly with the injection sites.  In the original dataset, the composite data is obtained from many brains and a template brain is formed. This should be explained. More importantly, expressions such as ""the number of voxels in the brain"" should be avoided.",2-Confident (read it all; understood it all reasonably well),,,,,,
Statistical Inference for Pairwise Graphical Models Using Score Matching,"Ming Yu, Mladen Kolar, Varun Gupta",https://proceedings.neurips.cc/paper/2016/hash/411ae1bf081d1674ca6091f8c59a266f-Abstract.html,"This paper develops an analysis in terms of asymptotic normality for a single edge parameter in graphical models using the Hyvarinen scoring rule. The Hyvarinen scoring rule means the log-partition function does not need to be known and consequently applies in settings where errors may be made in the model selection process. Section 4 provides the main theoretical result, asymptotic normality under two conditions, an assumption on the sparsity level and a sparse eigenvalue condition. ","This paper addresses a relevant problem of proving asymptotic normality and consequently  confidence intervals for learning edge parameters for pairwise graphical models. The score matching approach which has received recent interest overcomes the challenging of requiring knowledge of the log-partition function which is a useful contribution. The proofs appear to be correct but in Section 4.1, a slightly greater discussion on comparison of proof techniques in prior work that considers score-matching and confidence intervals in high-dimensional statistics would be useful. I believe this is a useful contribution to the graphical model learning and high-dimensional statistics communities.  ",2-Confident (read it all; understood it all reasonably well),"This paper studies the problem of estimating parameters in a pairwise graphical model and constructing confidence intervals for the parameters. As part of this an asymptotically normal estimator is constructed. The key progress made in this paper is that inference (i.e., confidence intervals) is done in a setting where computation of basic quantities (e.g. partition function) is intractable. (In contrast, for Gaussian graphical model one can efficiently compute the normalizing constant.)  Specifically, an estimator based on Hyvarinen score is given for estimation of a single edge in the pairwise undirected graphical model. The new scoring rule uses the conditional density of two variables given the rest. A first step forms a preliminary Markov blanket for a pair of variables, and the estimator then re-optimizes over parameter value, which has a sort of decoupling effect. The estimator is shown to satisfy asymptotic normality subject to regularity assumptions on the model (Corollary 2). Finally, experiments on simulated data show the estimator behaves as one would expect, and an experiment on protein signaling data agrees with results from previous papers analyzing that dataset.    The estimator is  ","This paper was a pleasure to read: the results address a problem of significant interest, the writing is excellent, and the overall level of scholarship is high with context and background carefully placed within the broader literature. The proofs are fairly simple, making good use of existing technology developed in the cited work, but the estimator is novel.   It would be nice to remark if possible on what does the assumption SE (line 176) really mean as far as modeling power of the model. ",2-Confident (read it all; understood it all reasonably well),"The paper presents a novel inference procedure for pairwise graphical models based on score matching. The proposed method is applicable to a variety of graphical models, going beyond the typical Gaussian case. Theory is very elegant. Illustrations using simulated data and real data are also very clear and convincing. Overall, this is an excellent paper with a novel method and good presentation. ","A few minor concerns are: (1) In Corollary 2, one condition is that (s\log p)^2/n = o(1). Here is s the same as the one defined on page 2 line 33? If so, then s is a very large parameter. I wonder is the s in this Corollary be the sparsity parameter m. (2) The simulations reveal very good performance of the proposed method. Do you have any results of comparing the current method with existing methods, especially since there are available works under the Gaussian graphical models settings?  Notations: (1) In Theorem 1 and Corollary 2, what is the vector w^*? Although it can be seen from the proof later what w is, it might be good to introduce the notation while presenting the main results. (2) In Corollary 2 line 195 definition of V: it should be E[\sigma_n]^{-2}.  (3) The first line of equation (12) on page 6: tilde w rather than tilde w ^*. (3) Lemma 7 statement of the main equation: it should be tilde w - w^*. (4) Lemma 8 proofs line 378 first line of the equation: it should be w^*.  Grammatical errors: (1) line 97 page 3: shown to be asymptotically ... (2) line 254 page 7: We should point out that ...  ",2-Confident (read it all; understood it all reasonably well),This paper works on the construction of estimates and inferences for pairwise graphical models with exponential family distributions. The paper employs the score matching trick and combines it with the post double selection idea for high dimensional regression to construct the estimates. This procedure provides a root-n consistent estimate of the pair-wise parameters as well as asymptotic normality. This involves establishing a Bahadur representation of the root-n scaled centered estimate of the parameter estimates. The authors also provide simulation studies to provide empirical validity of their methods.,"The paper is well written and tackles an interesting problem. I liked the clever combination of post double selection methods and score matching to construct such estimate. I have the following specify comments:  1. I believe the authors use ""m"" to denote sparsity as shown in Assumption M but use ""s"" in Corollary 2 (and also later) in Page 7. ""s"" is used to denote total number of parameters in Page 2 which is O(p squared).   2. Could the authors highlight the sparsity requirements in terms of degree of the precision matrix etc? is ""m"" of the same order as degree?  3. Are the results in this paper also valid for non-negative data?  ",2-Confident (read it all; understood it all reasonably well),"The paper proposes a parameter estimator for pairwise Markov networks with continuous random variables based on the Hyvärinen scoring rule. It proves that it is \sqrt{n} consistent and asymptotically normal, which enables the construction of confidence intervals for the parameters. Experiments on synthetic data confirm the estimator's properties. An experiment on a protein signaling dataset produces results consistent with estimates produced by other methods.","This paper addresses the hard problem of dealing with the partition function in undirected graphical models. Work on the Hyvärinen scoring rule is promising because it only requires knowing a density function up to a normalizing constant. The technical work all appears sound, although I am not 100% confident that I haven't missed something.  However, I think the paper is only narrowly applicable. Is there another distribution beyond the exponential graphical model that this method can be used for? I also do not understand the focus on Gaussian graphical models in the experiments section. The related work discusses several other methods that are applicable to Gaussian graphical models, but it seems the only difference discussed (lines 62-64) is that this method is applicable to models for which the partition function is intractable. I'm unclear on whether the authors are including Gaussian graphical models in that category. (I wouldn't.)",1-Less confident (might not have understood significant parts),"The paper suggests a novel method to learn pair-wise graphical model defined over continuous but not necessarily Gaussian variables. The method consists in suggesting a new score function based on Hyavarinen scoring rule.  Importantly,  the authors built a strong theoretical foundation for the method - proving/claiming that the method is $\sqrt{n}$ consistent and asymptotically normal (where $n\to\infty$ is the number of samples). ","Few questions/comments: 1) I wonder if the authors have anything to say about how the number of samples sufficient for achieving a fixed accuracy scales with the system size?  According to the experiments  the scaling does not look exponential (difficult).  Can the authors prove it,  or at least argue?    2) I would also like to see discussion of how the approach explained in the paper is related to approaches seeking for objectives beyond these based on sufficient statistics, see e.g. https://arxiv.org/abs/1411.6156, https://arxiv.org/abs/1605.07252 and references therein.   3) Comment on the relation between the new score function and standard pseudo-log-likelihood score function would be useful. In this regards,  experimental illustrations would certainly benefit from comparison with some other methods, e.g. based on the pseudo-log-likelihood and/or Gaussian fitting. ",2-Confident (read it all; understood it all reasonably well),,,,,,
Learning shape correspondence with anisotropic convolutional neural networks,"Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael Bronstein",https://proceedings.neurips.cc/paper/2016/hash/228499b55310264a8ea0e27b6e7c6ab6-Abstract.html,"The authors propose the ""anisotropic neural network"", a novel method for intrinsic deep learning, i.e. directly computing functions computed on a non-Euclidean domain. They are particularly interested in computing correspondences between different non-Euclidean objects, e.g. computing corresponding points on a body in different poses.  I'm not familiar with this area, but I think that the anisotropic network works as follows: You learn (?) several heat kernels, which each implement a mapping from the intrinsic space onto a polar system of coordinates. For each of the heat kernels, several filters are applied. The i'th filter from each kernel is summed to form the i'th output plane.  The authors show qualitative results on several benchmarks, which strongly outperform the prior GCNN approach.","The paper is presented very clearly, giving ample references, excellent diagrams, and clear explanation of the problem, theoretical work, and evaluation. I wonder if some of the explanation in Sec. 2 and 3 can be made a bit more accessible to a wider audience.  I don't have suitable background to evaluate the theoretical approach or comparison with other methods for instrinsic learning on non-Euclidean domains.  The qualitative results, both in isolation and in comparison to prior work, look quite remarkable to me. The correspondence mapping shown in the supplementary material is particularly impressive. Even though I am not familiar with this area, I would love to learn more about this work.",1-Less confident (might not have understood significant parts),"This paper introduces Anisotropic CNN, a new framework generalizing convolutional neural networks to non-Euclidean domains, allowing to perform deep learning on geometric data. This is basically a machine learning application of computer graphics and geometry processing applications. The experiments (on benchmarks such as SHREC’16 Partial) show that ACNN outperforms previously proposed intrinsic CNN models, as well as additional state-of-the-art methods in the shape correspondence application in challenging settings.","This paper is generally well written. The application of finding shape correspondence is quite important for many fields, especially for shape analysis related fields. The proposed approach seems quite effective as well. The experiments are convincing in general, although some additional improvements can be made. For example, more datasets and also comparisons against more methods. It is also interesting to try more advanced network architectures and see if this can improve the performance. It is mentioned that ""First 80 shapes for training and the remaining 20 for testing"". I assume these 80 shapes are from 8 subjects, and the 20 testing ones are from the remaining 2 subjects? In this case, it makes more sense to use leave-one-out considering that there are only 10 subjects.  Overall the quality of this paper is good and I didn't notice any major flaw. My main concern is that whether this fits the scope of NIPS. This is more like a graphics paper (most compared methods are published in that community as well). Other than this, I have no concerns.   ",1-Less confident (might not have understood significant parts),"The paper aims at learning intrinsic correspondence using convolutional neural network, The proposed method, Anisotropic Convolutional Neural Network (ACNN), is a variant of CNN that can deal with non-Euclidean domains. Overall, the paper is well written.","(1) The novelty. The paper is not so novel. Basically, the proposed method extends ADD [5] to deep learning framework, which is proposed in GCNN [16]. Meanwhile, this paper is not the first one to learn shape correspondence using deep learning. In this sense, the importance of this paper might be limited.  (2) The experiments. The experimental comparison is sufficient. However, only one method [5] is based on deep learning as the proposed method. Such a comparison is somewhat unfair. As the most relevant paper, why [16] is not compared?  ",2-Confident (read it all; understood it all reasonably well),"This paper proposes a novel type of CNN that operates on Riemannian manifolds. Different from existing such manifold CNNs, the proposed method uses anisotropic diffusion kernels to reparameterize functions defined in a local neighborhood of a point on surface. This new architecture, called ACNN, is tested on the point correspondence problem between shapes. Experiments show that it outperforms previous approaches on standard benchmark datasets of both complete and partial 3D shapes.","It is still an open problem of what the best practice is to design convolutional neural networks on manifold. Unlike CNNs on Euclidean space, parallel translation and the   statistical invariance coupled with this operation are not well defined. This core problem has stimulated several recent attempts, and this paper also falls into this category. In my opinion, it is reasonable to use heat diffusion process to define the proximity of a point. However, it is still unclear why the invariance assumption should hold using this parameterization. I would be more satisfied if the authors could do more in-depth statistical analysis over the distribution of the output from the patch operator, as defined in Eq (7). ","3-Expert (read the paper in detail, know the area, quite certain of my opinion)","This paper proposes a novel model called Anisotropic Convolutional Neural Network (ACNN), which generalizes classical convolutional neural networks to non-Euclidean domains. This work builds on two methods, namely, the Geodesic Convolutional Neural Network (""Geodesic Convolutional Neural Networks on Riemannian Manifolds"") and the concept of Anisotropic Diffusion Descriptors. While sharing many foundational ideas, it describes a new ""fused"" approach resembling the idea of the Geodesic Convolutional Neural Network, but using kernels of the anisotropic Laplacian operator and avoiding a construction of geodesic local patches (which, if I understand everything correctly, appears to be an artificial and inconvenient construct). The suggested approach is demonstrated to outperform other approaches when applied to the problems of full and partial mesh correspondence.","> Technical quality.  I believe the paper to be of a high technical quality. The calculations are sound and appear to not have any major flaws or mistakes (other than small inaccuracies outlined below). The experimental methods used to evaluate the proposed method seem to be appropriate.  > Novelty/originality.  This work builds on the publication ""Geodesic Convolutional Neural Networks on Riemannian Manifolds"" and the idea of Anisotropic Diffusion Descriptors. It shares the foundational concepts and notation with these articles, but proposes a novel ""fused"" approach combining the strengths of these two prior publications. While not entirely novel in its details, the combination of two approaches seems to be a successful idea and as such a promising ""step in the right direction"".  > Potential impact or usefulness.  This work proposes a model which shows a significantly improved performance compared to prior approaches and seems to show a potential of having a large impact in its specialized sub-field.  > Clarity and presentation.  The paper is well written. However, the clarity of the presentation is lacking, seemingly due to the fact that the authors expect the reader to be familiar with prior publications on Geodesic Convolutional Neural Networks and Anisotropic Diffusion Descriptors. Some of the minor inaccuracies/problems include: (a) the conductivity tensor defined in Eq. (2) should have an additional x-dependent multiplier; (b) in Eq. (7), D should have subscripts; (c) the subsection about the discretization of the anisotropic Laplacian operator should include at least some references (Anisotropic Diffusion Descriptors, ...?); (d) instead of ""the solution of heat equation (1) at time t is obtained by applying the anisotropic heat operator ..."", the sentence should read ""the solution of heat equation (1) with D=D_{\alpha \theta} at time t is obtained by applying the anisotropic heat operator ...""; (e) although the meaning of the notation < a,b >_H should be clear for the majority of the readers, including a definition or a reference would improve the clarity of the article.",1-Less confident (might not have understood significant parts),This paper uses anisotropic heat kernel as local intrinsic filters to construct convolutional neural network. The network is used to construct correspondence between deformed shapes. Experiments on FAUST and SHREC'16 show good performance for both full shape and partial shape correspondence.,"Compared to GCNN, this paper simplifies the construction of discrete patch operator. It also avoided using angular max pooling by using local curvature direction as reference frame. However the paper did not make it clear why ACNN does not need mesh as input. In construction of discrete anisotropic Laplacian a mesh is used with its faces and edges.",2-Confident (read it all; understood it all reasonably well),,,,,,
Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity,"Amit Daniely, Roy Frostig, Yoram Singer",https://proceedings.neurips.cc/paper/2016/hash/abea47ba24142ed16b7d8fbf2c740e0d-Abstract.html,The article introduces a new computational model called computation skeleton. It makes it possible to develop a duality between feed-forward neural networks and compositional kernel Hilbert spaces. The main results established are lower bounds on the replication parameter r.  ," This contribution to the theory of feed-forward neural networks, through the introduction of a new computational model, the computation skeleton, seems both original and promising. The reader who is not very familiar with deep neural networks (this is my case) will find it harder to understand the paper. The main weakness of the presentation rests in the fact that the motivations for some of the major hypotheses made (see Section 2) are to be found in the supplemnetary material. To my mind, the 9-page paper should be more self-contained. Furthermore, there are quite a few typos.  ",2-Confident (read it all; understood it all reasonably well),The authors give a dual relationship between neural networks and compositional reproducing kernel Hilbert spaces. This is based on an introduced notion of computation skeleton. The corresponding optimization problems are also considered. ,"The duality is well described and the notion of computation skeleton is well presented. The theoretical bounds involving the optimization problems seem reasonable, but I did not check the proofs. However, no numerical experiments are provided. It would be good to demonstrate a new concept by some experiments. ",2-Confident (read it all; understood it all reasonably well),The paper introduced a concept of computation skeleton associated to neural networks and defined a composition kernel. This kernel can be approximated by the empirical kernel induced by an random initialization of the neural network.,"This paper discussed the relationship between several concepts associated to neural networks, especially the relation of composite kernel and empirical kernel. Although the results looks somewhat novel and interesting, the main problem of the paper is lack of discussion on why these results are important to understand the neural network learning either theoretically or empirically. ",1-Less confident (might not have understood significant parts),"The authors introduce the notion of computational skeleton and show how a skeleton can be related to a RKHS (more precisely, to a compositional kernel) through the concept of ""dual activation"".  They also introduce a class of neural networks, called ""realisation of a skeleton"", defined through a given number of replications of a computational skeleton and a given number a of outputs (r and k, respectively).  The main results of the paper consist in analysing in what sense the representation generated by a random initialisation of such a neural network (i.e., a skeleton realisation) approximates the compositional kernel related to the underlying skeleton. ","# Some typos/remarks:  -At line 154, do the authors mean ""any continuous PSD function""? -Two typos at lines 171 (y instead of x') and 177 (""of in the induced space"").  -I am not sure about the meaning of the ""2-norm"" mentioned at lines 173 and 184, maybe the author should specify it.  -At line 190, S is a computational skeleton, and k_{S} is a compositional kernel.  -In Lemma 11 (appendix), I do not really understand what does b_{|A|} mean.    #Questions (maybe not founded...):  -Is it possible to interpret the fact that ""for a random activation, the empirical kernel k_w approximates the kernel k_S"", as a kind of ""law of large numbers"", in the sense: ""the replication of the computation skeleton counterbalances the fact that the weight of the network are random""?   -What kind of impact the operation of ""learning the weights of the network"" could have, for instance, on the relation between k_w and k_S (and on the related Hilbert spaces)?   -Is it possible, in view of Thm. 5 and 6, to draw a parallel between ""learning with a skeleton realisation"" and ""learning with a compositional kernel""?   ########################################### ### After rebuttal  My major concern about the paper is the lack of discussion on ""why these results may be important or interesting"". In fact, there is almost no discussion at all about the implication of the results of the paper.   The notions of ""computational skeleton"" and ""realisation of a skeleton"" seem relatively interesting and aim at generalising the kernel construction proposed in [13] and [29]. This ""construction"" is in my view the main contribution of the paper.   On a theoretical point of view and from my understanding, the main results of the paper are Thm 3. and 4. However, these results are not ""so surprising"", since, as confirmed by the authors, they can be interpreted as a kind of ""law of large number"": the random activation of the network is ""counterbalanced"" by the replication of the skeleton. Note that there is a big typo in eq. (12) of the supplementary material. As indicated by Table 1, some results of this type are already known.   From my understanding, Thm 5 and 6 are in fact just ""advanced corollaries"" of Thm 3. and 4 (as indicated at line 586 of the appendix, with a typo).  In their feedback, the authors indicate (about Thm 5 and 6) that: ""this shows that training the final layer is essentially equivalent to learning with the corresponding kernel"". However, for ""kernel-based learning"" (line 82), the authors deal with the classical ""loss + regularisation"" formulation, while classically, ""only loss"" is considered for ""Neural Network-based learning"" (line 74); so that I do not really understand how ""training the final layer"" could be sensitive to ""the size of the ball in the RKHS"".  In my view and from my understanding, these results are about ""the difference between learning with K_S or K_w in the classical framework of kernel-based learning"", so that I do not really agree with the answer of the authors.   Another ""drawback"" of the approach is, in my opinion, that the input space is restricted to be of ""hypersphere-type"".    Finally, the paper is quite difficult to read and its construction may, in my opinion, be improved in order to try to clarify and simplify the paper.   ",2-Confident (read it all; understood it all reasonably well),"The paper introduces a new connection between neural networks and kernel methods.  The paper introduces the concept of the computational skeleton representing a neural network, then defines a kernel function from this which may be viewed as a mean kernel when weights are drawn from a normal distribution.  The paper investigates how well this kernel approximates the kernel induced by a skeleton with randomly selected weights, and uses this for a basis for discussion of how deep networks are able to perform so well when much theoretical work often predicts that they will barely work better than random.",(caveat: while I have a reasonably strong background in kernel methods my understanding of (deep) neural networks is less well developed).  The paper presents an interesting and (to the best of my knowledge) new approach to investigating the performance of neural networks in terms of kernel methods.  The results appear sound and have interesting implications for both fields.  Minor points:  - I'm not sure that citing wikipedia is advisable (line 341). - Lemma 11: it is unclear what b is in this lemma.  Does it refers to the Taylor expansion of mu?  I assume that |A| refers to the cardinality of A?,2-Confident (read it all; understood it all reasonably well),"The paper expands previous interpretations of deep learning as a kernel machine which were limited to two-layer fully connected networks to any kind or architecture. In order to do so, the concept of computation skeleton is introduced. Using those skeletons, it proves that if the network fulfils a set of conditions on the number of hidden neurons and its depth and on the type of activation function, the resulting network approximates the kernel generated by the skeleton of that network.","First of all, I must admit that I probably lack the expertise to fully grasp the content of this paper, yet I will try to contribute with my point of view as kernel svm and deep learning newbie researcher.  Although I honestly failed to understand the main results, I found very exciting how the authors model neural networks as computation skeletons and then interpret them as kernels. My personal opinion is that one of the main drawbacks of deep learning is the lack of mathematical foundation for the architectures, which makes finding the optimal topology to be a costly and poor trial and error process. It seems to me that most of the research to solve this problem comes the probabilistic approach, so a work which uses the kernel framework is very interesting to me.   Conclusively, the modelization of neural networks as computation skeletons and kernels proposed by the authors alone justify the high evaluation I give, to my humble opinion. However, further review of the main results section is required. Please accept my apologies for not being of more help.",1-Less confident (might not have understood significant parts),,,,,,
Disentangling factors of variation in deep representation using adversarial training,"Michael F. Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, Yann LeCun",https://proceedings.neurips.cc/paper/2016/hash/ef0917ea498b1665ad6c701057155abe-Abstract.html,The authors presented a new generative model that learns to disentangle the factors of variations of the data. The authors claim that the proposed model is pretty robust to supervision. This is achieved by combining two of the most successful generative models: VAE and GAN. The model is able to resolve the analogies in a consistent way on several datasets with minimal parameter/architecture tunning. ,"The experimental results are very weak (non-existent). Based on this, I believe this work is very preliminary and is not is a position to be published in NIPS. I would encourage the authors to do a more thorough experimental study. ",2-Confident (read it all; understood it all reasonably well),"This paper presents a way to learn latent codes for data, that captures both the information relevant for a given classification task, as well as the remaining irrelevant factors of variation (rather than discarding the latter as a classification model would). This is done by combining a VAE-style generative model, and adversarial training. This model proves capable of disentangling style and content in images (without explicit supervision for style information), and proves useful for analogy resolution.","This is a paper presenting a significantly useful and novel technique, and doing so with a great deal of attention to detail and clear presentation. The experimental section might need improvement.  The authors might want to put up a stronger argumentation as to why learning to disentangle relevant and irrelevant information (with regarded to a supervised task) is useful and important, v.s. discarding irrelevant information altogether. It might be obvious to the authors but might not be self-evident for people coming from a classification background. Currently this is justified by stating that it is ""a more satisfying approach"", with no real justification given. Clearly citing what the practical applications are would make a better point.  Section 3 and in particular 3.3 is somewhat confusing (although the paper is clear and well-explained overall).   The experimental section should do a better job at presenting clearly what hypothesis the authors are trying to prove, and how the results do prove it. Implicitly it seems that the unstated hypotheses vary from experiment to experiment, and it isn't always clear what the presented results are supposed to show. The experimental section currently reads along the line of ""here's what we did, here's what we got"", which is detrimental to the overall quality of the paper.  Despite these issues, overall it's definitely a good paper with interesting results.",2-Confident (read it all; understood it all reasonably well),"This paper introduces a generative model for learning to disentangle hidden factors of variation. The disentangling separates the code into two, where one is claimed to be the code that descries factors relevant to solving a specific task, and the other describing the remaining factors. Experimental results show that the proposed method is promising.","The fact that disentangling does not depend on strong supervision of the particular sources of variation makes this approach powerful and practically very relevant. To me, the experiments presented on various different datasets and tasks are satisfactory to show that this approach might be a promising direction.",2-Confident (read it all; understood it all reasonably well),"The authors combine state of the art methods VAE and GAN to generate images with two complementary codes: one relevant and one irrelevant. They major contribution of the paper is the development of a training procedure that exploits triplets of images (two sharing the relevant code, one note sharing) to regularize the encoder-decoder architecture and avoid trivial solutions.  The results are qualitatively good  and comparable to previous article using more sources of supervision. ","The article visibly integrate a detailed exploration of the most recent generative deep-learning literature. The overall model is composed by a combination of multiple recently introduced loss functions, often differently from the original papers, conditioned on class labels. It is quite difficult to understand the exact contribution to the final qualitative result of each of the components. Since so much care is used into avoiding trivial solution between Z and S it would be nice to understand how much shared the parameters can be, and how much care is necessary to train the models on different dataset.  The limitation of the paper are given by lengthy description of the model and the fact that the triplet type of training is not extensively explained, but seems to be the fundamental contribution improving on the necessity of having a label for each factor of variation of the previously published literature. It would be interesting to see novel experiments enabled by the contribution of the paper. On the overall I think the paper deserves to be accepted for a poster contribution as it properly integrates state of the art results with an interesting training procedure enabling similar performance with reduced sources of supervision.",2-Confident (read it all; understood it all reasonably well),"This paper proposes a method to separate style and content in a neural representation. This is achieved by defining a latent code with vectors for specified (""content"") and unspecified (""style"") variation. A GAN loss encourages samples from the unspecified vector conditioned on the specified vector to be indistinguishable from empirical samples from a specified content class. This way the unspecified vector is encouraged to model all variation not related to class. Unlike previous methods, this is achieved without forcing the latent code to explicitly include a categorical class variable. Experiments show that style and content are successfully separated on a few simple datasets. ","I found this to be a quite interesting paper. It is well-written, except for many typos throughout, which are easily fixable. I did, however, find the model somewhat hard to follow. I think it would help to clarify early on what is the difference between using s in the latent code versus using a class label indicator. This distinction strikes me as a primary contribution of the paper.  The model is novel to my knowledge, and uses GANs in an intriguing way. The experiments are convincing that style and content were successfully separated.  I think the main weakness of this paper is that the experiments do not convincingly show that the novelty of the model paid off. A primary novelty is that the proposed model does not require class labels in the code layer. Instead the code layer encodes class-specific information as a continuous vector of features. The paper argues that an advantage might be that the model can be applied to novel classes, not seen at training time. Unfortunately, the experiments show little evidence that this actually works. The NORB experiments are the only ones that test performance on novel classes, and in these examples it does not look like style is well separated from content. As a result, it is not clear what is gained by coding class-specific features s rather than using the older approach of coding a discrete class label in the latent code. The paper does, however, show that using s allows for smooth interpolation between classes, which is an interesting property.  Overall this paper has demonstrated a novel method to disentangle factors of variation, employing a GAN, and this method has potential advantages in terms of how it codes class-specific information (and that it doesn't rely on aligned training data). I think the paper could be improved by including more tests of generalization to novel classes (surely this works on MNIST?), and perhaps finding other ways to exploit the novel encoding of class-specific information.",2-Confident (read it all; understood it all reasonably well),"Paper seeks to explore the variations amongst samples which separate multiple classes using auto encoders and decoders. Specifically, the authors propose combining generative adversarial networks and variational auto encoders. The idea mimics the game play between two opponents, where one attempts to fool the other into believing a synthetic sample is in fact a natural sample. The paper proposes an iterative training procedure where a generative model was first trained on a number of samples while keeping the weights of the adversary constant and later the adversary is trained while keeping the generative model weights constant. The paper performs experiments on generation of instances between classes, retrieval of instances belonging to a given class, and interpolation of instances between two classes. The experiments were performed on MNIST, a set of 2D character animation sprites, and 2D NORB toy image dataset.  ","The author(s) seek to explore the factors of variations which exist amongst the samples within and between classes, but their work instead proposes a framework that expresses the variations amongst classes. The paper failed to provide a qualitative analysis about such factors, but instead takes an almost black box approach that provides ""pretty"" results. Perhaps reasoning and backing of how such variations can be extracted/utilized would be helpful in bridging your work to wider subject areas. The author(s) stress the originality of their framework; however, putting together two well known frameworks is common in vision tasks and the basis of the idea for their training procedure is analogous to that of the famous AlphaGo. Although the results are visually appealing, the MNIST and 2D NORB datasets in which the experiments were performed on is widely considered to be toy datasets (I am unfamiliar with the sprites dataset). The authors may consider including experiments on more difficult dataset (ie. evaluating a mixture of multiple expressions in Face dataset). In addition, the paper should be checked for spelling, typos and grammatical errors as some of such error caused a number of confusions during the review process.",2-Confident (read it all; understood it all reasonably well),,,,,,
Lifelong Learning with Weighted Majority Votes,"Anastasia Pentina, Ruth Urner",https://proceedings.neurips.cc/paper/2016/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html,"The paper presents a learning algorithm that can learn many (different but related) prediction tasks by transferring knowledge obtained from previously solved tasks. The algorithm receives the tasks sequentially and choose to either: - build a majority vote predictor by weighting previously learned predictors; or - learn a new predictor ""from scratch"". The algorithm is inspired by VC-dimension regret bounds and is designed to obtain theoretical generalization and sample complexity guarantees. However, no empirical experiments are performed.","From my very personal point of view, the lifelong learning paradigm is a vague concept and is sometimes evoked for studying different scenarios that can be named otherwise (like transfer learning). However, I think that the framework studied here (from Balcan et al., 2015) is a very pertinent ""lifelong problem"". The authors present an honest work in the right direction. The proofs are not trivial but, as I explain below, the contribution appears to me insufficient for NIPS.  The risk bound minimized by the learning algorithm may be very high, as it relies on the VC-dimension of the predictors. Also, the role of the constant C in Theorem 1 is never discussed, and little is said about the way to handle it in the algorithm. Therefore, I am unconvinced that the algorithm will lead to a successful empirical procedure. I consider that the authors should provide empirical evidences that the approach is sound (at least on a synthetic dataset).   It is also likely that a tighter bounding technique (such as Rademacher or PAC-Bayes, both used by some of the cited works) provides better theoretical guarantees and/or empirical results. If there is a particular reason to rely on VC-dimension, the authors should discuss their choice.  In the introduction (Lines 47 to 50, and 80-81), the authors claim that, as opposed to Balcan et al. (2015), their algorithm learns ""with weighted majority votes rather than linear combinations over linear predictor"" and that ""the shift from linear combinations to majority votes introduces a stability to the learned ensemble that allows exploiting it for later tasks."" However, this comparison is not explicitly discussed later on. The meaning of the claim is unclear to me; In general, a linear combination of predictors can be expressed as a majority vote with a simple reparameterization.  ***  Post-rebuttal comments:  Considering the authors’ rebuttal, I raised my ""technical quality"" score a bit.   I still think that the paper would benefit from some experiments, in order to assess if Algorithm 1 would make an empirically good learning algorithm. It would also help to answer practical questions like: - Is the ""\gamma-effective dimension"" assumption realistic?  - Does the number of required labels for each task will be reasonable? Or should we manually ""override' the constant C to make the algorithm work empirically?   The authors argued that we must consider the paper as a theoretical work, and leave these questions for future work. This is defendable. Personally, I feel that a work presenting a learning algorithm without any empirical results (not even on a toy dataset) is a bit incomplete (sure, Balcan et al. did it beforehand, but in COLT, which is strongly theory oriented).   Finally, regarding the linear combinations vs majority votes misunderstanding, I think the distinction must be clearly specified in the paper. From my point of view, a linear *separator* is given by \sign( < v_i, x > ), albeit a linear *regressor* is given by < v_i, x >. Moreover, a majority vote ca be a aggregation of either classifiers or regressors.",2-Confident (read it all; understood it all reasonably well),"The authors consider lifelong learning using weighted majority votes of functions that previously had to be learned from the original hypothesis space. The algorithm first tries to learn a newly encountered function from the weighted majority vote. If it fails, it tries to learn from the original hypothesis space. When the number of functions that cannot be well approximated by the weighted majority is small, the sample complexity of the lifelong learning method is correspondingly shown to be small compared to learning all the functions from the original hypothesis space.","The authors show that to learn the weighted majority of previously learned functions, you only need to learn the weighted majority of functions where learning previously failed, assuming that the functions to be learned all come from the same base class and the tasks have small discrepancy in the marginal distributions. This allows life-long learning to be more sample  (as well as other resource) efficient. The authors introduce the $\gamma$-effective dimension, which bounds the number of failures.  I think that the result shows that weighted majority of previously learned functions may potentially be useful for lifelong learning. However, the extent of its usefulness is unclear. In particular: - Are there base function classes with small $\gamma$-effective dimension (for the worst case sequence)? - How do we compute the $\gamma$-effective dimension so that we can tell whether a class H is  suitable to use with weighted majority for lifelong learning? - I suppose the ordering of the task matters. Would be good to see some discussion on that. Experimental results would also be useful to see the performance of weighted majority for lifelong learning.  Using a good example in the writing would make the paper more accessible to a wider audience. For example, learning a linear classifier from image features for object classification for a very large number of classes (where each class is a task) from the same distribution of images (same distribution for each task, so zero discrepancy).",2-Confident (read it all; understood it all reasonably well),The paper analyzes a streaming model of lifelong learning in which the learner can only retain the learned classifier from previous tasks. This is a significantly harder setting for lifelong learning than those studied previously in the literature. The paper presents a general learning algorithm that uses majority votes of base learners to learn tasks in this setting. It also introduces a new notion of dimension for sequences of learning tasks which bounds the sample complexity of the presented learning algorithm. This sample complexity can be significantly smaller than the sample complexity of the naive algorithm that learns each task independently.,"Using representations learned from previous tasks to be more efficient at new learning tasks is an important problem in machine learning, both from a theoretical and a practical perspective. The model introduced in this paper seems to be cleaner and perhaps more realistic than lifelong learning models in the previous literature.   The proposed algorithm maintains a set of majority votes over the base hypotheses, uses already learned majority votes when they have low enough empirical risk, and learns a new weighted majority vote and adds them to the set otherwise. As the authors remark, this is a very natural paradigm and similar uses of majority votes in learning algorithms are fairly standard in the literature. Nevertheless, the analysis is nontrivial and the presentation is elegant and clear.  On the other hand, more context for the newly introduced dimension and the sample complexity bounds would have been helpful. For one, the γ-effective dimension introduced here, unlike most classical dimensions that characterize learnability in various models, depends on the distributions on the data. If such a dependence is necessary, an example demonstrating how worst-case distributions can make lifelong learning impossible would help the reader understand the model.   Some context and examples for the sample complexity bounds would help, too. Although it is clear from the statement of the theorem that, if the γ-effective dimension is small enough, we can get a big sample complexity improvement, it is less clear what kind of learning tasks can be expected to realize this. Also, it would be interesting to understand how much further improvement we could hope for in this model. Even if the authors have been unable to prove any sample complexity lower bounds that would give us this context, conjectures or some discussion about the true sample complexity would be interesting.  It seems that the use of ``min’’ in the definition of distance, and the claim (11) require justification.  Overall, this is a well-written paper that makes a significant contribution to an important field of machine learning. ",2-Confident (read it all; understood it all reasonably well),This paper attempts to improve sample complexity with lifelong learning in the setting where tasks arrive in a stream.,"The algorithm proposed in this paper is similar to the algorithm of stacking, which trains multiple weak classifiers and then learns a weighted combination of the outputs of these weak classifiers. The most important issue of stacking is overfitting, so I think there is the same issue in the algorithm proposed in this paper, since the hypothesis learnt for the i-th task heavily depends on the tasks from 1 to i-1. Some experimental results should be conducted to show the effectiveness of the proposed algorithm.  There is no significant contribution in the theoretical part, it is a straightforward result with the VC dimension of the weighted combination.   I do not understand why the section 4.2 is necessary in this paper, this paper has little to do with neural networks. ",2-Confident (read it all; understood it all reasonably well),"This paper proposed a lifelong learning algorithm based on weighted majority vote and provided performance analysis. From a general view, the algorithm learns a binary classifier by using weighted majority vote on previous classifiers with sufficient large labeled data for each sequential task. If the learned classifier cannot reduced the error rate to certain level, then it will re-train an classifier from the ground base classifier set. ","The idea of using weighted majority votes in lifelong learning setting is novel. In the analysis part, this paper provides guarantee on maximum error rate for each task instead of expected error rate compared with previous literature cited in the paper. The authors also draw some interesting connections between the proposed learning algorithm with neural network with rectifier activation function.   One question I have is that in both Alg.1  and Alg.2, there are critical steps to draw training set which depend on an condition on $\Delta$, which is further depend on an unknown parameter C. How can resolve this issue in practice?   ",1-Less confident (might not have understood significant parts),"The paper studies the problem of lifelong learning with ""safety"" constraints, where a learner needs to solve a sequence of tasks (up to some  accuracy), but are not allowed to keep previous training data as it attempts to solve a new task. The authors make no distribution assumption on the tasks. They show that if the tasks are related (characterized by a notion called the ""effective dimension""), then in theory, one can achieve a significant reduction in sampling complexity by transferring information from previous tasks, as opposed to learn each individual task in isolation.","           Pros:                      The paper is clearly written and easily understandable. It provides an algorithmic framework for lifelong learning task, including strategies for both the case when the number of tasks and base solvers are known, and the case when such information is not available. For for the latter the authors use a doubling trick, which adaptively chooses the model parameters on the fly.                      The inductive proof is sound and explained well; it is interesting to see the authors' view on its connection with learning representations in a neural network.                      Cons:                      My main concern is the lack of empirical evidence, to show that why majority votes are superior to linear combination of hypotheses. Since the paper suggests a new algorithmic tool and claim it is suitable for ""realistic"" scenarios, it would have made the claim much stronger if the authors can design experiments to conduct some (at least preliminary) empirical evaluation.                      While the theoretical propositions in the paper are mostly clearly explained, intuitively, can the authors please explain what is the main advantage of majority vote hypothesis class, over the linear combinations of bases linear predictors? Why does it offer robustness behavior in comparison with linear combinations? A high-level explanation will be helpful to make the paper clearer.",1-Less confident (might not have understood significant parts),,,,,,
The Power of Optimization from Samples,"Eric Balkanski, Aviad Rubinstein, Yaron Singer",https://proceedings.neurips.cc/paper/2016/hash/c8758b517083196f05ac29810b924aca-Abstract.html,"This paper considers the problem of maximizing a monotone submodular function under cardinality constraints given the valuation of polynomially many sets chosen uniformly from the feasible region. While there are previous lower bounds for performing this task in general they show how to obtain improved results when the function as bounded curvature.  Formally they show that for any submodular function f with curvature c it is possible to obtain a (1 – c)/(1 + c – c^2) – o(1) approximation. Moreover, they show that this is tight by proving that for every c there is a submodular function with curvature c for which it is impossible to obtain a better than  (1 – c)/(1 + c – c^2) approximation ratio in general using a polynomial number of samples.","The paper is clear, well, written, and provides a tight characterization of a fairly natural problem regarding submodular function maximization. Moreover, they motivate their problem well by contrasting their result with results on learning submodular function and highlighting that learning doesn’t necessary explain whether or not the function is optimizable.  Finally, since the algorithm they provide and analyze is not necessarily the simplest one would imagine, their paper is possibly motivating new algorithms for submodular maximization in certain contexts. While a little more discussion of how the lower bound in this paper compares to previous work and whether or not certain parts of the algorithm are essential (e.g. can the random R be replaced with sets seen) would benefit the paper, overall it is a solid submission.  Detailed Comments • Line 6: maybe the words with high probability or something about the success probability should be added? • Line 10: maybe worth saying how many query can rule out? Is it exponential or something between exponential and polynomial? • Line 62: what is meant by impossible?  No multiplicative approximation ratio? • Line 68: there should be some citations right after saying that curvature has been heavily studied • Line 115-120: are these bounds tight? The best lower bound on the problem in the full information case should be stated for completeness. • Line 152: putting the definition or construction of R into the algorithm might help the clarify somewhat ",2-Confident (read it all; understood it all reasonably well),"The paper considers the problem of maximizing a submodular function with a cardinality constraint, under the model in which the algorithm only sees the function value at random points of the input space.   For monotone submodular functions of bounded curvature 'c', the paper presents matching upper and lower bounds on the approximation ratio. The lower bound is an information theoretic one (as opposed to a hardness), which is nice. ","The model considered in the paper, obtaining function values at random feasible points of the input space, seems realistic enough, and the algorithm is simple to state.  The analysis is elegant and short, and it's nice that such a simple algorithm is optimal. The lower bound technique is also clean, and the paper gets matching bounds.  The only negative is the somewhat limited applicability. Bounded curvature submodular functions are considered ""very easy"", and it's almost immediate to obtain a constant approximation. Figuring out the precise constant is the main contribution.",2-Confident (read it all; understood it all reasonably well),"The paper studies submodular maximization subject to a cardinality constraint of k and the access to the function is restricted to uniform samples. While this task is impossible in general, the paper considers the case of bounded curvature and shows matching upper and lower bounds for the approximation factor of (1-c)/(1+c-c^2), where c is the curvature.  The key insight is to estimate the marginal gain of each element with respect to a random set of size k-1. The solution is either the set of k elements with maximum estimated marginal gains over random sets or a random set.","The result is interesting as approximation from samples is not possible in general but the author identified the case of bounded curvature as a common scenario where good approximation is possible and derive best possible approximation bound for this setting. The lower bound example is quite intricate and highlight the difficulty with the uniform sample model.  The experiment could be better, is there no real dataset that makes sense for this problem?",2-Confident (read it all; understood it all reasonably well),"Optimization from samples of monotone submodular functions with bounded curvature is studied. The authors provide and prove hard bounds on the approximation ratio for this problem. Specifically, using a probabilistic argument, they show that the approximation bounds are tight. They focus on the constrained case and consider the problem of maximizing the function with a cardinality constraint and provide a simple algorithm to solve the problem. Simulation study suggests that the algorithm is better than the best sample and a random set.","The paper is overall well written, in particular: section 1. sets up the problem well; intuition for the proofs were helpful in checking them and I think it should be accessible to readers with some basic understanding of submodular optimization literature. The references given by the authors are standard, good and popular which is good. In particular, I liked the hardness section where the idea is to classify elements of the ground set into two categories: good, bad and poor. Poor elements are only useful for making the constructed function monotone submodular whereas the good and bad are elements that carry very less ""information"". I am not sure if information is the right word here since usually it is related to entropy functions, it will be nice if the authors can describe why they call it that way.   One major concern that I have is the simulation experiments. In general, I think  that a single random set is not fair for comparison, if you are going to compare it with randomized selection, I think many draws should be conducted for evaluation (similar to boot strapping) and a confidence interval or bar would convey more than a single random draw. Secondly, while this seeme like a theoretical paper, nonetheless, I feel like it can/should be supplemented with some experiments using real datasets on problems that the authors mention in section 1. This is the main reason for my score for question 5, 7 (technical quality, Potential impact or usefulness).  While the analysis works for the special case of cardinality constrained problems, any thoughts on how to adapt the analysis to other constraints like cover or knapsack constraints? (I suppose that this is not simple as it sounds and can be highly nontrivial?)",2-Confident (read it all; understood it all reasonably well),This paper shows that for any monotone submodular function with curvature c there is an approximation algorithm for maximization under cardinality constraints when polynomially-many samples are drawn from theuniform distribution over feasible sets.,"General comments:  -The paper is very well written. The presentation is easy to follow.  -The theoretical results seem strong and sound. Both upper and lower bounds are proved.  -On the machine learning application side, it is less clear how to use these results because of the polynomial sampling complexity. It would also be interesting to add numerical experiments on a real learning task (eg document ranking) and to compare the results to existing algorithms.   ",1-Less confident (might not have understood significant parts),,,,,,,,,
Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods,"Antoine Gautier, Quynh N. Nguyen, Matthias Hein",https://proceedings.neurips.cc/paper/2016/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html,"This paper investigates a special class of neural networks, where the author succeed to show that it has only one stationary point. The author introduce a fast method converging to the unique fixed-point that is the global minimum of the cost function.","The paper investigates an interesting problem of showing a relatively complex model with wisely chosen constraints can have only one stationary point. Apart from convexity (or its generalizations like geodesic convexity), the common practice for showing uniqueness of the stationary point is through using fixed-point theory; the approach that is used in this paper. The author construct a contractive map, that its fixed-point is the unique minimizer of the cost function.   One drawback of the proposed method is dependence of the convergence proof on the boundedness of spectral radius of some non-trivial matrix. As authors mention, that is the reason why they did not try networks with larger number of hidden units. Another problem with the method is many parameters that should be tuned that makes the use of the procedure rather unintuitive. Furthermore, for each choice of the parameters the spectral condition should be verified. In the experimental results, the authors tried about 150,000 different combination of possible parameters values and chose the best of them based on cross-validation accuracy. Their method, however, could outperform linear-SVM for only 2 out of 7 datasets. This result questions, the real power of the neural network they are investigating. ",2-Confident (read it all; understood it all reasonably well),This paper studied a particular class of feedforward neural networks that can be trained globally optimal with a linear convergence rate using nonlinear spectral method. This method was applied to deep networks with one- and two-hidden layers. Experiments were conducted on a series of real world datasets.,"This paper studied a particular class of feedforward neural networks that can be trained globally optimal with a linear convergence rate using nonlinear spectral method. This method was applied to deep networks with one- and two-hidden layers. Experiments were conducted on a series of real world datasets.  As stated by authors, the class of feedforward neural networks studied is restrictive and counterintuitive by imposing the non-negativity on the weights of network and maximizing the regularization of these weights. Moreover, the less popular activation function called generalized polynomial is required for the optimality condition. All these assumptions are not quite reasonable. Detailed and careful explanations from perspectives of applications or real world datasets are helpful instead of making more restricted assumptions to achieve global optimum.   Authors claimed that the targeted class of neural networks still have enough expressive power to model complex decision boundaries and achieves good performance. However, the empirical experimental results do not give the strong support for the above claim. It seems that linear SVM performs much better than NLSM1 and NLSM2 in the cases that linear SVM seems not to be properly tuned, while NLSM tune their network structures. Moreover, as we know, NLSM is a nonlinear model, it is strange why a linear SVM is used as baseline, not a nonlinear SVM such as polynomial kernel or Gaussian kernel. Thus, the comparisons are not fair, and the conclusions based on these comparable results are not convincing. In other words, these results cannot prove the enough expressive power of the studied class of neural networks.  In the experiments, the datasets from UCI repo is too small from both sample size and feature dimensionality. Based on the analysis, the bounds on the spectral radius of the matrix A grow with the number of hidden units, so it is problematic for high-dimensional data. Is there any way to deal with this issue? This will greatly prevent this method from being applied to a large proportion of applications.  It seems that the optimal condition is not guaranteed by any settings of parameters possibly used in the studied neural networks. Although the condition can be checked without running the neural networks, it is still problematic if we do not have any priors or the possible parameters are extremely huge. Any fast approach to get the possible small set of valid candidates?  Does the optimal property of the studied model come from the special objective function? It is interesting to see if stochastic gradient descent can also converge to global optimal empirically on some tested datasets. ",2-Confident (read it all; understood it all reasonably well),"This paper presents a nice theory that demonstrates that a certain class of neural network has a global optimal which can be achieved in a linear convergence rate. The main constrain is that all the input data and the weights of the linear layer are positive and activation functions are generalized polynomial. The proposed theory is first proved on the neural network with a single hidden layer and later extend to the networks with multiple hidden layers. The authors also proposed an optimization algorithm based on nonlinear spectral method. Experiments show that the proposed algorithm works quite well on a small 2D synthetic dataset, and it also achieves a reasonable performance on some small real-world datasets (UCI datasets).","Pros: As claimed by the authors, this is the first work that proposed a potentially practical neural work with optimal guarantee and convergence guarantee. Previous work that have convergence guarantees requires a complicated method to check its preconditions, some of them even impossible to check in practice, as they depend on the data-generating measure. The theory proposed in this work has comparatively simple preconditions that only depends on the spectral radius of a nonnegative matrix consists of the parameters of the network. The authors also demonstrate that the proposed theory can be applied to network of arbitrary depth and show a reasonable result on small real dataset.  Cons: The proposed network still has limited applications. The theory requires both the input data and network parameters be positive, activation functions be generalized polynomials, and spectral radius (Theorem 1) be smaller than 1. I think the non-negative constrains on the input data should not block too much potential usage, as most of train data can be easily shift to be all positive. However, the fact that network parameters are positive might limit the expressiveness of the model. Also, the fact that all activation functions of all hidden layers are different is unnatural and it might also limit its application. At last, the authors said that in order to satisfy the spectral radius constrains, they need to increase p1 and p2 and decrease pw and pu, which effectively decreases the weights of linear units. And if I understand correctly, in order to satisfy the spectral radius constrains, the higher dimension of the input and network parameters, the lower the weights of linear units. Since when the linear weight is very small, the activation functions will almost act the same as a linear function. In that case, adding more layers is just adding another linear function, which won't increase the expressiveness of the network. Thus, I am little worried that this method might not easy to extend it to deep architectures. Despite all these concerns, this is still an encouraging result as an initial attempt and I think this paper should be accepted.  Minor comments:  I am curious the how p1, p2, pw, pu, the dimension of the input and the size of the network parameters affects the spectral radius rho(A). For example, when the dimension of input increases, does rho(A) increases linearly or exponentially, and in order to counter effect that, should we linearly or exponentially decreases pw and pu. I know it might not be easy to prove that, but it might be possible to empirically plot that relationship on a synthetic data.",1-Less confident (might not have understood significant parts),The paper presents nonlinear spectral method which optimally trains a particular class of feedforward neural networks with a linear convergence rate. The condition which guarantees global optimality depends on the parameters of the architecture of the network and boils down to the computation of the spectral radius of a small nonnegative matrix.  ,"It seems new and interesting to propose a  nonlinear spectral method which optimally trains a particular class of feedforward neural networks. At the same time, I'm worried about the limitation of the approach; it imposes the condition of non-negativity on the weights of the network. In addition, the choice of the activation functions in the neural network are non-standard and very specific ones. The limitation seems very strong, which may hinder the good performance of neural networks. Indeed, numerical experiments show not-so-good performance over SVM.  Minor comment: L.90-91:  The sentence saying ""Note that the nonlinear spectral method has a linear convergence rate and thus converges quickly typically in a few (less than 10) iterations to..."" seems exaggerated because the linear convergence rate does not necessarily imply a few iterations. ",1-Less confident (might not have understood significant parts),"The paper discusses and defines a specific kind of feedforward neural networks for which a unique global optimizer can be obtained. As a product, it defines the algorithm that finds the global optimizer and the algorithm has a linear convergence rate.","The mathematical derivation is instructive although a few parts are not very clear to me. 1. Theorem 3 involves the computation of first order derivatives as shown in those inequalities, but Theorem 4 gets to the second derivative level. What causes that? 2. The design of this kind of network seems to treat the last layer separately from any hidden layer. Since u is also a matrix representing the parameters connecting to the hidden layer, the gradient with respect to u in G^\Phi (3) is just a gradient in terms of the vectorization of u? (Similar questions arise in several other places, like bounding u to be within a ball).  A concern (do I miss anything?) is that why the model needs to maximize the 1-norm of the connection parameters in (2). In most NN models, weight decay requires to reduce the 1-norm. It appears that this is because the arbitrarily small epsilon in the (2) is just needed to make the gradient strictly positive so it's in V_++. This sounds just needed for their proof but does not justify well in practical NNs. What warrants that there is no free parameter here?  It requires all the data to be non-negative and then the model uses all positive weights (not even zero), so it keeps adding up. How practical is this design?  In the experiments, the proposed NN is compared with SVM (because SVM also has the global optimizer?). However, in practice, will this kind of NNs brings additional values in some aspect than other NNs (those more commonly used NNs, e.g., with ReLU)?",2-Confident (read it all; understood it all reasonably well),"         The paper proposes a method to train -under certain assumptions- a specific type of feed-forward neural networks.         The authors prove optimal convergence and demonstrate that their method can be applied on a selection of ""real world"" problems.       ","The problem under investigation seems interesting to me and any theoretical advance in this direction is appreciated. But the paper falls a bit short in convincing that the proposed class of neural networks is useful in practice. In particular, if the authors raise the issue of the expressiveness of their restricted model (and they do so twice), then I would recommend adding some thoughts about what can be done and what is out of reach for their approach. Moreover, concurrent approaches should be discussed and contrasted.   The experimental results do not clarify the limitations of applicability of their model either. When reading the experimental results, I was surprised that the authors only compare their approach to some SVM and ignore other methods, both other training methods for some neural networks and other algorithmic paradigms. Also, I would have expected some information about the computational resources demanded by the three algorithms.          The introduction is well written, subsequently the presentation has some room for improvement. In particular, Section 2 does not read fluently.         There are some typos, I have collected a few below.          Some specific remarks:         While the definition of the loss function is straight-forward, the function $\Phi$ lacks motivation: the role of the epsilon term was explained in the rebuttal, I think this should be added to the paper as well.         Theorem 1 should be explained informally before being stated. The statement itself is hard to parse. Below Thm 1: The authors explained in the rebuttal that the very general statement that ""the nonlinear spectral method [...] converges quickly typically in a few (less than 10) iterations to the global maximum"", given immediately after Theorem 1, stems from their experimental observation. This should be clarified.       60: polyomial         67: one CAN model.         67: p_1 and p_2 are not introduced.         67: R_{++} is not introduced, which is unfortunate since there are several other variables whose definition points to that one.  Thanks for clarifying this in the rebuttal, I had not seen this notation before.",1-Less confident (might not have understood significant parts),,,,,,
