Real Time Image Saliency for Black Box Classifiers,"Piotr Dabkowski, Yarin Gal",https://proceedings.neurips.cc/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html,"The paper proposes an approach to learn saliency masks. The proposed approach is based on a neural network and can process multiple images per second (i.e. it is fast).
To me the paper is borderline, I would not object rejection or acceptance. I really believe in the concept of learning to explain a model and I think the paper has some good ideas. 

There are no obvious mistakes but there are clear limitations. 
-The saliency is only indirectly measured. Either through weakly supervised localisation or by the proposed saliency metric both these methods have clear limitations and I think these limitations should be discussed in the paper.

The weakly supervised location is not perfect as a measure. If the context in which an object appears is essential to determine its class, the object localisation does not have to correlate with saliency quality. The results on weakly supervised localisation are interesting, but I think there is a big caveat when using them as a quality metric for saliency.

The saliency metric is not perfect because of how it is applied. The estimated salient region is cropped. This crop is then rescaled to the original image size, with the original aspect ratio. This could introduce two artefacts. First, the change of the aspect ratio might impact how well it can be classified. Second in the proposed metric a small salient region is preferred. Since a small region is blown up heavily for re-classification the scale at which the object is now presented to the classifier might not be ideal. (Convnets are generally pretty translation invariant, but the scaling invariance must be learned, and there are  probably limits to this).

What is not discussed here is how much the masking model depends on the architecture for learning the masks? Did the authors at one point experiment with different architectures and how this influenced the result?


Minor comments
Are the results in Table 1 obtained for all classes or only for the correct class?

- Please specify with LRP variant and parameter setting was used for comparison. They have an epsilon, alpha-beta, and more variants with parameters.  

*** Post rebuttal edit ***

The fact that  how well the saliency metric works depends on the quality and the scale invariance of the classifier is strongly limiting the applicability of the proposed method. It can only be applied to networks having this invariance. 
This has important consequences:
-  The method cannot be used for models during the training phase, nor for models that do not exhibit this invariance. 
- This limit the applicability to other domains (e.g. spectrogram analysis with CNN's).
- The method is not generally applicable to black-box classifiers as claimed in the title.

Furthermore, the response hints at a strong dependence on the masking network. 
- As a result, it is not clear to me whether we are visualizing the saliency of the U-network or the masking network.

If these effects are properly discussed in the paper I think it is balanced enough for publication. If not it should not be published","This paper introduces a NN to predict the regions of an image that are important for another NN for object categorization. The paper reads well and it is interesting that the experiments show that it works so well compared to previous works. The results are in challenging datasets with state-of-the-art NN. 

I may suggest to motivate a bit better in which applications one may need real time efficiency. Also, I would have liked to see some insight from an analysis of the learnt salient regions (eg. which object categories exploit biases in the background to recognize the object) ","This paper image saliency mask generation approach that can process a hundred 224x224 images per second on a standard GPU. Their approach trains a masking model that finds the tightest rectangular crop that contains the entire salient region of a particular requested class by a black box classifier, such as Alexnet, GoogleNet, and ResNet. Their model architecture requires image feature map, such as those by ResNet-50, over different scales. The final scale feature will be passed through a feature filter that performs the initial localisation, while the following upsampling blocks fine-tune the produced masks. Experiment shows that their method outperforms other weakly supervised techniques at the ImageNet localisation task. 

This paper appears to have sufficient references and related works. Do not completely check. 
This paper appears to be technically correct. Do not completely check.
This paper present a number of intuition and discussion on how they design their approach. 
This paper's presentation is good.

Overall, this paper presents interesting technical results that I am a little concerned about the real time speed claim and applications to real world images.

Comments:

- Does the processing time for 100 images per second include the image resizing operation? If so, what is the running time for other larger images, such as 640 X 480 images taken from iPhone 6s?

- Salient objects in this paper is quite large, what if the requested class object is small in the images? Will 224x224 image be enough?

- In Table 3, is there any corresponding metrics for other works in Table 2, such as Feed [2]?

MISC
- LN 273: ""More over, because our model"" -> ""Moreover, because our model""
- LN 151: ""difference that [3] only optimise the mask"" -> ""difference that [3] only optimises the mask"""
Joint distribution optimal transportation for domain adaptation,"Nicolas Courty, Rémi Flamary, Amaury Habrard, Alain Rakotomamonjy",https://proceedings.neurips.cc/paper/2017/hash/0070d23b06b1486a538c0eaa45dd167a-Abstract.html,"The manuscript introduces a new technique for unsupervised domain adaptation (DA) based on optimal transport (OT). Previous related techniques (that are state-of-the-art) use optimal transport to find a mapping between the marginal distributions of source and target.  
The manuscript proposes to transports the joint distributions instead of the marginals. As the target does not have labels, the learned prediction function on the target is used as proxy. The consequence is an iterative algorithm that alternates between 1) estimating the transport for fixed data points and labeling, and 2) estimating the labeling function for fixed transport. As such, the algorithm can be seen as a self-learning inspired extension of previous work, which executed 1) and 2) only once. 
In addition to the algorithm, the manuscript contains a high probability generalization bound, similar to the discrepancy-based bounds of Ben-David et al [24]. It is used as justification that minimizing the joint OT distance should result in better predictions. Formally, this is not valid, however, because the bound is not uniform in the labeling function, so it might not hold for the result of the minimization. 
There are also experiments on three standard datasets that show that the proposed method achieve state-of-the-art performance (though the differences to the previous OT based methods are small). 

strength:
- the paper is well written, the idea makes sense
- the method seems practical, as both repeated steps resemble previous OT-based DA work
- the theory seems correct (but is not enough to truly justify the method, see below)
- the experiments are done well, including proper model selection (which is rare in DA)
- the experimental results are state-of-the-art

weaknesses:
- the manuscript is mainly a continuation of previous work on OT-based DA
- while the derivations are different, the conceptual difference is previous work is limited 
- theoretical results and derivations are w.r.t. the loss function used for learning (e.g. 
  hinge loss), which is typically just a surrogate, while the real performance measure would 
  be 0/1 loss. This also makes it hard to compare the bounds to previous work that used 0-1 loss 
- the theorem assumes a form of probabilistic Lipschitzness, which is not explored well. 
  Previous discrepancy-based DA theory does not need Prob.Lipschitzness and is more flexible 
  in this respect.
- the proved bound (Theorem 3.1) is not uniform w.r.t. the labeling function $f$. Therefore, 
  it does not suffice as a justification for the proposed minimization procedure. 
- the experimental results do not show much better results than previous OT-based DA methods
- as the proposed method is essentially a repeated application of the previous work, I would have
  hoped to see real-data experiments exploring this. Currently, performance after different number
  of alternating steps is reported only in the supplemental material on synthetic data.
- the supplemental material feels rushed in some places. E.g. in the proof of Theorem 3.1, the 
  first inequality on page 4 seems incorrect (as the integral is w.r.t. a signed measure, not a 
  prob.distr.). I believe the proof can be fixed, though, because the relation holds without
  absolute values, and it's not necessary to introduce these in (3) anyway.  
- In the same proof, Equations (7)/(8) seem identical to (9)/(10)


questions to the authors:
- please comment if the effect of multiple BCD on real data is similar to the synthetic case 

***************************
I read the author response and I am still in favor of accepting the work. ","This paper proposes a straightforward domain adaptation method using the optimal transport plan. Since the optimal transport is coupled with the prediction function of the target domain, it is not immediately clear how the optimal transport can help find a good prediction function for the target domain. Nonetheless, Section 3 presents a theoretical justification based on some assumptions. It looks like the justification is reasonable. 

The paper provides comparison experiments on three datasets, but doest not have any ablation studies or analyses of the method. Some of the practical choices seem like arbitrary, such as the distance between data points and the loss between labels. What are the effects of different distance/loss metrics over the final results? Are there any good principles to choose between the different metrics? 

Overall, the proposed method is simple and yet reads reasonable. The experiments lack some points but those are not critical. 

In the rebuttal, it would be great to see some experimental answers to the above questions. ","Summary:

- The authors propose theory and an algorithm for unsupervised domain adaptation. Unlike much work on this topic, they do not make the covariate shift assumption, but introduce the notion of Probabilistic Transfer Lipschitzness (PTL), an Lipschitz-assumption about the labelling function across domains. They give a bound on the transfer/generalization error in terms of the distance between source and target joint distributions as well as the error of the best PTL hypothesis. An experimental evaluation is performed on three real-world datasets, with the proposed method JDOT performing very well. 

Clarity:

- Overall, the paper is clear and easy to follow. 

- What is the motivation/implications of choosing an additive/separable loss? It involves a trade-off between marginals yes, but how general is this formulation?

- How much does the example in Figure 1 rely on the target function being an offset version of the source function? What would happen if the offset was in the y-direction instead?

- In the abstract, the authors state that “Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain Ps and Pt.” Is this meant to describe the PTL assumption? In that case, I think the interpretation made in the abstract should be made in the text. 

Theory: 

- I would like the authors to comment on the tightness/looseness of the bound (Thm 3.1). For example, what happens in the limit of infinite samples? Am I right in understanding that the W1 term remains, unless the source and target distributions coincide? If they are different but have common support, and we have access to infinite samples, is the bound loose?

- Could the authors comment on the relation between the covariate shift assumption and the proposed PTL assumption? 

Experiments: 

- There appears to be a typo in Table 1. Does Tloss refer to JDOT? Tloss is not introduced in the text. "
Learning A Structured Optimal Bipartite Graph for Co-Clustering,"Feiping Nie, Xiaoqian Wang, Cheng Deng, Heng Huang",https://proceedings.neurips.cc/paper/2017/hash/00a03ec6533ca7f5c644d198d815329c-Abstract.html,"This paper achieves co-clustering by formulating the problem as optimization of a graph similarity matrix with k components.
This paper is well motivated, the formulation of the proposed method is interesting, and its empirical performance is superior to other co-clustering methods.
However, I have the following concerns about the clarity and the technical soundness of the paper.
- Do Algorithm 1 and 2 always converge to a global solution? There is no theoretical guarantee of it, which is important to show the effectiveness of the proposed algorithms.
- Section 4 is almost the same as Section 3.2, and the only difference is that the Laplacian matrix is normalized. This is confusing and the readability is not high. I recommend to integrate Section 4 into Section 3.2.
- Moreover, the title of Section 4 is ""Speed Up the Model"", but how to speed up is not directly explained.
- Algorithms 1 and 2 assume that lambda is large enough. But this assumption is not used in experiments. In addition, in experiments lambda is divided or multiplied by 2 until convergence, which is not explained in Algorithms 1 and 2, hence the explanation is not consistent and confusing.
- Clustering accuracy is reported as 100% when noise = 0.6. However, in Figure 2, there are blues pixels in clusters and illustrated results do not look like 100% accuracy. More detailed explanation is helpful.
- In Table 2, there is a large difference of the accuracy between the proposed method and BSGP, although the strategy is similar. What makes the difference? Discussing the reason would be interesting.
","The authors propose a new method for co-clustering. The idea is to learn a bipartite graph with exactly k connected components. This way, the clusters can be directly inferred and no further preprocessing step (like executing k-means) is necessary. After introducing their approach the authors conduct experiments on a synthetic data set as well as on four benchmark data sets.

I think that the proposed approach is interesting. However, there are some issues.

First, it is not clear to me how the synthetic data was generated. Based on the first experiment I suppose it is too trivial anyway. Why did the authors not use k-means++ instead of standard k-means (which appears in 3/5 baselines)? Why did they not compare against standard spectral clustering? How did they initialize NMF? Randomly? Why not 100 random initializations like in the other methods? Why did they not use a second or third evaluation measure like the adjusted rand index or normalized mutual information? Since the authors introduce another parameter lambda (beside k) I would like to see the clustering performance in dependence of lambda to see how critical the choice of lambda really is. I see the argument why Algorithm 2 is faster than Algorithm 1, but I would like to see how they compare within the evaluation, because I doubt that they perform exactly the same. How much faster is Algorithm 2 compared to Algorithm 1?

Second, the organization of the paper has to be improved. Figure 1 is referenced already on the first page but appears on the third page. The introduced baseline BSGP should be set as an algorithm on page two. Furthermore, the authors tend to use variables before introducing them, i.e., D_u, D_v and F in lines 61-63. Regarding the title: In what sense is the approach optimal? Isn't bipartite and structured the same in your case? 

Third, the language of the paper should be improved. There are some weird sentence constructions, especially when embedded sentences are present. Furthermore, there are too many ""the"", singular/plural mistakes and a problem with tenses, i.e. Section 5.2 should be in present tense. In addition to grammar, the mathematical presentation should be improved. Equations are part of the sentences and hence, need punctation marks.

Due to the inadequate evaluation and the quality of the write-up, I vote for rejecting the paper.

More detailed comments:

*** Organization:
- title:   I do not get where the ""optimal"" comes from. Lines 33-34 say Bipartite=Structured, so why are both words within the title?
- line 35: Figure 1 should be on the top of page two
- lines 61-63 shoud appear as an algorithm
- line 61: in order to understand the matrices D_u and D_v, the reader has to jump to line 79 and then search backwards for D, which is defined in line 74..
- line 63: define D_u, D_v and F before using them..

*** Technical:
- line 35: B is used for a graph but also for the data matrix (lines 35 and 57)
- line 38: B_{ij} should be small (see lines 53 and 60)
- almost all equations: math is part of the text and therefore punctuation marks (.,) have to be used
- equation (2) and line 70, set Ncut, cut and assoc with \operatorname{}
- line 74: order: first introduce D, then d_{ii} and then L
- line 74: if D is a diagonal matrix, then D_u and D_v are diagonal too. Say it!
- equation (4): introduce I as the identity matrix
- lines 82-85: this paragraph is problematic. What is a discrete value? Should the entries of U and V be integer? ( I think there a just a few orthogonal matrices which are integer ) How does someone obtain a discrete solution of by running k-means on U and V?
- line 93: if P (and therefore S) should be non-negative (see Equation (9)), then you can already say it in line 93. Furthermore, isn't this an assumption you have to make? Until line 93, A and B could contain negative numbers but in the proposed approach, this is no longer possible, right?
- line 113: a sentence does not start with a variable, in this case \sigma
- line 132: get rid of (n=n_1+n_2) or add a where before
- line 149: \min
- line 150: Algorithms 1 and 2 are working on different graph laplacians. I understand that Alg.2 is faster than Alg.1 but shouldn't there be also a difference in performance? What are the results? It should be noted if Alg.1 performs exactly the same as Alg.2. However, another graph laplacian should make a difference.
- line 160: Why does NMF need a similarity graph?
- line 161: Did you tune the number of neighbors?
- line 164: Why did you not use k-means++?
- line 167: How does the performance of the proposed method vary with different values for the newly introduced parameter lambda?
- line 171: How does the performance of the proposed method vary with different values for k. Only the ground-truth k was evaluated..
- line 175: ""two-dimensional matrix"" Why not $n_1 \times n_2$?
- Section 5.2: Please improve the description on how the synthetic data was generated.
- line 182: if \delta is iid N(0,1) and you scale it with r greater than 0, then r*\delta is N(0,r^2)
- line 183: according to the values of r, the noise was always less than the ""data""?
- Figure 2: Improve layout and caption.
- Table 1: After having 100 executions of all algorithms, what are the standard deviations or standard errors of the values within the table?
- line 183: r is not the portion of noise
- line 183: you did not do a evaluation of robustness!
- line 194: setting r=0.9 does not yield a high portion of noise. Perhaps the description of the synthetic data needs improvement.
- Table 2: NMF has to be randomly initialized as well, hence I am missing the average over 100 executions
- Table 2: What is +-4.59? One standard deviation? Standard error? Something else?
- line 219: You forgot to mention the obvious limitation of your approach: you assume the data to be non-negative!
- line 220: \ell_2-norm vs. L2-norm (line 53)
- line 224: Please elaborate the sentence ""This is because high-dimensional...""

*** Language:
- line 4:  graph-based
- line 20: feature space
- line 22: distribution (singular)
- line 27: the duality [...] is
- line 33: In graph-based
- line 35: of such a bipartite graph
- line 37: The affinity [...] is
- line 38: propose, stay in present tense
- line 39: conduct
- line 40: ""in this method"" Which method is meant?
- line 43: graph-based
- lines 53-54: L2-norm vs. Frobenius norm, with or without -
- line 58: ""view""
- line 66: In the following, 
- line 72: component (singular)
- lines 76-77: ""denotes"" is wrong, better use ""Let Z=""
- line 82: Note that
- line 88: We can see from the previous section
- lines 88-90: this is a strange sentence
- line 93: we learn a matrix S that has
- line 96: If S has exactly k connected
- line 101: S, which have exactly k connected
- lines 104-105: In the next subsection
- line 132: to conduct an eigen-decomposition
- line 134: to conduct an SVD
- Algorithm 1: while not converged
- line 141: ""will be hold""
- line 148: to conduct an SVD
- line 149: ""Algorithm 2 is much efficient than Algorithm 1""
- Section 5.1: the section should be written in present tense (like Section 5.2) not in past tense
- line 159: Non-negative Matrix Factorization (NMF)
- line 204: data sets do not participate in experiments
- line 229: graph-based
- line 232: ""we guaranteed the learned grapg to have explicitly k connected components""
- unnecessary use of ""the"" in lines 33, 68, 76, 77, 81, 82, 102, 104, 111, 114, 118, 119, 120, 122, 125, 129, 141, 143, 145

"
Learning to Inpaint for Image Compression,"Mohammad Haris Baig, Vladlen Koltun, Lorenzo Torresani",https://proceedings.neurips.cc/paper/2017/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html,"This paper proposes a progressive image compression method that's ""hybrid"". The authors use the framework of Toderici et al (2016) to setup a basic progressive encoder, and then they improve on it by studying how to better propagate information between iterations. The solution involves using ""temporal"" residual connections, without the explicit need to have an RNN per se (though this point is a bit debatable because in theory if the residual connections are transformed by some convolution, are they acting as an additive RNN or not?). However, the authors also employ a predictor (inpainter). this allows them to encode each patch after trying to predict an ""inpainted"" version first. It is important to note here that this will only work (in practice) if all the patches on which this patch depends on have been decoded. This introduces a linear dependency on patches, which may make the method too slow in practice, and it would be nice to see a bit more in the text about this issue (maybe some timing in formation vs. not using inpainting). 

Overall, I think the paper was well written and an expert should be able to reproduce the work.

Given that the field of neural image compression is still in its infancy and that most of the recent papers have been focusing on non-progressive methods, and this paper proposes a *progressive* encoder/decoder, I think we should seriously consider accepting it.","Paper modifies a version of auto encoder that can progressively compress images, by reconstructing the full image rather then residual at every state. This improves performance in low loss regime. In addition they learn an in paining network that learns to predict an image patches sequentially from previous patches that are located to the left and above of them. This results in the improved performance overall. Advantages: Interesting experiments and observations and improvement over previous residual encoder. Drawbacks: Why is the network applied in patches rather then over the whole image since it is convolutional network? The inpainting predictions are unimodal - whereas the distribution of the next patch is highly multimodal - this produces limitations and can be significantly improved. The resulting performance therefore is not state of the art (compared to standard non-progressive method like jpeg 2000).

Other comments:
- Have you tried feeding the image into encoder at every stage?
","The authors take a model from the paper Toderici et al. [18] for the task of image compression and add their ideas to improve the compression rate. The proposal that the authors make to this model are:
1. To provide an identity shorcut from the previous stage to the output of the current stage
2. Add projection shortcuts to Encoder and/or Decoder from their previous stage
3. Inpainting model (with multi-scale convolution layers) at the end of each stage. 

The authors provide many different experiments (with/without inpainting network) and observe that their model with projection shortcuts only for the Decoder and jointly trained with an inpainting network provides for the best performance. Particularly, the inpainting network(by using multi-scale convolutions), as the authors claim, helps in improving the performance significantly at lower bit-rates.

The paper shows many results to claim that the Decoder with parametric connections perform better than their other models. The reviewer finds it unclear with their argument that adding connections to Encoders would burden it on each stage. Perhaps an additional experiment / more explanation might give insights on how and why the Encoder with connections make the residuals harder to compress. 

Typos: Line 231 - ""better then"" -> ""better than""

It is an interesting paper with several experiments to validate their ideas."
Inverse Filtering for Hidden Markov Models,"Robert Mattila, Cristian Rojas, Vikram Krishnamurthy, Bo Wahlberg",https://proceedings.neurips.cc/paper/2017/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html,"The paper addresses recovery of the observation sequence given known posterior state estimates, but unknown observations and/or sensor model and also in an extension, noise-corrupted measurements.  There is a nice progression of the problem through IP, LP, and MILP followed by a more careful analytical derivation of the answers in the noise-free case, and a seemingly approximate though empirically effective approach (cf. Fig 2) using spherical K-means as a subroutine in the noisy case.

Honestly, most of the motivations seem to be unrealistic, especially the cyber-physical security setting where one does not observe posteriors, but simply an action based on a presumed argmax w.r.t. posteriors.  The EEG application (while somewhat narrow) seems to be the best motivation, however, the sole example is to compare resconstructed observations to a redundant method of sensing -- is this really a compelling application?  Is it actually used in practice?  

Some additional details are provided on page 8, but this is not enough for me to fully understand the rationale for the approach.  One is given posterior state estimates of sleep stage for a new patient and the goal is to provide the corresponding mean EEG frequency observations?  I don't see the point.

Minor terminological question: I've always viewed the HMM to be a type of sequential graphical model structure allowing any type of random variable (discrete or continuous) of which discrete HMMs and Kalman filters (continuous Gaussian) are special cases.  Does the terminology ""HMM"" really assume discrete random variables?

Overall, though the motivation for the work is not very strong for me, the results in this paper address a novel problem, and make technically innovative and honestly surprising insights in terms of the tractability and uniqueness of the recovery problems addressed in the paper.  I see this paper as a very technically interesting solution that may find future application in use cases the authors have not considered.
","The paper describes the solutions to three separate (related) inverse problems for the HMM, and then applies this to polysonmography data for automatic sleep staging. In general, the paper is well written. The method appears to be novel, although I don't have extensive knowledge of this area. The weakest part of the paper is the experimental section: a single set of experiments is performed (with testing on a separate subject), whilst varying the noise. This does show that the method works for this particular application, but doesn't show how generally applicable the method is. Also, I'm not sure why the posteriors would be corrupted by noise? What's the use case?

Specific comments
- L16 transitions according to the row-stochastic: missing ""are sampled""?
- last line in eq 4 - column vector times column vector equals column vector?? B1 = 1?","This is a well written paper considering an inverse filtering problem in HMMs. The material is easy to follow and makes sense, as well as the experimental section. As far as I know this problem has not been previously addressed.

My main criticism is that the experimental section is there mainly to show that the algorithms performs as promised, as opposed to backing up the original motivation for considering the inverse filtering problem in the first place. I see that the inverse filtering problem in HMMs might be useful, but the experimental results provided do not reinforce it in any way. For example, are there any examples that the HMM sensing mechanism fails? Could those example be simulated in some reasonable way and then the proposed algorithm actually used to perform the fault detection?

Anyhow, I still consider this paper as a valuable material for NIPS.

Other comments:
Eq. (22) in the supp. material - I’m not sure I’m following the reasoning here. Is there some kind of inequality that is used to show that the sum of two matrices is no less than …?
I understand that there are no statistical assumptions on the noise? (e.g. iid?) Could some of those assumptions lead to theoretical results from the clustering algorithm perhaps?
"
On clustering network-valued data,"Soumendu Sundar Mukherjee, Purnamrita Sarkar, Lizhen Lin",https://proceedings.neurips.cc/paper/2017/hash/018dd1e07a2de4a08e6612341bf2323e-Abstract.html,"The paper is interesting, and it has merit, but its limited level of novelty and achieved gain are hardly supporting acceptance.
For what I checked, the proposed methods are correct, but the authors spend too much providing mathematical details rather than performing a more convincing comparison with other methods, which, at the moment, is showing only a fair improvement over similar algorithms.Furthermore, the paper is describing a clever combination of well known techniques, rather than introducing some novel ideas, and, overall, the role played by machine learning throughout the process is quite marginal.  

Finally, a few misprints occur in the main text, and the references are not homogeneously reported; in fact, some entries have all the authors with full name, others have initials, others have ""et al."" misplaced (see [3])

===============

The provided rebuttal addresses all the concern: overall rating modified accordingly.","Review Summary: This paper accomplishes a lot in just a few pages. This paper is generally well-written, but the first three sections in particular are very strong. The paper does a great job of balancing discussing previous work, offering theoretical results for their approach, and presenting experimental results on both generated and real data. The paper would benefit from just a touch of restructuring to add a conclusion and future work section as well as more details to some of their experimental methods and results. 

Summary of paper: This paper presents two versions of their framework for clustering networks: NCGE and NCLM. Each version tackles a slightly different variation of the problem of clustering networks. NCGE is applied when there is node correspondence between the networks being clustered, while NCLM is applied when there isn’t. It is impressive that this paper tackles not one but both of these situations, presenting strong experimental results and connecting their work to the theory of networks. 

This paper is superbly written in some sections, which makes the weaker sections almost jarring. For example, sections 2 and 3 do an excellent job of demonstrating how the current work builds on previous work. But then in sharp contrast lines 197-200 rely heavily the reader knowing the cited outside sources to grasp their point. Understandably (due to page lengths) one can not include detailed explanations for everything, but the authors may want to consider pruning in some places to allow themselves more consistency in their explanations. 

The last subsection (page 8, lines 299-315) is the most interesting and strongest part of that final section. It would be excellent if this part could be expanded and include more details about these real networks. Also the paper ends abruptly without a conclusion or discussion of potential future work. Perhaps with a bit of restructuring, both could be included. 

Section 3.2 was another strong moment of the paper, peppering both interesting results (lines 140-142) and forecasting future parts of the paper (lines 149-150). However, this section might be one to either condense or prune just a bit to gain back a few lines as to add details in other sections. 




A few formatting issues, questions, and/or recommendations are below:

Page 1, lines 23,24,26, 35: Citations are out of order. 

Page 2, line 74: The presentation of the USVT acronym is inconsistent with the presentation of the NCGE and NCLM acronyms. 

Page 3, line 97: Citations are out of order. 

Page 4, line 128: Based on the earlier lines, it seems that g(A) = \hat{P} should be g(A_i) = \hat{P_i} 

Lines 159-160: In what sense does the parameter J count the effective number of eigenvalues? 

Page 5, lines 180-183: It appears that $dn$ is a number, but then in line 182, it seems that $dn()$ is a matrix function. Which is it?

Lines 180-183: The use of three different D’s in this area make it a challenge to sort through what all the D’s mean. Perhaps consider using different letters here. 

Line 196: Citations are out of order. 

Page 6, line 247: How is clustering accuracy measured? 

Page 7, line 259: There appears to be an extra space (or two) before the citation [26] 

Lines 266-277: There appears to be a bit of inconsistency in the use of K. For example, in line 266, K is a similarity matrix and in line 268, K is the number of clusters. 

Lines 269-270, table 2, and line 284: What are GK3, GK4, and GK5? How are they different?

Line 285: There is a space after eigenvectors before the comma. 
","Authors present a novel approach for clustering networks valued data with and without node correspondence.  The approach is seemed to have the theoretical support for it’s consistency and tested through real and simulated data analyses.  Overall, the paper is well written and I have some concerns on aspects outlined below.

1.Line 266: In learning t, how do you decide on the range to maximize the relative eigen-gap? I assume this is application specific?

2.Line 261: Thera are various networks based statistics and calculations also have various parameterizations. For example,  how do you compute the clustering coefficient? You have a reference but there are various ways to define clustering coefficient using different measures and a brief discussion on this would enhance the paper. 

3.Line 270: It’s not clear what is meant by “boosted it’s performance”. Do you mean computing time, or misclassification or convergence?

4.Line 280-281: This observation is counter intuitive relative the way you have defined similarity matrix and eigengap. It would be more interesting to see examine the behavior when the networks are heavily sparse.

5.There should be a section on discussion highlighting main results/contributions and also limitations of the approach. This is missing in the current version.
"
Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks,"Nanyang Ye, Zhanxing Zhu, Rafal Mantiuk",https://proceedings.neurips.cc/paper/2017/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html,"The paper proposes a method for improving stochastic gradient HMC. They add an extra term gaussian noise term to the momentum update controlled by a new parameter alpha. And they do metadynamics to more efficiently explore the landscape of this extra parameter. They fine-tune the solution at the end with a noise free momentum SGD. They show experimental results on stacked denoising autoencoders on MNIST and sequence modeling on wikipedia hutter prize. They show that their method outperforms state of the art methods on both datasets.

evaluation : I think the paper is generally well written and the contribution is valuable as far as I can tell without being an absolute expert in this field. I have some doubts about the experiments which I detail below. I also have some doubts about the flat-sharp minimum argument and the relationship with the density lines 129-133.

questions: 
- you rely on a two step process of bayesian optimization followed by momentum SGD. How important is the fine tuning ? How long does it have to run for ? 
- what is Ls in the experiments? Were both phases run in figure 2 or is the sgdm only run at the end ?
- Why is SGD-M is missing form the figure 3 ? I came to think that well tuned momentum SGD matches performance in most state-of-the-art optimizers, judging by the tables in E1 and E2 the tuning involved is relatively basic. What are the results with 10 learning rates sampled on a log scale between 1e-4 and 1e-2 and a similar grid in momentum ? Are the best methods the same ? Can the proposed method beat that given the same computational budget ?
- I think the problems may be too weak for the method to show its strength. I think imagenet or any of the experiments in [19] could make the case more convincing.","The paper proposes a new method to optimize deep neural networks, starting with a stochastic search using 'original' Langevin (where the temperature appears as a function of an auxiliary variable), then transitioning to more classical, deterministic algorithm.

I enjoyed reading the paper - I am not an expert in the field but as far as I could tell the methods are novel, and the idea of treating the temperature as a function of an augmented variable seems elegant; since one can then change the landscape for temperature (tweaking g(\alpha) and \phi(\alpha)) without changing the optimum of the function. The numerical experiments seem to indicate that the method is not more computationally demand but improves optimization. I recommend acceptance, with minor caveats below.

- The numerical experiments show that the algorithm obtain better loss than a number of classical neural network optimization methods. However they don't explicitly investigate the ability of the algorithm to jump between modes, a property frequently mentioned in the body of the text. Having an experiment on a toy, lower dimensional function, where that property could be clearly highlighted would have improved the paper. Generally speaking, visual representation of the behavior of the algorithm in a couple of canonical situation would have provided more intuition on the inner workings of the algorithm. The choice of networks (stacked DAE) and dataset (language modeling) seem also arbitrary.
- Notation feels a bit clumsy. It is unfortunate to introduce an inverse temperature \beta, make it be equal to the output of a function g, and have it appear frequently as its inverse (i.e. the temperature). Why not introduce T(\alpha), \beta=1/T(\alpha) (just like Shannon's entropy does not require a k_B term, k_B does not need to appear here), and use either notation depending on which fits the equation most naturally (T for equations 4,9,10, beta for 6,7). It might also be arguably clearer to plot T(\alpha) in figure 1 (since the text talks about 'high temperature configuration' but the figure is showing low inverse-temperatures).
- The paper suggests tweaking the value of \phi(\alpha) to obtain a desired distribution on \alpha. But is it not hard to understand what the marginal distribution of \alpha is, given the multiplicative term H(\theta,r) g(\alpha)? 
- Nitpicky: The force well from equation (11), as it is written, does not appear to have non-zero gradients. Does it not need to be smoothened to obtain the behavior described in the following lines?
- In section 4.2, lines 206-211 are not particularly clear. Furthermore, wouldn't adding those gaussian kernel to the hamiltonian effectively act as 'walls' preventing alpha from mixing (transitioning from one side of the wall to the other seems to require going through a high energy configuration)?
- Figure 2, \tilde beta is missing a backslash.
","Disclaimer: I have never run Santa from [1] in practice, and might therefore have missed some key subtleties.

The paper takes stochastic gradient Langevin dynamics, with Hamiltonian H(\theta, r),
and basically makes the annealing schedule adaptive by introducing another parameter \alpha to the Hamiltonian H. \alpha adapts H with g(\alpha) * H and gets its own momentum variable. Via g(\alpha), the original Hamiltonian H can be suppressed, allowing the annealing schedule to be changed in the formulation. This set-up -- CTLD or continuously tempered Langevin dynamics -- is used in an algorithm to get parameters in a ""flat minimum"" area, after which standard SGD does some refinement.

The results show that CTLD finds better solutions than Santa and methods that don't adaptively change the annealing schedule. CTLD adapts the temperature dynamics.

A few pointers on the results:

** The paper added a prior or regularizer in line 72, which is (might be) required to let the posterior be normalizable and the dynamics simulate from the stationary distribution of the joint. However, in the results, the prior or a regularizer seems omitted?

** Why is drop-our required if there is already a large amount of extra 'exploration' noise introduced by CTLD? Is it fair to isolate a conclusion in the presence of this extra source of noise?

For Hamiltonian MCMC for neural nets, why not go ""all the way"" as you already have samples (approximately) from the marginal p(\theta | data)? This is very much in the spirit of the original samplers for neural nets, e.g. [Neal, R. M. (1994) Bayesian Learning for Neural Networks, Ph.D. Thesis].
"
Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization,"Omar El Housni, Vineet Goyal",https://proceedings.neurips.cc/paper/2017/hash/01a0683665f38d8e5e567b3b15ca98bf-Abstract.html,"This paper studies the contrast between the worst-case and the empirical performance of affine policies.  The authors show the affine policies produce a good approximation for the two-stage adjustable robust optimization problem with high probability on random instances where the constraint coefficients are generated i.i.d. from a large class of distribution.  The empirical results affine policy is very close to optimal and also several magnitudes faster.

In Page 3, the paper mentions MIP, but doesn’t define it.  I think MIP is mixed integer program, which is mentioned in page 8. The paper misses its reference page (it is in the paper’s supplementary file).
","The authors present an interesting analysis of a particular robust optimization problem: two stage adjustable robust linear optimization with an affine policy. Specifically, the authors explore the discrepancy between poor theoratical performance (according to worst-case theoretical bounds) and near-optimal observed empirical performance. Instead of looking at the worst case scenarios, the authors derive bounds on performance when the contraint matrix is generated from a particular class of probability dstitribution. They derive bounds for several families and iii/non iid generation scenarios. 

I am not particular familiar with this problem but the paper is written well, and the material is presented in a clear and accessible fashion.
My only comment would be to add an example of a specific application ( and motivation) of the two-stage adjustable robust linear optimization. Readers who are not familiar with the subject matter will have greater appreciation for the presented results. ","Review 2 after authors' comments:

I believe the authors gave a very good rebuttal to the comments made which leads me to believe that there is no problem in accepting this paper, see updated rating.

------------

This paper addresses the challenging question of giving bounds for affine policies for adjustable robust optimization models. I like the fact that the authors (in some probabilistic sense) reduced the large gap of sqrt(m) to 2 for some specific instances. The authors have combined various different techniques combining probabilistic bounds to the structure of a set, which is itself derived using a dual-formulation from the original problem. However, I believe the contributions given in this paper and the impact is not high enough to allow for acceptance for NIPS. I came to this conclusion due to the following reasons:

- Bounds on the performance of affine policies have been described in a series of earlier papers. This paper does not significantly close the gap in my opinion.

- The results strongly depend on the generative model for the instances. However, adjustable robust optimization is solely used in environments that have highly structured models, such as the ones mentioned in the paper on page 2, line 51: set cover, facility location and network design problems. It is also explained that the performance differs if the generative model, or the i.i.d. assumption is slightly changed. Therefore, I am not convinced about the insights these results give for researchers that are thinking of applying adjustable robust optimization to solve their (structured) problem.

- Empirical results are much better than the bounds presented here. In particular, it appears that affine policies are near optimal. This was known and has been shown in various other papers before.

- On page 2, lines 48-49 the authors say that ""...without loss of generality that c=e and d=\bar{d}e (by appropriately scaling A and B)."" However, I believe you also have to scale the right-hand side h (or the size/shape of the uncertainty set). And the columns of B have to be scaled, making the entries no longer I.I.D. in the distribution required in Section 2?

- Also on page 2, line 69 the authors describe the model with affine policies. The variables P and q are still in an inner minimization model. I think they should be together with the minimization over x?

- On page 6, Theorem 2.6. The inequality z_AR <= z_AFF does always hold I believe? So not only with probability 1-1/m?

- There are a posteriori methods to describe the optimality gap of affine policies that are much tighter for many applications, such as the methods described by [Hadjiyiannis, Michael J., Paul J. Goulart, and Daniel Kuhn. ""A scenario approach for estimating the suboptimality of linear decision rules in two-stage robust optimization."" Decision and Control and European Control Conference (CDC-ECC), 2011 50th IEEE Conference on. IEEE, 2011.] and [Kuhn, Daniel, Wolfram Wiesemann, and Angelos Georghiou. ""Primal and dual linear decision rules in stochastic and robust optimization."" Mathematical Programming 130.1 (2011): 177-209.].

-------

I was positively surprised by the digitized formulation on page 8. Is this approach used before in the literature, and if so where? The authors describe that the size can depend on desired accuracy. With the given accuracy, is the resulting solution a lower bound or an upper bound? If it is an upper bound, is the solution feasible? Of course, because it is a MIP it can probably only solve small problems as also illustrated by the authors.

-------

I have some minor comments concerning the very few types found on page 5 (this might not even be worth mentioning here):
- line 166. ""peliminary""
- line 184. ""affinly independant""
- line 186. ""The proof proceeds along similar lines as in 2.4."" (<-- what does 2.4 refer to?)

"
Few-Shot Learning Through an Information Retrieval  Lens,"Eleni Triantafillou, Richard Zemel, Raquel Urtasun",https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html,"This work is a great extension of few shot learning paradigms in deep metric learning. Rather than considering pairs of samples for metric learning (siamese networks), or 3 examples (anchor, positive, and negative -- triplet learning), the authors propose to use the full minibatchs' relationships or ranking with respect to the anchor sample. This makes sense from the structured prediction, and efficient learning. Incorporating mean average precision into the loss directly, allows for optimization of the actual task at hand.
Some comments:
1. t would be great to see more experimental results, on other datasets (face recognition). 
2. Figure 1 is not very powerful, it would be better to show how the distances change as training evolves.
3. Not clear how to set \lambda, as in a number of cases, a wrong value for \lambda leads to weak results.
4. Authors must provide code for this approach to really have an impact in deep metric learning, as this work requires a bit of implementation.","This paper suggests a novel training procedure for few-shot learning based on the idea of maximising information retrieval from a single batch. Retrieval is maximised by a rank-ordering objective by which one similarity ranking per sample in the batch is produced and optimised. The method and the results are clearly stated, as are the mathematical background and guarantees. On a standard Omniglot few-shot learning task the method reaches state-of-the-art results. What I am wondering about is the computational complexity of this ansatz in comparison to a Siamese or Matching Network approach. Can you report the timings for optimisation?","This paper proposes an interesting strategy for the important problem of few shot learning, comparing on relevant open benchmarks, and achieving impressive performance.  However, the claims of the few shot learning being state of the art seem somewhat overstated. No reference is given to recent work that has demonstrated comparable or better results than this paper does. 

Relevant Papers Include:
Kaiser et al., ICLR’17
Munkhdalai and Yu, ICML’17
Shyam et al., ICML’17

Overall, the approach is relatively simple from a neural network architecture perspective in comparison to these other techniques, which makes this performance of this paper impressive. 
"
Accelerated consensus via Min-Sum Splitting,"Patrick Rebeschini, Sekhar C. Tatikonda",https://proceedings.neurips.cc/paper/2017/hash/024d7f84fff11dd7e8d9c510137a2381-Abstract.html,"This paper applies an accelerated variant of the min-sum algorithm, called min-sum splitting, to the distributed consensus problem. The paper is very well written, with the contribution clearly placed in the context of the state of the art in the topic. To the best of my knowledge (although I am not an expert on the topic), the results are novel and constitute a qualitative advance. In particular, the paper presents a novel connection between min-sum algorithms and lifted Markov chain techniques.

There is a detail which is not clear in the presentation.  In page 4, when describing the equivalent objective function that is minimized by the min-sum algorithm to yield the min-sum splitting scheme, the authors write: ""...splitting each term $\phi_{vw}$ into $\Gamma_{vw}$ terms, and each term $\phi_v$ into $\delta$ terms,..."" However, it is not clear what this means, since $\delta$ and $\Gamma_{vw}$, as introduced on the previous page are real numbers.
","This paper studies the convergence rate of a so-called min-sum splitting method on the average consensus problem. In general he paper reads fine but the improvement of the result seems not impressive. Detailed comments are as follows.

(1) It writes that ``This rate is optimal for graphs with good expansion properties, such as the complete graph. In this case the convergence time, i.e., the number of iterations required to reach a prescribed level of error accuracy in the… of the dimension of the problem, as…’’. 
For complete graphs, the linear rate is 0 because everyone converges to the average in 1 step. Also complete graphs are too special to be representative.
So for which general category of graphs the complexity does not depend on the dimension (number of nodes)? Which general category of graphs is considered as good?

(2) In this paragraph (same as comment 1), the literature review should include ‘’Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized Optimization and Multi-Agent Control’’ by Olshevsky. Its convergence rate should be reported properly (more explanation will be given in comment 8). The reference mentioned here has reached a rather competitive or ever better bound compared the result of the submission. 

(3) At the top of page 2, for consensus optimization, important references like 
``On the Linear Convergence of the ADMM in Decentralized Consensus Optimization’’ by Shi, Ling, Kun, Wu, and Yin,
``Optimal algorithms for smooth and strongly convex distributed optimization in networks’’ by Scaman, Bach, Bubeck, Lee, Massoulié
should be cited. Also the authors should report the state-of-the-art algorithms for consensus optimization and their corresponding (linear) convergence rates. 

(4) When discussing lifted graph and Markov chain, this paper ignored a very related paper ``Markov Chain Lifting and Distributed ADMM’’ by Franca and Bento.

(5) The content of the the last paragraph of page 5 is a long known fact. Should refer to ``Generalized consensus computation in networked systems with erasure links’’ by Rabbat, Nowak, and Bucklew. In the sequel, the connection between those variants and Heavy ball/Nesterov/Polyak is known to the field.

(6) There are many important references regarding consensus optimization the authors have ignored. For example, 
``Extra: An exact first-order algorithm for decentralized consensus optimization’’ by Shi, Ling, Wu, and Yin.
``Fast distributed gradient methods’’ by Jakovetic, J Xavier, and Moura.

(7) Proposition 3 seems to be trivial and is a supplementary contribution. 

(8) The rate has reached by this paper, D log(D/eps), does not seem to have a significant improvement on the rate D log(1/eps) that has been reached by Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized Optimization and Multi-Agent Control (see comment 2). Especially in the worst case scenario (holds for all graphs), D~n, the bound is even worse than that has been achieved in ``Linear Time Average Consensus….’’.

(9) The paper``Linear Time Average Consensus…’’ improves the bound through Nesterov’s acceleration. The reviewer suspects that the so-called ``Auxiliary message-passing scheme’’ proposed by the authors is again a Nestov’s acceleration applied to min-sum algorithm. This is fine but the analysis is done for consensus which boils down to analyzing a linear system and is supposed to be not hard. The contribution of the paper becomes not clear given such situation.

(10) The tiny improvement may come from a careful handle on the spectral gap of graphs. Eventually the worst case bound is still O(n) because O(n)=O(D) for the set of all graphs with n nodes.

(11) Line 243 of page 6. The graph is simple but the author is using directed edges. This is confusing.

(12) Typo at line 220 of page 6. Laplacian—> Lagrangian.

After rebuttal:

The reviewer is satisfied with the authors' response. But the evaluation score from this reviewer stays the same.







","In this paper, the authors present an accelerated variant of the Min-Sum message-passing protocol for solving consensus problems in distributed optimization. The authors use the reparametrization techniques proposed in [Ruozzi and Tatikonda, 2013] and establish rates of convergence for the Min-Sum Splitting algorithm for solving consensus problems with quadratic objective functions. The main tool used for the analysis is the construction of an auxiliary linear process that tracks the evolution of the Min-Sum Splitting algorithm.

The main contributions of the paper can be summarized as follows: (i) provide analysis for the Min-Sum splitting algorithm using a new proof technique based on the introduction of an auxiliary process, (ii) design a Min-Sum protocol for consensus problems that achieves better convergence than previously established results, and (iii) show the connection between the proposed method, and lifted Markov chains and multi-step methods in convex optimization.

The motivation and contributions of the paper are clear. The paper is well written and easy to follow, however, it does contain several typos and grammatical mistakes (listed below). The proofs of Propositions 1 and 2, and Theorem 1 appear to be correct.

Typos and Grammatical errors:
- Line 34: “…with theirs neighbors…” -> “…with their neighbors…”
- Line 174: “double-stochastic” -> “doubly-stochastic”
- Line 183: “… can be casted as…” -> “… can be cast as…”
- Line 192: “…class of graph with…” -> “…class of graphs with…”
- Line 197: “…which seems to…” -> “…which seem to…”
- Line 206: “…additional overheads…” -> “…additional overhead…”
- Line 225: “…pugging…” -> “…plugging…”
- Line 238: “…are seen to…” -> “…are able to…”
- Line 240: “…both type of…” -> “…both types of…”
- Line 248: “…also seen to…” -> “…also shown to…”
- Line 279-280: “…to convergence to…” -> “…to converge to…”
- Line 300: “…,which scales like…” -> “…,which scale like…”
- Line 302: “…for the cycle,…” -> “…for cycle graphs,…”

Other minor comments:
- Lines 220 and 221: Do you mean “Lagrangian” and “Lagrange multipliers” instead of “Laplacian” and “Laplace multipliers”?
- The authors present 3 algorithms, and the quantities involved are not always explained or described. For example, what is R_{vw} and r_{vw} in Algorithm 2? Also, in Algorithm 2, the quantities \hat{R}^0 and \hat{r}^0 do not appear to be initialized. Moreover, since the auxiliary linear process is key to the analysis and the central idea of the paper, the authors show clearly state which variables correspond to this in Algorithm 3.

The paper also appears to be missing several references. More specifically:
- Lines 41 and 43: (Sub)gradient methods for consensus optimization. There are several more references that could be included:
-- Bertsekas and Tsitsiklis, Parallel and distributed computation: numerical methods, 1989
-- Sundhar Ram Srinivasan et. al., Incremental stochastic subgradient algorithms for convex optimization, 2009
-- Wei Shi, Extra: An exact first-order algorithm for decentralized consensus optimization, 2015
(and, of course, many more)
- Line 170: “The original literature…”
- Line 229: work by Polyak (Heavy-ball)
- Line 232: work by Nesterov

It would be interesting and useful if the authors could answer/comment and address in the paper the following:
- Although the paper is a theoretical paper, the authors should comment on the practicality of the method, and when such a method should be used as opposed to other distributed methods for consensus optimization. 
- What are the limitations of the Min-Sum Splitting method? 
- What is the intuition behind using the auxiliary process in the Min-Sum Splitting method?
- The results provided in this paper are for consensus problems with quadratic objective functions. Can this framework be extended to solve more general consensus problems that often arise in Machine Learning? 
- The authors should also clearly state why such an approach is of interest in the context of Machine Learning and for the Machine Learning community.

In summary, this paper is a purely theoretical paper in which the authors establish rates of convergence using a new proof technique and show the connections between their method and well-established methods in the literature. Overall, the ideas presented in this paper are interesting, however, the practicality of the method and intuition behind the results are missing, as well as some justification for the  importance of this result for the Machine Learning community."
Saliency-based Sequential Image Attention with Multiset Prediction,"Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang",https://proceedings.neurips.cc/paper/2017/hash/028ee724157b05d04e7bdcf237d12e60-Abstract.html,"In this paper,  the authors proposed a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism based on 2D Gaussian model. Furthermore this mechanism sequentially focuses on salient regions and takes additional glimpses
within those regions in multi-label image classification. This sequential attention model also supports multiset prediction, where a reinforcement learning based training procedure allows classification to be done on instances with arbitrary label permutation and multiple instances per label. 

Pros:
1) This paper proposes a novel saliency based attention mechanism that utilizes saliency in the top layer (meta-controller) with a new 2D Gaussian based attention map. This new attention map models the regional /positional 2D information with a mixture of Gaussian distributions, which is more general than the standard attention layer (in DRAW, Show-attend-tell), where attention is enforced based on softmax activation. 

2) The interface layer fuses the attentional inference and the hidden state to produce a priority map (which is a 2D map that is filtered by attention), and a glimpse vector for estimating glimpse location for the controller. This mechanism is intuitive as it's inspired by human-level attention mechanism.

3) The controller section combines the priority map and the glimpse vector, and it performs a k+1 step estimation, where the first k step refines the glimpse location (in terms of the glimpse vector), and the last step predicts the classification label. This architecture further refines the attention capability, and it improves the accuracy of the multi-label classification.

4) A multi-object (multi-set) multi-label classification problem is studied by the proposed architecture using a reinforcement learning based training process. This allows classification with arbitrary label permutation (changes sequentially I presume) and multiple instances per label. The simple experiments on MINST also demonstrate the effectiveness of sequential multi-set prediction with this attention mechanism. 

Cons:

1) The reference section is poorly written, please tidy it up in the final version.

2) The experiment of multi-set classification is interesting. But to the best of my knowledge, such a problem setting (the set-based prediction discussed in section 4.1) is sequential by nature (in the ordering part). May be I am missing something here, but since it is specific-tuned for demonstrating the effectiveness of sequential prediction, I am not sure how the comparison with binary-crossetnropy is justified here. Also, how does this approach compare with RAM, another popular attention based algorithm that has a sequential decision-making/RL flavor?

On the overall, I found this paper well-written and intuitive, as the hierarchical attention mechanism is inspired by how humans process visual scenes selectively and sequentially with attention. I think this paper has significant contribution to the work of attention and I would recommend its acceptance as a NIPS paper.","
	This submission proposes a recurrent attention architecture (composed of multiple steps) that operates on a saliency map that is computed with a feed forward network. The proposed architecture is based on the biological model of covert attention and overt attention. This is translated into a meta-controller (covert attention) that decides which regions need overt attention (which are then treated by lower-level controllers that glimpse in the chosen regions). This is implemented as a form of hierarchical reinforcement learning, where the reward function for the meta-controller is explicitly based on the idea of  multiset labels. The (lower-level) controllers receive reward for directing attention to the area with high priority, as output by a previously computed priority map. The whole architecture is trained using REINFORCE, a standard RL algorithm. The proposed architecture is used for multi-label, specifically multi-set setting where each image can have multiple labels, and labels may appear multiple times. The experiments are conducted on MNIST and MNIST multiset datasets, where images contain one single digit or multiple digits. 

	The paper is well written and the model is well explained. The detailed visualizations in the supplementary material are required to understand the architecture with many components. The details of the architecture when considered with the supplementary material, are well motivated and explained; the paper is comprehensive enough to allow the reproduction of results. The topic is interesting and relevant for the community.

	Some questions are as follows:
	- The “meta controller” unit takes a saliency map and the class label prediction from the previous step as inputs and learns a covert attention mask. How is the saliency map initialized in the first step? Which saliency map does the “interface” unit use, the saliency map learned in the previous step or the initial one?
	- The controller takes the priority map and the glimpse vector as inputs, runs them through a recurrent net (GRU) and each GRU unit outputs a label. How are the number of GRU units determined here? Using the number of gaussians learned by the gaussian attention mechanism? Is this the same as the number of peaks in the priority map?

	A missing related work would be Hendricks et.al., Generating Visual Explanations, ECCV 2016 that also trains a recurrent net (LSTM) using REINFORCE algorithm to generate textual explanations of the visual classification decision. However it does not contain an integrated visual attention mechanism. 

	The main weakness of the paper is the experimental evaluation. The only experimental results are reported on synthetic datasets, i.e. MNIST and MNIST multi-set. As the objects are quite salient on the black background, it is difficult to judge the advantage of the proposed attention mechanism. In natural images, as discussed in the limitations, detecting saliency with high confidence is an issue. However, this work having been motivated partially as a framework for an improvement to existing saliency models should have been evaluated in more realistic scenarios. Furthermore, the proposed attention model is not compared with any existing attention models. Also, a comparison with human gaze based as attention (as discussed in the introduction) would be interesting. A candidate dataset is CUB annotated with human gaze data in Karessli et.al, Gaze Embeddings for Zero-Shot Image Classification, CVPR17 (another un-cited related work) which showed that human gaze based visual attention is class-specific. 

	Minor comment:
	- The references list contains duplicates and the publication venues and/or the publication years of many of the papers are missing.


	"
Adaptive Bayesian Sampling with Monte Carlo EM,"Anirban Roychowdhury, Srinivasan Parthasarathy",https://proceedings.neurips.cc/paper/2017/hash/02a32ad2669e6fe298e607fe7cc0e1a0-Abstract.html,"This paper address the task of learning the mass matrices in samplers obtained from dynamics that preserve some energy function, especially the Hamiltonian Monte Carlo method. While previous works make use of the Riemannian preconditioning techniques, the paper suggests an alternative of using Monte Carlo EM to learn the mass matrix from a maximum likelihood problem. The experiments show that proposed sampler achieve comparable results to existing works, while being significantly faster.

I suggest the authors provide more intuitions and backgrounds about the connection between maximization of the likelihood of given samples, i.e., equation (3), and the performance of Monte Carlo method. If one accepts this, other parts in Section 3.2 and Section 3.3 make sense for me.

In the experiments, comparison of time per iteration seems unnatural since only the E-step of the proposed algorithm is considered. Especially, one can see that SGHMC, SG-NPHMC are slightly slower than SGHMC-EM and SG-NPHMC-EM, which seems strange to me. Moreover, more comparison between update of mass matrix using Monte Carlo EM and Riemannian manifold is necessary, e.g., HMC and RGMC. The scale of the first experiment to small and second experiment is not persuasive enough since the only algorithm using Riemannian manifold, SGR-NPHMC, performs even worse than the one without the update of mass matrix, i.e. SG-NPHMC. The authors should give a mid-scale experiment where they can compare HMC-EM and RHMC directly, in order to compare the effect of Monte Carlo EM update of mass matrix.

Minor: Figure 1-b), post-burnin time should be changed to wall-clock time.","The paper embeds an HMC method within an MCEM framework.
The iterative nature of the MCEM algorithm allows for online adaptation of the mass 
matrix $M$ along the EM step, by using the momentum samples derived in the E-step 
to update the mass matrix at the M-step; this happens via a Robbins-Monro-type of 
adaptation in equation (5) of the paper. The authors also show a method for adapting the MC-sample size using familiar techniques from the MCEM literature. 
The crux of the article is found in pages 8-11 where all the above are described in details.
Applications and comparisons with competing methods are shown, involving a trivial Gaussian target and an interesting non-parametric application.

\section*{Comments}

The paper is well-written and easy to follow. I find the method interesting. There is no great novelty, but I think that the proposed algorithm can be practically useful.

\begin{itemize}
\item Can the authors please explain a bit better the connection of their approach with convex optimisation over the mass matrix? (How is the ``MLE of the precision'' defined?)
\end{itemize}

","This paper considers the problem of learning the mass matrix within Hamiltonian type MCMC samplers.  The authors do so by proposing the use of an online EM style algorithm with a Monte Carlo sampling step that adjusts according to the sampling error estimates.

The issue of how to sample efficiently from complex target densities is a very important one and the authors make a very nice contribution in this paper.  It is rather densely written but the topics are well explained with thorough referencing to the existing literature. The idea of making use of EM to estimate the mass matrix in these types of algorithms is very interesting, the proposed algorithms are explained well and appear to perform well in practice.  I therefore only have the following relatively minor comments:

Line 128:  “we perform generalised leapfrog steps” - it seems to me that standard leapfrog integrator is used here, rather than the generalised one?

Algorithm 1:  I find the use of \epsilon in the algorithm description slightly confusing.  Perhaps you could use an alternative notation here to make clear that these integration steps are only the proposed points for the subsequent Metropolis-Hastings step?

Line 328:  typo “we put model the columns”

Section 4.1:  Why was the observed FI used here instead of the expected FI?  I think the expected FI might give better performance.  More generally, this example seems to be a bit of a straw man, given that there isn’t any strong correlation here in the posterior density - a simplified Riemannian MALA sampler would likely outperform any of these HMC methods, which makes the comparison rather moot.  More interesting would be a banana shaped distribution, which is where Riemannian MCMC methods are really a bit more useful.

More generally, it is not clear to me how well the EM versions would work with strong correlations that change over the posterior.  Since it will be converging to a single mass matrix, then I suspect they won’t work well in strongly banana-like cases.  This isn’t explored in the paper at all, which is a shame - I think an example like this would be much more illustrative and informative than the current (trivial) Gaussian example."
Scalable Levy Process Priors for Spectral Kernel Learning,"Phillip A. Jang, Andrew Loeb, Matthew Davidow, Andrew G. Wilson",https://proceedings.neurips.cc/paper/2017/hash/02b1be0d48924c327124732726097157-Abstract.html,"The paper proposes a spectral mixture of laplacian kernel with a levy process prior on the spectral components. This extends on the SM kernel by Wilson, which is a mixture of gaussians with no prior on spectral components. A RJ-MCMC is proposed that can model the number of components and represent the spectral posterior. A large-scale approximation is also implemented (SKI).

The idea of Levy prior on the spectral components is very interesting one, but the paper doesn't make it clear what are the benefits with respect to kernel learning. Lines 128-135 analyse the levy regularisation behavior briefly, but this should be made more explicit and concrete. The final optimisation target doesn't seem to be explicit. 

A comparison to simple gaussian/laplace mixture without any priors would clarify the benefits of placing the levy prior. One would assume that the levy prior should distribute the spectral components evenly by having some 'clumped' clusters and few that are far away from each other, leading to potentially simpler models. The paper should comment more about this. There could also be some interesting connections to determinental point processes, that could also regularise the frequency components. Finally its a bit unclear what kind of Levy process the authors suggest. There are lots of options, is there some recommendation that works especially well for kernel learning?

The levy kernels are sampled, but there is no experiments showing posterior distributions of e.g. J or the kernels. How much variance is there? What's the benefit of sampling, wouldn't optimisation suffice? How much slower is the method compared to e.g. the SM kernel?

The experiments highlight a cases where the LKP works nicely. The first two examples actually seem to show some problems with the LKP. In fig3 the fits fall short, is this a fundamental problem with LKP or inference artifact? In fig4 the left component fit seems to be too wide. The whole 5.2. is extremely unfair experiment since the data was generated from laplacian model in the first place. Simulated data should not come from the same model.

The main problem is that the SM kernel would work in all of these cases equally well or only slightly worse. There is no clear example where SM kernel isn't already good enough. In the airline example there is very little difference, and the SM kernel surely would work in 5.4. also with the SKI implementation. There's also no running time information apart from brief mention in line 335. Was 5.4. really learned with all 100,000 datapoints? Learning curves should be added. 

Since the main contribution of the paper is the Levy regularisation introduced to the spectral domain, a key experiment should be to compare a 'naive' L1 mixture spectral kernel to the LKP variant, and show how the LKP variant behaver better in terms of model complexity, regularisation, inference, running time or accuracy. Also comparisons to SM kernel should be included in all experiments.

Nevertheless I liked the paper. The contribution is significant and well defined. The key problem in spectral kernel learning is the spectral regularisation, and this paper is one of the first papers that adresses it.

Minor comments:
79: what is R^+
eq2: what is \mu_X
eq9: the LKP definition is too informal",The paper is very well written. It presents an interesting idea to generalize the Gaussian process with a fixed pre-specified kernel to one which includes a distibution over kernel functions. This problem has received quite a lot of attention in recent years. As the current paper models a spectral density it most closely resembles earlier work of Wilson et al. which has received quite a number of citations indicating there is interest in these kind of methods. I also liked reading the presented method works well with simple initializations as this was a problem in earlier work.,"This paper models the spectrum of a Gaussian process kernel with a location-scale mixture of Lalpacians, using the LARX method of [1] for a non-parametric prior over the coefficients. It is claimed that the levy prior promotes sparse solutions. A reverse jump mcmc method is proposed, following [1]. Inference is accelerated using a SKI approximation from [2], and results are presented on the Airline dataset and also a dataset from natural sounds.

The paper is nicely written and the idea is interesting and novel, but the evaluation of the method is extremely weak. In particular, there are no quantitive comparisons, nor any mention of other kernel learning approaches (e.g. compositional kernels [3]) or other spectral approaches to GP regression (e.g. sparse spectrum GPs [5])

The first experiment presented is actually not the approach of the paper, but instead the method of [1]. It is unclear how this provides evidence for the claims of the novel contributions of the paper. The second experiment shows (qualitatively) that the model can fit a sample from its prior better than a different model. It is hardly surprising that the method of [5] fares less well on this task as Gaussian model over the spectrum is misspecified. The third experiment provides a comparison with the method of [5], but from the graph it is extremely unclear which of the two method is better, and no results on held-out data are presented. The final experiment serves only as a demonstration of scalability as the original data from the in-filled section is not shown, and the only evaluation given is that ‘the reconstucted signal has no discernable artifacts when played back as sound’. Presumably the sound file will be included in the final version, but even so, it is not easy to assess the significance of such evidence. Do other more simple approaches achieve this property? (Incidentally, the quoted sentence appears not to have been spell-checked - fortunately it seems to be the only one!)

Beyond the experiments, I feel the paper should be improved by distinguishing its key contributions. It appears that there are two: the use of laplacian basis functions to model the fourier spectrum (as opposed to Gaussian in [5]), and the use of the levy prior. Is maximum likelihood over the laplacian features as effective as [5]? It should be made clear whether this is a modelling implication or chosen for convenience. Secondly, it should be explained in what circumstances a ML approach leads to overfitting, requiring the use of the levy prior. 

Finally, I feel that the use of ‘scalable’ in the title is a little misleading. The kernel interpolation approach is an approximation (the quality of which is not discussed), and does not extend readily to high dimensions.

In summary, to improve this paper I suggest the results section is made more quantitive and emphasis on scalability is dropped and replaced by a focus on exactly why such priors are useful over a simpler ML approach.


[1] Wolpert, R.L., Clyde, M.A., and Tu, C. Stochastic expansions using continuous dictionaries: Levy adaptive regression kernels. The Annals of Statistics, 39(4):1916–1962, 2011.

[2] Wilson, Andrew Gordon and Nickisch, Hannes. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). International Conference on Machine Learning (ICML), 2015.

[3] Duvenaud, David, et al. ""Structure discovery in nonparametric regression through compositional kernel search."" arXiv preprint arXiv:1302.4922 (2013).

[4] Quinonero-Candela, Joaquin, Carl Edward Rasmussen, and Annbal R. Figueiras-Vidal. ""Sparse spectrum Gaussian process regression."" Journal of Machine Learning Research 11.Jun (2010): 1865-1881.

[5] Wilson, Andrew, and Ryan Adams. ""Gaussian process kernels for pattern discovery and extrapolation."" Proceedings of the 30th International Conference on Machine Learning (ICML-13). 2013.
APA	
"
Model-Powered Conditional Independence Test,"Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G. Dimakis, Sanjay Shakkottai",https://proceedings.neurips.cc/paper/2017/hash/02f039058bd48307e6f653a2005c9dd2-Abstract.html,"This paper proposed a model powered approach to conduct conditional independent tests for iid data. The basic idea is to use nearest neighbor bootstrap to generate samples which follow a distribution close to the f^{CI} and a classifier is trained and tested to see if it is able to distinguish the observation distribution and the nearest neighbor bootstrapped distribution. If the classification performance is close to the random guess, one fails to reject the null hypothesis that data follows conditional independence otherwise one accept the null hypothesis.

Authors developped some bounds on the closeness of the nn bootstrapped distribution and f^{CI} in term of total variantional distance and bounds on empirical risks under ideal classification settings and for near-Independent samples.

In general, the paper is trying to address an important problem and the paper is presented in a clear way.

Detail comments:
Major:
1. It seems that the whole method can be decoupled into two major components. One is to generate samples that mimic the f_{CI} and the other one is to use a classifier for determination. My question is whether we can replace any one step by existing solutionsfrom literature. For example, we used the permutation based method proposed in [7] and then the classification based approach to solve the problem. Or we use the nn bootstrap and the kernel two sample test approach together to solve the problem. I am couriois about the performances.

2. The nearest neighbor bootstrap distribution is close to f_{CI} in the finite sample size setting. In this case, if the groundtruth is x and y are weakly dependent given z, how the proposed method is going to tell?

3. Is the method totally symmetric with respect to x and y? In the causal perspective, x and y can be dependent given z due to two different causal models: x causes y or y causes x. In this two different scenarios, shall we need to determine which variable to sample. Authors did not consider this problem.

4.I am seriously concerned about how to choose the parameter $\tau$ in algorithms 2 and 3.
Minor:
In algorithm 2 and 3, it seems that the empirical risk should be devided by the sample size.","The paper proposes a  new conditional independence test.

Strengths
1) The paper identifies that in ordered for a test to work an assumption on the smoothness of conditional density should be made. Authors propose such an assumption and bound distance in total variation between density under the null hypothesis and density obtained from bootstrap under this assumption. This is novel.
2) Paper uses this  assumption to  bound error of the optimal classifier on the training set (1.1 (iii))
3) Theorem 1, which combines the assumption on smoothness, the assumption that hat R can be achieved with error eta and the assumption on difference in errors between optimal and trained classifier, is novel, insightful and non-trivial. 

Weaknesses

Paper is hard to read and does not give much interpretation of the results, to an extent that I am not sure if I understand them correctly. E.g. consider inequality in the section 1.1 iii) assume the alternative hypothesis holds and r_0=0.1.  Suppose G is a small class, say linear functions, and suppose they are not expressive enough to distinguish between f and f^CI at all. Can you explain, intuitively, how the small probability of the error can be achieved ( hat R < 0.1 + o_1 < 0.2 for large n)?
  
                
 ","The authors propose a general test to test conditional independence between three continuous variables. The authors don't have strong parametric assumptions -- rather, they propose a method that uses samples from the joint distribution to create samples that are close to the conditional product distribution in total variation distance. Then, they feed these samples to a binary classifier such as a deep neural net, and after training it on samples from each of the two distributions, use the predictive error of the net to either accept or reject the null hypothesis of conditional independence. They compare their proposed method to those existing in literature, both with descriptions and with experimental results on synthetic and real data.  

Conditional independence testing is an important tool for modern machine learning, especially in the domain of causal inference, where we are frequently curious about claims of unconfoundedness. The proposed method appears straightforward and powerful enough to be used for many causal applications.

Additionally, the paper is well-written. While the details of the theoretical results and proofs can be hard to follow, the authors do a good job of summarizing the results and making the methodology clear to practitioners. 

To me, the most impressive part of the paper is the theoretical results. The proposed method is a somewhat straightforward extension of the method for testing independence in the paper ""Revisiting Classifier Two-Sample Tests""  (https://arxiv.org/pdf/1610.06545.pdf), as the authors acknowledge (they modify the test to include a third variable Z, and after matching neighbors of Z, the methods are similar). However, the guarantees of the theoretical results are impressive. 

A few more comments:
-Is there a discrete analog? The bootstrap becomes even more powerful when Z is discrete, right?
-How sensitive is the test on the classification method used? Is it possible that a few changed hyperparameters can lead to different acceptance and rejection results? This would be useful to see alongside the simulations
-Section 1.1 of Main Contributions is redundant -- the authors summarize their approach multiple times, and the extent appears unnecessary. 
-Writing out Algorithm 2 seems unnecessary — this process can be described more succinctly, and has been described already in the paper
-Slight typos — should be “classifier” in line 157, “graphs” in 280"
Learning Multiple Tasks with Multilinear Relationship Networks,"Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, Philip S. Yu",https://proceedings.neurips.cc/paper/2017/hash/03e0704b5690a2dee1861dc3ad3316c9-Abstract.html,"I found the paper rather interesting. The idea of using tensor-variate priors for the weights was excellent. However, it could have been motivated better. In particular, the comparison with matrix-variate priors should have been explained better and the discussion on ll.237-242 on previous work should be expanded a lot more, possibly  in the related works section. The experimental section seems ok, however I would strongly suggest to remove results on the Office-Caltech dataset as the dataset has served its purpose and is not useful anymore for most deep models (very few datasamples, label pollution etc).","This paper proposes a solution to a very interesting problem, namely multi-task learning. The authors tackles learning task similarities. It is known to be very hard problem and it has been ignored in many problems that were proposed to solve multi-task learning. The authors proposes a Bayesian method by using tensor normal priors. The paper is well written and well connected to literature. The idea is quite novel. The authors supply a set of convincing experiments and to support their method. 

Although I am quite positive about paper, I would like to get information to the following questions:
1) Although experiments are convincing, I would like to see the standard deviations (stds)for experiments? Since the dataset sizes are small then reporting stds are quite important.
2) The authors uses AlexNet as a baseline. However, AlexNet architecture is not anymore the state of the art architecture. It would be great if authors uses GoogleNet or something similar and show the improvement of their algorithm. 
","The paper proposes an interesting approach for improving multi-task learning by modeling task relationships. The paper is well-written, compares to relevant benchmarks on relevant datasets, and provides additional insight about what the classifier is learning. 

While the results are impressive in the settings explored, the method seems unlikely to scale well to a large number of tasks or lifelong settings. "
Query Complexity of Clustering with Side Information,"Arya Mazumdar, Barna Saha",https://proceedings.neurips.cc/paper/2017/hash/03e7ef47cee6fa4ae7567394b99912b7-Abstract.html,"This paper studies query complexity for clustering with 'side information'. The 'side information' is provided as a similarity matrix W, whose Wij entry is distributed as f+ or f-, depending on if i,j belong to the same cluster or different cluster. The main result of the paper is that one requires Theta(nk) queries without side information whereas they show that only O(k log n/H^2(f+||f-) ) queries are sufficient with side information, where H is the Hellinger distance. They also show that up to a log(n) factor, this is tight. 

I find this to be an interesting theoretical result for several reasons. This is closely related to SBM which has seen a lot of activity in recent years, but asks some interesting questions as to how allowing queries change the threshold. It shows  that allowing side information can drastically change query complexity. This itself is probably not surprising as the side information can have the complete cluster information, but the paper identifies how to assess the quality of the similarity matrix.  ","NOTE: I am reviewing two papers that appear to be by the same authors and on the same general topic. The other one is on noisy queries without any side information.

This paper gives upper and lower bounds on the query complexity of clustering based on noiseless pairwise comparisons, along with random side-information associated with every pair. While I am familiar with some of the related literature, my knowledge of it is far from complete, so it's a little hard to fully judge the value of the contributions.  The paper is also so dense that I couldn't spend as much time as ideal checking the correctness -- especially in the long upper bound proof.

My overall impression of both papers is that the contributions seem to be valuable (if correct), but the writing could do with a lot of work. I acknowledge that the contributions are more important than the writing, but there are so many issues that my recommendation is still borderline. It would have been very useful to have another round of reviews where the suggestions are checked, but I understand this is not allowed for NIPS. 

Regarding this paper, there are a few very relevant works that were not cited:
- The idea of replacing Bernoulli(p) and Bernoulli(q) by a general distribution in the stochastic block model is not new.  A Google (or Google Scholar) search for ""labeled [sometimes spelt labelled] stochastic block model"" or the less common ""weighted stochastic block model"" reveals quite a few papers that have looked at similar generalizations.
- There is also at least one work studying the SBM with an extra querying (active learning) element: ""Active Learning for Community Detection in Stochastic Block Models"" (Gadde et al, ISIT 2016).  It is with a different querying model, where one only queries a single node and gets its label.  But for fixed k (e.g., k=2 or k=100) and linear-size communities, this is basically the same as your pairwise querying model -- just quickly find a node from each community via random sampling, and then you can get the label of any new node by performing a pairwise query to each of those.  At first glance it looks like your lower bound contradicts their (sublinear-sample) upper bound, but this might be because your lower bound only holds as k -> Infinity (see below)

In any case, in light of these above works, I would ask the authors to remove/re-word all statements along the lines of ""we initiate a rigorous theoretical study"",  ""significantly generalizes them"", ""This is the first work that..."", ""generalize them in a significant way"", ""There is no systematic theoretical study"", etc.

Here are some further comments:
- I think the model/problem should be presented more clearly.  It looks like you are looking at minimax bounds, but this doesn't seem to be stated explicitly.  Moreover, is the minimax model without any restriction on the cluster sizes?  For instance, do you allow the case where there are k-1 clusters of size 1 and the other of size n-k+1?
- It looks like Theorem 2 can't be correct as stated.  For the SBM with k=2 (or probably any k=O(1)) we know that *no* queries are required even in certain cases where the Hellinger distance scales as log(n)/n.  It looks like part if the problem is that the proof assumes k -> Infinity, which is not stated in the theorem (it absolutely needs to be).  I'm not sure if that's the only part of the problem -- please check very carefully whether Theorem 2 more generally contradicts cases where clustering is possible even with Q=0.
- The numerical section does not seem to be too valuable, mainly because it does not compare against any existing works, and only against very trivial baselines. Are your methods really the only ones that can be considered/implemented?
- There are many confusing/unclear sentences, like ""assume, otherwise the clustering is done"" (is there text missing after ""assume""?) and ""the theorem is already proved from the nk lower bound"" (doesn't that lower bound only apply when there's no side information?)
- The paper is FULL of grammar mistakes, typos, and strange wording. Things like capitalization, plurals, spacing, and articles (a/an/the) are often wrong. Commas are consistently mis-used (e.g., they usually should not appear after statements like ""let"" / ""we have"" / ""this means"" / ""assume that"" / ""since"" / ""while"" / ""note that"", and very rarely need to be used just before an equation). Sentences shouldn't start with And/Or/Plus.  The word ""else"" is used strangely (often ""otherwise"" would be better).  Footnotes should start with a capital letter and end with a full stop. This list of issues is far from complete -- regardless of the decision, I strongly urge the authors to proof-read the paper with as much care as possible.

Some less significant comments:
- The abstract seems too long, and more generally, I don't think it's ideal to spend 3 pages before getting to the contributions
- Is your use of terminology ""Monte Carlo"" and ""Las Vegas"" standard?  Also, when you first mention Las Vegas on p4, it should be mentioned alongside *AVERAGE* query complexity (the word 'average' is missing)
- Footnote 1: Should the 1- really be there after p= ?
- In the formal problem statement ""find Q subset of V x V"" makes it sound like the algorithm is non-adaptive.
- I disagree with the statement ""clearly exhibiting"" in the last sentence of the Simulations section.
- Many brackets are too small throughout the appendix


[POST-AUTHOR FEEDBACK COMMENTS]

The authors clarified a few things in the responses, and I have updated the recommendation to acceptance.  I still hope that a very careful revision is done for the final version considering the above comments.  I can see that I was the only one of the 3 reviewers to have significant comments about the writing, but I am sure that some of these issues would make a difference to a lot more NIPS readers, particularly those coming from different academic backgrounds to the authors.

Some specific comments:
- The authors have the wrong idea if they feel that ""issues like capitalization and misplaced commas"" were deciding factors in the review.  These were mentioned at the end as something that should be revised, not as a reason for the borderline recommendation.
- Thank you for clarifying the (lack of) contradiction with Gadde's paper.
- It is very important that theorems and lemmas are stated precisely.  e.g., If a result only holds for sufficiently large k, this MUST be stated in the theorem, and not just in the proof.
- Regarding the specific grammar issues addressed in the responses, I still suggest revising the use of Else/And/Or/Plus, the sentence ""assume, otherwise..."", and so on.  I am convinced that most of these are grammatically incorrect as used in the paper, or at least unsuitable language for an academic paper.  (The authors may disagree)
"
Non-parametric Structured Output Networks,"Andreas Lehrmann, Leonid Sigal",https://proceedings.neurips.cc/paper/2017/hash/04048aeca2c0f5d84639358008ed2ae7-Abstract.html,"The paper proposes Non-parametric Neural Networks (N3) a method that combines advantages of deep models for learning strong relations between input and output variables with the capabilities of probabilistic graphical models at modeling relationships between the output variables.

Towards this goal, the proposed method is designed based on three components:
a) a deep neural network (DNN) which learns the parameters of local non-parametric distributions conditioned on the input variables, b) a non-parametric graphical model (NGM) which defines a graph structure on the local distributions considered by the DNN. and, c) a recurrent inference network (RIN), which is trained/used to perform inference on the structured space defined by the DNN+NGM.
The proposed method is sound, well motivated and each of its components are properly presented. 

The method is evaluated covering a good set of baselines and an ablation study showing variants of the proposed method. The evaluation shows that state of the art results are achieved by the proposed method. However, due to its technical nature, there is a large number of variables and terms that complicate the reading at times. Still this is just a minor point.

Overall all the paper proposes a method with good potential in a variety of fields. However, one of my main concern is that its evaluation is conducted in the self-defined task of analyzing the statistics of natural pixels from the MS COCO dataset. In this regard, I would recommend adding more details regarding the evaluation, especially on how performance is measured for that task. Moreover, given that the evaluation is related with visual data, I would highly recommend evaluating their method on a more standard task from computer vision.

In addition, very recently a paper with the very same title as this manuscript (""non-parametric neural networks"") came out, i.e. Phillip and Carbonell, ICLR'17, while the method and goal approached in this paper is different from that from ICLR, I would strongly suggest either, a) make an explicit and strong positioning of your paper w.r.t. Phillip and Carbonell, ICLR'17, or b) consider an alternative (or extended) title for your paper. In this regard, I consider that your paper is better suited for the N3 title, however, this is a conflict that should be addressed adequately.

Finally, given the potential of the proposed method I would encourage the authors to release the code related to the experiments presented in the manuscript. Are there any plans for releasing the code?
I believe, this should encourage the adoption of the proposed method by the research community. 

I would appreciate if the authors address the last three paragraphs in their rebuttal.","The paper proposes a novel way to build a non-parametric graphical model with continuous variables using deep neural networks. 
Deep network  is trained to predict parameters of local non-parametric distributions. Then the distributions are transformed to conditional distributions and recurrent network perform message passing-like procedure to infer required statistics.

The paper is clearly written and the proposed approach seems novel and elegant to me. My main concern is the experimental evaluation in the paper. The authors use only small synthetic data. While the performed comparison on the dataset gives some insights, I strongly believe, that real world experiments would make the paper potentially very impactful.","It's my impression that an article should be self-contained. In the case of your article, it seems that a number of central details have been relegated to appendices. The lack of detail concerning how you condition via the conditioning operator is particularly problematic. 

1) how the inference network is run and trained is less than clear. I understand you're doing approximate neural message passing via an RNN, but do you merely pass messages once or does the setup allow for loopy belief propagation? If yes, when does it terminate? 

2) continuing the above line of inquiry, I assume you have to train a separate RIN for each statistic. If yes, how is the inference loss L_I selected? The loss you picked for marginal inference seems appropriate, but it's unclear what has to change for other statistics.

3) are you actually enforcing structure in your experiement? It goes unmentioned. I assume structure is decided a priori, unless its inference is a detail hidden away in the appendix?

4) I am not convinced that the way you use BB-VI in the experiments makes for a fair comparison. One, it's hardly the gold standard for variational inference given its instability problems. In your case you should at the very least be able to get away with reparameterized gradient estimators. Also, the advantage of BB is that you can apply it to any q-model while you use a notoriously weak mean-field approximation.  

Aside from the above, I enjoyed the article and its perspectives."
Robust Imitation of Diverse Behaviors,"Ziyu Wang, Josh S. Merel, Scott E. Reed, Nando de Freitas, Gregory Wayne, Nicolas Heess",https://proceedings.neurips.cc/paper/2017/hash/044a23cadb567653eb51d4eb40acaa88-Abstract.html,"The paper proposes a deep-learning-based approach to imitation learning which is sample-efficient and is able to imitate many diverse behaviors. The architecture can be seen as conditional generative adversarial imitation learning (GAIL). The conditioning vector is an embedding of a demonstrated trajectory, provided by a variational autoencoder. This results in one-shot imitation learning: at test time, a new demonstration can be embedded and provided as a conditioning vector to the imitation policy. The authors evaluate the method on several simulated motor control tasks.

Detailed comments.

Pros:
1) The architecture seems clean and logical, and seems to do the job well. 
2) In particular, VAE for trajectory embedding is new compared to recent related approaches.
3) The proposed approach is able to learn complex and diverse behaviors and outperforms both the VAE alone (quantitatively) and GAIL alone (qualitatively).
4) Interpolation between different policies/styles is impressive.

Cons:
1) Comparisons to baselines could be more detailed. Currently the comparison to GAIL is purely qualitative, as far as I can see, and only performed on one task. It intuitively seems that GAIL would not perform well, but perhaps it is worth showing clearly in the paper.
2) A discussion of sample efficiency compared to GAIL and VAE would be interesting. What if one trains GAIL per style or target speed - would that work as well as the proposed method? Multi-modality shouldn’t be an issue then. Will VAE work given much more data?
3) Presentation is not always clear, in particular I had hard time figuring out the notation in Section 3.
4) There has been some work on hybrids of VAEs and GANs, which seem worth mentioning when generative models are discussed, like:
  Autoencoding beyond pixels using a learned similarity metric, Larsen et al., ICML 2016
  Generating Images with Perceptual Similarity Metrics based on Deep Networks, Dosovitskiy&Brox. NIPS 2016
These works share the intuition that good coverage of VAEs can be combined with sharp results generated by GANs.
5) Some more extensive analysis of the approach would be interesting. How sensitive is it to hyperparameters? How important is it to use VAE, not usual AE or supervised learning? How difficult will it be for others to apply it to new tasks?
6) A related submission mentioned in lines 124-126 seems quite similar. I have no way to judge to which extent, but seems like it may be worth double-checking. If the work is from the same authors, it can be problematic.

Overall, the work is interesting and proposes an elegant and well-performing approach. I think it should be accepted.","The paper proposes to combine VAE with GAIL to make the GAIL method more robust by resolving the mode-collapsing problem. For this, the latent feature distribution of demonstration trajectories is learned using VAE and the GAIL objective is modified to be optimized for the expectation over the latent feature distribution. Overall, it becomes solving conditional GAILs where the conditioned embedding is given by VAE encoder and this leads to robust policy learning. The authors claim that, because the learned trajectory feature captures the semantic properties of the demonstration, conditioning on this the resulting generation distribution becomes close to uni-modal. The experiment shows the effectiveness of the proposed method on three continuous control tasks. 

The paper is well-written and the proposed approach and the experimental results are interesting. I overall enjoyed reading the paper. The followings are some of the questions and comments.

Could you elaborate more why q(z|x) is only conditioned on the states x but not along with actions a? And how this is related to the one-shot imitation learning (described in line 127-128)?

It could be interesting to see the t-SNE visualization of the trajectory embedding generated by the vanilla GAIL policy? This would provide some additional evidence on the mode-collapsing claim.

The proposed method seems helpful in improving sample complexity. Experiment on the varying number of demonstrations or policies would have been interesting to see (e.g., on the speed difference metric).","This work deals with the problem of modeling joint action/state trajectory spaces of dynamical systems using a supervised imitation learning paradigm.  It approaches the problem by defining a Variational Autoencoder (VAE) that maps the state sequence, via a latent representation, into a (state,action) pair.  

The approach is validated on three articulated body controller modeling problems: a robotic arm, a 2D biped, and a 3D human body motion modeling.

Prior work: the work closely follows (extends) the approach of Generative Adversarial Imitation Learning [11].  The difference here is that, through the use of VAE, the authors claim to have avoided the pitfall of GAN know as mode collapsing.

Summary

+ Addresses a challenging problem of learning complex dynamics controllers / control policies
+ Well-written introduction / motivation
+ Appealing qualitative results on the three evaluation problems. Interesting experiments with motion transitioning.  

- Modeling formulation is somewhat different from GAIL (latent representation) but it rather closely follows GAIL
- Many key details are omitted (either on purpose, placed in appendix, or simply absent, like the lack of definitions of terms in the modeling section, details of the planner model, simulation process, or the details of experimental settings)
- Experimental evaluation is largely subjective (videos of robotic arm/biped/3D human motion)
- Paper appears written in haste or rewritten from a SIGGRAPH like submission 
- Relies significantly on work presented in an anonymous NIPS submission

Detailed comments

In general, I like the focus of this paper; designing complex controllers for dynamic systems is an extremely challenging task and the approach proposed here looks reasonable.  The use of the VAE is justified and the results, subjectively, are appealing.

However, so many details are missing here and some parts of the discussion are unclear or not justified, in my view.

For instance, the whole modeling section (3), while not so obscure, still ends up not defining many terms.  Eg, what is \pi_E in the first eq of 3.2? (Not numbered, btw) What is \theta in eq3? (also used elsewhere) .  What are all the weight terms in the policy model, end of sec 3 (not numbered)?  You say you initialize theta to alpha wights, but what does that mean?

What is the purpose of Lemma 1?  The whole statement following it, l 135-145 sound unclear and contradictory.  Eg, you say that eq5 avoids the problem of collapse, yet you state in those lines that is also has the same problem?

The decoder models for (x,a) are not explicitly defined.

How is the trajectory simulation process actually accomplished?  This is somewhat tersely described in l168-172.

How are transitions between categories of different motion types modeled as you do not explicitly encode the category class?  It is obviously done in the z-space, but is it some linear interpolation?  It appears to be (l174-176) but is it timed or instantaneous?

What are all the numbers in the Action Decoder Sizes column of Tab 1, appendix?"
High-Order Attention Models for Visual Question Answering,"Idan Schwartz, Alexander Schwing, Tamir Hazan",https://proceedings.neurips.cc/paper/2017/hash/051928341be67dcba03f0e04104d9047-Abstract.html,"General Impression:

Overall I think the proposed method is interesting. The results are quite good and the attention maps seem to illustrate these gains might be due to improved attention mechanisms and not simply increased model capacity. I found the writing to be a bit weak and sometimes a bit confusing, though I imagine given more time the authors could improve the submission appropriately. 


Strengths:

- Interesting model with good results.
- Thorough qualitative and quantitative experiments.
- I'm fairly impressed by the shift between unary/pairwise attentions and the final attention. Though I would have liked to see marginalized trinary attention maps somewhere as well. Did I miss these in the text or supplement? 


Weaknesses:

- As I said above, I found the writing / presentation a bit jumbled at times.

- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).

- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.

- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly. 

- Figure 3 is never referenced unless I missed it.


Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.

- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?

- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?","The paper presents a novel method to model attention on the image and the question for the task of Visual Question Answering (VQA). The main novelty lies in proposing a generic formulation of attention for multiple modalities – a combination of unary, pairwise and ternary potentials for 3 modalities (image, question and answer) in VQA. The proposed approach is evaluated on the VQA dataset and outperforms existing models when ternary potentials are used. 

Strengths:

1.	A generic formulation of attention for multimodal problems that is not specific to a particular task is useful because especially if it can be shown to generalize to more than one task. 
2.	The ablation study showing the performance without ternary potential is useful to understand how much ternary potential helps.
3.	The paper is clearly written.

Weaknesses:

1.	The approach mentions attention over 3 modalities – image, question and answer. However, it is not clear what attention over answers mean because most of the answers are single words and even if they are multiword, they are treated as single word. The paper does not present any visualizations for attention over answers. So, I would like the authors to clarify this.

2.	From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.

3.	Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).

4.	The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.

5.	Other errors/typos:
a.	L38: mechanism -> mechanisms
b.	L237 mentions that the evaluation is on validation set. However Table 1 reports numbers on the test-dev and test-std sets?

Post-rebuttal comments:

Although the authors' response to the concern of ""Proposed model not outperforming existing models for 2 modalities"" does not sound satisfactory to me due to lack of quantitative evidence, I would like to recommend acceptance because of the generic attention framework for multiple modalities being proposed in the paper and quantitative results of 3-modality attention outperforming SOTA. The quantitative evaluation of the proposed model's attention maps against human attention maps (reported in the rebuttal) also looks good and suggests that the attention maps are more correlation with human maps' than existing models. Although, we don't know what this correlation value is for SOTA models such as MCB, I think it is still significantly better than that for HieCoAtt.

I have a question about one of the responses from the authors --

> Authors' response -- “MCB vs. MCT”: MCT is a generic extension of MCB for n-modalities. Specifically for VQA the 3-MCT setup yields 68.4% on test-dev where 2-layer MCB yields 69.4%. We tested other combinations of more than 2-modalities MCB and found them to yield inferior results.

Are the numbers swapped here? 69.4% should be for 3-MCT, right? Also, the MCB overall performance in table 1 is 68.6%. So, not sure which number the authors are referring to when they report 68.4%."
FALKON: An Optimal Large Scale Kernel Method,"Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco",https://proceedings.neurips.cc/paper/2017/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html,"Comments:
- line 19. ""... the a variety of ..."" -> ""... a variety of ...""
- line 25. ""... show that there are large class..."" -> ""... show that there is a large class...""
- line 113. ""... that in this case t=sqrt(n)log(n) are needed to achieve..."" I guess that the word ""iterations"" is missing? The same concern is with line 114
- Table 1. The last but one line in the table. There is a problem with a reference for ""Nystrom + sgd""
- line 195. ""Intuitevely, these conditions measure the size of H, if gamma is small then H is small rates are faster, and the regularity of f, if r is big f is
regular and rates are faster."" -> ""Intuitevely, these conditions measure the size of H (if gamma is small then H is small and rates are faster) and the regularity of f (if r is big f is regular and rates are faster).""?
- line 204. There should be a space between ""O(1/n)."" and ""By selecting""

Conclusion:
1) In FALCON we need to use the number of inducing points M ~ sqrt(n). Thus in general we are limited in the number of inducing points we can use if n >> 1.

Gaussian Process regression is a probabilistic counterpart of the kernel ridge regression and provides the same regression function, as well as uncertainty quantification, not accessible in case of KRR. Sparse Gaussian Processes (in fact this is the Nystrom approximation), are used to process large samples. Recently, they propose to approximate not the GP model (using Nystrom approximation), but its posterior by optimizing evidence lower bound (ELBO), see 
https://www.prowler.io/sparse-gps-approximate-the-posterior-not-the-model/
Thus, by considering inducing points as latent variables, we can optimize ELBO to tune their values. By restricting the family of variational distributions and limiting inducing points to belong to some tensor (see applications of tensor methods for GP e.g. in ""A. G. Wilson and H. Nickisch. Kernel interpolation for scalable structured gaussian processes // In International Conference on Machine Learning, pages 1775–1784, 2015"", ""E. Burnaev, M. Belyaev, E. Kapushev. Computationally efficient algorithm for Gaussian Processes based regression in case of structured samples // Computational Mathematics and Mathematical Physics, 2016, Vol. 56, No. 4, pp. 499–513, 2016"") we can use tensor algebra to efficiently perform SGD optimization of ELBO even in case when M ~ n and so we can better approximate the posterior. Could the authors comment on this approach to kernel regression learning? What connections does ELBO optimization by SGD wrt inducing points may have with the proposed learning scheme?

2) The topic of the paper is important. The paper is well written and contains not only efficient algorithm, but also useful theoretical results. Do the authors have available programming code with results of experiments? It could be useful to specify a link to github with this code.","The authors introduce a general framework for solving Kernel Ridge
Regression problems. They demonstrate and prove that their approach
has probably one of the lowest time and memory complexity. The efficiently
reduced  complexity is mainly reached by combining several well known
methods, e.g. the Nyström
approximation and a variant of the preconditioned conjugate gradient
method. The exploitation of some sophisticated technical tricks also yield
reasonable contribution. 

The presented method has an extensive theoretical foundation and
encouraging results on reasonable large data set are provided as
well. 
      
I have some concerns on the results compared to other methods in Table
2 and Table 3. It is not clear how the computational environments
of the different methods were compared. What was the source of the computational
times of some alternatives; do they come from
the references i.e. they are not directly computed on the same
configuration? The comparison could be 
more convincing if they are obtained at the same conditions. 

Another remark relates to the conditions exploited in Theorem 1,
since all other theorems depend and refer to them. What is assumed about the
family of the distribution appearing in Expression (1), e.g. bounded
support? It should be explicitly stated there.
","Summary:

A number of approaches developed in recent years are combined to develop a new fast kernel ridge regressor that can be computed in O(n^1.5) and which achieves state-of-the-art rates of convergence in certain situations.


Review:

The paper is well written and has good experimental results. The downside of this paper is that it is weak on novelty and it is heavily based on “A. Rudi, R. Camoriano, and L. Rosasco. Less is more: Nyström computational, NIPS 2015”. In the first 2-3 pages there is quite some overlap between these two papers: preliminaries overlap quite a bit, sentences appear to be at times slightly perturbed versions of the NIPS 2015 paper, e.g. “Extensive experimental analysis
shows that the considered approach achieves state of the art performances on
benchmark large scale datasets.” =>  “Extensive experiments show that state of the art results on available large scale datasets can be achieved even on a single machine.”  etc. Preliminaries do not need to be rewritten every time and it is fair enough to reuse parts. Similarly with parts of the introduction, but the amount of similarity suggests strongly that this is a somewhat incremental paper.  

The main new idea seems to be that random projections are used to speed up preconditioning of the regularized kernel matrix. The paper also develops theory around this idea, showing how you can gain speedups while preserving optimal rates of convergence. I did not go through the supplementary and it is hard for me to judge how much novelty is in the proofs. I imagine that on a high level you have to deal with a number of triangular inequalities: relating the new approximation to Nystrom, Nystrom to KRR, KRR to the true solution and that the main work lies in bounding the distance between the new approximation and Nystrom. 

In total, I think it is an incremental but valuable paper and it contains just enough material for acceptance. 


Further comments:

Abstract:
-“They rely on … and enjoy optimal statistical properties.”  This sentence doesn’t make sense to me. A particular method can be optimal in a particular statistical setting but kernel methods are not per se optimal.


P.2

— The KRR runtime seems too pessimistic — I assume the O(n^3) is because of the kernel inverse. Inverting the kernel matrix can apparently done in less than O(n^2.4). This should also reduce the computation time for D&C to below n^1.7 (I assume you use m=n^1/2 many samples) — vs n^1.5 for your method. "
Generalized Linear Model Regression under Distance-to-set Penalties,"Jason Xu, Eric Chi, Kenneth Lange",https://proceedings.neurips.cc/paper/2017/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html,"The paper proposes a new approach for learning a generalized linear model 
that penalizes the distance between the weight parameters and a set.
A majorization-minimization algorithm is presented for learning the model
with the proposed penalty. 

The proposed approach of penalizing distance-to-set is potentially significant
in that it provides a framework that encompasses many of the popular penalized
(generalized) regression methods such as sparse regression, regression with rank restriction etc.
The optimization algorithm generalizes well to other losses beyond the generalized linear models.
The proposed approach could be of interest to many researchers in the community.

The paper is well-written.

The paper provides evidence that the proposed approach works well in the experiments 
section. The learning algorithm is reasonably fast for analysis of real-world datasets.

Minor comments: 
It would be good to provide details of the temperature anomaly count data and mean squared errors
on this dataset.
","I found much to like about this paper. It was very clear and well-written, and the proposed method is elegant, intuitive, novel (to the best of my knowledge), seemingly well motivated, and widely applicable. In addition, the main ideas in the paper are well-supported by appropriate numerical and real data examples.

I would like to see the authors compare their method to two other methods that I would see as major competitors: first, the relaxed LASSO (Meinshausen 2007) wherein we fit the lasso and then do an unpenalized fit using only the variables that the lasso has selected. As I understand the field, this is the most popular method and the one that Hastie, Tibshirani, and Wainwright (2015) recommend for users who want to avoid the bias of the lasso (but note that the shrinkage “bias” is sometimes desirable to counteract the selection bias or “winner’s curse” that the selected coefficients may suffer from). Second, best-subsets itself is now feasible using modern mixed-integer optimization methods (Bertsimas, Kind, and Mazumder, 2016) (if one of these competitors outperforms your method, it would not make me reconsider my recommendation to accept, since the method you propose applies to a much more general class of problems).

Bertsimas, Dimitris, Angela King, and Rahul Mazumder. ""Best subset selection via a modern optimization lens."" The Annals of Statistics 44.2 (2016): 813-852.

Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the lasso and generalizations. CRC press, 2015.

Meinshausen, Nicolai. ""Relaxed lasso."" Computational Statistics & Data Analysis 52.1 (2007): 374-393.

"
Fisher GAN,"Youssef Mroueh, Tom Sercu",https://proceedings.neurips.cc/paper/2017/hash/07042ac7d03d3b9911a00da43ce0079a-Abstract.html,"In this work, authors present a novel framework for training GAN based on Integral Probability Metrics (IPM).

The different notions are introduced gradually which makes the article easy to read.
Proof are provided in appendix.
Experiments are well managed and complete.

- Section 2 -

Lines 109 110: I don't see the point of fixing M=N.

- Section 5 -

One points author could have provided more information is a complexity analysis comparing Wasserstein GAN and Fisher GAN. Where comes the speed up of Fisher-GAN ? ","Overall the paper is mathematically strong and provides a good and well-founded basis for constraining the discriminator in an adversarial framework trained on IPM. The method is indeed simpler than WGAN-gp and provides a way of constraining the discriminator without the sort of clipping in the original WGAN paper. I liked the paper, but I have several concerns:

1) The constraining factors do indeed have computational advantages over WGAP-gp, where you have to compute the norm of the gradients backpropped onto the input space. Instead we only need to compute the covariance, which is in the simple case just the expectation of the square of the outputs. However, looking at this it seems possible that E[f^2] over samples from the generator go to 0 and 1 over the real samples. In this case, it seems like the gradients in the neighborhood of the samples could still go to 0 with this constraint and a neural network discriminator while the distance given the optimal discriminator are non-zero. WGAN-gp, on the other hand, actually uses a combination of generated and real samples in its regularization term, instead of something over the 2 distributions independently. This likely breaks a symmetry implicit in the Fisher IPM-based method that keeps the gradients from vanishing in the neighborhood of the generated samples. Could the authors comment on this?

2) The stability experiments are lacking. Notably, the authors show stability when the discriminator batch norm is off, but I'm fairly certain that turning the discriminator batch norm off is a strong source of instability. Rather, turning the batch norm off on the generator, for instance, is much stronger. It is interesting that WGAN/gp doesn't perform as well, but it's always unclear if this is due to hyperparameter choices. Why not run the same exps as they did in the WGAN to compare?

3) The story assumes a lot of prior knowledge into IPM, and I think there's a bit of a disconnect between the math and the issues with GANs in general.  It could be a bit more accessible. Some of the comments below reflect this.

Additional comments:
Line 21: Aren’t f-divergences generalizations of the KL and LSGAN (Xi-squared) divergences?
Line 27: I'm fairly certain (95%) that WGAN-gp does reduce capacity, and this is evident in the reduced inception scores.
Line 115: The authors don't do a good job establishing what v is and how it relates to GANs as most people understand them. In the end, it just ends up being a 1-dimensional thing that disappears under the change of variables, but this point would be lost I think on most readers.
","This paper proposes a new criterion for training a Generative Adversarial Network and shows that this new criterion yields stability benefits.

The criterion is related to Fisher discriminant analysis and is essentially a normalized IPM.
The authors show that this criterion is equivalent to the symmetric chi-squared divergence.

One reason for not being fully enthusiastic about this paper is that the fact that the proposed criterion is equivalent to chi-squared, which is
an f-divergence, reduces the novelty of the approach and raises several questions that are not addressed:
- is the proposed implementation significantly different than what one would obtain by applying f-GAN (with the appropriate f corresponding
to the chi-squared divergence)? if not, then the algorithm is really just f-GAN with chi-squared divergence and the novelty is drastically reduced. If yes,
then it would be great to pinpoint the differences and explain why they are important or would significantly address the issues that are inherent
to classical f-divergences based GANs in terms of training stability.
- how do the results compare if one uses other f-divergences (for example non-symmetric chi-squared but other f-divergences as well)?
- because the criterion is an f-divergence, it may suffer from the same issues that were pointed out in the WGAN paper: gradients would vanish
for distributions with disjoint support. Is there a reason why chi-squared would not have the same issues as Jensen-Shannon or KL ? Or is
it that the proposed implementation, not being exactly written as chi-squared and only equivalent at the optimum in lambda, doesn't suffer from
these issues?

Regarding the experimental results:
The DCGAN baseline for inceptions scores (Figure 4) seem lower than what was reported in earlier papers and the current state of the art is more around 8
(not sure what it was at the time of the writing of this paper though).
The SSL results (Table 2) are ok when compared to the weight clipping implementation of WGAN but this is not necessarily the right baseline to compare with,
and compared to other algorithms, the results are not competitive.

My overall feeling is that this paper presents an interesting connection between the chi-squared divergence and the normalized IPM coefficient which
deserves further investigation (especially a better understanding of how is this connected to f-GAN with chi-squared). But the comparison with the
known suboptimal implementation of WGAN (with weight clipping) is not so interesting and the results are not really convincing either.
So overall I think the paper is below the acceptance bar in its current form."
Minimax Estimation of Bandable Precision Matrices,"Addison Hu, Sahand Negahban",https://proceedings.neurips.cc/paper/2017/hash/070dbb6024b5ef93784428afc71f2146-Abstract.html,"+++++++
Summary
+++++++
The paper establishes the first theoretical guarantees for statistical estimation of bandable precision matrices. The authors propose an estimator for the precision matrix given a set of independent observations, and show that this estimator is minimax optimal. Interestingly the minimax rate for estimating banded precision matrices is shown to be equal to the corresponding rate for estimating banded covariance matrices. The upper bound follows by studying the behavior of inverses of small blocks along the diagonal of the covariance matrix together with classical random matrix theory results. The lower bound follows by constructing two specially defined subparameter spaces and applying testing arguments. The authors support their theoretical development via representative numerical results.

+++++++
Strengths
+++++++

Clarity: The paper is very nicely written and the authors appropriately convey the ideas and relevance of their work.

Novelty: While I am not too familiar with the surrounding literature, the ideas in this paper appear novel. In particular, the estimator is both intuitive, as well as (relatively) easy to compute via the tricks that the authors discuss in Section 2.2. The techniques used to prove both upper and lower bounds are interesting and sound. 

Significance: The authors establish matching upper and lower bounds, thereby effectively resolving the question of bandable precision matrix estimation (at least in the context of Gaussian random variables).

Typo: Should it be F_alpha in (3), not P_alpha?

Update after authors' response: Good paper; no further comments to add.","This paper analyzes an algorithm for estimating so-called bandable precision matrices from iid samples -- vectors, drawn from a distribution with the unknown precision matrix.  Error bounds are established, in terms of the spectral norm, and a complementary minimax analysis establishes the optimality of the proposed procedure.

This paper is extremely well-written, concise, and complete.  The actual problem being considered here fills in a ""gap"" in the existing literature, and is well motivated and placed well within the context of the existing literature.  Overall, this should be a definite accept.  In fact, this paper should serve as a model example for how to motivate, present, and discuss problems of this general flavor.

I have only a few very minor points that the authors may wish to address in revision:
* Section 2.2 on implementation details is welcome, given that the ""naive"" interpretation of the algorithm would require many matrix inversions.  That said, I did find it a bit strange to discuss the implementation details in this much detail without providing some quantification of the overall implementation complexity.       
* The alpha=0.2 case seems to be the ""outlier"" in each of the experimental cases.  Especially for plot 2, I might recommend omitting it to make the other three cases easier to see.  In Figure 1, the growth doesn't look linear for that case, and the growth is even difficult to see in the other cases. 
* There seems to be mix of styles in the references, with respect to the author name convention.","The paper developes new minimax bounds on the estimation of banded precision matrices and propose a low-complexity estimation algorithm that acheive the proposed lower bound.


Comments:

1. is the constraint on the minimal eigen value of \Omega necessary in the proof? since this condition is not typically met in the high-dimensional setting as n- > infty.

2. From the simulations in Fig. 1 it seems that the error in spectral norm is around 1. This can be very good or terrible depending on how large the spectral norm of the true matrix is. It would be better to write the normalized error. 

3. Figure 2 is really confusing. SInce \alpha  > 0 then n^{-2\alpha/(2\alpha+1)} is a decreasing function of n, which means the larger $n$ values correpond to the region very close to 0 and I do not see a constant slope in the results! May be it would be better to draw in logaritmic scale and for larger n to clearly illustrate the results.





Some suggestions to improve readibility:

1. It would be better to denote vectors by boldfase small and matrices by boldface capital letters. Currently the notation is really mixed: X_i for data -- x_i 

2. It is good to mention that \omega in (3) denotes the components of the matrix \Omega

3. the notation is confusing in (8) what is 1-k and 1-k/2?

4. there should be a negative sign in (9). Also it is better to unfy the notation since now in (9) we have both X and v for vectors.

5. line 64: improve the text  

6. line 79: past that - beyond that 

"
Kernel functions based on triplet comparisons,"Matthäus Kleindessner, Ulrike von Luxburg",https://proceedings.neurips.cc/paper/2017/hash/07211688a0869d995947a8fb11b215d6-Abstract.html,"The authors propose two kernel functions based on comparing triplets of data examples. The first kernel function is based on ranking of all the other objects with respect to the objects being compared. The second function is based on comparing whether the compared objects rank similarly with respect to the other objects. The idea is interesting and it is potentially impactful to create such “ordinal” kernels, where the data are not embedded in metric spaces. 

Comments:
1. The authors should provide their main claims somewhere and demonstrate it well in the paper. Why should anybody use a triplet comparison kernel? From what I understand, with triplet comparison kernel, less number of triplets are sufficient to capture the same information that a full pairwise similarity matrix can capture (sec. 2.2). It is also faster. Is that correct?

2. How is the first kernel (k_1) and the one proposed in (Jiao & Vert 2015) different?

3. In Figure 3, can you quantify how well the proposed kernel performs with respect to computing all pairwise similarities? The synthetic datasets are not hard, and it may also be useful to demonstrate some harder case?

4. The kernels seem to have time advantages compared to other embedding approaches such as GNMDS (from the experiments in Sec. 4.2). However in terms of clustering performance they are outperformed by embedding approaches (the authors claim that the performance is close for all algorithms). How many landmark points were chosen for computing the kernels? Can you comment on the size of landmark set vs. running times? 

5. Also, in terms of running times, can you comment whether the reason for the slow performance of the embedding algorithms is the inherent computational complexity? Could it be an implementation issue?

6. The authors should attempt to provide results that are legible and easy to read. If there is lack of space, supplementary material can be provided with the rest of results.

Update
-------
The authors have answered several of my comments satisfactorily. ","
Paper Summary:
The authors propose a methodology to define a similarity kernel function from ordinal data (triplet based comparisons). They estimate the (dis)similarity score between two objects x1 and x2 by comparing rankings induced the objects that are estimated from the ordinal data. 
Experiments on benchmark datasets show that their proposed technique yields comparable quality results but with significant speedups.

Review:
Overall I like the idea of estimating kernel functions from ordinal data. To best of my knowledge this has not been done before and I believe that the intuitive idea proposed in this paper can help practitioners do standard kernel-based learning tasks from ordinal data. 

I hope that the authors do the following:
- include all the ‘omitted’ figures and experiments on a more diverse set of benchmarks in the appendix / supplementary materials. 
- provide the demo code
- more visualizations like Figure 4

These would help make the message of the paper even stronger.

","The authors propose kernels on the top of non-metric, relatively
ordered data sources. In the construction they define explicit
feature spaces to represent the kernels, thus those  kernels could
be relatively easily applied for example to describe outputs in a structured
learning environment as well.  The geometry of the feature space is also
discussed to illuminate the potential behavior which can help in the
interpretation of the relationships expressible by these type of kernels. 

The paper is clearly, and concisely  written. The introduction of the
underlying problem can be well understood. The definition of the
kernels via explicit feature space is significant advantage against
other kernels built up on a graph structure.      

Firstly it seems the data model
is very restricted, but this type of approach to learning
from non-metric data is inherently allows to tackle with highly
nonlinear problems, for example learning on a manifold where only
local relations are available.

The handling of contradictions might be oversimplified since they
could be also produced by non-acyclic graphs and not only by
statistical noise. Perhaps a better approach could be that where the category of
the underlying order structures are defined more explicitly. In that
case methods dealing with the different type of contradictions can be
developed.        
"
Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization,"Fabian Pedregosa, Rémi Leblond, Simon Lacoste-Julien",https://proceedings.neurips.cc/paper/2017/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html,"This paper considers solving the finite sum optimization via the SAGA framework. This work extends from Leblond et al. 2017 by considering the composite optimization, where the additional part is the non-smooth separable regularization. The key improvement lies on how to deal with the non-smooth regularization while obey the principle of sparse update. The proposed trick to split the non-smooth regularization looks interesting to me. The analysis basically follow the framework in Leblond et al. 2017, the proof for the asynchronous variant follows the idea in Mania et al. 2015.

Major comments:
- Authors need to explain the ""inconsistent read"" more clear. In particular, what is hat{x}_k and how to decide the index of k. Is it the same as Lian et al. 2016 and Liu and Wright 2015? What is the key difference?
- It is unclear to me how to obtain the neat representation of the difference between hat{x}_t and x_t.
- To obtain the linear convergence rate and speedup, this paper makes several implicit assumptions. Authors should explicitly indicate the assumption used in this paper. 

Minor comments / typos:
- The definition of \Delta in line 217 was incorrect.

Missing reference:
The following is related to asynchronous greedy SDCA.
- Asynchronous parallel greedy coordinate descent, NIPS, 2016.","This paper proposes an asynchronous variant of SAGA, a stochastic gradient method for finite sum convex optimization (Defazio et al, 2014). To my understanding, the paper addresses two challenges: asynchronicity and proximity. The authors achieve three improvements over the original SAGA. First, they propose a sparse update based on block coordinate wise using extended support for the gradients when the regularization is decomposable. Second, they design an asynchronous variant of SAGA where the delay quantity can be up to sqrt(n)/10. Finally, they can deal with nonsmooth regularizers via proximal operators. In terms of convergence results, the still achieve a linear convergence rate under the strong convexity of the overall sum function f, and individual Lipschitz gradient of fi. Although the step-size is slightly smaller than the one of SAGA, the convergence factor remains comparable. This is probably due to different assumption. 
In fact, the paper combines several advanced ideas from existing works such as SAGA with variance reduction, sparse updates, Hogwild, asynchorinization such as Arock, etc, to improve over all these techniques. The proof is quite clean and well organized.
In my opinion, such improvements are significant and are important in practice due to obvious reasons. The numerical results also strongly support their theoretical contribution. These experiments are carried out on three large-scale data sets, and empirically show a linear speed up of the new algorithm.
Overall, this paper has significant contribution both in terms of theory and experiments. It merits to be accepted for NIPS.
Minor comments.
Some concepts and notation should be defined. For example, support (supp), \Omega(\cdot), inconsistence read.
The phrase “step size” should be consistent on all text. For example, on line 46 and line 142 they are not consistent.
Line 152, delete “a” or “the”.
Line 361, the y should be in bold.
Line 127: remove one “a”.
","The main contribution of this paper is to offer a convergence proof for minimizing sum fi(x) + g(x) where fi(x) is smooth, and g is nonsmooth, in an asynchronous setting. The problem is well-motivated; there is indeed no known proof for this, in my knowledge.

A key aspect of this work is the block separability of the variable, which allows for some decomposition gain in the prox step (since the gradient of each fi may only effect certain indices of x.) This is an important practical assumption, as otherwise the prox causes locks, which may be unavoidable. (It would be interesting to see element-wise proxes, such as shrinkage, being decomposed as well.)

Their are two main theoretical results. Theorem 1 gives a convergance rate for proxSAGA, which is incrementally better than a previous result. Theorem 2 gives the rate for an asynchronous setting, which is more groundbreaking.

Overall I think it is a good paper, with a very nice theoretical contribution that is also practically very useful. I think it would be stronger without the sparsity assumption, but the authors also say that investigating asynchronosy without sparsity for nonsmooth functions is an open problem, so that is acknowledged.

potentially major comment:
 - Theorem 2 relies heavily on a result from leblond 2017, which assumes smooth g's. It is not clear in the proof that the parts borrowed from Leblond 2017 does not require this assumption. 

minor comments about paper: 
 - line 61, semicolon after ""sparse""
 - line 127: ""a a""

minor comments about proof (mostly theorem 1):
 - eq. (14) is backwards (x-z, not z-x)
 - line 425: do you mean x+ = x - gamma vi?
 - Lemma 7 can divide RHS by 2 (triangle inequality in line 433 is loose)
 - line 435: not the third term (4th term)
 - line 444: I am not entirely clear why EDi = I. Di has to do with problem structure, not asynchronasy?
 - eq (33) those two are identical, typo somewhere with missing Ti?
 - line 447: Not sure where that split is coming from, and the relationship between alpha i, alpha i +, and grad fi.
 - line 448: there is some mixup in the second line, a nabla f_i(x*) became nabla f_i(x).
Some of these seem pretty necessary to fix, but none of these seem fatal, especially since the result is very close to a previously proven result.

I mostly skimmed theorem 2 proof; it looks reasonable, apart from the concern of borrowing results from Leblond (assumptions on g need to be clearer)"
A New Theory for Matrix Completion,"Guangcan Liu, Qingshan Liu, Xiaotong Yuan",t,"The authors study matrix completion from a few entries.  They propose a new
proprty of (matrix,sampling pattern) which they term ""Isomery"". They show that
the well known (incoherence,low rank,random sampling) assumption implies this
condition. The authors then consider a nonconvex bilinear program for matrix
completion. Under the Isomery assumption, they prove that the exact solution is
a critical point of this program (with no assumptions on rank, it seems).  Some
empirical evidence are presented, whereby the nonconvex program is superior to
the convex Nuclear Norm minimization method for a certain pattern of nonuniform
sampling.

The paper is relatively well written (see comments on English below). The
results are novel as far as I can tell, and interesting. This work touches on
fundamental aspects of matrix recovery, and can certainly lead to new research.
There are a few major comments I'd be happy to see addressed. That said, I vote
 accept. 

Major comments:

1. The authors say nothing about the natural questions: Can the Isometry
condition be verified in practice? Is it a reasonable assumption to make?

2. The authors discuss in the introduction reasons why random sampling may not
be a reasonable assumption, with some real data examples. It would be nice if,
for a real full data matrix, and a sampling pattern of the kind shown, the
    authors can verify the Isomry assumption. 

3. The matrix recovery problem, as well as all the algorithms discussed in this
paper, are invariant under row and col permutations. The authors discuss
sampling patterns which are ""concentrated around the main diagonal"" - this
condition is not invariant under permutations. Only invariant conditions make
sense.

4. The main contributions claimed (l.79) are not backed up by the paper itself. 
Specifically, 
    - it's unclear what is meant by ""our theories are more flexible and
powerful""
    - the second bullet point in the main contribution list (l.83) referes
 to ""nonconvex programs"" in general
    - The third bullet point (l.87) is not actuallty proved, and should not be
      advertised as a main contribution.  The authors 
  proved Theorem 3.1, but its connection to matrix completion is only vageuly described
  in the paragraph above Theorem 3.1.

minor comments: 
- English throughout
- Theorem 3.2: formally define ""with high probability""
- The word ""theory"" and ""theories"" in the title and throughout: I'm a non-native English speaker, yet I don't see
why you use the word. 

- l.56: ""incoherent assumption"" -> ""incoherence assumption""
- l.75 ""by the commonly used"" -> ""of the commonly used""
- l.79 ""hypothesis"" -> condition
- l.92, l.129 ""proof processes"" -> ""proofs""
- l.188 ""where it denotes"" -> ""where we denote""
- l.208 ""that"" -??
- Eq. (8) and Eq. (9) - maybe missing =0 on the right hand side?
- Figure 2: label vertical axis
- what do you mean by ""proof of theory"" (eg line 92)?

"
A Bayesian Data Augmentation Approach for Learning Deep Models,"Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, Ian Reid",https://proceedings.neurips.cc/paper/2017/hash/076023edc9187cf1ac1f1163470e479a-Abstract.html,"Data augmentation for neural networks generates synthetic examples, which are then used for additional training of a classifier. This can be achieved by label preserving transformations in a generic way or adaptive by training a generative adversarial network (GAN). The authors propose a new network architecture for this task consisting of three deep neural networks, two classifiers and one generator. The training algorithm described in the paper combines Monte Carlo expectation maximization (MCEM) and generalized expectation maximization (GEM). Experiments on standard data sets for image classification show that the new method is significantly better than data augmentation with label preserving transformations.

The paper is well written. It clearly explains the proposed network architecture as well as the algorithm for training the three neural networks. However, ""-log(1 - fA(fake))"" in (15) is not consistent with the other cost functions and should be corrected. As the new approach is an extension of AC-GAN, a comparison with that architecture would be most interesting in order to see the effect of adding a third network directly. Therefore I recommend to add results for AC-GAN to the next version of this paper. While the results provided in the paper show that GAN methods are better than label preserving transformations or no augmentation, this does not tell if adding a third network is justified by a significant increase in performance.

The comparisons provided in the author feedback show that the proposed network architecture has advantages over previous data augmentation methods. Including these results as well as the correction for (15) in the paper will improve it. However, a more systematic comparison would be even better.","The authors propose a Bayesian approach to handle Data Augmentation (DA) for learning deep models. Data augmentation, in this context, addresses the problem of data sparsity by generating more training samples to make the training process of a (heavily parameterized) Classifier Network (CN) robust. The standard Generative Model (GM) for obtaining the augmented training set is typically trained once and then new data are generated based on this GM and used to estimate the CN. The authors suggest to also learn the GM itself jointly with the CN as new augmented training data are produced - which is straightforward with a Bayesian approach. Using three different data sets and two classifiers,  they demonstrate that the Bayesian DA is superior to regular DA, which in turn is superior to not having DA at all (which likely suffers from some overfitting).

The authors claim in the introduction that they provide a novel Bayesian formulation of DA. While this may be true, from a Bayesian point of view their idea is quite simple, as this is what any Bayesian statistician would do. Having said that, I think this is a good paper which is clearly written. If it really is the first attempt to think about this problem in a Bayesian way it is by itself worthy of publication, but I am not sufficiently familiar with the deep learning literature to judge this.

The main problem with the pure DA is that one is heavily relying of the GM to be producing correct data, while the proposed approach would allow the GM to be improved. This makes the authors' approach more challenging as both the CN and the GM need to be trained and, in top of this, both are deep models. Hence, making inference using traditional powerful Bayesian tools seems impossible and the authors propose an algorithm which is in spirit with the EM-algorithm to learn the ""latents"" (here the artificial data) conditional on model parameters and vice versa. Hence my main concern regards the optimization and its robustness to different choices of tuning parameters. For example, Wilson et al. (2017) show that adaptive step-size choices (such as ADAM) are prone to find poor modes in heavily parameterized models. This model is significantly more parameterized than the standard DA, and, although you generate more data, you also have the problem of training the GM. Can you comment on this?  
 
I think that Lemma 1 should be expressed more carefully, stating the assumptions made.

@article{wilson2017marginal,
  title={The Marginal Value of Adaptive Gradient Methods in Machine Learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  journal={arXiv preprint arXiv:1705.08292},
  year={2017}
}
"
Deep Hyperalignment,"Muhammad Yousefnezhad, Daoqiang Zhang",https://proceedings.neurips.cc/paper/2017/hash/0768281a05da9f27df178b5c39a51263-Abstract.html,"Summary
This paper proposes a hyperalignment method based on using neural network in place of basis functions under a framework similar to kernel hyperalignment. The method is applied to a number of real fMRI datasets. Increased classification accuracy with reduced computation time compared to another deep learning architecture is shown.

Comment
1. The novelty of this paper is unclear. From the text, the proposed method seems to be a combination of previous contributions. Does using neural network in place of basis functions result in an objective that is much harder to optimize? The optimization procedure seems to be “inspired” by previous works also.

2. Works on functional alignment by Georg Lang should be cited.

3. The original purpose of alignment is to establish subject correspondence so subject variability can be modelled. The best way to demonstrate better correspondence is to visually show that the subjects’ activation patterns better align. By using a classification task for assessment as done in this work, it is difficulty to tell if it is better alignment that resulted in better classification or simply a better functional representation that resulted in better performance. In fact, it is unclear how to map the aligned data to the original anatomical space for interpretation purposes.

4. Since G is not unique, would that have a major impact on the results?

5. The results might reflect overfitting, i.e. biased, since the best performance over a range of parameter values is reported, as opposed to conducting proper nested cross validation. Models with greater flexibility would tend to perform better. Also, the range of parameter values would have a great influence on the results.

6. Functional hyperalignment presumably does not require pre-anatomical alignment. Does the performance gain hold if no spatial normalization is performed?
","This work describes an extension of the hyperalignment (HA) method,
i.e. functional alignment across subjects. This extension directly
addresses some limitations of previous HA methods in the literature:
nonlinearity, scalability and high-dimensionality. The proposed
method, called deep hyperalignment (DHA), defines the objective
function to minimize by reformulating the HA objective function in
terms of multiple stacked layers of nonlinear transformations with a
(multi-layer) kernel function. The objective function is reformulated
in terms of templates and then eigendecomposition. Explicit derivative
is derived. Thus, optimization can be carried out also for a dataset
with large number of subjects. The proposed method is tested using
standard SVM classification and compared to the main HA algorithms in
the literature. The experimental setup is based on 5 task-based
datasets and 2 movie-based datasets. DHA shows superior performance
with respect to other algorithms, in all cases.

The paper is well written, even though complex to read in Section 3
(""Deep Hyperalignment""). I invite the authors to make that part more
accessible. I see no major issues in the paper, but I am not expert in
the mathematical derivations of the proposed method. The experimental
evidence is convincing and significant.
","Deep Hyperalignment

[Due to time constraints, I did not read the entire paper in detail.]

The problem of functional alignment of multi-subject fMRI datasets is a fundamental and difficult problem in neuroimaging. Hyperalignment is a prominent class of anatomy-free functional alignment methods. The present manuscript presents a extension to this using a deep NN kernel. I believe this to be novel, though I don’t know the literature well enough to be confident of that. While not a surprising research direction, this is a natural and promising method to consider, and the experimental results are excellent.

The methods are explained thoroughly, with detailed proofs in supplementary material. I did however find this section too dense and without a good summary of the final design and parameter choices. A figure sketching the method would have been very helpful. Related work is surveyed well. The new method could be described as an amalgam of several existing approaches. However the various elements have been combined thoughtfully in a novel way, and carefully implemented.

The experiments are impressively thorough, involving several open datasets and several competing methods, with detailed reporting of methods, including the parameters used for each of the alignment methods. Some more detail on preprocessing would have been helpful. It seems there has been some validation of parameters on the test data (“the best performance for each dataset is reported” p6), which is not ideal, however the number of parameter values tested is small and the results are so good that I feel they would stand up to a better training/validation/test split. My main concern about this section is that all of the results reported were computed by the present authors; it would be helpful to also flag any comparable results in the literature. 

The quality of the English is mostly good but with occasional problems. E.g “time complexity fairly scales with data size” is unclear, and the sentence in which it appears is repeated three times in the manuscript (including once in the abstract). Some proof-reading is in order.

"
Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model,"Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra",https://proceedings.neurips.cc/paper/2017/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html,"This paper describes an improved training procedure for visual dialogue models.
Rather than maximizing the likelihood of a collection of training captions, this
approach first trains a discriminator model to rank captions in a given context
by embedding them in a common space, then uses scores from this discriminator as
an extra component in a loss function for a generative sequence prediction
model. This improved trainin procedure produces modest improvements on an
established visual dialogue benchmark over both previous generative approaches
as well as adversarial training.

I think this is a pretty good paper, though there are a few places in which
the presentation could be improved.

SPECIFIC COMMENTS

The introduction claims that the discriminator ""has access to more information
than the generator"". I'm not clear on what this means. It is
true that in a single update step, the discriminator here is trained to
distinguish the ground truth answer from a collection of answers, while in the
standard conditional GAN setting the discriminator compares only one sample at a
time to the ground truth. But over the course of training a normal GAN
discriminator will also get access to lots of samples from the training
distribution. More generally, there's an equivalence between the problem of
learning to assign the right probability to a sample from a distribution and
learning to classify whether a sample came from the target distribution or a
noise distribution (see e.g. the NCE literature). Based on this section, I
expected the discriminator to be able to learn features on interactions between
multiple candidates, and was confused when it wound up assigning an embedding to
each candidate independently.

Related work: given the offered interpretation of D as a ""perceptual loss"", I
was surprised not to see any mention in the related work section of similar loss
functions used image generation tasks (e.g. Johnson et al). I was not able to
find anything in a quick lit search of similar losses being used for natural
language processing tasks, which is one of the things that's exciting about this
paper, but you should still discuss similar work in other areas.

@137 it's standard form

The explanation offered for the objective function at 177 is a little strange.
This is a logistic regression loss which encourages the model to assign highest
probability to the ground-truth caption, and I think it will be most
understandable to readers if presented in that familiar form. In particular, it
does not have an interpretation as a metric-learning procedure unless the norms
of the learned representations are fixed (which doesn't seem to be happening
here), and is not a margin loss.

I don't understand what ""assigned on-the-fly"" @186 means. Just that
you're reusing negative examples across multiple examples in the same batch?

@284 modifications to encoder alone

Table captions should explain that MRR -> ""mean reciprocal rank"" and especially
""mean"" -> ""mean rank"".
","This paper proposes a novel neural sequence model for visual dialog generation. Motivated by the flexibility of generative dialog model (G) and the good performance of discriminative dialog model (D), authors propose an interesting way to combine advantages of both models by building communication between two models. This is an interesting idea. Different from the adversarial training used in GAN models, they pretrain G with MLE loss and D with a metric learning loss respectively in advance and then further train G with both metric learning loss and MLE loss. By using metric learning loss, it is encouraged to learn a semantic embedding space. The use of Gumbel-Softmax sampler makes it possible to have end-to-end training for metric learning loss.

Besides, a more advanced attention model is proposed to effectively model image, history and question into an embedding vector.

In the experiment section, authors have done extensive ablation study to validate the effectiveness of different components of the proposed method.

There is a mistake on line 304 where it says '45.78% R@5' but I can not find this number in Table. 4.
I suggest authors to mention how they choose other options for N-pair loss. Are all other 99 options are used or only a subset are sampled and how?

Overall, it is a good paper which proposes a novel and effective method for the task of visual dialog generation.","This paper proposes a training procedure for visual dialog models.
The paper first notes that the strength of discriminatively trained models has supervisor performance over generative models, 
but have the limitation of not being useful in practical settings, where novel responses are  needed and no ranked list is available; and the strength of generative training is their ability to deal with unknown dialogs but have the limitation of generating safe responses,  sticking to known responses.
The authors seek to draw from the strengths of both of these training paradigms through knowledge transfer from
a discriminative model to a generative model.
For an end-to-end differentiable loss,  the paper makes use of Gumbel-Softmax.

A new encoder is proposed, which does not treat the image as a single entity, instead the proposed
image attention model considers the fact that the question  is likely to be relevant to a specific part of the image.
The proposed encoder uses the question to attend to the dialog  history, and then uses the
question and attended dialog  history to perform attention on the image.

Experiments were carried out on the VisDial dataset, a visual dialog dataset generated
using mechanical turk users.

1. When updating the discriminator during training of the generator, i,e. adversarial training
the reason provided for bad performance is that the the discriminator in this setting
updates its parameters based on the ground truth and the
 generated sample  but does not make use of the additional incorrect options. One
would imagine that the objective function of the adversarial discriminator could be augmented to also
take into account the additional incorrect options, in addition to the adversarial part of the loss?

2. In terms of empirical results, although modest gains in performance are observed at R@5 and R@10 (~ 2%)
 it is unclear if the improvements at R@1 are significant. 

3. Overall, taken individually, the contributions of the paper are mostly based on prior
work, however, their combination is conceptually interesting. On the other hand, performance improvements
at R@1 are not significant.
"
PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference,"Jonathan Huggins, Ryan P. Adams, Tamara Broderick",https://proceedings.neurips.cc/paper/2017/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html,"The authors propose to use polynomial approximate sufficient statistics for non-exponential family models in the generalised linear models. More specifically the authors use Chebyshev polynomials to approximate the mapping function \phi among other polynomials, since the error with the Chebyshev polynomials is exponentially small with order M when the mapping function is smooth. The authors show the theoretical results in Section 4, in which how the error in the MAP estimate decreases exponentially with order M. The authors also show how the posteriors with and without the polynominal approximation differ in terms of the Wasserstein distance, in Section 4.2. 

I found it a bit difficult to understand what the Corollary 4.3 an 4.4 mean. Can authors elaborate what these mean in plain English in their rebuttal? Also, how are their theoretical results reflected in the experiments? 

--- after reading their rebuttal ---
Thanks the authors for answering my questions. I keep my rating the same.   ","[EDIT] I have read the rebuttal and I have subsequently increased my score to 7. 
 
# Summary of the paper
When data are too numerous or streaming, the authors investigate using approximate sufficient statistics for generalized linear models. The crux is a polynomial approximation of the link function.

# Summary of the review
The paper is well written and the method is clear and simple. Furthermore, it is computationally super-cheap when the degree of the polynomial approximation is low, a claim which is backed by interesting experiments. I like the underlying idea of keeping it simple when the model allows it. My main concern is the theoretical guarantees, which 1) lack clarity, 2) involve strong assumptions that should be discussed in the main text, and 3) are not all applicable to the recommended experimental setting. 

# Major comments
- L47 ""constant fraction"": some subsampling methods have actually been found to use less than a constant fraction of the data when appropriate control variates are available, see e.g. [Bardenet et al, MCMC for tall data, Arxiv 2015], [Pollock et al., the scalable Langevin [...], Arxiv 2016]. Actually, could your polynomial approximation to the likelihood could be useful as a control variate in these settings as well? Your objection that subsampling requires random data access still seems to hold for these methods, though.
- L119 ""in large part due to..."": this is debatable. I wouldn't say MCMC is expensive 'because of' the normalizing constant. ""Approximating this integral"": it is not the initial purpose of MCMC to approximate the normalizing constant, this task is typically carried out by an additional level above MCMC (thermodynamic integration, etc.).
- Section 4: the theoretical results are particularly hard to interpret for several reasons. First, the assumptions are hidden in the appendix, and some of them are quite strong. The strongest assumptions at least should be part of the main text. Second, in -say- Corollary 4.1, epsilon depends on R, and then L207 there's a necessary condition involving R and epsilon, for which one could wonder whether it is ever satisfied. Overall, the bounds are hard to make intuitive sense of, so they should be either simplified or explained. Third, some results like Corollary 4.2 are ""for M sufficiently large"", while the applications later typically consider M=2 for computational reasons. This should be discussed.
- The computational issues of large d and large M should be investigated further.
- Corollary 4.2 considers a robust loss (Huber's) but with a ball assumption on all data points, and a log concave prior on theta. I would typically use Huber's loss in cases where there are potential outlier inner products of x and theta; is that not in contradiction with the assumptions?
- Corollary 4.3 Assumption (I) is extremely strong. The appendix mentions that it can be relaxed, I think it should be done explicitely.
","I found the introduction and motivation for this paper to be quite good. General Linear Models (GLM) are certainly a widely used tools so improvements for large scale data analysis problems is impactful.  The basic idea of expanding the link function to provide approximate sufficient statistics seems clear enough. 

I'm not familiar enough with the full literature on scalable versions of GLM to know how this work compares. A more complete literature review would have helped. The abstract for instance says existing approaches either don't scale or don't provide theoretical guarantees on quality, and then says the method shows ""competitive performance"" (to what?).  

After reading the introduction (line 43), the authors say variational Bayes  ""lacks guarantees on quality"" without explaining what they mean. I assume this is also what is being referred to in the abstract where they write ""don't provide theoretical guarantees on quality"". I assume this  refers to the fact that the recognition model or approximate posterior in variational Bayes need not be the true posterior if the variational family isn't expressive enough. This seems to be a fairly weak argument against variational Bayes, given that this method is also using approximate sufficient statistics. Regardless, more some more text dedicated to the problem with methods like SGD and VB would be welcome.

On page 3 line 116, I would strike the somewhat silly line that the MAP is a strict generalization of the MLE b/c in some situations they are equal. They are different concepts and they have different transformation properties. Eg. from theta -> theta' = g(theta) the MAP can change, but the MLE is invariant.

On page 4 line 138 it mentions the log likelihood isn't a dot product of a statistic and the parameter, hence the argue not in the exponential family. This should be clarified as the Poisson distribution is in the exponential family and the footnote on page 3 mentions that they are using a non-standard notation for the exponential family as they have suppressed the base measure and log-partition function. 

I have a hard time reading Figure 2 given the colored labels. It's also a little strange that some are curves vs. time and the others are simply dots. Also, what should the reader make of the fact the -LL for PASS-LR2 is less than that of the true posterior ? It seems like this is a measure of uncertainty or variance in that estimate.

The NIPS review guidelines say:
""We would prefer to see solid, technical papers that explore new territory or point out new directions for research, as opposed to ones that may advance the state of the art, but only incrementally. A solid paper that heads in a new and interesting direction is taking a risk that we want to reward.""
Overall my impression is this is an incremental advance the state of the art."
Online multiclass boosting,"Young Hun Jung, Jack Goetz, Ambuj Tewari",https://proceedings.neurips.cc/paper/2017/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html,"This paper aims to develop online version of the multiclass boosting algorithm. The authors combine the ideas in [4] and [7] I order to develop their framework. Overall, the paper is well written and easy to follow. It also seems that framework is technically sound. However, there are certain parts of the paper that is not well-justified or motivated. I talk about them below:

1- The title of Section 3 is “an optimal algorithm”. It is not clear in what sense the developed algorithm in this section is optimal. There is no theory proving that the bounds obtained using the developed algorithm is optimal for online multi-class boosting. If mistakes bounds of O(T) (for example in case of 0-1 loss) is optimal for online multi-class boosting, it should be proved or there should be some reference to where is has already been shown.

2- Now, let's say it can be proved that such bounds are optimal for online boosting. What is the motivation behind online boosting when the bounds are suboptimal when compared to online learning with expert advice (O(log(T))? Why having multiple weak learners are useful at all when it doesn’t lead to better results in terms of (mistake) bounds compared to standard online learning models?

3-  Given that in online boosting, you already have all the weak learners available, why do you still calculate the potential function sequentially, assuming you don’t have the knowledge of labels for any weak learner after the current weak learner i. This is reflected in designing the potential function in Equation (3) which results in computing the weights w^i(t) and cost matrix D_t^i sequentially. To me, it seems that computing the potential function by using the outputs of all weak learners is more precise and I am not sure why such an approach is not taken here.  Will that result in better bounds?

And here are some minor problems:
Section 3.1: what do you mean by “For optimal algorithm, we assume that \alpha_t^{i}=1,  \forall i,t”?

The notation is sometimes difficult. As an example, it took me some times to understand s(r) in Section 3.1 refers to element r of vector s. I would have used s[r] instead of s(t) to prevent the confusion with a function.

Section 3.2: “Under 0-1 loss” is repeated twice in 3rd sentence. 
","This paper develops appropriate theory and proposes two algorithms for online multi class boosting. One particularly notable result is a weak learning condition for online multi class boosting. This paper would be a notable addition to this recently growing literature, therefore, I recommend acceptance.

A few comments:

1. On page 3, in definition 1, it is worth commenting on whether there are any theoretical developments describing how S, the excess loss, decreases with the number of training examples. This would seem a basic requirement for weak learning in an online setting.

2. In the experimental section, it is worth noting that the comparison to the original online boosting algorithm is not quite fair since it was developed as a two-class algorithm. The experimental results on the online boosting algorithm are worth retaining because, as the authors point out, it is a good baseline.","In this paper the authors introduce two learning algorithms dealing with online learning in a multi-class setting. The novelty resides in combining a multi-class boosting framework providing solid theoretical results with an online learning framework. Notable contributions of the paper include: a weak learning condition for multi-class online learning, a general boosting algorithm and a more practical version. I have two main concerns about this paper.

The paper is overall well written, the key ideas are ordered in an easy to follow manner and, as far as I can tell, the theoretical results are sound. However, at times the paper is hard to follow without switching over to the supplementary material. For instance, the link between the weak learning condition in batch learning and the one presented in the paper, is not obvious without reading the supplementary material; Section 3.2 is also hard to follow since the supplementary material is needed in order to grasp key ideas. While I do understand the authors' eagerness to include most of their work in the paper, in my opinion, the main paper should be as close as possible a stand alone paper, which isn't the case for this paper.

My second concern is related to the experimental and (the missing) discussion sections. The experimental results suggest that the proposed methods can achieve better results than other online methods, however their run time is obviously far more important. As such two questions arise : 1) Is there a case where the proposed methods are more interesting to use than existing ones wrt the ratio performance/run time? 2) Is it possible to speed up the proposed methods without significant loss in their performances? A discussion section treating both questions would've been appreciated. I also think that more experimental results are needed in order to evaluate how the proposed methods scale wrt the number of classes and examples.

When introducing the adaptive algorithm the authors propose to discard the weak learning condition and introduce an empirical edge. However the empirical edge takes its values in [-1,1], while the edge in the weak learning condition takes its values in (0,1). In practice, how is a negative edge dealt with? Is the training sample discarded?"
State Aware Imitation Learning,"Yannick Schroecker, Charles L. Isbell",https://proceedings.neurips.cc/paper/2017/hash/08e6bea8e90ba87af3c9554d94db6579-Abstract.html,"This paper proposes a framework for MAP-estimation based imitation learning, which can be seen as adding to the standard supervised learning loss a cost of deviating from the observed state distribution.
The key idea for optimizing such a loss is a gradient expression for the stationary distribution of a given policy that can be estimated online using policy rollouts. This idea is an extension of a previous result by Morimura (2010). 

I find the approach original, and potentially interesting to the NIPS community. The consequences of  deviating from the demonstrated states in imitation learning have been recognized earlier, but this paper proposes a novel approach to this problem.

I have some problems with the presentation. The paper heavily builds on technical results from Morimura (2010). It would be helpful to present the work in a more self-contained format. See also the detailed comments below.

The correctness of Eq. (3) and (4) depends on certain ergodicity assumptions of the MDP (required for detailed balance to hold), which are mentioned in Morimura (2010) but not in the current paper. Such assumptions should be added and discussed. 

The requirement of taking additional policy rollouts (in addition to the imitation learning data) is quite restrictive, even if these samples are unsupervised. In this respect, I wonder if a relevant comparison would be to take additional RL rollouts with a cost for ‘staying close’ to the demonstrated data. Such a cost might be hard to tune, and I would thus expect the current method to perform much better.

Detailed comments:

158: This equation, which is fundamental to the work, is non-trivial, and uses ‘backward’ MDP transitions which are non-standard in RL/policy gradient. It would be helpful for the reader to provide more explanation, and explicitly refer to Morimura’s work for a proof. 

172: Perhaps add some motivation for this constraint?

193: where is this assumption mentioned in this work? This whole paragraph can only be understood after reading the work of Morimura (2010).

205: How is this assumption justified?

I have read the author feedback.","This paper presents a new approach to imitation learning called State-Aware Imitation Learning. Rather than just trying to match the actions of the demonstrations, it also tries to match the state distribution from the demonstrations. The paper adds matching that state distribution as part of the objective function and they derive a way to approximate the gradient for the state distribution. Results on two domains show that this approach is better able to overcome stochasticity in the environment than pure supervised learning.

The idea of looking at the distribution over states seems very similar to the recent work on Generative Adversarial Imitation Learning (GAIL), where the discriminator network looks at the distribution over states or state-actions to classify whether the policy is from the demonstrations or the generating network. This work should be included in the related work and ideally compared to in the experiments.

The paper says that sample episodes are collected in an ""unsupervised"" way multiple times throughout the paper. What exactly does this mean? They're collected by following the current policy of the algorithm, right?

Both of the domains are setup in a particular way to benefit this algorithm: there is stochasticity in the environment to push the agent out of the state distribution of the demonstrations, and there is a reward penalty for leaving that distribution by too much (either going off track in the racing case, or the robot falling over). It would be interesting to see if this approach provides a benefit in other domains that are not so well tuned to its strengths.

In both experiments, the authors train a supervised baseline first and then run their algorithm starting from that policy network. What happens if SAIL is trained from the start without the supervised training of the policy network first? 

I think this is a good contribution to the imitation learning literature, and the results look nice, but it should compare with GAIL, at least in the related work.

I read your feedback, thanks for the clarifications and glad to see you'll add the GAIL comparison. 
","I've read the feedback, thanks for the clarifications.

%%%

This paper deals with imitation learning. It is proposed to maximize the posterior (policy parameters given the expert state-action pairs). The difference with a classic supervised learning approach is that here the fact that the policy acts on a dynamical system is taken into account, which adds the likelihood of states given policy (the states should be sampled according to the stationary distribution induced by the policy). This additional term is addressed through a temporal difference approach inspired by the work of Morimusa et al. [12], other terms are handled in the classic way (as in supervised learning). The proposed approach is experimented on two domains, a small on and a much larger one, and compared mainly to the supervised baseline (ignoring the dynamics).

This paper proposes an interesting, sound and well motivated contribution to imitation learning. I found the paper rather clear, but I think it could be clarified for readers less familiar with the domain. For example:
* in Eq. 3, the reverse probability appears. It would help to at least mention it in the text (it is not that classical)
* the constraint (l.168, expected gradient null) comes simply from the fact that the probabilities should sum to one. It would help a lot to say it (instead of just giving the constraint)
* the function f_w is multi-output (as many components as policy parameters), it could help to highlight this in the text
* the off-policy approach should be made explicit (notably, as it is a reversed TD approach, due to the reversed probabilities behind, do the importance weights apply to the same terms?)

Section 2 is rather complete, but some papers are missing. There is a line of work that tries to do imitation as a supervised approach while taking into account the dynamics (which is also what is done in this paper, the dynamics inducing a stationary distribution for each policy), for example :
* ""Learning from Demonstration Using MDP Induced Metrics"" by Melo and Lopes (ECML 2010)
* ""Boosted and Reward-regularized Classification for Apprenticeship Learning"" by Piot et al. (AAMAS 2014)
The second one could be easily considered as a baseline (it is a supervised approach whit an additional regularization term taking into account the dynamics, so the used supervised approach in the paper could be easily adapted)

Regarding the experiments:
* it would have been interesting to vary the number of episodes collected from the expert
* for the off-policy experiment, it would be good to compare also to (on-policy) SAIL, as a number of gathered trajectories
* for the walker, giving the number of input/output neurons would also help (28 in/4 out for the policy, 28 in/3200 out for the gradient estimator? so a 28:80:80:3200 network?) This makes big output networks, it could be discussed.
* still for the walker, what about off-policy learning?

Regarding appendix A, the gradient of the log-policy plays the role of the reward. Usually, rewards are assumed to be uniformly bounded, which may not be the case here (eg, with a linear softmax policy based on a tabular representation, the probabilities could be arbitrary close to zero, even if ever strictly positive). Isn't this a problem for the proof?
"
Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter,"Yi Xu, Qihang Lin, Tianbao Yang",https://proceedings.neurips.cc/paper/2017/hash/09fb05dd477d4ae6479985ca56c5a12d-Abstract.html,"The results of this paper appear to be interesting. Here are some comments:

1. To ensure the solution stays in a compact set, the author add a constraint R(x) \leq B. Then, authors have to use a sub-algorithm for evaluating the proximity operator of R under this constraint. This is a main drawback of this work and the author should address carefully the effect of inexact to rate of convergence.
2. The current state of this paper consider the case where \theta is in (0, 0.5). In some application, theta can be greater than 0.5. Can the authors provide some comments to explain why ?
3. More example for HEB condition should be provide.

2. The notation is overlap: R is the number of iteration and function. ","The paper studies the SVRG algorithm (stochastic variance reduced gradient) under quadratic error bound condition and more generally under Holderian error bound condition. The main contribution of the paper is to adapt the error bound parameter without knowing it in advance, where the so called error bound parameter is a similar quantity as the usual strong convexity parameter. 

The idea of the paper is very interesting and I have checked the proof is correct. However, I find the presentation of the algorithm a bit obscure, it might be better to give more intuitions and explanations on it in the main paper instead of going into the details of the proof. For exemple, it would be nice to comment on the difference between the complexity before and after the restart strategy and compare it with the lower bound if possible. 

The main idea of the paper is to predefine the number of iterations R and T for the outer and inner loop operations and check whether a sufficient decrease condition is satisfied, if not the estimate error bound parameter is too small compare to its true value and we should increase R and T. In contrast to the algorithm 1 which can run until any accuracy, the adaptive method requires to set the target accuracy in advance, because R depend on \epsilon. Is there a way to remove such dependency and how does it set in the experiments? In the experiments, a flat plateau sometimes occurs, can authors comment on such observation?

Overall, the theoretical result of the paper is solid with promising experimental evaluations but the presentation is improvable, a major revision of the text is needed.  

#EDIT Thank you for author's feedback. I keep my score because of the presentation is largely improvable. ","This paper introduces a restarting scheme for SVRG specialised to the case where the underlying problem satisfies the quadratic error bound (QEB) condition, a weaker condition than strong convexity that still allows for linear convergence. The algorithm solves the problem of having to know the value c of the QEB constant before hand.

The restarting scheme applies full SVRG repeatedly in a loop. If after the application of SVRG the resulting iterate is less than a 25% relative improvement (according to a particular notion of solution quality they state), the value of c is increased by \sqrt{2}, so that the next run of SVRG uses double the number of iterations in it's inner loop (T propto c^2).

The use of doubling schemes for estimating constants in optimisation algorithms is a very standard technique. It's use with SVRG feels like only an incremental improvement. A particular problem with such techniques is setting the initial value of the constant. Too small a value will result in 5-10 wasted epochs, where as too large a value results in very slow convergence.

The experiments section is well-explained, with standard test problems used. I do have some issues with the number of steps shown on the plots. The x axis #grad/n goes to 1000 on each, which is unrealistically large. It masks the performance of the algorithm during the early iterations. In the SVRG and SAGA papers the x axis spans from 0 to 30-100 of #grad/n, since this represents the range where the loss bottoms out on held-out tests sets. It's hard to tell from the plots as they are at the moment if the algorithm works well where it matters. This would be much clearer if error on a test set was shown as well.

On the first plot, a comparison is made against SVRG with a fixed number of inner iterations T, for a variety of T. The largest T tried had the best performance, I would suggest adding larger values to the plot to make clear that using too large a value results in worse performance (i.e. there is a sweet spot). I would remove the T=100 from the plot to make room.

In terms of language, the paper is ok, although in a number of places the language is a little informal. There are a moderate number of grammatical issues. The paper does require additional proof reading to get it up to standard."
Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data,"Wei-Ning Hsu, Yu Zhang, James Glass",https://proceedings.neurips.cc/paper/2017/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html,"This paper presents a factorized hierarchical variational autoencoder applied to unsupervised sequence modeling. The main claim of the paper is that the proposed model can disentangle the sequence representation into frame-level and sequence-level components. The sequence-level representation can be used for applications such as speaker verification, without any supervision in learning the representation, and they show that it is better than competitive unsupervised baselines such as using i-vector representations. 

The model is a mostly straightforward adaptation of sequential VAEs, with the addition of a discriminative regularizer that encourages sequence-level features to be able to predict sequence indices. Does this mean the actual index of a sequence in the training set? Or an index into list of sequences comprising a single training example? 

In experiments, the authors show that the proposed method achieves strong results on a variety of discriminative tasks using the proposed unsupervised feature learning method.

Questions:

- “Note that the mean of the posterior distribution of μ2 is not directly inferred from X, but instead is built into the set of inference model parameters with one for each sequence.”
Does this refer to training set sequences? If that is the case, does this not present a problem when applying the model to new data? In general the usage of the terms “sequence” and “frame” are somewhat confusing throughout the paper. 

- How do the frame-level features perform on speaker verification? If there is some simple heuristic that could be used to derive a sequence embedding from the frame features, that would be a very interesting control experiment. Likewise, is there a way to derive frame-level features from sequence-level components of the model? This would be a way to quantitatively assess the disentangling. (It looks like table 4 row 5 may already contain this second control experiment).

- Have you experimented with adversarial losses using sequence index prediction? For example one could encourage frame-level features to not be able to predict sequence indices, and vice-versa.

- Have you tried generating audio from this model, and if so how to the samples sound?
","The paper proposes to the Factorized Hierarchical VAE for unsupervised hierarchical representation learning in sequential data. The authors pursue computational efficiency by making the inference possible at frame (sub-sequence) level. To complement the induced problem of this decoupling, the authors also propose to augment the objective with the discriminative objective. Qualitative and quantitative evaluations are performed by visualizing the learned latent features and on speaker verification and word error rate with i-vector baseline.

The paper is well-written and easy to understand. I enjoyed reading the paper. It also tackles an important problem of learning the hierarchical representation of sequential data in an unsupervised way. It would have been nice to add another baseline which works without frame-level decoupling. Then, in addition to the accuracy, having some learning curves (for varying sequence lengths) in comparison of the baseline would be interesting because the main claim is to make the inference scalable with decoupling at the frame level. I found the comparison of different architectures in Table 1. is somewhat not much important.","Summary

This paper proposes a hierarchical probabilistic generative model for sequential data. The proposed model structure distinguishes between higher-level sequence and lower-level frame latent variables in order to capture multiple scales. A discriminative regularizer is added to the ELBO to encourage diversity across sequence latent variables with the aim of disentangling the latent representations. This work is related to approaches for learning disentangled representations in probabilistic generative models, such as DC-IGN (Kulkarni et al. 2015), InfoGAN (Chen et al. 2016), and Beta-VAE (Higgins et al. 2017). It is also related to hierarchical VAE-style models such as Structured VAEs (Johnson et al. 2016) and the Neural Statistician (Edwards & Storkey 2017) and latent variable models for sequential data including VRNN (Chung et al. 2015), SRNN (Fraccaro et al. 2016) and the hierarchical multiscale RNN of Chung et al. (2017).

Strengths

- Disentanglement in sequential data is not studied sufficiently in current literature
- Seq2Seq model within each frame is sensible
- Paper is overall well-written and clear

Weaknesses

- The proposed hierarchical model makes frame-level independence assumptions
- Discriminative objective is reminiscent of a supervised objective and thus it not clear that the resulting disentanglement of latent representations is in fact unsupervised
- Experiments are somewhat incomplete, both in comparing to baselines and in empirically validating disentanglement

Quality

Two design decisions regarding the graphical model did not make sense to me. First, if z2 is meant to capture sequence-level information, why is it placed inside the frame-level plate? Second, modeling frames independently for a given sequence discards the correlations between frames. Even if this facilitates learning at the frame level, it does not seem to be an entirely appropriate assumption for sequential data. I am wondering if the authors can clarify the motivation behind these choices.

In particular, the first decision seems to necessitate the addition of the discriminative regularizer (equation between ll. 110-111). However such an objective could be naturally incorporated into the model by adding an optional fully observed sequence-level side information variable y^{(i)} which is generated from z2.

For the variational lower bound results, I would suggest adding a comparison to baseline models such as VRNN (Chung et al. 2015) or SRNN (Fraccaro et al. 2016).

The disentanglement properties of the proposed FHVAE seem to be due to the discriminative objective, since the speaker verification error rate drops as alpha increases. However, since in TIMIT the sequences vary primarily on speaker identity (as mentioned in the paper), the discriminative objective is roughly equivalent to a supervised loss in which the goal is to predict speaker identity. In this case, the disentanglement is no longer unsupervised as originally claimed. Adding a speaker verification experiment on a dataset where the sequences vary less based on speaker identity would help clarify the role of the discriminative objective in disentangling the latent representation.

Clarity

The paper is generally well-written and structured clearly. The notation could be improved in a couple of places. In the inference model (equations between ll. 82-83), I would suggest adding a frame superscript to clarify that inference is occurring within each frame, e.g. q_{\phi}(z_2^{(n)} | x^{(n)}) and q_{\phi}(z_1^{(n)} | x^{(n)}, z_2^{(n)}). In addition, in Section 3 it was not immediately clear that a frame is defined to itself be a sub-sequence.

Originality

The paper is missing a related work section and also does not cite several related works, particularly regarding RNN variants with latent variables (Fraccaro et al. 2016; Chung et al. 2017), hierarchical probabilistic generative models (Johnson et al. 2016; Edwards & Storkey 2017) and disentanglement in generative models (Higgins et al. 2017). The proposed graphical model is similar to that of Edwards & Storkey (2017), though the frame-level Seq2Seq makes the proposed method sufficiently original. The study of disentanglement for sequential data is also fairly novel.

Significance

Unsupervised discovery of disentangled latent representations is an important problem. There has been some prior work in learning disentangled representations for image data but to my knowledge not for sequential data. However the proposed discriminative objective sufficiently resembles supervised identity prediction that it is not clear that the proposed method actually addresses the problem.

Chen, Xi, et al. ""Infogan: Interpretable representation learning by information maximizing generative adversarial nets."" NIPS 2016.
Chung, Junyoung, et al. ""A recurrent latent variable model for sequential data."" NIPS 2015.
Chung, Junyoung, Sungjin Ahn, and Yoshua Bengio. ""Hierarchical multiscale recurrent neural networks."" ICLR 2017.
Edwards, Harrison, and Amos Storkey. ""Towards a neural statistician."" ICLR 2017.
Fraccaro, Marco, et al. ""Sequential neural models with stochastic layers."" NIPS 2016.
Higgins, Irina, et al. ""beta-VAE: Learning basic visual concepts with a constrained variational framework."" ICLR 2017.
Johnson, Matthew, et al. ""Composing graphical models with neural networks for structured representations and fast inference."" NIPS 2016.
Kulkarni, Tejas D., et al. ""Deep convolutional inverse graphics network."" NIPS 2015.

-----

**Edit:** I have read the authors' rebuttal. The clarification was helpful regarding the importance of the discriminative regularizer in achieving disentanglement. I would recommend emphasizing in the paper (1) the presence of multiple utterances per speaker and (2) the speaker verification results are still good when alpha = 0. I am updating my rating of this paper from 4 to 6."
Recurrent Ladder Networks,"Isabeau Prémont-Schwarz, Alexander Ilin, Tele Hao, Antti Rasmus, Rinu Boney, Harri Valpola",https://proceedings.neurips.cc/paper/2017/hash/0a5c79b1eaf15445da252ada718857e9-Abstract.html,"This paper presents a natural extension to ladder networks, which it interprets as implementing one step of message passing, to the recurrent case implementing multiple steps of massage passing amongst latent variables. The experiments seem to support this being a good move. I am fairly convinced of the value of the architecture being presented here, but the paper presents a few weaknesses which should be addressed. I will focus on the main two in this review.

First, simply put: section 2 needs a formal description of the architecture being proposes, and it raises my eyebrows that the authors did not think this should be required in the body of the article. This can easily be rectified, and I am fairly confident that the description that will eventually be provided will match my educated guesses based on the diagrammatic representation of the model, but it’s poor form to not include it in the review draft.

Second, the experimental section is adequate and convincing, but in 2017 (or in fact in 2015 or 2014) I roll my eyes at experiments involving MNIST. The structure of the occluded image experiment is cool, and I would love to see discussion of results on a similar set up with, say, CIFAR image classification. I don’t think this can be done on short notice for the response, but it would be nice to see in a final version of the paper, as the work would be stronger for it.

Overall, not a bad paper, but could have been stronger. I think the idea is decent, so wouldn’t argue against it being accepted","In this paper, the authors introduce an extension of ladder networks for temporal data. Similarly to ladder network, the model proposed in the paper consists in an encoder-decoder network, with ""lateral"" connections between the corresponding layers of the encoder and the decoder. Contrary to the original model, recurrent ladder networks also have connection between different time steps (as well as skip connection from the encoder to the next time step). These recurrent connections allows to model temporal data or to perform iterative inference (where more than one step of message passing is required). The proposed model is then evaluated on different tasks. The first one is a classification task of partially occluded and moving MNIST digit, where the authors show that the proposed model performs slightly better than baselines corresponding to the proposed models with various ablation. The second task is generative modeling of various piano pieces. Finally, the last experiment is a classification task of MNIST digits with Brodatz textures, making the classification very challenging.

While the ideas presented in this paper are natural and well founded, I believe that the current version of the papers has some issues. First, the proposed model is very succinctly described (in section 2), assuming that the reader is familier with ladder networks. This is not the case for me, and I had to look at previous papers to understand what is the model. I believe that the paper should be self-contained, which I do not think is the case for the submitted version. I also believe that some statement in the paper are a bit unclear or misleading: for example, I do not think that Fig. 1c corresponds to message passing with latent variables varying in time (I think that it corresponds to iterative inference). Indeed, in the case of a temporal latent variable model, there should also be messages from t to t-1. I also found the experimental section a bit weak: most of the experiments are performed on toy datasets (with the exception of the piano dataset, where the model is not state of the art).

Overall, I am a bit ambivalent about the paper. While the ideas presented are interesting, I think that the proposed model is a bit incremental compared to previous work, and that the experiments are a bit weak.

Since I am not familiar at all with this research area, I put a low confidence score for my review.
","This paper presents a recurrent version of the Ladder network that was presented in NIPS 2015. 

The Ladder network, in brief, is a bottom-up/top-down hierarchical network with lateral connections between layers. The recurrent extension uses this network recurrently with connections between successive time slices.

I am somehow divided about the value of this paper. From an architectural point of view, the contribution is minimal. It only consists of a recurrent connection that could be added to basically any network. Another contribution is the use of auxiliary loss functions. However, on this point, I am not clear whether the various losses are optimized jointly or alternately. More details would have been useful.

However, the experimental results are interesting. A first experiment in Section 3.1 is with a Moving, Occluded version of the MNIST dataset prepared by the authors themselves. This experiment covers two reasonable baselines and various ablated versions. The best accuracy in Table 1 is achieved with all the proposed components. In addition, the next-frame predictions displayed in Fig. 4 illuminate about the internal behaviour. Also the semi-supervised experiment reported in Table 2 is interesting.

The second experiment is a prediction of piano notes from music from 19 different classical composers. In this experiment, the proposed network performs well and is surpassed only by models that predict the simultaneous notes jointly.

The experiment in section 4 shows that the proposed network not only can predict next-time frames, but next-time separate objects. This is done by using multiple such networks with tied parameters in parallel. Fig. 5 is another very interesting display of behaviour.

The last experiment (Section 4.1) shows the remarkable performance of the network on a very challenging task where the MNIST digits have been almost completely destroyed by heavy texturing.

Overall, in my opinion the experiments are very well-designed, diverse and probing and deserve attention for the proposed network."
Distral: Robust multitask reinforcement learning,"Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, Razvan Pascanu",https://proceedings.neurips.cc/paper/2017/hash/0abdc563a06105aee3c6136871c9f4d1-Abstract.html,"Summary
The described approach improves data efficiency for deep reinforcement learning through multitask learning by sharing a distilled policy between the individual task learners.

Comments
* Overall, most of the paper seems to be very nicely done, e.g.:
** Complete and concise description of related work and underlying reasoning/idea for the approach (section 1)
** Detailed mathematical derivation for both tabular based approaches as well as policy-gradient based models.
** Interesting connections are highlighted (e.g.reminiscence of DisTral to ADMM in section 2.3)
** Experiments seem reasonable.
* I miss direct comparison(s) to the somewhat related 'Policy destillation' paper [21]. it would have been nice to see some of the same tasks used as there. The same holds also true for [18].
 ","The paper presents an approach to performing transfer between multiple reinforcement learning tasks by regularizing the policies of different tasks towards a central policy, and also encouraging exploration in these policies.  The approach relies on KL-divergence regularization.  The idea is straightforward and well explained.  There are no theoretical results regarding the learning speed or quality of the policies obtained (though these are soft, so clearly there would be some performance loss compared to optimal).  The evaluation shows slightly better results that A3C baselines in both some simple mazes and deep net learning tasks.

While the paper is well written, and the results are generally positive, the performance improvements are modest.  It would have been nice also to quantify the performance in *runtime* instead of just in terms of number of samples used (since the networks architectures are different).  In the tabular case, it would really have been nice to show some more detailed experiments to understand the type of behaviour obtained.  Eg. what happens if goals are sampled uniformly (as in these experiments) vs all sampled in one or the other of the extreme rooms, and what happens as the number of sampled tasks increases. Also, another baseline which does not seem to be included but would be very useful is to simply regularize task policies towards literally a ""centroid"" of the suggested actions, without learning a distilled policy.  One could simply average the policies of the different tasks and regularize towards that quantity.  It is not clear if this would be competitive with the proposed approach, but conceptually it should lead to very similar results.  If the number of tasks increases, one would expect the distilled policy would in fact simply become uniform - is this the case?

Finally, It would really have been nice to see some attempt at theory in the tabular case. It seems to me that one ought to be able to bound the loss of this approach wrt optimal policies using a variational approach, since the optimization seems to suggest that the algorithm computes a variational approximation to what would be the optimal policy for a distribution of MDPs. Such a result would make the paper more interesting conceptually and give some confidence in its applicability beyond the domains used in the experiments.

Smaller suggestions:
- The title refers to ""robustness"" which in RL is a technical term regarding optimization wrt an unknown distribution of dynamics (using typically min-max algorithms). This is not the type of approach proposed here, so some clarification is needed.","The paper proposes an approach (DISTRAL) for the multitask learning setting. DISTRAL shares/transfers a 'distilled policy' between different tasks. The aim is that this distilled policy captures common behaviour across tasks. Experiments on several domains are performed to evaluate the approach.

The topic of this paper, multitask learning within RL, is very relevant and important. Furthermore, the paper is well-written, contains clever optimization strategies and extensive experiments.

The main drawback of the paper, in my opinion, is that the central learning objective (Equation 1) is not well motivated: it is not clear why this approach is a good one to follow for multitask learning. Personally, I don't see why and how it would scale to complex domains and/or domains where individual task policies are very different from each other. The experiments actually seem to confirm this: for the complex tasks of Section 4.2, the performance improvement of DISTRAL compared to the baselines (A3C/A3C 2col/A3C multitask) is minimal.

What would make the paper stronger is if a careful analytical and/or empirical evaluation was done on what type of multitask domains this method does well on and on what type of domains it does not well on.

Furthermore, while the underlying concept of policy distillation is interesting, it has already been published in an ICLR'16 publication (citation 21). 

That being said, I think the pros of this paper outweigh the cons and recommend acceptance."
Real-Time Bidding with Side Information,"arthur flajolet, Patrick Jaillet",https://proceedings.neurips.cc/paper/2017/hash/0bed45bd5774ffddc95ffe500024f628-Abstract.html,"The paper analyzes the problem of a bidder participating in multiple auctions with a budget B and a horizon T. The objective is to optimize the bids over time, which is modeled as a contextual bandit problem. The paper develops an algorithm with regret of order O(d\sqrt{T}) when there is no budget or when B scales with T.

The paper needs to discuss the current literature on learning in Auctions (with or without budgets). For example, see
Learning in Repeated Auctions with Budgets: Regret Minimization and Equilibrium. Balseiro and  Gur, 2017
and earlier references therein. For example, the bid shading parameter $\lambda$ that appears is in line with what has appeared in the earlier literature. The paper would also benefit from better comparing to other papers that deal with similar knapsack constraints: see, e.g.
Close the Gaps: A Learning-while-Doing Algorithm for Single-Product Revenue Management Problems  Zizhuo Wang Shiming Deng and Yinyu Ye. Operations Research, 2014

The novelty is the introduction of context in conjunction with budget. As a result, the paper would benefit from highlighting more why the conjunction of context and budget is challenging? Where are the exact novel technical ideas needed for this compared to existing ideas in the contextual bandit literature and in the ``learning with knapsack constraints'' literatures? Relatedly, the lack of lower bound that captures the dependence on d and T significantly weakens the submission. A sharp lower bound would allow to much better appreciate the quality of the algorithms proposed. ","The paper studies the problem of repeated bidding in online ad auctions where the goal is to maximize the accumulated reward over a fixed horizon and in the presence of budget constraint. It is also assumed that a context vector is available at each auctions which summarizes the specific information about the targeted person and the website. The expected value of an ad is considered to be a linear function of this context vector. This is a generalization of the setting considered in Weed et al (2016) in two directions: first having a budget constraint and second the availability of the side information. The paper proposes a bidding strategy in such a setting and provide a regret bound for it. 

While the paper technically sounds and is well-written, here are a few comments on the formulated problem.

1. It has been assumed in lines 51-53 that p_t is an iid process. This seems to be far from reality as p_t is the maximum bid of the competitors each of which might be a learning agent (just like us). In this case, competitors select their bids based on his observed history. This suggests that competitors bid (and hence p_t) is not a stationary process and more importantly is correlated with our bids. This is true even if each competitors values the ad differently and in a way specific to him. This adversarial correlation is actually one of the main differences between auctions and conventional bandit problems which has been ruled out in the paper by assuming that p_t is generated from a stationary distribution in an iid manner. 

2. Assumption 1 which plays an important role in the results of the paper mentions that p_t and v_t are conditionally independent given the side information x_t. Aside from the above reason (comment 1) and even if we assume that the competitors are not learning agents, this seems a very restrictive assumption. x_t is commonly shared between all the bidders and all of them select their bids based on it. Consider for example a scenario that a group of the bidders (including us) are interested in a specific type of (target person, website) pair and they will bid higher when  the associated x_t is reported. Thus, there is a correlation between those competitors bids (and hence p_t) and our bids given x_t. There is clearly a correlation between our bid and our value v_t (because we are a smart bidder) and hence, v_t and p_t cannot be independent given x_t. 

A few typos:
1. Assumption 1 in the Supplementary Materials is i fact Assumption 2 of the main paper.
2. There is a . missing at the end of line 313 and one at the end of Lemma 8. 
3. At the end of Section C in the Supplementary Materials it should be Lemma 3 of [2] not Lemma 11 of [2]. 
"
Learning Spherical Convolution for Fast Features from 360° Imagery,"Yu-Chuan Su, Kristen Grauman",https://proceedings.neurips.cc/paper/2017/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html,"This paper describes a method to transform networks learned on perspective images to take spherical images as input. This is an important problem as fisheye and 360-degree sensors become more and more ubiquitous but training data is relatively scarce. The method first transforms the network architecture to adapt the filter sizes and pooling operations to convolutions on a equirectangular representation/projection. Next the filters are learned to match the feature responses of the original network when considering the projections to the tangent plane of the respective feature response. The filters are pre-learned layer-by-layer and fine-tuned to output features as similar as possible to the original network projected to the tangent planes. Detection experiments on Pano2Vid and PASCAL demonstrate that the technique performs slightly below the optimal performance using per-pixel tangent projections (however significantly faster) while outperforming several baselines, including cube map projections.

The paper is well written and largely easy to read. While simple and sometimes quite ad hoc (Sec 3.2), I like the idea and am not aware of any closely related work (though I am not an expert in that domain). The technical novelty is quite limited though as it largely boils down to retraining a newly structured network on equirectangular images based on a pre-trained model. I think the experiments still warrant publication (though I would have believed that a pure computer vision venue would be more appropriate than NIPS due to the specific application), so overall I am slightly on the positive side.

Detailed comments (in order of the paper):

Introduction: The upper bound the method can achieved is defined by running a CNN on all possible tangent projections. This seems fair, but does not deal with very large and close objects which would be quite distorted on the tangent plane (and occur frequently in 360 degree videos / GoPro Videos / etc). I don't think this is a weakness but maybe a comment on this might be good.

Related work: I am missing a section discussing related work on CNNs operating on distorted inputs and invariance of CNNs against geometric transformations, eg., spatial transformer networks, warped convolutions, and other works which consider invariance.

l. 117: W -> W x W pixels

Eq. 2: I find "":="" odd here, maybe just ""="" or approximate? Also, it is a bit sloppy to have x/y on the lhs and \phi/\psi on the rhs without formalizing their connection in that context (it is only textual before). Maybe ok though.

After this equation there is some text describing the overall approach, but I couldn't fully grasp it at this point. I would add more information here, for example that the goal is to build a network on the equirectangular image and match the outputs after each convolution to the corresponding outputs of the CNN computed on the tangent plane.

Fig. 2b: It is unclear to me how such a distortion pattern (top image) can arise. Why are the boundaries so non-smooth (double curved left image boundary for instance). I don't see how this could arise.

Fig. 2 caption: It would be good to say ""each row of the equirectangular image (ie, \phi)"" and in (b) what is inversely projected.

l. 131: It is never clearly described how these true outputs are computed from exhaustive planar reprojections. Depending on the receptive field size, is a separate network run on each possible tangent plane? More details would be appreciated.

Sec. 3.2: I found the whole description quite ad-hoc and heuristic. I don't have a concrete comment for improvement here though except that it should be mentioned early on that this can only lead to an approximate result when minimizing the loss in the end.

l. 171: It is unclear here if the loss is over all feature maps (must be) and over all layers (probably not?). Also, how are the weights initialized? This seems never mentioned.

l. 182: It is unclear why and how max-pooling is replaced by dilated convolutions. One operation has parameters while the others has not.

l. 191: It is not Eq. 2 that is used but a loss that considers the difference between the lhs and the rhs of that equation I assume? Why not stating this explicitly or better reuse the equation from the beginning of Sec 3.3.?

l. 209: What happens to the regions with no content? Are they excluded or 0-padded?

l. 222: Why is detection performance evaluated as classification accuracy. Shouldn't this be average precision or alike?

Experiments: Some more qualitative results would be nice to shed some light. Maybe some space can be made by squeezing the text partially.

","
	Summary
	-------
	
	The paper proposes an approach for making existing CNNs that
	have been trained on standard, 'perspective' imagery,
	available for being applied to 360-degree, spherical imagery. The
	proposed approach is phrased as a regression problem that
	mimicks the convolutional response of a 'target network'
	(such as VGG [25] etc.)	on exhaustive tangent plane
	projections of a spherical image by responses of a 'spherical
	convolution network' that is applied to a single
	equirectangular projection.

	The result is a method that produces convolutions on spherical
	images that are both accurate (since they resemble the
	responses on all possible tangent planes) and fast to compute
	(actual	tangent planes are not constructed, but replaced by a
	single equi-rectangular projection).

	Apart from the basic idea of formulating the problem of
	applying existing networks to spherical imagery, the paper
	proposes a specific NN architecture and a number of technical
	modifications to traditional NNs that are claimed to be
	crucial for making the method work: separate convolution
	kernels are learned for different output rows, kernel shape is
	relaxed to being rectangular (not square), and pooling layer
	size is adapted to pixel size. In addition, a training
	procedure is suggested that pre-trains individual kernels
	independently before jointly fine-tuning the entire network.

	Experiments are conducted on existing datasets Pano2Vid [27]
	and PASCAL VOC, based on existing network Faster-RCNN
	[22]. Several variants of the proposed method (with and
	without kernel-wise pre-training) and baselines are
	compared both w.r.t. accuracy in convolution results (RMSE)
	and final object class detection performance. To that end,
	PASCAL VOC images are artificially transformed into spherical
	images. The proposed method is demonstrated to perform better
	than the baselines and almost on par with exhaustive tangent
	plane projections, at a lower computational cost.


	Novelty and significance
	------------------------

	To my knowledge, the proposed method is novel in that is the
	first to propose a learning-based framework for making
	existing, trained CNNs available for application to spherical
	images and also in the particular technical
	implementation. Given the demonstrated performance benefits
	over the equirectangular projection on one and the
	computational savings compared to the exhaustive computation
	of tangent planes on the other hand, it has the potential to
	advance or at least inspire the next standard method for
	processing spherical images via CNNs.


	Technical correctness
	---------------------
	
	The proposed method seems plausible and technically correct.



	Experimental evaluation
	-----------------------

	The given experimental evaluation is extensive in terms of
	examined aspects (baselines, method variations, tasks and
	applied measures) and convincingly demonstrates both superior
	performance of the proposed method and computational
	efficiency in comparison to existing approaches.

	Specifically, the experiments verfiy both accurate recreation
	of convolutional responses and benefit for an object detection
	task.


	Presentation
	------------
	
	The presentation is excellent; the paper follows a clear line
	of argumentation and is well written.

      ","The paper presents an approach to feature extraction on 360-degree images. The approach is to learn a projection operator into R^2, where standard ConvNets take over. The results show a mild accuracy gain due to the learned operator.

Overall this is a competently executed and well-written paper. The work strikes me as a bit incremental, but it will be of interest to some people and will not be out of place at NIPS."
Approximate Supermodularity Bounds for Experimental Design,"Luiz Chamon, Alejandro Ribeiro",https://proceedings.neurips.cc/paper/2017/hash/0d9095b0d6bbe98ea0c9c02b11b59ee3-Abstract.html,"I believe that the authors have missed an important reference:
[1] “Guarantees for Greedy Maximization of Non-submodular Functions with Applications”, by Andrew An Bian, Joachim M. Buhmann, Andreas Krause, and Sebastian Tschiatschek, ICML 2017.
Given the results in [1] (in particular, see section 4.1 therein), this paper became significantly less novel. Currently I can see a few improvements of this paper over [1]:
1) The result of the additive approximate supermodularity (Theorem 2), and its corresponding consequence in E-optimal designs (Theorem 4), seems to be new.
2) The results in this paper apply to general covariance matrix Rθ, whereas [1] considers only isotropic covariance.
3) The notions of approximate supermodularity (or rather of approximate submodularity in [1]) are slightly different, but the proof techniques are essentially the same. Overall, this paper still contains nontrivial extensions, but it is really not at the level of NIPS. I therefore suggest reject.

Some minor suggestions:
1) The related work is given at the end of the paper which confuses the reader. It’s better to put it at the end of the introduction.
2) Some corrections on the language usage is necessary, for example:
 - line 110: surrogate of
 - line 142: should be close to
 - line 166: it is worth
 - line 413 (at appendix): it suffices

","This paper defines two types of approximate supermodularity, namely alpha-supermodularity and epsilon-supermodularity, both of which are extensions of the existing supermodularity definition. Then the authors show that the objectives of finding the A- and E-optimal Bayesian experimental design respectively belong to those two approximate supermodularity. Therefore, the greedy selection algorithm can obtain a good solution to the design problem that is within (1-e^{-1}) of the optimal, which is guaranteed by the standard result of supermodularity minimization.

This paper is nicely written and everything is easy to understand. I really enjoy reading it. The result gives a theoretical justification of the effectiveness of the greedy algorithm in finding A- and E- optimal design, which is further confirmed by experiments. 

However, I also see some limitations of the proposed framework:

1. The analysis seems to apply only to Bayesian experimental design that the theta must have a prior. If not for the regularization term in the equation 4, it might be impossible to use any greedy algorithm. On the contrary, existing approach such as [10] does not have such limitation.

2. The relative error is different from the traditional definition, as the objective function f is normalized that f(D) <= 0 for all D. I suspect this is the key that makes the proof valid. Will this framework be applied to the unnormalized objective?

Although there are limitations, I still think the paper is above the acceptance threshold of NIPS.
","This work provides performance guarantees for the greedy solution of experimental design problems. Overall, this is a good paper and followings are detail comments.

1 In line 128-129, the condition should be for all A\subset B and for all u\in E\setminus B. 

2 In equation (5), it is better to say e \in \argmax than e = \argmax since \argmax may return more than 1 element.

3 For Theorem 1, have you seen this paper?

Bian, Andrew An, et al. ""Guarantees for Greedy Maximization of Non-submodular Functions with Applications."" arXiv preprint arXiv:1703.02100(2017).

They got similar results. You may want to cite their works.

4 In figure, why are A-optimality and E-optimality positive? In lemma 1 and theorem 3, it is claimed than the objective is decreasing and normalized. Did you reverse the axis?
"
Differentiable Learning of Logical Rules for Knowledge Base Reasoning,"Fan Yang, Zhilin Yang, William W. Cohen",https://proceedings.neurips.cc/paper/2017/hash/0e55666a4ad822e0e34299df3591d979-Abstract.html,"This paper develops a model for learning to answer queries in knowledge bases with
incomplete data about relations between entities. For example, the running example in the paper is answering
queries like HasOfficeInCountry(Uber, ?), when the relation is not directly present in the
knowledge base, but supporting relations like HasOfficeInCity(Uber, NYC) and CityInCountry(NYC, USA).
The aim in this work is to learn rules like HasOfficeInCountry(A, B) <= HasOfficeInCountry(A, C) && CityInCountry(C, B). Note that this is a bit different from learning embeddings for entities in a knowledge
base, because the rule to be learned is abstract, not depending on any specific entities.

The formulation in this paper is cast the problem as one of learning two components:
- a set of rules, represented as a sequence of relations (those that appear in the RHS of the rule)
- a real-valued confidence on the rule

The approach to learning follows ideas from Neural Turing Machines and differentiable program synthesis,
whereby the discrete problem is relaxed to a continuous problem by defining a model for executing the rules
where all rules are executed at each step and then averaged together with weights given by the confidences.
The state that is propagated from step to step is softmax distribution over entities
(though there are some additional details to handle the case where rules are of different lengths).
Given this interpretation, the paper then suggests that a recurrent neural network controller can
be used to generate the continuous parameters needed for the execution. This is analogous to the neural
network controller in Neural Turing Machines, Neural Random Access Machines, etc.
After the propagation step, the model is scored based upon how much probability ends up on the
ground truth answer to the query.

The key novelty is developing two representations of the confidence-weighted rules (one which is straightforward to optimize as a continuous optimization problem, and one as confidence weights on a set of discrete rules) along with an algorithm for converting from the continuous-optimization-friendly version to the rules version.
To my knowledge, this is a novel and interesting idea, which is a neat application of ideas from the
neural programs space.

Experimentally, the method performs very well relative to what looks to be a strong set of baselines.
The experiments are thorough and thoughtful with a nice mix of qualitative and quantitative results,
and the paper is clearly written.

One point that I think could use more clarification is what the interpretation is of what the
neural network is learning. It is not quite rules, because (if I understand correctly), the neural
controller is effectively deciding which part of the rules to apply next, during the course of execution.
This causes me a bit of confusion when looking at Table 3. Are these rules that were learned relative to
some query? Or is there a way of extracting query-independent rules from the learned model?
","This paper extends TensorLog to do rule learning. It transforms a weighted logic program into an approximate objective that is easy to optimize with backprop.

The paper is rather clear and well-written. Although there are so many approximations, transformations and tricks involved that it may be hard to reproduce and fully understand what goes on.

The selling point of the learner is that it is differentiable and interpretable. I'm not really convinced of either point.

Yes, rules are interpretable, and even the objective of (2) is still quite intuitive and elegant. But once you flip sum and product (for no reason except that we can) in (5), any claim of interpretability becomes void. The recurrent attention model then goes further down the rabbit hole.

Being differentiable is a trendy phrase, but in this context I wonder what it really means. When learning rules of length 2, one can just generate all possible rules and reduce the problem to parameter learning on this large rule set. And parameter learning on these (semi-)probabilistic models is always differentiable. For example, you can optimize the parameters of a set of MLN rules by differentiating the pseudolikelihoood. I don't find the argument of being differentiable particularly compelling or meaningful (unless I'm missing some important detail here).

Can you tie the variables between relations in a rule? Seems like that is not possible to express? Such limitations should be discussed clearly.

If the advantage compared to embeddings is that you can predict for unseen entities, why not set up a convincing experiment to prove that? Is this covered by any of the existing experiments?

The experiments look good at first sight, but also appear to be somewhat cherry-picked to work with the learner. Rule lengths are 2-3 (why?), the limitations and approximations of the learner do not seem to matter on these tasks. Overall I don't extract any insight from the experiments; it's just a big win for the proposed learner with no explanation as to why or when not.","This paper suggest a rule-learning approach knowledge-base reasoning. Unlike traditional approaches that define rule-learning as a combinatorial search problem this paper proposes a differentiable model for learning both the rule structure and its parameterization for the knowledge base completion task.  The learning problem is defined over an RNN.  The approach was evaluated over three KB tasks, achieving state-of-the-art results.

I found the paper difficult to follow, as in many cases the authors made claims that were either unexplained or out of context. Specifically, it would be helpful if the authors clarified in the related work section the differences and similarities to other neural program inductions frameworks (it's not clear how the example explains this difference).
The transition from Eq.5 to Eq. 6-8 should also be explained more carefully. Terminology such as memory and attention should be explained before it is used. The equivalence between NL and rules in Sec. 4.3 was also not clear.





"
When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent,"Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo A. Parrilo, Nuri Vanli",https://proceedings.neurips.cc/paper/2017/hash/0e7c7d6c41c76b9ee6445ae01cc0181d-Abstract.html,"This paper gives nice examples to show that cyclic coordinate descent can be asymptotically faster than randomized coordinate descent.
Though different to those existing non-asymptotic iteration complexity analysis, this paper is still of certain interest for nonlinear optimization. I therefore recommend this paper to be accepted.
My detailed comments are as follows.
1. The claim that people have the perception that CCD is always inferior is false. At least in the case that n is small, this can be not the case. The paper by Sun and Ye only claimed that it scales worse with respect to n. [6] also provided an example that random permutation CD (which usually behaves closer to RCD than to CCD) can be worse than CCD when n = 2.
2. In line 110: the result for the case l approaches infinity is called Gelfand's formula (as mentioned in [11]).
3. I would still hope to see the rate of RCD represented in E || x - x^* ||, as this value can be directly related to the expected objective value, which is the one that is really of interest. As noted in [20], the current analysis for RCD cannot be easily linked with the expected objective and therefore the result is weaker. It should not be too difficult to obtain such results by utilizing Nesterov's results in [14].
4. I am somehow confused by the notations: in definition 4.3, is the inequality element-wise or is it for the eigenvalues? In the notation section it is claimed that the norm is always the Euclidean norm, but there are also norms applied on the matrices, which clearly cannot be the Euclidean norm.
5. [17] and [18] are referring to the same paper, and the correct papers to refer should be ""Richtárik, Peter, and Martin Takáč. ""Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function."" Mathematical Programming 144.1-2 (2014): 1-38."" and ""Lu, Zhaosong, and Lin Xiao. ""On the complexity analysis of randomized block-coordinate descent methods."" Mathematical Programming 152.1-2 (2015): 615-642.""
6. In the outline section, description of sec. 4 was ignored.
7. In the notation section, the part about the superscripts is mentioned twice.
8. The D matrix was used in line 51 without a definition, and later it's mentioned that the diagonal parts of A is denoted by I while in (4) D is used. Though under the assumption that all diagonal entries are 1, they are equivalent, it is still better to unify the notations.

=== After reading the authors' feedback ===
I thank the authors for the feedback and have no further comments.","In this paper, the authors analyze cyclic and randomized coordinate descent and show that despite the common assumption in the literature, cyclic CD can actually be much faster than randomized CD. The authors show that this is true for quadratic objectives when A is of a certain type (e.g., symmetric positive definite with diagonal entries of 1, irreducible M-matrix, A is consistently ordered, 2-cyclic matrix). 

Comments:
- There are more than 2 variants of CD selection rules: greedy selection is a very valid selection strategy for structured ML problems (see ""Coordinate Descent Converges Faster with the Gauss-Southwell Rule than Random Selection, ICML 2015 by Nutini et. al.)

- The authors state that it is common perception that randomized CD always dominates cyclic CD. I don't agree with this statement. For example, if your matrix is diagonal, cyclic CD will clearly do better than randomized CD.

- The authors make several assumptions on the matrices considered in their analysis (e.g., M-matrix, 2-cyclic). It is not obvious to me that these matrices are common in machine learning applications when solving a quadratic problem. I think the authors need to do a better job at convincing the reader that these matrices are important in ML applications, e.g., precision matrix estimation https://arxiv.org/pdf/1404.6640.pdf ""Estimation of positive definite M-matrices and structure learning for attractive Gaussian Markov Random fields"" by Slawski and Hein, 2014.

- The authors should be citing ""Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems"" NIPS 2015 (Sun & Hong)

- I have several issues with the numerical results presented in this paper. The size of the problem n = 100 is small. As coordinate descent methods are primarily used in large-scale optimization, I am very curious why the authors selected such a small system to test. Also, it seems that because of the structure of the matrices considered, there is equal progress to be made regardless of the coordinate selected -- thus, it seems obvious that cyclic would work better than random, as random would suffer from re-selection of coordinates, while cyclic ensures updating every coordinate in each epoch. In other words, the randomness of random selection would be unhelpful (especially using a uniform distribution). Can the authors please justify these decisions.


Overall, I'm not convinced that the analysis in this paper is that informative. It seems that the assumptions on the matrix naturally lend to using cyclic selection as there is equal progress to be made by updating any coordinate. At least that is what the numerical results are showing. I think the authors need to further convince that this problem setting is important and realistic to ML application and that their numerical results emphasize again the *realistic* benefits of cyclic selection.

============
POST REBUTTAL
============
I have read the author rebuttal and I thank the authors for their details comments. My comment regarding ""equal progress to be made"" was with respect to the structure in the matrix A and the authors have address this concern, pointing my attention to the reference in [20]. I think with the inclusion of the additional references mentioned by the authors in the rebuttal that support the applicability of the considered types of matrices in ML applications, I can confidently recommend this paper for acceptance.","This paper proved that for two classes of matrices, cyclic coordinate descent (CCD) is asymptotically faster than randomized coordinate descent (RCD). In particular, they consider two classes of matrices: M-matrices and matrices with Young’s property A (called 2-cyclic in the paper), and prove that the asymptotic convergence rate of CCD is twice as fast as RCD. 

The comparison of CCD and RCD is an interesting problem. While the recent work [20] proved in the worst case CCD can be O(n^2) times slower, it remains an interesting question why in many practical problems CCD can be faster than or comparable to RCD. This paper gives one of the first positive results for CCD, for some special classes. These results shed light on the understanding of CCD and RCD. 

To improve the paper, I have a few suggestions:
  (i) In line 34, the authors said that “these rate estimates suggest that CCD can be slower (precisely O(n^2) times slower)“. This is not “precise” to my knowledge. The rate estimate of [1] for CCD actually can be O(n^3) times slower than RCD. This gap was shrunk to O(n^2) for quadratic problems in [Sun, Hong ’15 NIPS], but the O(n^3) gap for the general convex problem was still there (though partially reduced for some problems in that paper). In [20], the gap for general convex problem was mentioned as an open question.

  (ii) “The perception that RCD always dominates CCD” (line 37) was mentioned in [6], but probably not “in light of this result“ of [20].  In fact, [20] did give numerical examples that CCD is faster than RCD: for matrices with standard Gaussian entries, CCD is indeed faster than RCD in the simulation. [20] also mentioned that the worst-case analysis does not mean CCD is bad, and other analysis methods are needed to explain why CCD is usually faster than GD. 
    I think at least the numerical example of Gaussian case should be mentioned in the paper. 

  (iii) It is well known that when Young’s property A holds, CCD is twice as fast as Jacobi method. So for the 2-cyclic case, the Jacobi method is approximately as fast as RCD, right? Could the authors explain the connection between Corollary 4.16 and the classical result (or claim it is just a coincidence)?

Overall, I think this is a good paper that proved some interesting results. Only a few minor issues regarding the literature need to be explained before acceptance. "
Principles of Riemannian Geometry  in Neural Networks,"Michael Hauser, Asok Ray",https://proceedings.neurips.cc/paper/2017/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html,"This paper interprets the deep residual networks from an interesting perspective, i.e., based on the Riemannian geometry. First, the authors use dynamical systems to explain the performance of the residual network, compared with the ordinary static networks. Thus, given an initial condition/input data, the differentiable equation maps this point to its target. As the number of layer goes to infinity, the authors show that the metric tensor for residual networks converges and is smooth, and thus defines a Riemannian manifold. Experiments validate part of the proposed theory. 

There are two suggestions about the paper.
1. When illustrating (residual) deep neural network based on the Riemannian geometry, it is better to compare with the latent variable explanation. The author introduces these two perspectives in section 1, but, I think, there need more discussions and comparisons in the derivations of the theory and in the experiments. 
2. In addition to the illustration, is there any insights for the design of deep neural network based on the Riemannian geometry?
","This paper is an attempt at explaining the mechanism at work in deep learning methods. The authors propose to Interpret neural networks as methods for solving differential equations. 

So far, we could not deny the effectiveness of deep learning approaches but were eager to understand it. This paper seems to contribute to that line of research. Although my understanding of the technical arguments are limited, I believe that it could be of great interest to the NIPS community.

I found the following papers :
- Building Deep Networks on Grassmann Manifolds - Huang et al. 
- Generalized BackPropagation Etude De Cas : Orthogonality - Harandi et al.
- A Riemannian Network for SPD Matrix Learning -  Huang et al. 
-Geometric deep learning: going beyond Euclidean data - Bronstein et al.
which integrate notions of Riemannian geometry into a deep learning approach. 
What are the links between the submitted paper and those four approaches ?

Finally, could the authors makes some connections between their interpretation of deep learning and natural gradients ?

After reading my fellow reviewers' comments and the authors' rebuttal, I am still convinced that the paper is a good fit for NIPS. ","The paper develops a mathematical framework for working with neural network representations in the context of finite differences and differential geometry.

In this framework, data points going though layers have fixed coordinates but space is smoothly curved with each layer.

The paper presents a very interesting framework for working with neural network representations, especially in the case of residual networks.

Unfortunately, taking the limit as the number of layers goes to infinity does not make practical application very easy and somewhat limits the impact of this paper.

The paper is not always completely clear. Since the goal of the paper is to present a minority perspective, clarity should be paramount.

experiments are a bit disapoiting. Their goal is not always very explicit. What the experiments do exactly is not very clear (despite their obvious simplicity). The experiments which show how a simple neural networks disentangle data points feel well known and their relation to the current paper feels a bit tenuous.

Line 62: The . in tensors (…) ensure consistency in the order of the superscrips and subscripts.
=> Not very clear. I assume the indices with a dot at the end are to be on the left of indices with a dot at the beginning (so that the dots would sort of align).

line 75, Eq 1:

Taking the limit as L -> /infty seems to pose a problem for practical applications. Wouldn’t an equation of the form bellow make sense (interpolation between layers):

x^a(x + \delta l) = x^a(l) + f^a(x^b(l); l) \delta l
instead of
x^a(x + 1 ) = x^a(l) + f^a(x^b(l); l) \delta l

6.1 & Figure1: The Harmonic oscillator

Section 6.1 and the corresponding figures are unclear.
=> The given differential equation does not indicate what the parameters are and what the coordinates are. This is a bit confusing since \xi is a common notation for coordinates in differential geometry. We are told that the problem is two dimensional so I assume that x is the two dimensional variable.
=> Figure 1 is confusing. What does the finite differencing plot show ? what do the x and y axis represent ? What about metric transformations ? What about the scalar metric values ?
The fact that the particle stands still is understandable if in the context of a network but confusing w.r.t. the given differential equation for x.
=> a, b = 1,2 is not clear in this context. Does this mean that a = 1 and b = 2 ? can the state space representation be written in the given way only if a, b = 1,2 ?

Figure 2: Every plot has the legend “layer 11”. Typo ?

section 6.3 is a bit superfluous."
Continual Learning with Deep Generative Replay,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim",https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html,"Quality:
The paper is technically sound. Extensive comparisons between the proposed approach and other baseline approaches are being made.

Clarity:

The paper is well-organized. However, insufficient details are provided on the architectures of the discriminator, generator and the used optimizer. 

I would be surprised if the cited papers refer to the hippocampus as a short-term memory system since the latter typically refers to working memroy. I assume they all refer to hippocampus as a system for long-term memory consolidation. I advice to adapt accordingly.

A bit more attention should be paid to English writing style:
primate brain’s the most apparent distinction 
inputs are coincided with 
Systems(CLS) 
Recent evidences 
networks(GANs) 
A branch of works 
line 114:  real and outputs 1/2 everywhere => unclear; please reformulate
line 124:  real-like samples => realistic; real-life (?)
in both function 
past copy of a self. 
Sequence 
of scholar models were 
favorable than other two methods 
can recovers real input space 
Old Tasks Performance 
good performances 
with in same setting employed in section 4.3.  

Originality:
The proposed approach to solve catastrophic forgetting is new (to my knowledge) yet also straightforward, replacing experience replay with a replay mechanism based on a generative model. The authors do show convincingly that their approach performs much better than other approaches, approximating exact replay. 

A worry is that the generative model essentially stores the experiences which would explain why its performance is almost identical to that of experience replay. I think the authors need to provide more insight into the how performance scales with data size and show that the generative model becomes more and more memory efficient as the number of processed training samples increases. In other words: the generative model only makes sense if its memory footprint is much lower than that of just storing the examples. I am sure this would be the case but it is important and insightful to make this explicit.

Significance:
The results are important given the current interest in life-long learning. While the idea is expected in the light of current advances, the authors give a convincing demonstration of the usefulness of this approach. If the authors can improve the manuscript based on suggested modifications then I believe the paper would be a valuable contribution. ","This paper introduces a model for performing continual/transfer learning by using a generative network/solver pair (old scholar) to continually replay fake data based on previously learned information for the new scholar to learn. The generative network is trained using the GAN framework. The entire model is trained on a sequence of tasks, with the n'th scholar training on real data from the current task Tn as well as fake data generated by the previous scholar n-1. They tested on MNIST and SVHN, examining transfer from one dataset to the other as well as from fewer classes to more classes in MNIST.

This is a great idea, and overall the results are impressive. However the paper is a bit confusingly written and the model needs to be explained in more detail. For instance, what is the ratio of real to replayed samples that new scholars learn on? Does this hyperparameter need to be tuned for different tasks? The paper also needs some proofing, as many sentences had grammatical errors, such as the caption for Table 2: ""Our generative replay algorithm is favorable than other two methods in that it is equivalent to joint training on accumulated real data as long as the trained generator can recovers real input space.""

I like that both a ceiling control (Exact Replay) as well as a random control (Noise) were both provided, as this gave a great way to assess the performance of their model in absolute terms. However, a baseline comparison with another SOTA model for continual learning such as EWC or LwF would have been even better. This lack of a baseline is I think the biggest weakness of this submission.
I am appreciative of the fact that performance comparisons with other models are not always interpretable, given different constraints on capacity, flexibility, etc, that the authors outline in Table 2, but some attempt to quantify these differences would have been appreciated, rather than asking the reader to simply take these relative advantages/disadvantages on faith."
Nonlinear random matrix theory for deep learning,"Jeffrey Pennington, Pratik Worah",https://proceedings.neurips.cc/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html,"This looks like a good theoretical contribution and an interesting direction in the theory of deep learning to me.

In this paper, the authors compute the correlation properties (gram matrix) of the vector than went through some step of feedforward network with non linearities and random weights. Given the current interest in the theoretical description of neural nets, I think this is a paper that will be interesting for the NIPS audience and will be a welcome change between the hundreds of GAN posters. Some findings are particularly interesting.They applied their result to a memorization task and obtained an explicit characterizations of the training error. Also the fact that for some type of activation the eigenvalues of the data covariance matrix are constant  suggests interesting directions for the future.

Few comments and suggestions follows;:

- In section 1.3 : I think that the interest for Random Network is slightly older than the authors (maybe) know. In particular, these were standard setting in the 80s (more than 20 years before any work presented by the authors) and have generated a lot of attention, especially in the statistical physics community where the generalization properties were extensivly studied, (see, just to name a few:  -https://doi.org/10.1209/0295-5075/27/2/002 -https://doi.org/10.1088/0305-4470/21/1/030-10.1103/PhysRevLett.82.2975 )

- In connection with the random kitchen sinks: isn't the theory presented here a theory of what is (approximatly) going in in the Kernel space ? Given the connection between random projections and kernel methods there seems to be something interesting in this direction. Section  4.2 ""Asymptotic performance of random feature methods"" should propbably discuss this point. Perharps it is linked to the work of El Karoui, et al ?

- Can the author provided an example of an activation that leads to the 0 value for the parameter zeta? This is not clear from the definition (10) (or maybe I missed something obvious ?) is it just ANY even function?

I did not check all the computations in the supplementary materials (espcially the scary expansion in page 11!) but I am a bit confuse about the statement ""t is easy to check many other examples which all support that eqn. (S19) is true, but a proof is still lacking."" . Why is it called a lemma then? I am missing something?","the idea of addressing deep learning using random matrix theory seems to have a lot of potential but it appears to be in the beginning stages.  Seems the major contribution is to show that their main be some advantages in keep zeta = 0 which may give future guidance when selecting and analyzing activation functions.  Numerical results are provided demonstrating alignment with theory predicted.  I am not sure how applicable this paper will be for deep learning in practice other than the suggested future analysis for activation functions.  My background is not in random matrix theory, but the proofs appear correct as best I can tell.

Minor comments:

Line 73-80.  Use of definitions defined in Theorem 1 before theorem 1 is presented.  I would prefer a brief definition/intuition of these terms to be presented in this section.  Otherwise reader needs to jump around to fully appreciate this paragraph.

Line 102-103.  Just English preference:  ""There has been minimal research performed on random matrices with nonlinear dependencies with a predominant focus on kernel random matrices."" 

Line 122-123.  Equation 4.  I think it might add clarity to say that delta is the Dirac delta and make the definition depend on t:  rho_M(t) = (1/n) sum delta(t-lambda_j(M)).  Also, it might help to give dimensions of M in definition.

Line 135-137:  Equation (9) might add a reminder of how M depends on n0 -> infinity

Line 214-217: Can the author point to any references/paper's why equilibrating the singular values would speed up training.

I think overall it might be useful if a discussion of the practical applications for this analysis is added.


","Among all the papers trying to contribute to the theory of deep learning this seems to me to be a very important contribution. This paper solves one of the open problems in random matrix theory that allows to describe spectral density of matrices that went trough a non-linearity such as used in neural nets. It is a very nice piece of work in random matrix theory with some interesting speculations about consequences for training of deep neural nets. I am very positive about interest of this paper to the NIPS crowd. 

Some little suggestions for improvement: 

Section 1.3. on related work on networks with random weights seems to be largely biased. There is a number of older works in the statistical physics community studying random neural networks (starting with works o Derrida, Gardner, Sompolinsky, some of those attempt to study multilayer networks). There is also: ""Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?"" of more recent ""Multi-Layer Generalized Linear Estimation"" by Manoel et al. Surely the authors can do a better job in covering the literature related to random weights. This will be interesting for the general NIPS reader. 

I understand that this kind of theory is still far from saying something about generalization in neural networks. But since this is such a central question perhaps the authors could comment on this so that the reader does not overlook this point. 

I've read the author's feedback and took it into account in my score. "
Identification of Gaussian Process State Space Models,"Stefanos Eleftheriadis, Tom Nicholson, Marc Deisenroth, James Hensman",https://proceedings.neurips.cc/paper/2017/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html,"This paper is concerned with identification of Gaussian process state space models. The focus is on deriving a variational lower bound on the process combined with the reparameterization trick.

The paper is generally well explain and the key concepts are demonstrated in the experiments. This kind of models have been extensively studied in the past. Similar constructions with linear measurement noise can be found in the literature, whereas the variational methodology to get a bound augmented with an RNN recognition model have been proposed by Frigola et al. and Mattos et al. respectively. The paper also follows a recent trend on building on past approaches by augmenting variational methods with the reparameterization trick. The above facts reduce the novelty of the approach. However, there are two key points to make here. Firstly, the reparameterization trick is particularly advantageous in this setting, because dynamical systems (in contrast to standard regression) very often need non-smooth and non-standard kernels. Secondly, although the model for the dynamics is linear (eq. 10,11), the recognition network parameters take up on driving the non-linear effects (eq. 12-14). The authors do not comment on that aspect, but I think it is a nice idea which can facilitate the optimization.

The experiments disappointingly do not consider comparison with baselines and do not consider datasets from classical systems identification (e.g. Mattos et al. both for the method and the data). Other than this important omission, the results on the RL task are nice.

== Post-rebuttal edit ==
I have read the authors' reply, thanks. ","The paper proposes some advancements in the field of Gaussian process state space models. The main ideas are in the use of a recurrent neural network to parameterize the posterior over latent states.

The paper is well written overall, although the use of a recurrent neural network model is not very well explained, and it assumes that the reader figures out some of the details.

As a nonexpert in this particular domain and the proposed applications, I think that the idea is interesting, however I have some reservations about novelty. Most of the elements used to derive the inference scheme are based on standard variational inference, sparse GP approximations, and the reparameterization trick, and what is different is the parameterization of the posterior. Also, the proposed model follows from Frigola et al. (2014), so it feels as if this paper is a good realization of an incremental idea.

The results are interesting but perhaps a more extensive evaluation would have been useful to strengthen the paper. The experiments are missing comparisons with simpler approaches (e.g., what if g() and/or f() are linear?) or any other competitor - without these, I believe is difficult to draw any conclusions on the potential impact of this work.
","The authors derive a variational objective for inference and hyperparameter learning in a GPSSM. The authors apply a mean field variational approximation to the distribution over inducing points and a Gaussian approximation with Markov structure to the distribution over the sequence of latent states. The parameters of the latter depend on a bi-RNN. The variational bound is optimised using doubly stochastic gradient optimisation. 

The authors apply their algorithm to three simulated data examples, showing that particular applications may require the ability to flexibly choose kernel functions and that the algorithm recovers meaningful structure in the latent states.

Overall, the paper is well written and provides an interesting combination of sparse variational GP approximations and methods for armotised inference in state space models. However, it is difficult to judge how much the proposed algorithm adds compared to existing sparse variational GPSSM models, since the authors did not make any comparisons or motivate the algorithm with an application for which existing approaches fail. From the paper alone, it is not clear whether related approaches could perform similarly on the given examples or what extensions of the model make it possible to go beyond what previous approaches can do. Comparisons to related models and a more quantitative analysis of the given algorithm would be essential for this. (The authors include RMSE comparisons to a Kalman filter and autoregressive GP in the rebuttal) 

Even though the motivation of the paper focuses on system identification, it would be interesting to see if the given approach is successful in correctly learning the model parameters (i.e. linear mapping of observations) and hyperparameters. Currently all results refer to the latent states or transition function. This would be interesting in order to evaluate biases that may be introduced via the choice of approximations and could also help with highlighting the need for the approximate inference scheme presented here (e.g. if a naive Gaussian parameterisation without the bi-RNN introduces more biases in learning).

Lastly, the authors do not apply their algorithm to any real data examples, which again would be useful for motivating the proposed algorithm.

"
Estimation of the covariance structure of heavy-tailed distributions,"Xiaohan Wei, Stanislav Minsker",https://proceedings.neurips.cc/paper/2017/hash/10c272d06794d3e5785d5e7c5356e9ff-Abstract.html,"The paper considers covariance matrix estimation in high-dimensions under heavy-tailed and non-centered data. Since the data is non-centered, the mean has to be estimated (under heavy-tail assumptions) to certain required accuracy and plugged-in to estimate the covariance matrix. Furthermore, the estimation method requires knowledge of certain parameters of the true covariance matrices. In order to overcome that, the paper proposes an adaptive procedure based on Lepski's method to automatically select the required parameters. 

The results are interesting and enables practical covariance matrix estimation high-dimensions. The main drawback of the paper is that most of the techniques used are already proposed in the literature and well-known (at least to researchers working in this area). Hence essentially this paper combines such techniques (in a slightly non-trivial way) to obtain the required results. ","The paper provides a truncation estimator for the covariance matrix. The idea is simple yet useful, enjoying very good theoretical properties. I strongly welcome this contribution to the robust statistics literature.

I have only several very minor comments/suggestions.
(1) The authors should discuss relevance to covariance estimation under elliptical distributions, like ECA proposed by Han and Liu (JASA, 2017+) (which is also a robust estimator of the covariance when the eigenvalues are bounded; see, e.g., https://arxiv.org/pdf/1705.06427.pdf). See below for more details.  
(2) Can we derive estimation accuracy under other norms (such as L_infty norm and restricted spectral norm, defined in the above ECA paper)? These alternative norms are crucial in high dimensional statistics (e.g., graphical model estimation and sparse PCA). See the ECA paper for an example.
(3) Centeredness is not crucial in covariance matrix estimation as long as the data are iid: We could use the pairwise difference X_i-X_j, which is centered no matter whether X_i is centered or not. This alternative should also be more efficient than the proposed one since it avoids estimating the mean. See again the ECA paper for discussions. Relative comments have also been put in Chen, Gao, and Ren (AoS, 2017+), which the authors cited. 
","This paper studies robust estimation of covariance matrix using adaptive truncation, where existing results work for centered data.  The main contribution is to control the additional error incurred by the use of a plug-in mean estimate (Lemma 4.2), which requires a bit technical work.  The results a valid and make a fine contribution to this topic.

This paper provides results on the error bounds in terms of the intrinsic dimension, which is useful and interesting.  However, the claim in the abstract that the ""results are applicable in the case of high-dimensional observations"" is a bit far-fetching, as the results are useful only when the intrinsic dimension is low, which is not the case if the data is from a sparse high dimensional model, such as in the case of sparse PCA (intrinsic dimension is high, but only a few relevant variables).

Typo: the math display right above line 220, there is a missing supremum over $\mathbf{v}$ . "
Robust Optimization for Non-Convex Objectives,"Robert S. Chen, Brendan Lucier, Yaron Singer, Vasilis Syrgkanis",https://proceedings.neurips.cc/paper/2017/hash/10c66082c124f8afe3df4886f5e516e0-Abstract.html,"The paper discusses approximate robust optimization over a finite set of loss functions (of size m), which are possibly nonconvex.  It is shown that the proposed algorithm provides a distribution over a set of solutions (of size T) that achieves a theoretical bound of order sqrt(log(m)/T) on the distance to an alpha-approximate solution of the robust optimization problem.  The results apply for a single solution point rather than a distribution, when the loss functions and the solution space are convex.  Two application examples are presented and employed for the numerical testing.  However, I believe that there are a few points that needs clarification.

1. The contribution over the work in reference [17] should be clearly stated as the setup looks very similar.  As fas as I can see Theorem 1 in [17] applies to general loss functions, with an error bound on subproblem minimization. Thus it is important to stress the distinction.  

2. It is not clear to me whether Algorithm 1 and Definition 1 exactly correspond to the terms 'Bayesian optimization' and 'Bayesian oracle' used in relevant literature.  It would be good to have a short explanatory paragraph if so, and change the terminology otherwise.  ","This study showed that a minimax problem can be approximately solved via approximate Bayesian oracle (Theorem 1).  

Overall, the study has a solid contribution. I checked all the proofs and did not found a serious flaw. Even the technique is not very involved, it has wide applications.

[Concern]
Theorem 1 seems a standard argument that converts min-max and min-sum, which is sometimes used to prove the minimax theorem via no-regret learning. (I would like to hear a key-point of the proof that I missed)

The title is misleading. First, it does not tackle the non-convexity (the difficulty of non-convex problem is the hardness of constructing an approximation oracle; thus the existence of the oracle is too strong assumption for this title). Second, it is not limited for continuous problems (""non-convex"" is usually used for continuous problem, but the only (theoretically solid) application is a discrete problem). 
I guess the authors want to apply the method to train the neural network (as they did in the experiment) so the paper has this title. But I think it is not a good title.

[Minor]
line 155 and 156, ""\subseteq 2^V"" should be ""\subseteq V"" or ""\in 2^V"".","Summary:
This work provides a method to optimize the worst-case loss over a set of different class of non-convex objective functions. In the real world, observed signals may contain various source of noise, which requires the learning method to be robust enough to the noise. The author converted the robust improper optimization to a Bayesian optimization problem. The proposed learning procedure consists of two iterative steps: the learner tries to minimize the loss with an alpha-approximate Bayesian oracle; the adversary aims to find a distribution over losses that maximize the loss of the learner. The author provide detailed proofs for the convergence and the theoretical bounds of the algorithm. Specifically, a robust statistical learning and a robust submodular maximization cases were discussed. Supportive application experiments were performed to validate the method. The result shows that the proposed method has substantial improvements in the worst-case scores.

Quality:
The paper is well written with theoretical proofs and supportive experiments. The author compared the proposed method with many modifications to support their choice.

Clarity:
The paper is well organized. The experiment details are clearly written. 

Originality:
There are a lot of works in robust optimization in machine learning. This work focuses on more general setting of robust optimization where loss functions are non-convex. The application of alpha-approximate Bayesian oracle is a novel approach.

Significance:
This work provides a general approach for robust optimization of non-convex objective functions. The method is validated from both a neural network learning task and an influence maximization task and it substantially outperformed the baseline approaches. It leads the state-of-art in robust optimization for non-convex problems.
"
Exploring Generalization in Deep Learning,"Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nati Srebro",https://proceedings.neurips.cc/paper/2017/hash/10ce03a1ed01077e3e289f3e53c72813-Abstract.html,"This paper presents an initial empirical analysis of different metrics which can be used to predict the generalization error of a neural network. These metrics include sharpness, lipschitz continuity and various norms  on the weight matrices. Finally they suggest three different conditions to prevent pathological solutions by bounding the sharpness of solution.

While the analysis presented in the paper is inconclusive, these intermediate results are of interest.

Minor details : There are a few grammatical mistakes and typos in the paper. 
e.g. 
Ln 103 : we check weather -> we check whether
Ln 141 - This suggest -> This suggests","Update after rebuttal: quote the rebuttal ""Even this very simple variability in architecture, proves challenging to study using the complexity measures suggested. We certainly intend to study also other architectural differences.""

I hope the authors include the discussions (and hopefully some experiment results) about applying the proposed techniques to the comparison of models with different architectures in the final version if it get accepted. The current results are already useful steps towards understanding deep neural networks by their own, but having this results is really a great add on to this paper. 

-------------------------
This paper investigated primarily two (related) questions: what is the complexity measure that is implicitly enforced to allow good generalization performance of large deep neural networks? Given different global minimizers of the empirical risk, can we tell which one generalize better based on the complexity measure.

Although no definite answer is given in this paper, a few options are explored and some of them seem to be positive choice. More specifically, norm based complexity measure and (average) sharpness based complexity measures are empirically investigated. The sharpness based measure are further justified via PAC-Bayes bound on generalization. 

There is no discussion on how or why those complexity measures are enforced during training of neural networks, but the explorations are interesting by themselves.

Although a theorem is given to control the sharpness of a network under a number of assumptions (C1-C3), this is primarily an empirical paper. Therefore, the reviewer thinks it would be more useful to include more extensive experiments. More specifically, could the proposed measures be useful when comparing across different architectures? Could it be useful when compare to different global minimizers from different algorithms (variants of SGD, or SGD with small / large batches)? etc.","Summary:
Paper studies how one can measure how well a network will generalize. (This is linked to implicit regularization present in our current optimization methods.) In particular, the paper links the capacity of neural networks to four different path-norm measures of the weights of the network. 

They revisit/define margins, lipschitz continuity and (expected) sharpness as measures of network capacity/generalization.

An interesting experiment is done where the ability of these measures to predict how well a network can generalize is carried out: a network is trained on a subset of a training set, along with different fractions of corrupted labels for other datapoints (different fractions for different networks). The measures are then used to predict how well a network might generalize.

An interesting paper. (Some of the text has minor typos though.) 

But it seems that the conclusion from the middle pane of Figures 3, 4 that spectral norm and path l2 norm are the best measures for generalization, as they seem to correlate with test error in both cases, whereas the others don't. (The path l1 norm also does badly). 

For better comparison, it would have been good to see Figure 4 for CIFAR10 also.


Minor comments:
I assume the epsilon is added after the softmax (in (2))? Is there some rescaling then?
Why does the left of Figure 2 show that sharpness is unstable? This seems to be more of a feature of Figures 3, 4? 
 

"
Spherical convolutions and their application in molecular modelling,"Wouter Boomsma, Jes Frellsen",https://proceedings.neurips.cc/paper/2017/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html,"The authors formulate a spherical convolution operation for features on a sphere that can be efficiently implemented using tensor operations on a GPU. This operation is evaluated as part of deep learning models for molecular modeling. The authors demonstrate that CNNs with spherical convolutions outperform fully connected models on two tasks in that domain and that three flavors of the spherical convolution operation results in similar performance. However, a more interesting baseline than a fully connected model would be a CNN with standard convolution operations. In particular, what would happen if the molecules were parameterized in the euclidean space and modeled with conventional CNNs? It is not clear if spherical convolutions would outperform standard convolutions on these tasks. There are several existing models that successfully utilize conventional 3D convolutions for molecular modeling (AtomNet, for example). While this proof of concept is intriguing, without demonstrating the added value of spherical convolutions as compared to regular convolutions, it is not clear that this extension could be useful in practice.","Paper summary:
The authors propose to separate the surface of a sphere using the equiangular cubed sphere representation of Ronchi et al.  (1996) in order to do a spherical convolution. This convolution has the advantage of being almost invariant to rotation of the volume and should have application in earth science, astronomy and in molecular modeling. The authors demonstrate the usefulness of their approach on practical tasks of this later field.
 
Strengths and weaknesses:
The paper is clearly written and easy to read. Also, I appreciate the amount of work that has been put in this paper as molecular modeling is a field with a steep learning curve, where the data is hard to pre-process, and where clear prediction task is sometime hard to define.
 
Although I appreciate the importance of spherical convolution on spherical data and the comparison with a purely dense model, there are other comparisons I believe would be of interest. Namely, I would like to know if empirical comparison with 3D convolution (ex: https://keras.io/layers/convolutional/#conv3d) in a cube is possible. For example, in the case of molecular modeling, the cube could be centered on the atom of interest. Also, it would also be of interest to compare the spherical convolution using different coordinate system: the equiangular cubed sphere (as proposed, Figure 1b) and the standard equiangular spacing (Figure 1a).
 
Finally, it would be nice if the conclusion contained ways to improve the proposed approach.
 
Quality:
I found the paper well written and easy to understand. I found few minor typos that are highlighted at the end of this review.
 
Clarity:
The figures are well done and help to understand the concepts that are important to the comprehension of the paper. I liked that the contribution of the authors is well-defined and that the authors didn’t try to oversell their ideas.
 
The authors should define the Q20 prediction score at line 229.
 
On line 245, the author state ""Although there are a number of issues with our approach …"". I appreciate that the authors were honest about it but I would like them to elaborate on the said issues.
 
Originality:
I found the paper interesting and refreshing. I liked that the authors have identified a niche problem and that they addressed it simply.
 
Significance:
The authors were honest about their work being preliminary and still at an early stage, to which I agree. However, I believe that their work could have a significant impact in domains where data are in a spherical format or when 3D data is acquired from a fixed point sensor.
 
I would like to emphasize that the impact of this paper will be greatly reduced if the author do not make the source code for the spherical convolution available online. In addition, having the source code to reproduce the results presented in the paper is good practice. For these reasons I strongly encourage the authors to make their source code available if the paper is accepted.
 
Errors / typos:
Line 68: ""... based on the on the cubed …""
Line 101: ""... we can make the make the …""
Line 117: The sentence seems unfinished or has a missing piece."
Safe Adaptive Importance Sampling,"Sebastian U. Stich, Anant Raj, Martin Jaggi",https://proceedings.neurips.cc/paper/2017/hash/1177967c7957072da3dc1db4ceb30e7a-Abstract.html,"The authors present a ""safe"" adaptive importance sampling strategy for coordinate descent and stochastic gradient methods. Based on lower and upper bounds on the gradient values, an efficient approximation of gradient based sampling is proposed. The method is proven to be the best strategy with respect to the bounds, always better than uniform or fixed importance sampling and can be computed efficiently for negligible extra cost. Although adaptive importance sampling strategies have been previously proposed, the authors present a novel formulation of selecting the optimal sampling distribution as a convex optimization problem and present an efficient algorithm to solve it.

This paper is well written and a nice contribution to the study of importance sampling techniques.

Comments:
Proof of Lemma 2.1 -- seems to be missing a factor of 2 in alpha^*.

Example 3.1 - In (7) you want to maximize, so in example 3.1, it would appear that setting c to the upper or lower bound is better than using uniform sampling. Is that what this example is intending to show? It is confusing with the statement directly prior claiming the naive approach of setting c to the upper or lower bound can be suboptimal.

Line 4 of Algorithm 4 - given that m = max(l^{sort}), will this condition ever be satisfied?
Line 7 of Algorithm 4 - should be u^{sort} instead of c^{sort}?

I think the numerical results could benefit from comparisons to other adaptive sampling schemes out there (e.g., [2], [5],[21]), and against fixed importance sampling with say a non-uniform distribution. 

Why are there no timing results for SGD?

Title of the paper in reference [14] is incorrect. 
Add reference: Csiba and Richtarik. ""Importance Sampling for Minibatches"" 2016 on arXiv. 

============
POST REBUTTAL
============
I have read the author rebuttal and I thank the authors for addressing my questions/comments. I think this paper is a clear accept.","I think this paper is interesting and provide some good insight for RCD and SGD algorithms. Conclusion 2 in Theorem 3.2 is very useful because it suggests that the new sampling method is better than L based sampling which is a commonly used strategy. I have the following comments which I want the authors to consider.

1. In Line 85, in the denominator of p^*_i, it should be x_k not x. 

2. In Line 135, you may want to remove the subscript k for \hat p and \hat c just to be consistent.

3. I cannot see any tick, label, number and word in all figures, which are too small. Please move some of them to appendix to gain space so that you can show larger figures.

4. If possible, please provide a result similar to Conclusion 2 in Theorem 3.2 but comparing with uniform sampling (p=1/n) instead of L_i based method

5. Please remark on how will this new sampling method affect the total complexity of RCD or SGD. For example, what will be the total complexity (including the O(n) time for updating l_k and u_k) for RCD to find an epislon-optimal solution?","This paper focuses on safe adaptive importance sampling, which is a popular topic in the community at the moment. Some of the key features of this work are that the proposed method is generic and can be integrated with many existing algorithms, and there are both theoretical results and practical experiments to support the proposed strategy. 
One issue with the paper that should be addressed is the figures in Section 5, which are very difficult to see properly. These figures should be made larger, and the labels also made larger, so that the reader can see them properly. "
Introspective Classification with Convolutional Nets,"Long Jin, Justin Lazarow, Zhuowen Tu",https://proceedings.neurips.cc/paper/2017/hash/11b921ef080f7736089c757404650e40-Abstract.html,"The paper proposes a technique to improve the test accuracy of a discriminative model, by synthesizing additional negative input examples during the training process of the model. The negative example generation process has a Bayesian motivation, and is realized by ""optimizing"" for images (starting from random Gaussian noise) to maximize the probability of a given class label, a la DeepDream or Neural Artistic Style. These generated examples are added to the training set, and training is halted based on performance on a validation set. Experiments demonstrate that this procedure yields (very modest) improvements in test accuracy, and additionally provides some robustness against adversarial examples.

The core idea is quite elegant, with an intuitive picture of using the ""hard"" negatives generated by the network to tighten the decision boundaries around the positive examples. The authors provide some theoretical justification showing that the distribution of these negatives examples converges to the distribution of positive examples.

While I find the approach intriguing, there are a few shortcomings. The improvement in classification accuracy demonstrated in the experiments is relatively small for the datasets shown, although the MNIST demonstration with varying sample sizes is very encouraging. A more complex dataset (e.g. ImageNet) would be very interesting to see, but potentially quite expensive to train... I'm unable to determine how many pseudo-negative samples are generated in each round (it says ""e.g. 200"" at one point; is it 200?), which affects the practicality of the approach (how much slower is training?).

While the Bayesian motivation is nice, it's not clear to me exactly what it means to sample from the distribution of negative examples using SGD. I'm not actively familiar with the references listed tying SGD to MCMC; if there is in fact a meaningful interpretation of the proposed sampling procedure that justifies the vague (but repeated) use of the term ""fair sample"", it would be nice to include a more explicit summary in the text.

Finally, it would have been nice to see comparisons against DCGAN for CIFAR10 and SVHN as well, as well as a simple baseline of label smoothing (instead of using one-hot target labels, use a mixture of e.g. .9 * one-hot + .1 * uniform). Label smoothing has been reported (e.g., in https://arxiv.org/pdf/1512.00567.pdf) to yield comparably sized improvements, essentially by encouraging the model to be less confident. One explanation for the success of the ICN approach could simply be that by training on negative samples that are close to the positive samples, the model is forced to be less confident overall, reducing overfitting. Comparing against label smoothing could help tease out whether that explanation is sufficient (and if so, label smoothing is substantially simpler and cheaper, if a bit of a hack).

Despite the above criticisms, I think the paper is well written and could be a valuable contribution; I recommend acceptance.
      ","Authors present the so called introspective convolutional network (ICN), a deep learning model that is able, using a single model, to generate negative example in order to improve the network training and in turn its classification performances. The proposed model is compared with a state-of-the-art discriminative convolutional neural network (CNN) for classification, namely ResNet, and with other two networks sharing a similar approach with the ICN. Classification scores over well known benchmark dataset confirms a performance gain.

The paper is well written and the proposed method is interesting in my opinion. 

I have one question: did you consider to compare your ICN against a deep belief network (DBN)? a DBN is in fact a probabilistic generative model also used for classification that, during the pre-trainining phase uses a generative model of the input data by means of a stack of Restricted Boltzmann machine and then, in the fine tuning, it is trained such a multilayer perceptron using gradient descent and backpropagation. I think that a comparison with that network could further improve the quality of the paper.

Minor remarks:
Which programming languages or libraries did you use for your network? Please clarify and let the source code be avilable, for example on github
What are the computation time? please add a table
Figure 1 is too small, please increase its size","The key idea of ICN is to: 1) train a base classifier and use it to generate pseudo-negative samples; 2) add the generated samples to the original training dataset and fit a new classifier on it. 

The main drawback of this approach is that it is very easy for the classifier to overfit on the positive samples, as indicated in Figure 1. Although the authors give convergence analysis (line 212), the assumption (that the classifier trained on new samples always improves over the old classifier) is too strong. As a result, the reviewer is not convinced by the proposed algorithm.

The experimental results are not convincing as well. The baseline results reported in the experiments are far from the state of the art (SOTA) results, e.g., 0.77 vs 0.21 (SOTA) on the MNIST dataset. The proposed algorithm is a meta-algorithm, which does not depend on the underlying architecture of the base classifier. Therefore, it is more convincing if the authors use the SOTA (or close to SOTA) neural classifier architecture as the baseline, and then see if using the idea of ICN could improve on it.

A key assumption in the formulation (line 106) is that the priors are equal: p(y=1)=p(y=-1). This can be problematic; as most real datasets have unbalanced class labels. 

The multi-class formulation (line 244) pushes the classifier away from the generated pseudo-negative samples. However, the pseudo-negative samples are generated for each class. A natural question is what if the pseudo-negative sample of class-1 is the pseudo-positive sample of class-2? In that case, pushing the classifier away from this sample could degrade the classifier accuracy for class-2.
"
Hybrid Reward Architecture for Reinforcement Learning,"Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, Jeffrey Tsang",https://proceedings.neurips.cc/paper/2017/hash/1264a061d82a2edae1574b07249800d6-Abstract.html,"R5:

Summary:
This paper builds on the basic idea of the Horde architecture: learning many value functions in parallel with off-policy reinforcement learning. This paper shows that learning many value functions in parallel improves the performance on a single main task. The novelty here lies in a particular strategy for generating many different reward functions and how to combine them to generate behavior. The results show large improvements in performance in an illustrative grid world and Miss Pac-man. 


Decision:
This paper is difficult to access. In terms of positives: (1) the paper lends another piece of evidence to support the hypothesis that learning many things is key to learning in complex domains. (2) The paper is well written, highly polished with excellent figures, graphs and results. (3) The key ideas are clearly illustrated in a small domain and then scaled up to Atari. (4) The proposed approach achieves substantial improvement over several variants of one of the latest high-performance deep learning systems.

My main issue with the paper is that it is not well paced between two related ideas from the literature, namely the UNREAL architecture [1], and Diuk’s [2] object oriented approach successfully used many years ago in pitfall—a domain most Deep RL systems perform very poorly on. It’s unclear to me if these two systems should be used as baselines for evaluating hydra. However, it is clear that a discussion should be added of where the Hydra architecture fits between these two philosophical extremes. 

I recommend accept, because the positives outweigh my concerns, and I am interested to here the author’s thoughts.   


The two main ideas to discuss are auxiliary tasks and object-oriented RL. Auxiliary tasks are also built on the Horde idea, where the demons might be predicting or controlling pixels and features. Although pixel control works best, it is perhaps too specific to visual domains, but the feature control idea is quiet general. The nice thing about the UNREAL architecture [1] is that it was perhaps one of the first to show that learning many value functions in parallel can improve performance on some main task. The additional tasks somehow regularize the internal state resulting in a better overall representation. The behavior policy did not require weighting over the different value functions it simply shared the representation. This is a major point of departure from Hydra; what is the best design choice here? Much discussion is needed.

Another natural question is how well does pixel control work in Miss Pac-man, this was not reported in [1].

I see hydra as somewhat less general than auxiliary tasks idea. In Hydra a person must designate in the image what pixel groupings correspond to interesting value functions to learn. One could view Hydra as the outcome of UREAL pixel control combined with some automatic way to determine which pixel control GVFs are most useful for sculpting the representation for good main task behavior. In this way Hydra represents a benchmark for automatic GVF construction and curation.

On the other extreme with have Diuk’s [2] object oriented approach in pitfall. Assuming some way to extract the objects in an Pitfall screen, the objects were converted into a first-order logic representation of the game. Using this approach, optimal behavior could be achieved in one episode. This is interesting because SOTA deep-learning systems don't even report results on this domain. Hydra is somewhat like this but is perhaps more general.

Carefully and thoughtfully relating the Hydra approach to these extremes is critical for accurately discussing the precise contribution of hydra and this paper under review here, and I invite the authors to do that.  
    
The final result of the paper (honestly admitted by the authors) exploits the determinist of Atari domain to achieve super-human play in miss packman. This result shows that A3C and others do not exploit this determinism. I am not sure how generally applicable this result is. 

Another method not discussed, Model Free Episodic Control [3], performs well in Miss Pac-man—much better than A3C. Episodic control defines a kernel function and memory built from agent states generated by a deep network. This allows the system to quickly the query the values of related previously seen states, resulting in much faster initial learning in many domains.  I would be interested to see the performance of episodic control in miss pac-man with both representations.

It is interesting that DQN does not seem to exploit the additional domain knowledge in the fruit experiment. I am trying to figure out the main point here. Is the suggestion that we will usually have access to such information, and hydra will often exploit it more effectively? Or that prior information will usually be of this particular form that hydra uses so effectively?  Could unreal’s aux tasks make use of manually ignoring irrelevant features? If that is not feasible in UNREAL, this would be yet another major difference between the architectures! This raises an interesting question: how often can this feature irrelevance be automatically observed and exploited without causing major problems in other domains. It seems like UNREAL’s independent GVFs could eventually learn to ignore parts of it’s shared representation. 

Low-dim representations: this is a meme that runs throughout the paper, and a claim I am not sure I agree with. Several recent papers have suggested that the capacity of deep learning systems is massive and that they can effetely memorize large data-sets. Do you have evidence to support this seemingly opposite intuition, outside of the fact that Hydra works well? This seems like a post-hoc explanation for why hydra works well. This should be discussed more in the paper

[1] Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., & Kavukcuoglu, K. (2016). Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397.

[2] Diuk, C., Cohen, A., & Littman, M. L. (2008, July). An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning (pp. 240-247). ACM.

[3] Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman, A., Leibo, J. Z., ... & Hassabis, D. (2016). Model-free episodic control. arXiv preprint arXiv:1606.04460.
   
Small things that did not effect the final decision:
line 23: performed > > achieved 
26: DQN carries out a strong generalisation > > DQN utilizes global generalization—like any neural net—
28: model of the optimal value function. I don’t think you mean that
230: how important are the details of these choices? Could it be -500 or -2000? 

","This paper proposes splitting up the Q-value prediction output of a DQN net into multiple different GVFs.  Each GVF is associated to a particular aspect of the task at hand (this is defined by the user) and these GVFs are then combined in some manner to predict the expected return (aka actual Q-value).    Performance is then demonstrated on two tasks to be state of the art.

Opinion:
To begin with, I am not entirely sure how this differs from the ""REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS"" paper.  The network architecture is not identical, but the fact that you are seeding your meta-agent with a bunch of local prediction tasks to speed up and robustify the overall Q-value prediction seems to be the same.  
That aside, here are some comments specific to your paper:

1. I am confused as to how you are aggregating your individual Q-values into the final Q-value.  From the definition of Q_hydra (Eq 7.5 (since there is no actual equation label)), it is a linear combination of the individual GVFs for the sub-policies, so I would assume you would aggregate according to the weights w_i.  However, on line 121 you mention a max or mean-based aggregation.  Indeed, if you use an arbitrary aggregation technique you are unlikely to find Q^*_env, but I am quite confused as to why you would use any other aggregation other than learned linear combinations of your GFVs.  

2. It's unfortunate that the tasks need to be specified by the user, and can't be somehow automatically determined.  This requires significant task-specific engineering and is not really in the spirit of making general control algorithms that can be easily applied to arbitrary problems.  I am also generally confused by your choice of high-level domain knowledge elements that you could leverage to improve performance.  For point 1, is the pruning to be done in an automated way? (L1 regularization on the weights of the final value function comes to mind, but doesn't seem to be the chosen approach).  For points 2 I am not sure how explicitly predicting the presence of a terminal state would 'free up' weights to better predict values elsewhere.  For point 3 it seems that pseudo-rewards is a superset of your reward-function decompositions, but to be honest, until I started getting confused by this point, I assumed you were using arbitrary features of the environment, and not necessarily limitting yourself to a-priori known components of the reward function.

3. Your state of the art is missing many elements, notably the unsupervised auxiliary task paper, but also the ""Universal Value Function Approximator"" paper by Schaul (and I'm sure quite a bit more, as both those papers have significantly larger related work sections).  I'm not judging by the count of papers, but the reduced section makes it difficult to properly compare and contrast your approach to existing and very similar approaches.

4. I am not entirely satisfied with the experimental results.  I have a series of concerns with the Ms. Pacman results:
  4.1 -- The choice of aggregator types seems somewhat arbitrary, is there any particular theoretical or intuitive motivation for the two aggregation methods?  
  4.2 -- You introduce very briefly your executive-memory node.  Would this node function in a non-deterministic environment?  Did you try running without this node?  It seems to me almost all your performance benefits may be coming from this one aspect, which is very specific to a deterministic environment.  

Overall, the experimental section seems to rely on a series of very task-specific design decisions.  It does not leave me enough clarity if the high scores on Ms. Pacman have anything to do with your particular approach to reward decomposition, or if the addition of these 3-4 task-specific design choices are responible.

My final opinion is that this paper introduces a series of interesting ideas, but in a hard to follow and somewhat unstructured manner.  I am not satisfied with how you motivate the reward decomposition as well as the manner in which the rewards are aggregated. I am also not satisfied that the performance on Ms. Pacman has anything to do with your reward decomposition and not with your executive-memory trick.  I understand that objectively comparing an algorithm which is designed to exploit domain knowledge with a series of more general algorithms is tricky, but if you want to exploit domain knowledge please use more real-world tasks that are hard to design algorithms for in general.  

Overall, although I appreciate the effort that went into this paper, I don't believe it is clear, concise and rigorous enough for inclusion into NIPS proceedings.


Details:
Line 97: Q-value functions are not typicall approximated with a deep net, this is still a relatively new occurence.  The reason you are likely using a deep convolutional net in this paper is because you are using 2d images as input and these are currently best processed with neural nets.

Line 172 I believe you meant HYDRA+1 and so on (and not HYDRO).

Line 205: ""with without""","This paper presents a novel way of decomposing the reward function into multiple smoother reward functions, thus making it easier to learn the task. In addition, decomposing the reward function this way enables things like doing state abstraction per new reward function, and specifying other domain info per reward signal. The authors demonstrate this learning speedup on a toy domain as well as the Atari game Ms PacMan. And with some additional engineering, they achieve maximum possible scores on Ms Pac-Man (super-human scores).

De-composing the reward function into multiple smoother rewards makes a ton of sense, and fits the existing Horde architectures very nicely. It is also really nice that it enables extra domain knowledge to be easily integrated in. 

One of the main benefits of this approach is the ability to easily add domain knowledge. This is very much emphasized in the Atari results and conclusions, but it is never mentioned in the intro or abstract - it should be included there as well. 

It's very clear which domain knowledge is being injected into HYDRA in the toy domain (i.e. HYDRA-1, HYDRA-2, etc), but it's not clear for the Atari case. Which pieces of domain knowledge is it using? All of them?

The extra head that repeats sequences of actions that beat a particular level is very hand engineered for Ms Pacman and the particular deterministic eval metric being used. It results in tremendous scores on Ms Pacman, but it feels like a hack added on just for those scores. I don't think its related to the contribution of the paper in any way (I imagine you could add this memory replay head to any agent and eventually defeat Ms Pacman this way). 

Some technical questions:
In equation 6, would it not make sense to weight the inidividual reward Q losses by the weights being used in the Q_hydra aggregation? In equation 7, instead of doing the max over that particular reward's Q-function, you could use Q_hydra for the argmax and then that individual rewards Q-function for the target values. Then all of the Q-functions would be consistently updating with the same max action and I think Q*_hydra should then equal Q*_env. 


Small edits:
Line 172 says HYDRO instead of HYDRA
Line 222: starts of -> starts off"
When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness,"Chris Russell, Matt J. Kusner, Joshua Loftus, Ricardo Silva",https://proceedings.neurips.cc/paper/2017/hash/1271a7029c9df08643b631b02cf9e116-Abstract.html,"This paper tackles the primary criticism aimed at applications of causal graphical models for fairness: one needs to completely believe an assumed causal model for the results to be valid. Instead, it presents a definition of fairness where we can assume many plausible causal models and requires fairness violations to be bounded below a threshold for all such plausible models.
The authors present a simple way to formally express this idea: by defining an approximate notion of counterfactual fairness and using the amount of fairness violation as a regularizer for a supervised learner. This is an important theoretical advance and I think can lead to promising work.

The key part, then, is to develop a method to construct counterfactual estimates. This is a hard problem because even for a single causal model, there might be unknown and unobserved confounders that affect relationships between observed variables. The authors use a method from past work where they first estimate the distribution for the unobserved confounders and then construct counterfactuals assuming perfect knowledge of the confounders. 
I find this method problematic because confounders can be a combination of many variables and  can take many levels. It is unclear whether an arbitrary parameterization for them can account for all of their effects in a causal model. Further, if it was possible to model confounders and estimate counterfactuals from observed data in this way, then we could use it for every causal inference application (which is unlikely). It seems, therefore, that the estimated counterfactuals will depend heavily on the exact parameterization used for the confounders.   I suggest that the authors discuss this limitations of their specific approach . It might also be useful to separate out counterfactual estimation as simply a pluggable component of their main contribution, which is to propose a learning algorithm robust to multiple causal models. 

That said, this is exactly where the concept of comparing multiple causal models can shine. To decrease dependence on specific parameterizations, one could imagine optimizing over many possible parameterized causal models. In the results section, the authors do test their method on 2 or 3 different worlds, but I think it will be useful if they can extend their analysis to many more causal worlds for each application. Not sure if there are constraints in doing so (computational or otherwise), but if so, will be good to mention them explicitly. ","The authors consider a novel supervised learning problem with fairness constraints, where the goal is to find an optimal predictor that is counterfactually fair from a list of candidate causal models. The parameterizations of each candidate causal model are known. The authors incorporate the fairness constraint as a regularization term in the loss function. Evaluations are performed on two real-world datasets, and results show that the proposed method balance fairness in multiple worlds with prediction accuracy.

While the idea of exploring a novel fairness measure in counterfactual semantics and enforcing it over multiple candidate models is interesting, there are a few issues I find confusing, which is listed next:

1. The Counterfactual Fairness definition (Def 1) is not clear. It is not immediate to see which counterfactual quantity the authors are trying to measure. Eq2, the probabilistic counterfactual fairness definition, measures the total causal effect (P(Y_x)) of the sensitive feature X on the predicted outcome Y. It is a relatively simple counterfactual quantity, which can be directly computed by physically setting X to a fixed value, without using Pearl’s algorithm of three steps.  

2. If the authors are referring to the total causal effect, it is thus unnecessary to use a rather complicated algorithm to compute counterfactuals (line87-94).
If the authors are indeed referring to the counterfactual fairness defined in [1], the motivation of using this novel counterfactual fairness has not been properly justified. A newly proposed fairness definition should often fall into one of following categories: i. It provides a stronger condition for existing fairness definitions; ii. It captures discriminations which are not covered by existing definitions; or iii. It provides a reasonable relaxation to improve prediction accuracy. I went back to the original counterfactual fairness paper [1] and found discussions regarding this problem. Since this a rather recent result, it would be better if the authors could explain it a bit further in the background section.

3. The contribution of this paper seems to be incremental unless I am missing something. The authors claim that the proposed technique “learns a fair predictor without knowing the true causal model”, but it still requires a finite list of known candidate causal models and then ranges over them. The natural question at this point is how to obtain a list of candidate models? In causal literature, “not knowing the true causal model” often means that only observational data is available, let alone a list of fully-parametrized possible models exists. The relaxation considered in this paper may be a good starting point, but it does not address the fundamental challenge of the unavailability of the underlying model.

Minor comments:
- All references to Figure2 in Sec2.2 should be Figure1.

[1] Matt J Kusner, Joshua R Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. arXiv preprint arXiv:1703.06856, 2017.

Post-rebuttal: 
 Some of the main issues were addressed by the authors. One of the issues was around Definition 1, and I believe the authors can fix that in the camera-ready version. Connections with existing fairness measures can be added in the Supplement. 

Still, unless I am missing something, it seems the paper requires that each of the causal models is *fully* specified, which means that they know precisely the underlying structural functions and distributions over the exogenous variables. This is, generally, an overly strong requirement. 

I felt that the argument in the rebuttal saying that “These may come from expert knowledge or causal discovery algorithms like the popular PC or FCI algorithms [P. Spirtes, C. Glymour, and R. Scheines. “Causation, Prediction, and Search”, 2000]”  is somewhat misleading.  Even when the learning algorithms like FCI can pin down a unique causal structure (almost never the case), it’s still not the accurate to say that they provide the fully specified model with the structural functions and distributions over the exogenous. If one doesn’t have this type of knowledge and setting, one cannot run Pearl’s 3-step algorithm. I am, therefore, unable to find any reasonable justification or setting that would support the feasibility of the proposed approach.","Summary. This paper addresses the problem of learning predictors that trade-off prediction accuracy and fairness. A fair predictor with respect to attribute A is defined using the notion of counterfactual fairness, which basically means that predictions should be independent of which value a sensitive attribute A attains (for example, predictions are the same in distribution for both A=male and A=female). The contribution of the paper is to relax the problem of attaining exact fairness according to a known causal model of the world, to the problem of attaining approximate fairness without assuming the correct causal model specification is known. Instead, a family of M model specifications is allowed. Concretely, this is manifested by incorporating new terms that are added to the loss function penalising deviations from perfect fairness for each of the causal models in the family. By varying the importance of these terms, we can then trade-off prediction accuracy and fairness. The model is applied to two real-world datasets, and trade-off curves are presented to showcase the functionality of the approach. 

Comments. The paper is clear, well-written, technically sound and addresses an important problem domain. The idea of trading-off predictability with fairness by introducing penalty terms for deviations from fairness within each causal model is natural and intuitive. The paper sets up the optimisation problem and proposes an algorithm to solve it. It does not address the issue of how to weigh the different causal models in the family, and it does not provide a baseline trading-off strategy for comparison with the proposed approach. This is perhaps ok but I found it weakens the contribution of the paper. Could the authors address these concerns in their response, please? Certainly, the simplicity of the approach is appealing, but it is not easy to infer from the text how practical the approach may be in its current form."
Dualing GANs,"Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel",https://proceedings.neurips.cc/paper/2017/hash/12a1d073d5ed3fa12169c67c4e2ce415-Abstract.html,"To avoid the instability of the training curves present when training GANs generally, the paper proposes to solve the dual optimization problem for the discriminator rather than the primal problem, which is the standard GAN's formulation. The results show that dualizing the discriminator yields to monotonous training curves, which is not the case when considering the standard formulation where the training curves are spiky.

Adopting the standard GAN approach, which is well-known in the literature, the paper proposes a new interesting way to solve the GAN's learning problem. The idea of dualizing the optimization problem is well motivated since it's fairly natural. Moreover, the proposed method is mathematically relevant and clearly validated by experiments. The paper is well written and the adopted formalism is consistent. Furthermore, the supplementary materials detail the experiments and the proofs of the main results, which are important to the paper's understanding.

I have just a little remark concerning the figure 3 of the main paper: after convergence, it seems that Dualing-GAN may introduce some variance in the estimation of the real data distribution. In fact, the variance per mode of the generated samples, using Dualing-GAN, seems to be bigger than the one, using standard GAN. I recommend the authors to discuss this point to make sure that there is no negative impact on the results in general.","This submission proposes an alternative view on GAN training by replacing the inner learning of the discriminator by its dual program. In the case of linear discriminators this leads to a stable and simpler maximization problem. The authors propose strategies to cope with non-linear discriminators, by either taking linear approximations to the cost function, or to the scoring function (typically the CNN in the case of image GANs). 

PROS:
- The paper proposes an alternative and simple view on GAN training.
- The paper reads well and the formalism is well chosen.
- The proposed algorithm (with scoring function linearization) leads to interesting initial results on CIFAR and MNIST.

CONS:
- The proposed approach requires defining a dual variable for each sample z_i. Therefore, the dual program needs to be solved to satisfactory precision at each step. 
- If I am not mistaken, Claim 1 is simply the dual of a binary logistic linear classification problem. Then, it might be good to present it that way, and avoid calling that a ""claim"" as this is a classical result. 
- From a purely formatting point of view, the graphical presentation of results could be improved, as it is hard to read. Moreover, only several runs, and the corresponding learning curves are given. Would it be possible to give other quantitative measures? How often does GAN training diverge? Is the proposed algorithm helping that metric?

Overall, I think that this paper gives a simple view on GAN training using the dual of the inner problem. I enjoyed reading the paper and both the formal and experimental results seem correct. Because of this I lean towards acceptance - however, since I am not a pure GAN expert, I may have misevaluated the importance of this work. ","This paper proposes a new method to train GAN: firstly it assumes the discriminator is linear, then the GAN model is solved by dual optimization, secondly since the discriminator is non-linear, to train the GAN model using dual optimization, local linearization for the cost or score function is repeated used. Experiments show the effectiveness of the proposed approach.

The innovation of in  the  scenario of optimization, is moderate. It is natural to derive corresponding dual optimization for the case of linear discriminator which has limited representation power for practical application as well as the case of local linearization.

The experiments are not strong enough, specially the so-called instability problem is still not overcome or mitigated. The author should better illustrate the advantage of the proposed approach through the experiments.
"
A Universal Analysis of Large-Scale Regularized Least Squares Solutions,"Ashkan Panahi, Babak Hassibi",https://proceedings.neurips.cc/paper/2017/hash/136f951362dab62e64eb8e841183c2a9-Abstract.html,"The paper tackles the problem of penalized least squares when the dimension scales linearly with the sample size and the penalty separates the variables (e.g., LASSO). The authors prove asymptotic results for the solution and its difference with the true (yet unknown) vector. 

The main assumptions are on the design matrix and the unknown vector, which are both assumed to be random, both with iid entries. In other words, the authors study how input distributions are processed by penalized least squares by describing the output distribution (i.e., the distribution of the solution), when the input distribution has an iid structure. 

As one of their main tools, the authors prove that the initial high-dimensional optimization problem is tied to two low dimensional optimization problems.


Major comments: 

- As a reader, I am very confused by the structure of the supplementary material. Some results are presented and labeled in the main text, but the labels (i.e., ""Theorem 2"") are used for other results in the Supplementary material, making it impossible to find the proofs of the results of the main text. The supplementary material is presented as a whole work, that could almost be submitted as is. However, its role should be to complete the main text with intermediate results and proofs. I believe it should be rewritten completely, with proofs that actually correspond to the results presented in the main text. If the authors wish to complete the main text and extend it with further results, they should rather consider to submit a longer version of their work to a conference. 
- I am not very familiar with this literature, but the assumption that the entries of the unknown vector x_0 are iid seems very artificial and not realistic to me: It gives symmetric and interchangeable roles to all the covariates in the regression. However, I think that on a theoretical level, the results are very interesting and already seem to require very technical proofs. I believe this fact should be emphasized in the main text (e.g., ""true vectors with a different structure"" are mentioned in Line 278 of the main text, but it is not clear what the authors have in mind and whether their results could be easily extended to the case where x_0 has independent entries that are not iid).



Minor comments in the main text:

- The way Equation (1) is written, the factor 1/n is confusing (it may be better to remove it, only for the reader's ease)
- At first reading, it is not clear what the curly E is (e.g., in Equation (4))
- Figure 2: There are no vertical lines that depict optimal values of \lambda
- Figure 3: Should it be \nu=2 or 3 ? (cf. Line 165)


Minor comments in the supplementary material:

- There is a confusion between f and f_n (I recommend that in the main text, Equations (1) and (3), f be replaced with f_n)
- Line 97: Replace the second occurrence of 1/n f_n^(k) with f_n
- Line 191, Equation (69): The 4 should be an exponent in ""z4""



I believe that the results presented in this work are worth acceptance at NIPS, but the paper - especially the supplementary material - needs to be rewritten.","The paper studies the universality of high-dimensional regularized least squares problems with respect to the distribution of the design matrix. The main result of the paper is that, under moment conditions on the entries of the design matrix A, the optimal value of the least squares problem, and characteristics of the optimal solution are independent of the details of the distribution of the A_ij’s. 

The proof uses the Lindeberg interpolation method, in a clever incremental way, to transform the problem to its Gaussian counterpart while preserving the characteristics of the optimization, then uses Gaussian comparison theorems (variants of Gordon’s escape through a mesh result) to analyze the Gaussian setting. 
The consequences are nice formulas predicting the exact values of the cost, the limiting empirical distribution of the entries of the solution and of the error vector. 
These formulas were previously known to be correct only in restricted settings. 

These exact asymptotic characterizations of random estimation problems have been studied by several authors and in different settings. There are similarities, at least in spirit, to the work of El Karoui [1], and Donoho-Montanari [2] on robust high-dimensional regression (a problem that could be seen as the dual of the one considered here), and to many papers that have appeared in the information-theory community on “single letter formulas” characterizing the MMSE of various estimation channels (linear regression, noisy rank-one matrix estimation, CDMA,…) See e.g., [3,4,5] and references therein. 
Although most of these efforts have mainly dealt with the Gaussian design setting, some have proved “channel universality”, which is essentially the focus of this paper. The authors should probably fill this bibliographic gap. 

Second, I would like to point out a different approach to the problem, inspired by rigorous work in statistical physics, and that will make the connection to the above mentioned work clearer: one could consider the log-sum-exp approximation to the optimization problem, with a “inverse temperature” parameter \beta. And one recovers the optimization in the limit \beta -> \infty. One could study the convergence of log-partition function obtained in this way, and obtain a formula for it, which in this setting would bare the name of “the replica-symmetric (RS) formula”.
The “essential optimization” problem found by the authors is nothing else but the zero-temperature limit of this RS formula. (One would of course have to justify swapping the limits n \to \infty and \beta \to \infty.) 

This being said, the techniques used in this work seem to be of a different nature, and add to the repertoire of approaches to compute asymptotic values of these challenging problems.      

[1] Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators : rigorous results https://arxiv.org/abs/1311.2445
[2] High Dimensional Robust M-Estimation: Asymptotic Variance via Approximate Message Passing https://arxiv.org/abs/1310.7320
[3] The Mutual Information in Random Linear Estimation : https://arxiv.org/abs/1607.02335                    
[4] Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula https://arxiv.org/abs/1606.04142
[5] Fundamental limits of symmetric low-rank matrix estimation : https://arxiv.org/abs/1611.03888","This work considers the asymptotic performance of regularized least squares solutions with random design matrices. Expressions for the asymptotic error of Lasso recently appeared in the literature work when the design matrix is i.i.d. Gaussian. Here the authors extend these and show universality results where the design matrix has i.i.d. non-Gaussian entries. The regularization function can also be a separable arbitrary strongly convex function, or the L1 norm. They define a two dimensional saddle point problem, whose solution can be used to  find the distribution of the estimate and its error which also involves a proximal map. They also provide limited numerical result which exhibit the predicted universality of the Lasso error with Bernoulli and t-distributed design matrices. Please see below for additional comments.

1. The tractability of evaluating the expectation of 4 seems to be limited. Are there other regularization functions where a statement similar to section 3.1.1 is possible ? For instance does the analysis work with L2 regularization when n larger than m.

2. The numerical illustration of the universality phenomenon seems very limited. What are other distributions that obey the theorem statement ? (For instance Bernoulli with different parameters).

3. It is not clear how sparse random design matrices fit into the main conclusion. When the design is very sparse, its nullspace will inevitably contain sparse matrices and recovery is not possible. 

4. The expectation notation seems to be non-standard. Is it defined over finite sample size, or asymptotically as n goes to infinity ?

5. Some typos line 104  'constant'. line 99  'and the error'"
Diffusion Approximations for Online Principal Component Estimation and Global Convergence,"Chris Junchi Li, Mengdi Wang, Han Liu, Tong Zhang",https://proceedings.neurips.cc/paper/2017/hash/13f3cf8c531952d72e5847c4183e6910-Abstract.html,"This work analyses the classical Oja iteration for finding the first principal component. They show it can be approximated using a stochastic process and provide global convergence guarantees which match the optimal .

Although the related work section is very dense, it is well organized and serves as a good starting point for the non-expert reader.

As a non expert in this area I find it quite hard to judge the merits of this paper in detail. Having said that it seems that providing the first global convergence guarantees for Oja’s rule ought to be of interest.

The analysis seems to thoroughly explain the different phases in the optimization procedure which perhaps could be useful as a diagnostic tool.

The analysis only considers the standard Oja rule which obtains the leading eigenvector. This is perhaps of limited use, but I appreciate the difficulty in extending this to the entire prinicpal subspace.

Line 27. A point which is tangential to the direction of a paper but still I believe a mischaracterization: We don’t need to compute the covariance matrix. Computing the SVD suffices and can be sped up using randomized techniques. At the very least, we don’t require O(d^2) storage.

There are plenty of typos and ungrammatical sentences throughout and should be proofread more thoroughly. E.g.
145: using an inﬁnitesimal generator analysis results
160: is name the generalized logistic curves.
165: for more on basic concepts and relates topics of SDE
171: a local maximizers that has negative curvatures in every direction.
171: In below, we consider","The paper analyzes a streaming algorithm for PCA, proves that the bounds achieved match the minimax rates and breaks the convergence speed of the algorithm in 3 phases thanks to the use of differential equations that capture the evolution state of the recursive power-method algorithm. 
The presentation of the somehow complex idea behind writing a ODE which captures the convergence of the algorithm is very clear. I wish the authors had reported the result stated at l.148 in the appendix. Finding the reference is not easy. 
I believe that using the same machinery for analyzing convergence of other nonconvex problems can lead to useful results on problems where the computational algorithms are hard to analyze with standard tools. Can the authors discuss this point in the concluding remarks section?","This is a fairly well written paper. The goal of the paper is to study the dynamics of Oja's online algorithm for the estimation of the principal component in PCA.

The innovative aspects of the method lie in  that  the author(s) resort to using a Markov process and via diffusion approximation arguments end up to an equivalent differential equation. In the sequel, they employ weak convergence arguments for Markov processes to reach their results. In this way, assumptions concerning initialization requirements are bypassed. Instead,  a global rate analysis philosophy is adopted. 

The obtained results are interesting in that they show that the algorithm has the potential to escape for unstable stationary points and finally the estimate oscillates around the true principle component. 

 Once the problem is stated, the paper follows well formed  arguments. I did not checked all the details of the proof, however, the proof evolves in a mathematically sound way.

My main criticism of the paper is that it would be helpful to the readers and beneficial to the paper to have some related simulation results and performance figures. "
k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms,"Cong Han Lim, Stephen Wright",https://proceedings.neurips.cc/paper/2017/hash/13fe9d84310e77f13a6d184dbf1232f3-Abstract.html,"abstract

This  paper  presents  a  unifying view  of  k-support/OWL  norms  and
overlapping  group-lasso norms.  It  results in  two  new families  of
norms,  called   ""latent  k-group-support""  (LGS)  norm   and  ""latent
group-smooth OWL"" (LGS-OWL)  norm).  Computation of these  norms is in
general NP-hard.   However, when the  underlying groups has  a special
form  characterised by  a tree  decomposition of  the grouping,  it is
possible to compute these norms with known complexity. This complexity
depends on  the tree width, which  is a measure of  overlapping of the
groups.  A dynamic program relying on the tree decomposition paves the
way  for projecting  onto the  LGS and  LGS-OWL norms.  Some numerical
examples of support recovery comparing the latent group-Lasso to these
extensions with varying k are studied.

comments

The paper  is well written,  even though  I found it  rather technical
since it is not  exactely my area of expertise. I  am impressed by the
computational results and the dynamic program derived y the authors to
compute  the   projection  onto  these   new  norms.   Thus,   on  the
computational and  practical side, I think  that there is a  truly new
contribution for  the special  topic of norms  crafting. On  the other
hand, I am  not completly convinced by the utility  of these new norms
compared  to  the  existing  OWL/k-support.   In  particular,  in  the
numerical studies,  it is obvious  that knowning the number  of latent
group is  very important  for achieving good  performance in  terms of
support recovery. This is not the case in practice, how do the authors
deal with  this issue?  Beside, I  think that  the unifying  papers of
Obozinski and Bach already characterized a very large family of norms.","The paper adapts recent sparsity-inducing regularizers (pOWL, k-support norm) to the overlapping group setting. Two norms are derived from formulations of pOWL and k-support in the combinatorial penalty framework of Obozinski and Bach, where the ""cardinal of the support"" submodular function is replaced by the minimum number of groups required to cover the support, which is not submodular. 

The papers shows most related problems (norm computation, projection and proximal operator) to be NP-hard in general. In the case where a graph constructed from the group overlapping patterns has small treewidth, the papers shows that level sets of the norm can be modelled using a small number of linear and l_p cone constraints. Experiments are based this formulation using a general purpose convex solver. On synthetic data, the proposed norms are compared to latent group Lasso.


The paper is well written. Sections 3.2 and 4 are a bit dense and dry, and could benefit from working out concrete examples, possibly in the supplemental material.

The paper is interesting, and the proposed methods appear promising for overlapping group problems where one wants to stay within the realm of convex methods. Experiments on synthetic data are rigorous and convincing, although overall the experimental section is a bit short. More experiments, including on non-synthetic data, would be interesting.


Remarks

L180: ""indices i assigned to nodes below and including X"" is a bit confusing to me. Nodes are sets of groups, so I am assuming this means ""indices i in the support of groups assigned to nodes below/in X""?
L196: ""the the unit ball""
L242: is ""..."" missing in the 3rd sample group?
L337: vertex -> vertices
","Summary: 

This paper designs new norms for group sparse estimation. The authors extend the k-support norm and the ordered weighted norm to the group case (with overlaps). The resulting (latent) norms are unfortunately NP-hard to compute, though. The main contribution is an algorithm based on tree decomposition and dynamic programming for computing the best approximation (under the Euclidean norm) under group cardinality constraints. This algorithm improves the previous work by a factor of m (# of groups). A second contribution is an extended formulation for the proposed latent group norms. Such formulation can be plugged into standard convex programming toolboxes, as long as the underlying tree width is small. Some synthetic experiments are provided to justify the proposed norms (and their extended formulations).

Overall this paper appears hard to follow, largely due to its technical nature. The organization is also a bit off: the utility of the dynamic programming algorithm in section 3 is largely unclear, and the experiment section did not verify this part at all. While being mostly a computational paper, strangely, the experiment section only compared the statistical performances of different norms, without conducting any computational comparison. While the technical results seem to be solid and improve previous work, it will be more convincing if more can be said on what kind of problems can benefit from the authors' results. With these much more sophisticated norms, one becomes easily overwhelmed by the fine mathematical details but somehow also gets more suspicious about their practical relevance (which unfortunately is not well addressed by the current experiments). 


Other comments:

Lines 93-95: the parameter nu disappears in the convex envelope?

Line 97: the weights need to be nonnegative.

Line 343: this proof of Theorem 2.1 works only for 1 < p < infty? For instance, if p = 1, then Omega = |.|_1 always. Please fully justify the claim ""Omega is the ell_p norm if and only if ..."", as the experiments did use p=infty. 

Line 346: please justify ""if lambda < 1, the solution is (1-lambda)y if and only if ...""

Line 141:  I do not see why the claim N <= |V| is correct. do you mean max_j |X_j| <= |V|?

Experiments: should also include a set of experiments that favors the latent group norm, to see that if the proposed norms perform (much?) worse under an unfavorable condition. Also, please compare with the elastic net regularizer (in the correlated design case). "
Learning to Model the Tail,"Yu-Xiong Wang, Deva Ramanan, Martial Hebert",https://proceedings.neurips.cc/paper/2017/hash/147ebe637038ca50a1265abac8dea181-Abstract.html,"
	Summary
	-------

	The paper proposes an approach for transfer learning for
	multi-class classification problems that aids the learning of categories
	with few training examples (the categories in the tail of the
	distribution of numbers of examples per category). It is based
	on ideas of meta-learning: it builds a (meta-)model of the dynamics
	that accompany the change in model parameters as more training
	data is made available to a classifier.

	Specifically, the proposed approach takes inspiration from
	existing work on meta-learning [20] but extends it by applying
	it to CNNs, utilizing deep residual networks as the
	meta-model, and applying the framework to a general 'long-tail
	problem' setting in which the number of training examples
	available is different and not fixed between categories.

	Experiments are conducted on curated versions of existing
	datasets (curated such that they exhibit strong long-tail
	distributions): SUN-397 [13], Places [7], and ImageNet
	[5]. The performance of the proposed method is demonstrated to
	be considerably higher than several more adhoc baselines from
	the literature.


	Novelty and significance
	------------------------

	While the proposed method draws inspiration from a variety of
	existing methods in transfer learning (notably [20]), the
	particular formulation using deep residual networks as a
	meta-model, modeling the dynamics of adding progressively more
	training examples, and handling the case of different numbers
	of available training examples per category in a unified and
	automatic way are novel aspects of the paper to my knowledge.

	These aspects could potentially be useful to a wider range of
	applications than (image) classification and facilitate the
	treatment of long-tail problems, as they arise in many
	real-world application scenarios.


	Technical correctness
	---------------------

	The proposed method of (meta-)modeling the gradual addition of
	training examples seems plausible and technically correct.

	It would, however, be interesting to learn more about the
	complexity of the proposed approach in terms of training time,
	since a significant number of individual models has to be
	trained prior to training the meta-model.



	Experimental evalution
	----------------------

	The experimental evaluation seems convincing: it is done on
	three different existing datasets, compares the proposed
	approach to existing, more ad-hoc baselines as well as [20],
	and highlights the contributions to performance of the
	different proposed system components (notably, taking the step
	from fixed to recursive splitting of categories). Performance
	is indeed best for the proposed method, but considerable
	margins, and particularly pronounced for tail categories.



	Presentation
	------------

	The paper text seems to suffer from a latex compilation issue
	that results in missing multiple bibliographic and figure
	references (questions marks instead of numbers: line 79, 176,
	239, 277, Tab. 1), which is unfortunate and a possible
	indication that the paper submission has been rushed or a
	premature version uploaded. The text, hoever, does contain
	enough information in order to know what is going on despite
	not having the references.

	The authors should please comment on this and what the correct
	references are in their rebuttal.

	Apart from this issue, the paper is well written and clearly
	presents the proposed ideas, experimental results, and
	relation to prior work.

	Typo line 74: 'changeling' - challenging; line 96: 'the the'
	

      ","*Approach:* The paper addresses the problem of learning models from a few examples by learning model ""dynamics"" from the large classes. By simulating the number of training examples in large categories the dynamics are estimated. This is then transferred to categories with few training examples, thereby simulating the presence of many training examples. The approach is implemented as a residual architecture based on the property of compostionality of the transformation as training data is added. 

The work is related to several recent approaches for meta-learning where a model is trained to predict the parameters of another model based on one or few training examples. This paper presents another way to parameterize the transformation using residual architecture that has some attractive properties. 

*Results* The approach is compared to predictive baseline [20] on several datasets (SUN, Places, ImageNet). The approach outperforms the sample reweighing baselines as well as [20]. 


*Evaluation* The paper is well written and presents a number of interesting ideas, which I found very enjoyable. I found the use of the residual architecture to model the diminishing transformations quite interesting. 

One issue with predictive models (including this work) is that they are limited to small size outputs. This work only considers the last layer weights. How can one generalize the approach to more complex model parameters?","This paper presents a work on tackling the challenge of learning classification models from datasets with imbalanced long-tail class distributions. The major idea is to learn a ""meta model"" to directly estimate the parameters of models trained with many samples using parameters learned on few, or even single, samples. This process is done in a chained manner, which is modeled by residual networks. Classification experiments are performed on image recognition datasets to verify the effectiveness of the proposed approach.

Using meta learning to address the problem of imbalanced data distribution is an interesting idea. It clearly differentiates this work from other approaches to solving this problem. However, the authors did not present well this idea. In particular, the writing is not easy to comprehend, and a lot of references are missing in the text (line 176, 239, 240, etc.).

In practice, the idea is implemented as residual networks and the meta model is trained to model the dynamics of model parameters when trained on different number of available sample. This is justified by the authors observation on identity mapping and compositionally. Here the authors rely on one key insight that classifier weights for different class share similar dynamics. This is an interesting claim and is crucial for the approach, because it ensures that the dynamics learned on data-abundant classes can be transferred to but we fail to see any analysis or theoretic proof.

Another weakness of this work is on the experiment side. The approach is only applied to the last layers of classification networks, which is equivalent to working with linear classifiers. It would be of great interest to see experiments with non-linear models, where this approach could see wide uses.

Overall, I believe the authors presents an insightful and interesting idea with potential applications. But in its current status this work is not mature to merit a publication at NIPS. I highly encourage the authors to make significant improvement to the presentation and experiments and resubmit it to another venue."
Neural Variational Inference and Learning in Undirected Graphical Models,"Volodymyr Kuleshov, Stefano Ermon",https://proceedings.neurips.cc/paper/2017/hash/14e422f05b68cc0139988e128ee880df-Abstract.html,"In this paper the authors essentially propose to train a MLP to generate proposal samples which are used to estimate the partition function Z of an undirected model. Instead of using straight importance sampling to estimate Z (which would be an unbiased estimator for Z), they propose a bound that overestimates Z^2 *in expectation*. While the authors highlight around line 70 that this only works when q is sufficiently close to p, I think it should be made even clearer that almost any estimate with a finite number of samples will *underestimate* Z^2 when q is not sufficiently close. I agree with the authors that this is probably not an issue at the beginning of training -- but I imagine it becomes an issue as p becomes multimodal/peaky towards convergence, when q cannot follow that distribution anymore. Which begs the question: Why would we train an undirected model p, when the training and evaluation method breaks down around the point when the jointly trained and properly normalized proposal distribution q cannot follow it anymore?
Nevertheless, I find the approach interesting and the experiments show that it seems to work for small scale dataset. I think the paper could be improved by paying more attention to this aspect and by investigating the behaviour at the point when q cannot track p anymore.   

-- update --

I would like to thank the authors (and the other reviewers) for their feedback. 


I'm still concerned that the focus on the upper bound property of the Z^2 estimator creates a false sense of reliability. I tested the estimator empirically: I used a 10 dim Gaussian as a proposal distribution to estimate Z^2 for another 10 dim Gaussian. Both shared their mean, but while p had a std-deviation of 0.2, q had a std.-deviation of 1. Here the ground truth Z^2 is 1 because p is already a properly normalised distribution. I found that the proposed estimator almost always severely *underestimates* Z^2 for most finite sets of samples from q, even though they constructed the estimator to be an upper bound on Z in expectation (code attached).

I would therefor encourage the authors to more explicitly state that the proposed estimator will underestimate Z^2, even though it is an upper bound in expectation.


Nevertheless, I think the paper is well written, technically correct and contributes an interesting idea. I therefore updated my overall rating to 6 (marginally above acceptance threashold). 

--
from __future__ import division 

import tensorflow as tf 

distributions = tf.contrib.distributions 
dim_x = 10 # dimensionality of samples 
n_samples = 1000 # number of samples form proposal q 

ones= tf.ones(dim_x) 
p = distributions.Normal(loc=0.*ones, scale=0.2*ones) # target dist 
q = distributions.Normal(loc=0.*ones, scale=1.0*ones) # proposal dist 

x = q.sample(sample_shape=(n_samples,)) 

log_p = tf.reduce_sum(p.log_prob(x), axis=-1) 
loq_q = tf.reduce_sum(q.log_prob(x), axis=-1) 

f_samples = tf.to_float(n_samples) 
z2_est = tf.reduce_logsumexp(2.*log_p - 2.*loq_q) - tf.log(f_samples) 

sess = tf.InteractiveSession() 

print ""Real log(Z^2) is 0.0 because P is a proper normalized distribution"" 
for i in xrange(10): 
print "" log(Z^2) estimate: %f5.3"" % z2_est.eval() 
--","The presented techniques are very interesting, and I believe they are also very useful.

However, I am too concerned about the finite-sample performance of Eq (2). So I strongly suggest a detailed and critical discussion on why and when this bound works, particularly in Eq. (7).
I think this paper would have impact and much wider audiences if the authors could provide more proofs to increase credibility. 

Minors:
1. Line 45, it should be Z(\theta) = \int \tilde p(x) dx.         Ex[a(x)] usually means \int a(x) p(x) dx;
2. Line 52, the variance of \hat I should be (E[w2]-I2) / n;
3. Line 148, Wrong parametrization of p(a|x);
4. Eq. (5), it should be \theta^T x instead of  log(\theta^T x);
5. Line 275, ""...to learn to(two) types..."";
6. Line 280, ""u \in R^1  0"";
7. The title of Figure 2 is not right.","# Overview

The paper presents approximate inference methods for learning undirected graphical models. Learning a Markov random field (MRF)  p(x) involves computing the partition function -- an intractable integral $Z(\theta) = \int \bar p_\theta(x) dx$ (where $\bar p_\theta(x)$ is the energy function). The authors use a tractable importance distribution $q_\phi(x)$ to derive an upperbound on the partition function. They argue that optimising the parameters of p and q under this bound can be seen as variational inference with \chi^2-divergence (instead of KL).

Essentially the proposed methodology circumvents sampling from an MRF (which would require computing the partition function or a form of MCMC) by sampling from an approximation q and using the same approximation to estimate an upperbound on the log partition function of the MRF (which arises in VI's objective). To obtain an approximations q that is potentially multi-modal the authors employ mixture of K components (termed ""non-parametric VI"" Gershman et al 2012) and auxiliary variables (in the approximation). 

The paper then presents 3 applications:

1. 5x5 Ising model: the idea behind this toy application is to assess the quality of the approximate partition function since one can compute the exact one. 
2. RBM for MNIST: here they model an RBM p(x) over MNIST-digits using their upperbound on partition function to optimise a lowerbound on the likelihood of the data. 
3. Variational auto encoder for (binarised) MNIST: here they model p(x|z)p(z) where the prior p(z) is an RBM. 

The main point of the empirical comparison was to show that the proposed method can be used to fit a variational auto-encoder with an RBM prior. The authors contrast that with a variational auto-encoder (with the standard Gaussian prior) whose approximate posterior is augmented with auxiliary random variables as to be able to model multiple posterior modes. 

# Qualitative assessment

* Quality

The paper is mostly technically sound, but I would like to have a few points clarified:

1. You refer to equation just before line 54 as ELBO, but the ELBO would involve computations about the posterior, which is really not the case here. I can see the parallel to the ELBO, but perhaps this should be clarified in the text.
2. In line 91, I find the text a bit misleading: ""the variance is zero, and a single sample computes the partition function"".  If p = q, the bound in Eq (2) is tight and the expectation equals Z(theta)^2, but that does not mean you should be able to compute it with a single sample.  
3. Most of the text in Section 3.4 (optimisation) assumes we can simulate x ~ q(x) directly, but that's not really the case when auxiliary variables are employed (where we actually simulate a ~ q(a) and then x ~ q(x|a)). I am curious about how you estimated gradients, did you employ a reparameterisation for q(a)? How did you deal with the KL term from q(a,x) in Section 5.3 where you have a VAE with an RBM prior?
4. Is the gradient in line 159 correct? I think it might be missing a minus. 
5. The energy function in line 204 should have been exponentiated, right?
6. Can you clarify why $\log B(\bar p, q)$ in Equation 7 does not affect p's gradient? As far as I can tell, it is a function of p's parameters (\theta).

* Clarity

A few results could have been derived explicitly: e.g. gradients of upperbound with respect to q in each of q's formulation that the paper explored; similarly, the estimator for the case where q is q(a,x).


* Significance

The main advance is definitely interesting and I can imagine people will be interested in the possibility of using MRF priors.


"
Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification,"Bikash Joshi, Massih R. Amini, Ioannis Partalas, Franck Iutzeler, Yury Maximov",https://proceedings.neurips.cc/paper/2017/hash/14ea0d5b0cf49525d1866cb1e95ada5d-Abstract.html,"This paper presents a double sampling strategy to improve the multiclass classification approach in [16]. The authors present both theoretical and empirical analysis of their proposed approach. Experiments conducted on text data set shows the proposed approach can handle multi-class classification problem with a large number of classes. 


Pros:

- The authors conduct comprehensive experimental comparisons with several baseline methods. Results show that the proposed approach achieves high performance with shorter training time on several datasets. 

- The extreme classification problem is an important direction of research and have many applications in practice. 

Cons:

- The paper is mostly based on [16]. Despite there are two main improvements: 1) proposed the double sampling strategy and 2) a new generalization bounds based on local Rademacher Complexities, the novelty is relatively thin.  

- Despite the proposed approach shorten the training time, it still requires long prediction time compared with other approaches. It is arguably the prediction time is more important in practice. 

Comments:

- It is unclear how to interoperate the generalization bound presented in the paper. How the generation bound compared with other approaches?

- Some methods presented in the experiments are batch learning algorithms and some are online learning algorithms. Therefore, the memory usages are very different. 
","Summary:

This paper proposes a new reduction from multi-class classification to binary classification that is especially suitable when the number of classes is very large. They consider a hypothesis that map (input,class) pairs to scores, and the underlying loss function counts the fraction of the wrong classes that are scored higher than the true class. More specifically, they suppose they have a feature transformation phi that maps (input,class) pairs to a p-dimensional feature space, and they learn a mapping from R^p to scores. 

 Their reduction extends the work of Joshi et al. (2015) which, for each data point (x,y), creates K-1 transformed points where each transformed point intuitively corresponds to the comparison of label y with some incorrect label y'. Given that the transformed dataset contains correlated training examples, many standard generalization bounds cannot be applied. The first contribution of this paper is an improved generalization analysis over Joshi et al. (2015) using new bounds on the fractional Rademacher complexity in this setting. The second main contribution is showing that it is possible to employ sampling techniques during the reduction to balance the class distributions and to decrease the training set size. In particular, they sample each class (with replacement) in order to ensure that each class has roughly equal representation in the transformed training data, and rather than including transformed points for all K-1 possible class comparisons, they use only a random sample of classes. They show that these two sampling procedures do not drastically affect their generalization bounds. Finally, they show that this new procedure performs very competitively against several strong baselines.

Comments:

This paper presents an interesting reduction from multi-class classification with many classes to binary classification and corresponding generalization bounds for dealing with the fact that the reduced dataset contains correlated data. The main novelty of the reduction presented in the paper is the sampling technique for rebalancing the class distributions and for reducing the size of the transformed dataset, while the basic reduction appeared already in an earlier paper by Joshi etl al. The empirical comparison to existing algorithms are very strong.

One thing that is missing from the paper is a theoretical analysis of the impact of the prediction algorithm given in section 2.3, where comparisons are only performed for the k classes whose centroids are closest to a given input point. 

I would have also appreciated some additional discussion of the sample complexity bounds, possibly comparing them to the generalization bounds obtained for the non-transformed multi-class problems. ","This paper presents a learning reduction for extreme classification, multiclass classification where the output space ranges up 100k different classes. 

Many approaches to extreme classification rely on inferring a tree structure among the output labels, as in the hierarchical softmax, so that only a logarithmic number of binary predictions need to be made to infer the output class, or a label embedding approach that can be learned efficiently through least-squares or sampling. The authors note the problems with these approaches: the inferred latent tree may not be optimal and lead to cascading errors, and the label embedding approach may lead to prediction errors when the true label matrix is not low-rank.

An alternative approach is to reduce extreme classification to pairwise binary classification. The authors operate in the second framework and present a scalable sampling based method with theoretical guarantees as to its consistency with the original multiclass ERM formulation.

Experiments on 5 text datasets comparing a number of competing approaches (including standard one-vs-all classification when that is computationally feasible) show that the method is generally superior in terms of accuracy and F1 once the number of classes goes above 30k, and trains faster while using far less memory, although suffers somewhat in terms of prediction speed.

The Aggressive Double Sampling reduction is controlled by two parameters, one of which controls the frequency of sampling each class, which is inversely proportional to the empirical class frequency in order to avoid entirely missing classes in the long tail. The second parameter sets the number of adversarial examples to draw uniformly. This procedure is repeated to train the final dyadic classifier.

At prediction time, inference is complicated by the fact that generating pairwise features with all possible classes is intractable. A heuristic is used to identify candidate classes by making an input space centroid representation of each class as the average of vectors in the class, with prediction being carried out by a series of pairwise classifications with only those candidates.

Theoretical analysis of the reduction is complicated by the fact that the reduction from multi-class to dyadic binary classification changes a sum of independent random variables into a sum of random variables with a particular dependence structure among the dyadic examples. The other complication is caused by the oversampling of rare classes, shifting the empirical distribution. The proof involves splitting the sum of dependent examples into several sums of independent variables based on the graph structure induced by the example construction, and using the concentration equalities for partially dependent variables introduced by Janson. Although the finite-sample risk bound is biased by the ratios between the true class probabilities and the oversampled ones, these biases decrease linearly with the size of the sampled and re-sampled training sets. The analysis appears sound but I have not verified the proof in detail.

Overall, this paper presents a strong method for extreme classification, with small memory and computational requirements and superior performance on especially the largest datasets. The binary reduction algorithm appears theoretically sound, interesting, and useful to practitioners."
Learning Linear Dynamical Systems via Spectral Filtering,"Elad Hazan, Karan Singh, Cyril Zhang",https://proceedings.neurips.cc/paper/2017/hash/165a59f7cf3b5c4396ba65953d679f17-Abstract.html,"The paper addresses an interesting issue and the proposed approach has merit.

However, the analysis is based on a number of strong conditions that significantly limit the class of LDS that are considered.

One of the assumptions is that the A matrix is symmetric and strictly psd for which the authors provide some justification.

A stronger assumption is that the input is bounded entrywise, which is much harder to justify.

Finally, stability of an LDS is usually defined through the spectral radius norm. On the other hand, in this paper the authors work with the operator norm that is significantly more restrictive. 

There is no discussion of the 2nd and 3rd assumption and which LDS systems fail to cover.","Linear dynamical systems are a mainstay of control theory. Unlike the empirical morass of work that underlies much of machine learning work, e.g. deep learning, where there is little theory, and an attempt to produce general solutions of unknown reliability and quality, control theorists wisely have chosen an alternative course of action, where they focused on a simple but highly effective linear model of dynamics, which can be analyzed extremely deeply. This led to the breakthrough work many decades ago of Kalman filters, without which the moon landing would have been impossible. 

This paper explores the problem of online learning (in the regret model) of dynamical systems, and improves upon previous work in this setting that was restricted to the single input single output (SISO) case [HMR 16]. Unlike that paper, the present work shows that regret bounded learning of an LDS is possible without making assumptions on the spectral structure (polynomially bounded eigengap), and signal source limitations. 

The key new idea is a convex relation of the original non-convex problem, which as the paper shows, is ""the central driver"" of their approach. The basic algorithm is a variant of the original projected gradient method of Zinkevich from 2003, The method is viewed as a online wave filtered regression method (Algorithm 1), where pieces of the dynamical system are learned over time and stitched together into an overall model. The paper shows that optimization over linear maps is appropriate convex relaxation of the original objective function,. 

The paper is well written and has an extensive theoretical analysis. It represents  a solid step forward in the learning of dynamical systems models, and should spur further work on more difficult cases. The convex relaxation trick might be useful in other settings as well. 

Some questions: 

1. Predictive state representations (PSRs) are widely used in reinforcement learning to model learning from partially observed states. Does your work have any bearing on the learnability of PSRs, a longstanding challenge in the field. 

2. There is quite a bit of work on kernelizing Kalman filters in various ways to obtain nonlinear dynamical systems. Does your approach extend to any of these extended models?

The approach is largely based on the simple projected gradient descent approach, but one wonders whether proximal gradient tricks that are so successful elsewhere (e.g, ADAGRAD, mirror prox etc.) would also be helpful here. In other words, can one exploit the geometry of the space to accelerate convergence? 
","An interesting and very strong paper.  The results and proofs are extremely relevant to the NIPS audience. The paper is clearly written and provides a method for improving bounds in online learning of LDS in an efficient manner, as stated.  The example applications show significant practical improvement over EM.  
Suggestions:
- Comparisons to other sys-id methods would be welcome (e.g. least squares, GLS, etc.)
- More information in the main paper on the nature of adversarial noise that can be tolerated would be helpful. An example would be ideal. This is presumably linked to performance on the nonlinear system considered.
- LIne 207 is missing words."
Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes,"Zhenwen Dai, Mauricio Álvarez, Neil Lawrence",https://proceedings.neurips.cc/paper/2017/hash/1680e9fa7b4dd5d62ece800239bb53bd-Abstract.html,"# Update after author feedback and reviewer discussion

I thank the authors for feedback. I maintain my assessment and do not recommend publication at this stage.

The core contribution is representing conditions with latent variables, and deriving a VI algorithm to cope with intractibility. This is interesting, but the discussion around it could be much improved. 

Some possible improvements are addressed in the author feedback, eg I'm not sure how Fig 1 could have been understood without the complementary explanation brought up in the feedback. 

Beyond what has been addressed in the author feedback, some work is needed to make this paper appealing (which the idea under study, the method and the results seem to call for):
- clarifying the mathematical formulation, eg what forms of $k_H$ are we examining, provide a full probabilistic model summary of the model, point out design choices
- pointing out differences or similarities with existing work
- remove gratuitous reference to deep learning in intro (it detracts)
- make sure that all important questions a reader might have are addressed

# Overall assessment

The issue addressed (modelling univariate outputs which were generated under different, known conditions) and the modelling choice (representing conditions as latent variables) are interesting.

I see no reason why to even mention and work with the (restrictive) parameterization of section 2.2, and not immediately the parameterization allowing ""missing data"" section 4. The latter is more interesting and equally simple.

Overall, the paper is reasonably clear, but seems lacking in originality and novelty. 

## Novelty
Wrt work on multi-output / multi-task GP, I can see that:
- this model has latent variables, not task IDs, and so can generalize over tasks (called conditions in the paper)
- the computational treatment using inducing points helps
The two seem very close though. The assumption of Kronecker structure is a staple. In particular, experiments should try compare to these methods (eg considering every condition a distinct task). 

The application of Titsias 2009 seems straightforward, I cannot see the novelty there, nor why so much space is used to present it.

## Presentation and clarity
1. Paper requires English proofreading. I usually report typos and mistakes in my reviews, but here there are too many grammar errors.
2. Motivating example with braking distance is helpful.
2. I don't understand fig 1 c,d at all: why do we not see the data points of fig 1a? where do these new points come from? what is depicted in the top vs the bottom plot?
2. References: the citation of Bussas 2017 is wrong. The complete correct citation is more like: Matthias Bussas, Christoph Sawade, Tobias Scheffer, and Niels Landwehr. Varying-coefficient models for geospatial transfer learning.. Machine Learning, doi:10.1007/s10994-017-5639-3, 2017.
","This paper presents a method for inference in GPs with multiple outputs where the outputs are highly correlated. The model is augmented with additional latent variables responsible to model this correlation, which are estimated with variational inference.

This is a interesting paper that introduces a (seemingly novel?) method to model the correlation between multiple GPs with a latent variable instead of imposing a certain structure in the kernel (such as the LMC does). The results looks good and comes with great computational complexity. 

I  would prefer a more though discussion investigating why this approach outperforms the standard methods. Also, Section 3 is a bit difficult to follow, where the intuition and the pedagogical skill of the presentation is somewhat lacking. If the presentation could be improved, it would help the accessibility of the paper.","The paper extends multi output GP models by modelling the outputs  as a function on the data and a shared latent space which aims to find embeddings for each condition. A variational GP approach is taken where the variational distribution is factorised as kronecker matrix Uhh \kron Uff (which seems a reasonable assumption), allowing fast inference.

The key contribution of this paper is that by modelling the conditions in the latent space, it is possible to marginalise these out at test time, providing a smooth average across different condition seen at training train, weighted by the similarity to these conditions. 

While not applicable to the general case of missing data, I would be interested to see how this model performs in the fully observed case compared to multi-output models like Wilson et al.'s Gaussian Process Regression Network. This is just curiosity and doesn't impact my positive review of this paper.

Minor errors:
line 45: ""physics about car""
"
Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks,"Wei-Sheng Lai, Jia-Bin Huang, Ming-Hsuan Yang",https://proceedings.neurips.cc/paper/2017/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html,"Summary:

The paper presents a semi-supervised approach to learning optical flow using a generative adversarial network (GAN) on flow warp errors.  Rather than using a handcrafted loss (e.g., deviation of brightness constancy + deviation from smoothness) the paper explores the use of a GAN applied to flow warp errors.

Strengths:

+ novel semi-supervised approach to learning; some concerns on the novelty in the light of [21]

+ generally written well

Weaknesses:

- some key evaluations missing

Comments:

Supervised (e.g., [8]) and unsupervised (e.g., [39]) approaches to optical flow prediction have previously been investigated, the type of semi-supervised supervision proposed here appears novel.  The main contribution is in the introduction of an adversarial loss for training rather than the particulars of the flow prediction architecture.  As discussed in Sec. 2, [21] also proposes an adversarial scheme.  A further comparative discussion is suggested.  Is the main difference in the application domain?  This needs to be clarified.  Also the paper claims that ""it is not making explicit assumptions on the brightness constancy"".  While the proposed approach does not use an Lp or robust loss, as considered in previous work, the concept of taking flow warp error images as input does in fact rely on a form of brightness constancy assumption.  The deviation from prior work is not the assumption of brightness constancy but the actual loss.

In reference to [39], it is unclear what is meant by the approach ""requires significantly more sophisticated data augmentation techniques"" since the augmentation follows [8] and is similar to that used in the current paper.

The main weakness of the paper is in the evaluation.  One of the main stated contributions (line 64) is that the proposed approach outperforms purely supervised approaches.  Judging from Table 3, this may be a gross misstatement.  For example, on Sintel-Clean FlowNet2 achieves 3.96 aEPE vs 6.28 by the proposed approach.

Why weren't results reported on the KITTI test sets using the test server?  Are the reported KITTI results including occluded regions (in other words all points) or limited to non-occluded regions?

The limitations of the presented training procedure (Sec. 4.4) should have been presented much earlier.  It would have been interesting to train the network with data all from the same domain, e.g., driving scenes.  For instance, rather than train on FlyingChairs and KITTI raw, it would have been interesting to train on KITTI raw (no ground truth) and Virtual KITTI (with ground truth) and evaluate on KITTI TEST.

Reported fine-tuned results should be presented for all approaches.

Minor point:

- line 22 ""Due to the lack of the large-scale ground truth flow"" --> ""Due to the lack of large-scale ground truth flow""
- line 32: ""With the limited quantity and unrealistic of ground truth"" 


Overall rating:

The training approach itself appears to be novel (a more detailed comparative discussion with respect to [21] is suggested) and will be of interest to those working on flow and depth prediction.  Based on the issues cited with the evaluation I cannot recommend acceptance at this point.

Rebuttal:

The reviewer acknowledges that the authors have addressed some of my concerns/suggestions regarding their evaluation. The performance is still below that of the state-of-the-art. Finetuning may help but has not been evaluated.  Is the performance deficit, as suggested by the authors, due to the capacity of their chosen network or is it due to their proposed GAN approach? Would the actual training of a higher capacity network with the proposed adversarial loss be problematic? These are topics to be explored.","The paper proposes a semi-supervised learning scheme with GANs for optical flow.
The proposed approach is reasonable, and some tiny improvements (Table 2 and 3) are achieved over fully supervised or baseline semi-supervised (similar) architectures. However, this is the case when the proposed approach uses all the train data of the supervised architectures and extra train data.

I am missing a simple yet very relevant experiment where the Flying Chairs train data is split for supervised training and remaining ""unlabeled"" data.
For example, the percent of supervised training can go from 10% to 100% and the results for both supervised and proposed semisupervised (that uses also the remaining part of the data as unlabeled) should be reported for all these partitions.
In this way we can see how much flexibility brings the proposed approach wrt the fully supervised approach.

Another experiments could mix the train data with Sintel or other sources as mentioned in section 4.4. 
Are there cases when using unlabeled data worsen the overall performance? such as it happens for KITTI 2012 when using the ""baseline semi-supervised"" with ""Chairs+KITTI"" in Table 2. Why? ","This paper discusses the training of an optical flow estimation model, and proposes two things: 1) train the model in a semi-supervised way by using a supervised loss on labeled data, and an unsupervised loss on unlabeled data.
2) use the labeled data to also learn the unsupervised loss using a discriminator (in a GAN setting).

I think this paper is interesting and well written.
The different components which can be thought of as 3 different losses (supervised, hand-crafted-unsupervised and learned-unsupervised) are explained clearly in the text and the experimental setup explicitly demonstrates the benefits of each. Even though the experimental results are inferior to the state-of-the-art both in hand-crafted methods and learned methods, they shed light on how the ideas presented can be incorporated with better models and lead to better results.

While the overall experimental setup seems to be fair, there is one issue that bothers me.
I don’t understand how the baseline semi-supervised method can be worse than the supervised method. If \lambda is the weight of the unsupervised loss then the semi-supervised method can always converge to the supervised method by making \lambda close to zero.
So was the \lambda not tuned to give the best result? 
Is it a generalization error? Perhaps seeing the training errors might help here.

Minor issues:
In the flow-warp definitions in line 148, I think it should be y = I_1 - W() .
Typo in line 184/5 “effect of using different the receptive”.
The numbers (mixed with equation numbers) in line 223-227 are confusing.




"
Phase Transitions in the Pooled Data Problem,"Jonathan Scarlett, Volkan Cevher",https://proceedings.neurips.cc/paper/2017/hash/1700002963a49da13542e0726b7bb758-Abstract.html,"Summary. This submission presents an information-theoretic study of the pooled data problem. The contributions are to provide a lower bound of the number of tests necessary for achieving decoding error less than 1 in the noiseless setting (Theorem 1), and a lower bound in noisy setting (Theorem 2, Corollary 1). The lower bound in the noiseless setting matches the previously obtained upper bound in the leading order in the number p of items, which in turn specifies the phase transition point between decodable and undecodable phases.



Quality. The key ideas for proving Theorem 1, especially the genie argument that is used in reducing the problem to a simpler one with a smaller number of labels, are properly described, although I have not checked details of the proof.



Clarity. In some places results are displayed without mentioning that they are asymptotic, i.e., in the limit p \to \infty, which might cause confusion.



Originality. Showing existence of the phase transition in the noiseless setting is new. Studying the noisy version of the pooled data problem is another novel contribution.



Significance. This work is of significance in that it has revealed existence of phase transition in the noiseless pooled data problem, as well as sample complexity lower bounds in the noisy setting. The latter would also encourage further studies on conditions under which perfect decoding should be possible in the noisy setting, as well as on efficient decoding algorithms from noisy observations.



Minor points:

Page 5, line 13-14: a concatenation (the) of vectors

Equation (22): The function H_2(\pi) seems undefined.

Equation (25): The inequality sign should be in the opposite direction.","In the pooled data problem there are p items, each of which is assigned 1 of d possible labels. The goal is to recover the vector of labels from n ""pooled tests."" In each of these tests, a ""pool"" (subset of the items) is chosen and we observe how many items in the pool have each label (but not which items have which). A random prior on the label vector is used: the proportion with which each label occurs in the population is fixed, and the true labelling is chosen independently among all labellings that exactly obey these proportions. This paper considers the non-adaptive case where all the pools (subsets) must be chosen in advance. The focus is mainly on the case of Bernoulli random pools: the incidence matrix of pools versus items has iid Bernoulli(q) entries for some fixed 0 < q < 1. The objective is exact recovery of the label vector, although extensions to approximate recovery are considered in the appendix.

This paper proves information-theoretic lower bounds for the pooled data problem, both in the noiseless setting described above and in a very general noisy model.

In the noiseless setting with Bernoulli random pools, they prove the following sharp phase transition. The regime is where the number of samples is n = c*p/log(p) for a constant c, with p tending to infinity. They find the exact constant C such that if c > C then exact recovery is possible with success probability tending to 1, and if c < C then any procedure will fail with probability tending to 1. The constant C depends on the number of labels and the proportions of each label (but not on the constant q which determines the pool sizes). The upper bound was known previously; this paper proves the tight lower bound (i.e. the impossibility result), improving upon a loose bound in prior work.

The sharp information-theoretic threshold above is not known to be achieved by any efficient (i.e. polynomial time) algorithm. Prior work (citation [4] in the paper) suggests that efficient algorithms require a different threshold (in which n scales proportionally to p).

The key technical insight used in this paper to prove the sharp lower bound is the following ""genie argument."" Let G be some subset of {1,..,d} (the label values), and imagine revealing the labels of all the items that have labels not in G. Now apply a more standard lower bound to this setting, and optimize G in order to get the strongest possible bound.

The other contribution of this paper is to the noisy setting, which has not previously been studied. This paper defines a very general noise model where each observation is corrupted via an arbitrary noisy channel. They give a lower bound for exact recovery in this setting, and specialize it to certain concrete cases (e.g. Gaussian noise). The proof of the lower bound is based on a variant of the above genie argument, combined with standard lower bounds based on Fano's inequality.

Unfortunately there are no existing upper bounds for the noisy setting, so it is hard to judge the strength of this lower bound. The authors do, however, argue for its efficacy in a few different ways. First, it recovers the sharp results for the noiseless case and also recovers known bounds for the ""group testing"" problem, which is essentially the special case of 2 labels: ""defective"" and ""non-defective"" (and you want to identify the defective items). Furthermore, the lower bound for Gaussian noise is strong enough to show an asymptotic separation between the noiseless and noisy cases: while in the noiseless case it is necessary and sufficient for n to be of order p/log(p), the noisy case requires n to be at least p*log(p).

In the conclusion, the authors pose the open question of finding upper bounds for the noisy setting and provide some justification as to why this seem difficult using existing techniques.

I think this is a good paper and I recommend its acceptance. The exposition is good. The sharp lower bound for the noiseless case is a very nice result, and the proof technique seems to be novel. The lower bound for the noisy case is also a nice contribution; although it is hard to judge whether or not it should be tight (since no upper bound is given), it is certainly nontrivial, using the genie argument to surpass standard methods.

Specific comment:
- I think there is a typo in Table 2: the heading ""exact recovery"" should be ""noiseless."" (If I understand correctly, the entire table pertains to exact recovery.)","This paper explores the pooled data problem introduced in [1], and derives a lower bound on the number of pooled tests required to identify the ground-truth labels of items. The derived bound gives two main implications: (1) The noiseless setting: Characterizing the sharp threshold on the required number of tests under i.i.d. Bern(q) measurement matrix, thereby proving the optimality of the message passing algorithm in [3]; (2) A noisy setting: Evaluating several mutual information terms that form the lower bound to investigate the effect of noise upon the performance under a Gaussian noise model.

The paper is well written - the main results are clearly presented and the proofs are well streamlined together with accessible outlines. In particular, in the noiseless setting, it tightens the lower bound in [3] via a converse technique, thus showing the tightness of the upper bound in [3]. The major concerns of this reviewer are as follows: 

1. (technical contribution): As mentioned in Example 2, Corollary 1 recovers the lower bounds given in the binary group testing problem [9], which can be viewed as a special case of d=2. I am wondering how distinct the noisy-case converse technique developed in this paper (which also covers the noiseless setting under the vanishing error probability criterion) relative to the one in [9], which is also based on Fano's inequality and a genie-aided argument. Perhaps a key contribution might be extending to an arbitrary d (in which a more involved combinatorics arise) and explicitly evaluating several mutual information terms being tailored for the model considered herein. If that is the case, the technical contribution looks marginal although its implication is interesting.
    
2. (i.i.d. measurement matrix) The above two implications are based on the i.i.d. assumption made on the measurement matrix. This assumption may be arguable though for some applications in which adaptive measurements are allowed and/or some measurement constraint (like locality constraint) is applied. In particular, compared to the adaptive measurement setting, the lower bound claimed in the noisy setting might not be that informative, as one can only do better in the adaptive setting. I am wondering how the developed bound can be extended (if possible) to a broader class of measurement models. This generalization is perhaps challenging though, due to the potential difficulty of single-letterization for beyond-iid models.

3. (Gaussian model) Not clear as to how practically-relevant is the Gaussian model under which the second implication is developed. Any important applications which respect the Gaussian model? 

4. (upper bound) Developing an algorithm for the noisy setting is deferred as a future work. 

5. (applications) It is not clear how the pooled data problem arises in the applications mentioned in the introduction. More elaboration may be helpful.

Minor comments: 

1. As for the indicator function near Eq. (2): Adding brackets in the two sets intersected can improve readability. 

2. Comparison to the very recent work [6] is not clear enough. For instance, what are qmax and \Delta? What does the condition \Delta >> \sqrt{qmax} mean? More detailed explanation might help. 

3. As mentioned in the paper, under the vanishing error probability criterion, the noisy-case bound recovers the one in the noiseless setting, in which the proof technique looks distinct. Any way to unify the two proofs?

4. One of the key techniques in the converse lie in the genie argument. But this part is relatively harder to follow. A more comprehensive illustration might help.

--------------------
(Additional comments)
I have read the rebuttal - some of the comments are addressed. Given that the converse proof in the noiseless setting is quite distinct relative to [9], the paper seems to make a technical contribution as well. Perhaps this can be further highlighted in a revision.   "
Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning,"Christoph Dann, Tor Lattimore, Emma Brunskill",https://proceedings.neurips.cc/paper/2017/hash/17d8da815fa21c57af9829fb0a869602-Abstract.html,"The paper defines ""Uniform-PAC"" where uniformity is over the optimality criterion, eps. It is PAC like in that optimal actions are taken in all but a bounded number of steps. It is also regret like in that the algorithm is eventually good relative to any epsilon---not just one it is told to meet.

I liked the paper. I thought the discussion of different performance metrics was thorough and informative. I would have liked more intuition about the iterated logarithm idea and its main properties, but I understand that the highly technical stuff had to be expressed in very limited space. I would also have liked to have seen a more thorough accounting of open problems, as arguing that we have a new algorithmic criterion means that some new problems arise and some older problems are reduced in priority. So, for example, do we need an analysis of the infinite-horizon case? Does it raise new issues or do we essentially replace H by 1/(1-gamma) and things go through? Does the criterion have interesting problems from a bandit perspective?

Detailed comments:

""UBEV leverages new confidence intervals which the law of iterated logarithm allows us to more tightly bound the probability of failure
events, in which the algorithm may not achieve high performance."": Confusing, if not broken. Rewrite.

""F_PAC(1/eps, log(1/delta)"": Missing paren. Also, F_PAC is used immediately after with several new parameters.

""F_UHPR(S,A,H,T,log(1/delta)"": Missing paren.

""(Delta_k)_k"": Isn't that second k incorrect?

""guarantees only bounds"" -> ""guarantees only bound"".

""criteria overcomes"" -> ""criterion overcomes""?

""that an algorithm optimal"" -> ""that an algorithm with optimal""?

""Uniform-PAC is the bridge"" -> ""Uniform-PAC is a bridge""?

""thet"" -> ""the""?

I'm confused about the implications of Theorem 3 on Theorem 2. Theorem 3b says the algorithm produces an (eps,delta)-PAC bound. Theorem 3c says it has T^1/2 high probability regret. But, Theorem 2a that an algorithm can't have an (eps,delta)-PAC bound and high probability regret better than T^2/3. Theorem 2b is even stronger, saying it can't do better than T. Oh, wait, maybe I see the subtlety here. The issue is not that *no* algorithm with a certain PAC bound can achieve a certain regret bound. It's that knowing that an algorithm satisfies the PAC bound is insufficient to guarantee that it has the regret bound. The UPAC bound is stronger and results in algorithms that have both kinds of guarantees. I think you can make this clearer in the paper. (It's very interesting.)

""rng"" is used without explanation. I found https://en.wikipedia.org/wiki/Rng_(algebra), but it doesn't seem to use rng as an operator, just a property. So, I think additional background is needed here.

""All algorithms are run with setting where their bounds hold."": Reword?

""increase. . One"": Extra period.

""the discounted for which"": Missing word?


Note: I was very impressed with the comments of Assigned_Reviewer_3, but they didn't significantly dampen my enthusiasm for the paper. To me, the interesting contribution is the definition of a criterion that implies both PAC and regret bounds. The analysis of the finite-horizon case was just a small downpayment on using this concept.

That being said, I would be much happier if an infinite horizon analysis was presented and I do not think your claim that it is ""even not clear how regret should be defined here"" is very convincing. There are several concepts used in the literature that would seem worth considering. Something related to the Strehl/Littman concept raised by Assigned_Reviewer_1 seems like it could work.
","SUMMARY.
This paper considers the setting of finite horizon episodic learning MDPs, and studies the performance of learning algorithms in terms of PAC and regret guarantees.
The paper introduces a novel algorithm called UBEV, together with a finite time regret and PAC analysis.

DETAILED COMMENTS.
I initially thought this would be a very promising article; In the end I am quite disappointed.

A) I find Section 2 a bit spurious. The whole discussion seems to be motivated only by the fact that you make use of a refined concentration inequality based on a peeling argument instead of the crude union bound that is sometimes used in MDP analysis. Further, the UBEV algorithm is nothing but a regret minimization algorithm for undiscounted MDPs, in the spirit of UCRL, KL-UCRL and so on, with the key notable difference that it applies to the episodic setting with fixed horizon H.
I recommend removing this section 2 entirely.

B) Finite Horizon H:
The assumption of an episodic setting with a fixed, known horizon H should really be discussed. 
I find it very misleading that in page 5, several differences are mentioned with other setting but this knowledge of the horizon is not even mentioned.
This vastly simplifying assumption is actually the key element that enables to obtain a simplified analysis. In particular, it naturally gives a known upper bound on the diameter (H) of the MDP, and more importantly it enables the use of Dynamic Programming, as opposed to Value Iteration, for which it is simple to enforce constrained/regularization on the MDP. 
In stark contrast, computing an optimistic value while enforcing constrained on the output value in the context of an infinite horizon is a key challenge;
See algorithms such as UCRL, KL-UCRL or REGAL that consider infinite horizon (they do not resort to DP due to the infinite horizon), plus discussions there-in regarding the difficulty of enforcing constrains on the MDP in practice.

C) Scaling of the regret:
The leading term of the regret in Theorem 4 scales with sqrt{S} but it also scales with H^2 with the horizon. In this context where H upper bounds the diameter D of the MDP, this dependency seems to be loose compared to the regret of other algorithms that scale with D, not D^2. Thus it seems in the analysis you just improve on one factor but loose on another one. Once again, the fact this ""detail"" is not even discussed is misleading.

D) Concentration inequalities:
The main building block of this paper, apart from the greatly simplifying assumption of having a finite horizon H, is to resort to an improved concentration inequality for the empirical transition distribution, in order to properly handle the random stopping time (number of observations of a state-action pair).
Indeed the naive way to obtain concentration inequalities for the empirical mean reward or transition probability at time t, with N_t(s,a) observations, is to apply a crude union bound on all possible values of N_t(s,a), that is t values, thus creating a factor t in the log terms (log(t\delta)).
However, improved techniques based on a peeling argument  (where the values of N_t(s,a) are localized) leads to better scaling essentially moving from a t term to a log(t) factor, up to loosing some constant factors. Such argument has been used in several analyses of bandit: see e.g. the analysis of KL-UCB by [Cappe et al.], or tools by Aurelien Garivier relating this indeed to the LIL. One can also see a similar type of argument in the PhD of Sebastien Bubeck, or in even earlier papers by Tze Leung Lai. More generally, the peeling technique is largely used is mathematical statistics, and is not novel.
Now, it is an easy exercise to show that the same peeling argument leads to concentration inequalities valid not for a single t, but uniformly over all t (actually for possibly unbounded random stopping time, and thus in particular for the random number of pulls) as well; thus I do not see much novelty either.
Let us go a little more in depth in the statements (e.g. G.1) regarding tightness of the bounds that is provided here, one may first question the use of a peeling with a geometric factor 2, that is arguably the most naive and sub-optimal one, and could be improved (see e.g. analysis of KL-UCB in [Cappe et al.]). Second, at least for sub-Gaussian random variables, one may instead resort to the Laplace method: this method leads to a \sqrt{t} term in the logarithm instead of a \log(t) and thus might be considered sub-optimal. However, the constant factors that we are used when applying the peeling argument, even after optimization of all terms (which is not done here), is still larger than the one we loose with the Laplace method. As a result, the bound resulting from the peeling argument term becomes only tighter than the one resulting from the Laplace method asymptotically for huge values of t (this actually makes sense as the regime of the LIL is also a quite asymptotic regime).
Thus, I feel that the discussion regarding the llnp(t) term is incomplete and misses an important block of the literature, once again.

E) The only possibly interesting part of the paper is Lemma D.1; but as mentioned, being able to enforce the constraints on the MDP is made easy by the fact that a finite horizon is considered. 

F) Minor point: the appendix seems too long for a NIPS paper.

DECISION.
The paper does not enough put in perspective its results with the existing literature, especially regarding the horizon H and the concentration inequalities.
Likewise, the scaling of the regret bounds is not discussed against other results.
These missing points tend to give the feeling that the authors oversell their contribution, and hide the more debatable aspects of their results.
This is also extremely misleading for the less expert readers that may not be able to see the loose aspects of this work.
Perhaps it is just that the paper was just written in a rush, but this is anyway bad practice. 

Points B,C,D should be improved/fixed.

Reject.

FINAL DECISION
I read the authors' rebuttal.
Their answer to point C) (scaling with H) should be included in the paper, as this is an important clarification of their result.

Even though the paper is restricted to the simpler finite horizon setting, I agree that this setting is also relevant and deserves attention.
I still believe that the selling point of the paper should not be section 2 and that much more credit should be given to previous works: indeed each separate idea presented in the paper existed before, so that the paper does not introduce a novel idea; what is novel however, is the combination of these.
We should thus judge how this combination is executed, and I consider this is well done.
Overall, given that the claims and proofs look correct (even though I haven't checked everything in detail), and that the paper is globally well executed, I accept to raise my score in order to meet a consensus.","		
This paper presents a novel framework for providing performance guarantees to reinforcement learning. I am somewhat familiar with some of the earlier results, but by no means an expert in this more theoretical corner of RL, so let me take some time to explain my understanding. Hopefully the authors might be able to use some of this to provide a bit more guidance for non-experts.

In theoretical RL, there have been a number of frameworks to provide guarantees. Several approaches have extended the notion of PAC learning to RL, leading (amongst others) to the notion of ""PAC-MDP"", which seems to correspond to what the current paper calls (eps,delta)-PAC. A method is PAC-MDP if *for any epsilon and delta*, there is a polynomial function F(S,A,1/eps,1/delta) that expresses the sample complexity of exploration (=the number of mistakes N_\epsilon) with prob. 1-delta.

This is nice, but many (all?) of the PAC-MDP algorithms actually take \epsilon as input: I.e., they do learning for a particular (s,a)-pair until they can guarantee (with high prob) that the accuracy is within \epsilon. Then they stop learning for this particular (s,a).

This paper proposes the ""\delta-uniform PAC framework"". In contrast to the above, a \delta-uniform PAC algorithm only takes \delta as its input, which means that it keeps learning for all (s,a) pairs, and which means that it will converge to an optimal policy. In addition, the authors show that if an algorithm has a uniform PAC bound that directly implies a tighter regret bound than what (eps,delta)-PAC would imply.





Questions:

-In how far is the proposed framework limited to the episodic setting with known horizon?

-If I correctly understand, the current approach still gives \delta to the algorithm. An obvious questions is whether this dependence can also be removed? 

-While not formalized, as in this paper, I think that many of the proposed PAC-MDP methods in fact support continue learning. E.g., Strehl&Littman'08 write ""important to note that for each state-action pair (s, a), MBIE uses only the first m experiences of (s, a)"" with the following footnote:
 ""This property of the algorithm is mainly enforced for our analysis. [...] However, in experiments we have found that setting m to be very large or infinite actually improves the performance of the algorithm when computation is ignored.""
This gives rise to 2 questions:
-in how far is the ""uniform PAC framework"" really novel? (don't we automatically get this when taking away the 'stop learning' switches from current PAC-MDP approaches?)
-in how far are the empirical results of the baselines included in the paper really representative of their best performance?

-The paper claims that the uniform PAC framework  is the first to formally connect PAC and regret. I have two reservations:
1) Theorem 2a seems to directly indicate a relation between the previous PAC framework and regret?
2) Strehl&Littman'08 prove a relation to ""average loss"" which seems very close to regret?



Details:

-""Algorithms with small regret may be arbitrarily bad infinitely often.""
->This is not clear to me. Seems that if they are arbitrarily bad (say, -infty) just once that would give a regret of -infty...?

-display formula below 54 mixes up 't' vs 'i'

-would be good to better emphasize the episodic setup, and the fact that 'T' means episode (not time step, this confused me)

-line 63, FPAC has wrong arguments and bracket problem.

-in the algorithm there are some strange remainders of comments...?

-line 163, what does 'rng' mean exactly?

-line 171 ""as in c) above"" - where do we find c) ? Is this Theorem 3c? (that does not seem to make sense?)



"
Expectation Propagation for t-Exponential Family Using q-Algebra,"Futoshi Futami, Issei Sato, Masashi Sugiyama",https://proceedings.neurips.cc/paper/2017/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html,"The paper discusses the t-exponential family, and derives an EP scheme for distributions in the t-exponential family. A q-algebra is presented, which allows for computation of the cavity distribution and thus EP. the method is demonstrated on a Bayes point machine and T-process classification.

The paper was a great read, really enjoyable, clearly written, fun and engaging. But it's not suitable for NIPS. The emprical validation is not very strong: the first experiment compares EP with ADF, and the result is exactly as expected, and does not really relate to the t-exponential family at all. The second experiment demonstrates that T-process classification is more robust to outliers than GP classification. But the same effect could be acheived using a slight modification to the likelihood in the GP case (see e.g. [1]).

There is some value in the work, and the paper is a very enjoyable read, but without a practical upside to the work it remains a novelty.

[1] http://mlg.eng.cam.ac.uk/pub/pdf/KimGha08.pdf","The authors proposed an expectation propagation algorithm that can work with distributions in the t-exponential family, a generalization of the exponential family that contains the t-distribution as a particular case. The proposed approach is based on using q-algebra operations to work with the pseudo additivity of t-exponential distributions. The gains of the proposed EP algorithm with respect to assumed density filtering are illustrated in experiments with a Bayes point machine. The authors also illustrate the proposed EP algorithms in experiments with a t-process for classification in the presence of outliers.

Quality

The proposed method seems technically sound, although I did not check the derivations in detail. The experiments illustrate that the proposed method works and it is useful.

Clarity

The paper is clearly written and easy to follow and understand. I only miss an explanation of how the authors compute the integrals in equation (33) in the experiments. I assume that they have to integrate likelihood factors with respect to Student t distributions. How is this done in practice?

Originality

The paper is original. Up to my knowledge, this is the first generalization of EP to work with t-exponential families.

Significance

The proposed method seems to be highly significant, extending the applicability of EP with analytic operations to a wider family of distributions, besides the exponential one.","The authors propose to use q-algebra to extend the EP method to t-exponential family. More precisely, based on the pseudo additivity of deformed exponential functions which is commonly observed in t-exponential family, the authors exploit a known concept in statistical physics called q-algebra and its properties such as q-product, q-division, and q-logarithm, in order to apply these properties to derive EP for t-exponential family. The paper is well written and easy to follow (although I haven't thoroughly checked their proofs for Theorems). I find the experiments section a bit unexciting because they showed known phenomena, which are (1) ADF depends on data permutation and EP doesn't, and (2) GP classification is more vulnerable to outliers than student-t process. But I would guess the authors chose to show these since there is no existing methods that enables EP to work for t-exponential family. 

--- after reading the author's rebuttal ---
Thanks the authors for their explanations. I keep my rating the same. "
Collaborative PAC Learning,"Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, Mingda Qiao",https://proceedings.neurips.cc/paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html,"**Edit** I have read the other reviews and rebuttal. I thank the authors for their feedback and clarification and my positive evaluation of this paper remains unchanged.

The authors give theoretical guarantees for collaborative learning with shared information under the PAC regime. They show that the sample complexity for (\epsilon,\delta)-PAC learning k classifiers for a shared problem (or a size k majority-voting ensemble) only needs to grow by a factor of approximately 1+log(k) (or 1+log^2(k)) rather than a factor of k as naive theory might predict.
The authors provide pseudocode for two algorithms treated by the theory. These were correct as far as I could see, though I didn't implement either.
The paper is mostly clearly written, and strikes a sensible balance between what is included here and what is in the supplementary material, and I enjoyed reading it. I checked the paper and sections A and B of the supplementary material quite carefully, and skimmed C and D. There were no obvious major errors I could find.
It would, of course, have been ideal to see experimental results corroborating the theory, but space is limited and the theory is the main contribution. Hopefully those will appear in a later extended version.

I have mostly minor comments:
Somewhere it should be highlighted explicitly that t is a placeholder for log_{8/7}(k) thus although t is indeed logarithmic in k it is also not *very* small compared to k unless k is large (few hundred say) and in this setting a very large k strikes me as unlikely. On the other hand some central steps of the proof involve union bounding over 2t terms, which is usually disastrous, but here the individual sample complexities for each classifier still only grow like log(2t/\delta), i.e. loglog(k) and you may like to highlight that as well.

I think it is also worth mentioning that the guarantees for the second algorithm depend on a large enough quorum of individual voters being right at the same time (w.h.p) thus the guarantee I think is for the whole ensemble and not for parts of it i.e. since we don't know which individual classifiers are right and which are not. In particular, in practice could early stopping or communications errors potentially invalidate the theoretical guarantees here?

Line 202 and elsewhere use the notation of line 200, i.e. n_{r-1,c} + \frac{1}{8}n_{r-1,c-1}, since it is much clearer.

Line 251 can you please clarify the notation? \mathcal{D}_{\orth} is a distribution over - what? - with relation to the \mathcal{D}_i? It seems like \mathcal{F}_k and \mathcal{D}_k have some dependency on one another?

Line 393 in supplementary should be \forall r < r' \in [t].

A comment - the theory for the second algorithm looks like it could be adapted to, say, a bagged ensemble with only very minor modifications. There is little in the way of good theory for ensemble classification, in particular when is an ensemble of ""weak"" classifiers better than a single ""strong"" classifier is not well understood.","SUMMARY

This paper considers a generalization of the PAC learning model in which there are k learners, each receiving i.i.d. data from a different distribution D_i but labeled by a common target function f*. The goal is either:

-- (personalized) obtain k functions f_1, ..., f_k that do well on distributions D_1, ..., D_k respectively

or 

-- (centralized) obtain one function f that does well on all k distributions.

It is assumed that we are in the ""realizable"" case, i.e. a class F is known that contains f*.

Suppose that for learning an epsilon-good function, the normal PAC bound (for just one learner) is m. The naive approach to solving either of the two generalized problems (again, with error at most epsilon) would result in a  total of O(km) samples being drawn, basically m per learner.

Instead, the authors present two simple and fairly clever schemes that solve the personalized and centralized problems with a total of just O(m polylog(k)) samples.

COMMENTS

This is a very well written paper that gives lots of intuition about the problem and the proof techniques.

The specific problems considered here have some of the flavor of domain adaptation, and should be of fairly broad interest, at least at a conceptual level. Still, it would have been nice if the authors had given some more compelling real-world examples as motivation, some for the personalized case and some for the centralized case.
","This paper studies the problem in which k players have examples labeled by the 
same function but with points wrawn from k different distributions. The goal is to 
use these examples to (a) obtain a classifier that has low error on all k distributions 
(b) obtain a personalized classifier for each of the players that has low error. The 
paper shows simple algorithms (or more accurately reductions to the usual PAC 
learning) that show how to achieve this with log^2 k and log k factor overheads 
respectively. This improves the naive implementation that would require factor k 
more samples. The problem with just combining the samples is that the player that 
has a harder distribution might get all the errors.  This work shows that log k factor 
overhead is unavoidable and also that achieving this upper bound requires an 
algorithm that uses a larger hypothesis class than that used to generate the labels. 
There is also an extension to non-realizable setting althougt only for tiny noise rate 
that allows the same analysis to go through. 

The problem setting is fairly natural and has been studied before (e.g. Balcan,Blum,Fine,Mansour 2012 although in the context of reducing communication rather than ensuring the largest error).
The question is also fairly natural and this work gives some basic insights into it. The slightly more involved centralized case uses a boosting like algorithm. It is unclear why 
the authors did not directly use a reweighting scheme from boosting (unless I'm missing something this would be both simpler and give log k instead of log^2 k dependence).

Overall the paper studies a clean theoretical model of collaborative learning and 
gives simple insights related to ensuring that all players have a maximum error 
guarantee. A weaker side of the paper is that the solutions are entirely 
straightforward given the problem and there is no treatment of the potentially more 
interesting non-realizable setting.

>> Thanks for the clarification about the log factors when using boosting. "
Polynomial time algorithms for dual volume sampling,"Chengtao Li, Stefanie Jegelka, Suvrit Sra",https://proceedings.neurips.cc/paper/2017/hash/18bb68e2b38e4a8ce7cf4f6b2625768c-Abstract.html,"The authors study the problem of sampling subsets of points proportionally to their volume-span. The main contributions are; (a) polynomial time sampling algorithms (using marginals and involved matrix manipulations) and (b) propose a MCMC sampler for volume sampling via a connection between dual volume sampling and the Strongly Rayleigh property of measures.

The paper is well written and the main contributions are well explained. The references to prior work is accurate (to the best of my knowledge). In my opinion, the connection between dual volume sampling and Strongly Rayleigh measures is interesting.

I have a few questions that I would like to share with the authors.

Major comments:
1) In L158-162, you state that determinantal point processes are known to be Strongly Rayleigh measures. If so, is Theorem 6 well known or it is part of your contributions here? Please clarify it.
2) Algorithm 1, ""while not mixed do"". Is it possible to have a stopping criteria here? Can you decide when to stop? If not, I would suggest to have an input parameter ""number of iterations""
3) Experiments: Legends on first 2 plots are missing
4) Experiments: Please replot the ""time-error trade-off"" plot. Use different scale.

Minor comments:
1) L110: Spell out ""Thm. 2."" to Theorem
","Summary: The authors present an algorithm for Dual Volume Sampling (DVS). This is an approach proposed by Avron and Boutsidis for experimental design and picks a subset S of k columns of a fat matrix A with probability proportional to det(A_S A_S') where A_S is the matrix restricted to columns in S. Avron and Boutsidis also showed that such a sampling provides a good approximation for E and A optimal experimental designs.

Their (first) algorithm samples columns iteratively. The authors derive explicit yet efficiently computable formulae for the marginal probabilities at each step. 

Next, the authors prove that the DVS distribution satisfies a powerful property known as the ""Strongly Rayleigh"" property, which almost immediately implies a fast mixing Markov chain for DVS sampling. 

Finally, the authors present experimental results for experiment design in regression problems. These algorithms demonstrate a clear advantage in terms of both running time and the error achieved.

Opinion: I think the authors make a fundamental contribution to understanding DVS and the first algorithms for efficiently sampling from them. The theoretical bounds on the algorithm are not very impressive, but the experiments demonstrate that for a good range of parameters, they offer a strong time advantage over several other methods used in practice.
Overall, I recommend acceptance.

Small typos: 
181: ""an diagonal"" typo
183: S <= T should be T <= S

Edit : I have read the authors' feedback, and the review stands unchanged.","The paper studies efficient algorithms for sampling from a
determinantal distribution that the authors call ""dual volume
sampling"", for selecting a subset of columns from a matrix.
The main results in the paper are two sampling algorithms for
selecting k columns from an n x m matrix (n<=k<=m):
- an exact sampling procedure with time complexity O(k m^4)
- an approximate sampling, with time complexity O(k^3 n^2 m) (ignoring
log terms). 
The approximate sampling algorithm makes use of Strongly Rayleigh
measures, a technique previously used for approximately sampling from
closely related determinantal distributions, like Determinantal Point
Processes. Compared to the exact sampling, it offers a better
dependence on m, at the cost of the accuracy of sampling,
and a worse dependence on k and n. The authors perform some
preliminary experiments comparing their method to other 
subset selection techniques for linear regression. It is worth noting
that the plots are not very informative in comparing dual volume
sampling to leverage score sampling which is to my mind the most
relevant baseline.  

This determinantal distribution was previously discussed and motivated
in [1], who referred to it just as ""volume sampling"", and suggested
potential applications in linear regression, experimental design,
clustering, etc., however without providing a polynomial time
algorithm, except for the case of k=n.

The paper addresses an important topic that is relevant to
the NIPS community. The results are interesting, however the
contribution is somewhat limited. To my knowledge, the algorithms (in
particular, the exact sampling algorithm) are much slower compared
to Leverage Score sampling, and the authors did not make a compelling
argument for why DVS is better suited for the task of experimental
design. 

[1] Avron and Boutsidis. Faster Subset Selection For Matrices and
Applications, 2013. https://arxiv.org/abs/1201.0127
"
Premise Selection for Theorem Proving by Deep Graph Embedding,"Mingzhe Wang, Yihe Tang, Jian Wang, Jia Deng",https://proceedings.neurips.cc/paper/2017/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html,"This paper applies deep learning to the problem of premise selection in HOL theorem proving.  The  paper is clear (with some issues listed below) and sophisticated in deep learning methods with an appropriate experimental methodology and thorough discussion of related work.  However, the main weakness in my opinion is the results.  While an improvement from 83% percent accuracy to 91% is reported, the results still exhibit the bizarre property that the performance is the same whether or not one uses the information of what the system is trying to prove.  There is no discussion of this bizarre aspect of the experiments.  Is the system only able to identify domain-independent lemmas that are always useful --- like basic properties of sets and functions?

I also have some technical questions.  The symbols (words) are initialized with one-hot vectors.  Word embeddings for natural language applications typically have hundreds of dimensions.  It would be nice to know how many distinct constants the corpus contains and hence what is the dimension of the embedding vectors being used.

Also I found the discussion of intra-graph batching interesting but would like some additional information.  It seems that intra-example batching would apply to vision CNNs as well as each filter of each layer is repeatedly applied across a single convolution.  My expectation is that this is not typically done because it is not supported by frameworks such as tensor flow where BN is a layer and minibatching is standardized to be be inter-instance.  Did the authors implement novel layers or framework features?

Finally, at an intuitive level one would expect relevance to be driven by types.  To prove a graph theory theorem one should use graph theory lemmas where ""graph"" is a datatype.  Was there any attempt to take this intuition into account?","The paper addresses the problem of premise selection for theorem proving. The objective in this line of work is to learn a vector representation for the formulas. This vector representation is then used as the input to a downstream machine learning model performing premise selection. 

The contributions of the paper are: (1) a representation of formulas as graph; (2) learning vector representations for the nodes and ultimately for the formula graphs; and (3) show empirically that this representation is superior to the state of the art on theorem proving data sets. 


Pros

* The paper is very well written. A pleasure to read and despite its technical content highly accessible. 

* The experimental results show a significant improvement over the state of the art. 

* The authors discuss all related work I'm aware of and do it in a fair and comprehensive way.

Cons

* There has been previous work on this topic. The novelty here is the graph representation and the way in which node embeddings are learned. The former is a nice idea. The latter is more interesting from a ML point of view but it would have made the paper much stronger if the authors had compared the averaging of neighboring node embeddings approach to other node embedding approaches such as DeepWalk.

This has been added after the rebuttal:

The main weakness of the paper is in my opinion the (lack of) novelty of the proposed approach. The difference to previous work is the graph representation of the formulas. To represent formulas as graphs is not new. Now, instead of learning a vector representation for the formulas directly, a vector representation of the graphs is learned. The downstream architectures are also more or less what has been done in previous work. It's a nice idea for premise selection that performs better than the SOTA by combining existing ideas (graph representation of formulas + graph vector representation learning). Since there's nothing really new here I would have liked more experimental results and a comparison of different graph embedding methods (many of these are cited by the authors). 


Minor comments:

Line 154: ""To initialize the embedding for each node, we use the one-hot vector that represents the name of the node."" You probably mean that you initialize the embeddings randomly? ","This paper proposes a deep learning approach to learn representation
of higher-order logic formulae in the context of theorem
proving. Formulae are first converted into directed graphs, from which
the embeddings are derived. The technique is reminiscent of recursive
neural networks (RNNs) but to the best of my knowledge RNNs have never
be applied to higher order logical formulae before.

The paper is well written and the proposed approach well
motivated. Experiments show the effectiveness of this deep embedding
technique.

I have some minor comments:

1. The paper that first introduced RNNs had the goal of learning
   representations of logical terms. It is so closely related to this
   work that it should be cited:
   
   C. Goller and A. K{\""u}chler. Learning task-dependent distributed
   structure-representations by backpropagation through structure.  In
   IEEE International Conference on Neural Networks, pages 347--352,
   1996.

2. It seems to me that the formula->graph transformation always yield
   acyclic graphs but this should be perhaps said explicitly (or give
   counterexamples).

3. I have problems with the example at the beginning of 3.1. Functions
   normally return objects, not predicates, therefore f(x,c) /\ P(f)
   does not seem to be well formed: what is the conjunction of a
   Boolean value --- P(f) --- and an object??
   Perhaps it should be something like
   \forall f \exists x Q(f(x,c)) /\ P(f)

4. You talk about higher-order logic but it seems you are just using a
   subset of second-order logic. Can you quantify over predicates?  A
   brief synopsis of the syntax and semantics of the logic assumed in
   the paper would be helpful.

5. Second-order logic is undecidable, is there any associated impact
   on theory behind your work? Related to this, Siegelman & Sontag
   showed that recurrent networks with arbitrary precision weights
   have super-Turing computational power, are there any links between
   these results and your work?
"
Differentiable Learning of Submodular Models,"Josip Djolonga, Andreas Krause",https://proceedings.neurips.cc/paper/2017/hash/192fc044e74dffea144f9ac5dc9f3395-Abstract.html,"This paper proposes a way to differentiate the process of submodular function minimization thus enabling to use these functionals as layers in neural networks. The key insight of the paper consists in the usage of the interpretation of discrete optimization of submodular functions as continuous optimization. As a concrete example the paper studies the CRF for image segmentation and creates and the graphcut layer. This layer is evaluated on the Weizmann dataset for horse segmentation and is reported to bring some improvements.

I generally like the paper very much, find the description of the method clear enough. In particular, I liked the short introduction into submodular functions and their connection to min-norm-point.

I have some comments that might allow to increase the impact of the paper. My comments mostly cover experimental evaluation and related works.
	1. The experimental evaluation presented in Section 6 is a bit disappointing to a practitioner. The results are clearly far below state-of-the-art in image segmentation. To demonstrate the full potential of the new layer, I would recommend to plug the new layer into one of the state-of-the-art systems for image segmentation such as DeepLab (Chen et al., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs, 2016). I understand that the graphcut layer is applicable only to binary problems, but it would be very interesting to try it e.g. on one class of PASCAL VOC.
	2. In particular, there is an extension of DeepLab (Chandra and Kokkinos, Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs, ECCV 2016 ) that puts a Gaussian CRF on top of the potentials learned by CNNs. They make the inference layer differentiable by connection using the fact that inference in Gaussian CRFs reduces to solving systems of linear equations. It would be very interesting to see how the graph layer compares against the Gaussian CRF layer.
	3. It would make sense to mention another line of works that incorporate discrete optimization into networks. In particular, if the final loss can be computed directly from the results of discrete optimization (for example, when the whole system is trained with a structured SVM objective: Jaderberg et al., Deep structured output learning for unconstrained text recognition, ICLR 2015; Vu et al., Context-aware CNNs for person head detection, ICCV 2015). Comparison to this approach can also strengthen the paper.
	4. As mentioned in line 65 a popular way of embedding algorithms into neural networks consists in unrolling a fixed number of steps of iterative algorithms into layers of neural network. This paper uses one of such iterative algorithms (a total variation solver) to do inference, so it would make sense to simply backprop through several iterations of it. Again, comparison to this approach would strengthen the paper.
	5. Are there any other examples (more rich than graphcuts) of the cases when minimization of submodular functions can be plugged into neural networks? Even naming such cases together with the appropriate submodular solvers would be very valuable.
	6. In terms of the potentials learned by the graphcut layer, it would be very interesting to visualize what the network has learned and, e.g., compare those with the standard potentials based on the gradient of the image.

Minor comments:
	- Line 130 says ""the Lovasz extension is linear in O"", but O is a set and it is clear what the phrase means.
	- Line 135. [27] looks like the wrong reference
	- Line 143. The definition of optimal partition is never given, so it remains unclear what it is.
	- Line 295 says that only a small fraction of labelled pixels was used for training. It is not clear why this is done.
	
","In this paper, the authors discuss the implement of discrete optimization on modern machine learning models. They provide an easily computable approximation to the Jacobian complemented with a complete theoretical analysis. Overall, this is a good paper and followings are detail comments.

1 In abstract, the authors said they can continuously relax the output without sacrificing guarantees.  What is the original guarantees? 

2 In line 33, it is better to say A^*\in argmax than A^* = \argmax since \argmax is actually a set.

3 Do you have an efficient way to test a submodular function is separable or not?

4 In your experiments, do you have a running time comparison of adding the graph cut layer?
","Summary: The paper develops a method to approximate the Jacobian of the isotonic regression and the min-norm problem. The paper gives the conditions under which the approximation is correct and shows how to compute the directional derivatives exactly in polynomial time.

Overall evaluation: I am not familiar with the topic in the paper and the relevant literature to comment on the significance of the results. However, this seems to be a good theory paper.

Some specific comments:
1. I think the paper should include more details on the application of the results to the log-supermodular models and the end-to-end optimization problem in Equation (3). The current manuscript only mentions them briefly in Sections 1 and 2.
2. The paper should also include some examples where Theorem 3 holds.
3. I think more experiments with other benchmark image segmentation tasks would be helpful. How is the CNN+GC compared to the current state-of-the-art on those benchmark datasets?
4. The term inside the double brackets [[ . ]] in Equation (1) is confusing to me. Is it missing the cardinality function? On the other hand, F(A) on line 99 does not need the cardinality function."
YASS: Yet Another Spike Sorter,"Jin Hyung Lee, David E. Carlson, Hooshmand Shokri Razaghi, Weichi Yao, Georges A. Goetz, Espen Hagen, Eleanor Batty, E.J. Chichilnisky, Gaute T. Einevoll, Liam Paninski",https://proceedings.neurips.cc/paper/2017/hash/1943102704f8f8f3302c2b730728e023-Abstract.html,"[UPDATE AFTER AUTHOR RESPONSE]

The authors' response confirms my rating – it's a valuable paper. I'm optimistic that they will address mine and the other reviewers' concerns in their revision.


[ORIGINAL REVIEW]

The paper describes YASS: Yet Another Spike Sorter, a well-engineered pipeline for sorting multi-electrode array (MEA) recordings. The approach is sound, combining many state-of-the-art approaches into a complete pipeline. The paper is well written, follows a clear structure and provides convincing evidence that the work indeed advances the state of the art for sorting retinal MEA data. 

While the paper is already a valuable contribution as is, I think it has much greater potential if the authors address some of the issues detailed below. In brief, my main concerns are

(1) the lack of publicly available code,
(2) the poor description of the neural network detection method,
(3) issues with applying the pipeline to cortical data.


Details:

(1) Code is not available (or at least the manuscript does not provide a URL). Since spike sorting is at this point mainly an engineering problem (but a non-trivial one), a mere description of the approach is only half as valuable as the actual code implementing the pipeline. Thus, I strongly encourage the authors to go the extra mile and make the code available.


(2) Neural network spike detection. This part seems to be the only truly innovative one (all other components of the pipeline have been described/used before). However, it remains unclear to me how the authors generated their training data. Section C.2 describes different ways of generating training data, but it is not clear which one (or which combination) the authors use.

(a) Using pre-existing sorts. 

First, most labs do *not* have existing, properly sorted data available when moving to dense MEAs, because they do not have a pipeline for sorting such array data and – as the authors point out – existing methods do not scale properly. 

Second, it is not clear to me how existing sorts should help training a more robust neural network for detection. Did the authors inspect every single waveform snippet and labeled it as clean or not? If not, based on what algorithm did they decide which waveform snippets in the training data are clean? Why do they need to train a neural network instead of just using this algorithm? How do they deal with misalignments, which create label noise?

If using pre-existing sorts is what the authors did, they need to provide more information on how exactly they did it and why it works. In the current form, their work cannot be reproduced.

(b) Generating synthetic training data by superimposing waveform templates on background noise. This could be a reasonable approach. Is it used for data augmentation or not at all and just described as a potential alternative? What is the evidence that this approach is useful? The synthetic data may not be representative of real recordings.


(3) Generalization to cortical data. I am quite confident that the pipeline works well for retinal data, but I doubt that it will do so for cortical data (some arguments below). I think this limitation needs to be discussed and acknowledged more explicitly (abstract, intro, conclusions).

(a) In cortical recordings, waveform drift is a serious issue that arises in pretty much all non-chronic recordings (and chronically working high-density MEAs are still to be demonstrated). Thus, modeling drift is absolutely crucial for recordings that last longer than a few minutes.

(b) Getting good training data for the NN detection is more difficult. Good ground truth (or well validated data such as described in appendix I) is not available and generating synthetic data as described in C.2 is not necessarily realistic, since background noise is caused by spikes as well and neurons often fire in highly correlated manners (thus rendering the approach of overlaying templates on spike-free noise problematic).



Minor comments:

- Fig. 3 bottom panel: Y axis is strange. Does 1^-x mean 10^-x? Also, it exaggerates tiny accuracy differences between 0.99 and 0.999, where both methods are essentially perfect.

- The authors use spatially whitened data (according to section 2.1), but I did not find a description of the spatial whitening procedure in the manuscript or supplement.","[UPDATE AFTER AUTHOR RESPONSE]

I have upgraded by assessment and confidence about this paper. The upgrade is due to the authors' commitment to including a performance breakdown in the paper, should it be accepted. I would also urge the author's to explicitly mention that their method has supervised stages, whereas some of the comparison methods (i.e., Kilosort) are unsupervised. I agree with the authors' response that if prior information is available, it should be utilized. However, since this is an important methodological detail relevant to the comparisons presented, it must be made explicit to facilitate the community's interpretation of the results.

[ORIGINAL REVIEW]


The authors develop a multistage spike sorter, cutely coined YASS, which follows the classical pipeline of spike detection, feature extraction, and clustering. The most appealing aspect of this work is that the developed YASS system outperforms relative to several state-of-the-art methods. Despite this laudable performance, this work raises several concerns, as enumerated below.

Major Concerns: 

1. Yass' impressive performance comes at the cost of requiring hand-labeled data (for the detection step), which is not required for all methods to which Yass is compared. In this sense, the comparisons provided are not as apples-to-apples. Requiring the experimenter to provide such hand-labeled data represents an additional burden of effort which is not required by completely unsupervised methods.

2. Many of the components in the Yass pipeline are derivative of previous work. While there are certainly novel components (possibly enough to merit publication at NIPS), the authors do not provide a performance breakdown to demonstrate that these novel contributions are required to achieve the stated levels of performance. For example, it might be the case that the overall performance is due to the particular selection of previously developed components, rather than due to the inclusion of newly developed ones.

Minor concerns

3. If my read was correct, the proposed method discards signals that appear on multiple electrodes, keeping only the waveform signal with the largest amplitude (lines 138-140). This is in contrast to the approach of Kilosort, for example. This seems disadvantageous, and raises another flag that the comparison may not be apples-to-apples against Kilosort. If the experimental data did not contain overlapping signals across channels, Yass' performance would not suffer due to this design choice, and Kilosort's performance would not shine in comparison since one of it's main features would not be in play. If my understanding is not correct here, this text should be clarified. Regardless, the authors should clarify the precise application setting with regard to overlapping signals across channels.

4. The writing style is rather opaque. I would encourage the authors to clarify their presentation by boiling down the critical mathematical and algorithmic details (be they words or equations), and including them in the main paper. At present, almost none of these critical details are available without diving into supplements. This separation of the details presents quite a burden to the reader and the reviewer.","This paper addresses the important contemporary problem of spike detection and sorting on multi-electrode arrays (MEA).   Efficient algorithms for this problem are critical given the increased number such experiments and the number of simultaneously recorded neurons.  The ability to scale into the regimes of 10^4 to 10^6 electrodes is challenging and requires sophisticated analysis methods.  

The authors have correctly identified key attributes of robustness, scalability, modularity and the use of prior information in the development of powerful algorithms.  The paper employs a substantial amount of methods and machinery that the authors are well familiar with and understand the problem space well.

Algorithm 1 placed early in the paper is not particularly helpful or readable, and doesn’t really offer anything that the text description doesn’t provide.  The overview section is important to approach first but the writing here is not as clear as it might be and moves over a wide range of topics very quickly.

It is very difficult to determine the real veracity of the algorithms proposed.  In particular the neural net training stage is compelling but difficult to penetrate how it would really work.  Having training data, potentially different for each labs use, may be problematic and it is difficult to gauge how well this will really work, although the authors provide some good simulations.

In summary, this is potentially an interesting approach although the complexity of the method, its seemingly ad hoc nature and its comparatively not transparent presentation make it less attractive.  The approach is interesting although very complicated with technical justification absent from the main paper or relegated to the Supplementary materials, which are substantial.


"
Variational Laws of Visual Attention for Dynamic Scenes,"Dario Zanca, Marco Gori",https://proceedings.neurips.cc/paper/2017/hash/194cf6c2de8e00c05fcf16c498adc7bf-Abstract.html,"This computational neuroscience paper proposes a bottom-up visual feature saliency model combined modified by dynamical systems modelling that aims at capturing the dynamics of scanpaths. I enjoyed reading this paper, it has a set of nice of ideas
in defining ""affordances"" (curiosity, brightness invariance) that drive the scan-path, as well as the idea of the Least Action Principle. These ""affordances"" are albeit perhaps a bit adhoc postulated and not further motivated.
In the context of the Least Action Principle and its derivation in this context it would have been interesting to related it to actual work
in eye movement research such as past and recent work by the Wolpert lab. While the lack of biological motivation is in itself alone a minor issue in the era of  benchmark-busting deep learning models of (static) visual salience, the issue with this model is the mixed performance it achieves with respect to these models is poor. Given that the model claims to capture biological eye movement dynamics, it would have been intersting to see in the main paper actual figures characterising the generated scan paths, not only in terms of their overall statics at matching eye movem
","Variational Laws of Visual Attention for Dynamic Scenes

The authors investigate what locations in static and dynamic images tend to be attended to by humans. They derive a model by first defining three basic principles for visual attention (defined as an energy function to be minimized by the movements of the eye): (1) Eye movements are constrained by a harmonic oscillator at the borders of the image within a limited-sized retina. (2) a “curiosity driven principle” highlighting the regions with large changes in brightness in both a fine and blurred version of the image, and (3) brightness invariance, which increases as a function of changes in brightness. Using a cost function derived from these three functions, the authors derive differential equations for predicting the eye movements across static or dynamic images (depending on the starting location and initial velocity). The authors evaluate their technique quantitatively on data sets of static and dynamic scenes coupled with human eye movements. They demonstrate that their method performs comparable to the state-of-the-art. 

Formal definitions of saliency and modeling eye movements are critical issues in computational vision and cognitive science. Psychologists have long been plagued by vague definitions of saliency, and the authors propose a novel and innovative model (as far as I am aware) that could aid the development of better understanding how what makes something salient and a formal model for eye movements (within the bottom-up tradition). Although it is not necessarily state-of-the-art on every metric for every data set, it performs well and provides a refreshingly different perspective on the problem. 

Unfortunately, some of what I wrote above is based on conjecture as the paper is poorly written and hard to follow. I recommend the authors have others proofread the paper and expand on abbreviations (both within the equations and also those used in Section 3). I would recommend they move the 2 page Appendix to supplementary material and use those extra pages to define each variable and function used (even if it is a convention within your own field – the NIPS audience comes from many different disciplines and some will have trouble following the mathematics otherwise). 

As a psychologist, I would have liked the authors to connect their work to some of the psychological literature on eye movements as optimal steps for gathering information. See for example, Najemnik, J., & Geisler, W. S. (2008). Eye movement statistics in humans are consistent with an optimal search strategy. Journal of Vision, 8(3), 1-14. There are other relevant articles (particularly from Geisler’s lab, but that should give the authors a pointer to follow into that literature). I would be interested to see a discussion of how their approach compares to their findings (e.g., are they restatements of similar models or provide independent information that could be integrated to produce a better model?)
","The paper proposes a new approach to the study of eye movements.  The authors correctly summarize the current state of the art (as far as I understand it, but I admit from the outset I am not an expert in perception or eye movements).  Basically these approaches are based on a saliency map that is defined over an image and fixations points are selected in a optimization/maximization approach.  This model (EYMOL) works according to a different assumption and instead is defined directly on trajectories of gaze according to a ""Least Action Principle.""  The details of some of the math were beyond my ability to evaluate because I don't have the necessary physics background (particularly the extensive appendix).  However, even still I was able to understand the key elements of the approach and how it differs from past models.  The model is helpfully applied to real data set of eye-movements and is compared against a variety of alternative models.  While it doesn't dominate on every measure, the results are favorable for the new approach and demonstrate that it has some validity.

I think the paper might be of interest to some people in the vision science community (e.g., attendees of VSS).  I'm not quite as convinced this makes a great contribution to NIPS, however I think that should be evaluated against other reviewer's opinions who are more expert.  I think it likely advances a new and interesting theory that could inspire further empirical research, and so has both novelty and merit for perception sciences."
On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm,"Masaaki Imaizumi, Takanori Maehara, Kohei Hayashi",https://proceedings.neurips.cc/paper/2017/hash/1b5230e3ea6d7123847ad55a1e06fffd-Abstract.html,"The authors propose two algorithms for fitting low-rank tensor-train (TT) decompositions using the Schatten TT norm as the low-rank inducing regularizer. The first (TT-ADMM) uses ADMM with the optimization variable being the tensor itself--- this has exponential space and time complexities. The second (TT-RALS) reduces these complexities to polynomial by optimizing in an alternating fashion (using ADMM) over the factors in a TT factorization of the tensor, and using randomized dimensionality reduction to estimate the Schatten TT norm.

Theoretical analysis is provided to argue that: under some regularity conditions (incoherence) the matrix estimate obtained by TT-ADMM is consistent, and if the initial guess is close enough to the optimal point, TT-RALS is also consistent. Quantitative rates of convergence are provided. I verified the correctness of the first claim. Experimental results are given comparing the performance of TT-ADMM and TT-RALS to other TT low-rank factorization methods for matrix recovery. These show that TT-RALS is much more scalable than the other algorithms.

TT factorizations are useful and preferable over other forms of tensor factorizations precisely because they allow you to express high-order tensors with a tractable number of parameters, but the challenge is in learning these factorizations efficiently. TT-RALS is a very useful algorithm to that end, and the theoretical guarantees given in this paper are worth dissemination.

-I suggest the authors change the name from TT-RALS since that suggest alternating least squares, which is not what the algorithm does.
- TT-ADMM is orders of magnitude more accurate on the synthetic data than TT-RALS, which is not surprising ... but on the example with real data, TT-RALS is the most accurate method, consistently beating TT-ADMM. Please comment on this counterintuitive result (is this due, e.g., to termination conditions?)
- Please do another round of careful proof reading: there are several grammatically incorrect phrases like ""the all"" instead of ""all the"".","This paper looks at the tensor train (TT) decomposition as an efficient storage method for tensors with statistical guarantees.
Traditional methods for tensor decomposition include Tucker decomposition [29] and CP decomposition. The advantage of TT decomposition is that it can avoid the exponential space complexity of Tucker decomposition. However, its statistical performance is less understood, which the authors attempt to address in this paper.
In addition, the authors provide an efficient alternative to existing TT algorithms.

It’s interesting to note that Tucker and TT decompositions have complementary bottleneck - One has space complexity issues and the second has computational complexity issues.

The authors look at the problem of tensor completion which is an extension of the matrix completion problem that has been extensively researched in the literature.
In matrix completion problem, the standard assumption made is that the matrix is low-rank so that the problem becomes well-posed. This assumption is also observed to be the case in practical applications such as recommender system.
The authors make a similar observation for the tensor to be estimated (i.e. a low-rank tensor). Just as nuclear norm/schatten-p norm is used a relaxation to the rank matrix, the authors propose the Schatten TT norm as a convex relaxation of the TT rank of a tensor.


Comments:
- The authors mention that the TT rank is a tuple in section 2, while mentioning later in section 6 that it can be described by a single parameter, making the definition confusing.
- The problem formulation (4), the algorithmic approach (section 3.1) all mimick the ideas in matrix completion literature. The innovation is more to do with the appropriate representation of tensors while extending these ideas from the matrix completion literature.
- Even in the matrix factorization literature, the scalable methods are the ones that already assume a low-rank factor structure for the matrix to be completed to save space and computation. For tensors, it is all the more imperative to assume a low-rank factor structure. The authors do this in (6) to save space and time.
- Theorem 1 is quite interesting and provides a more tractable surrogate for the TT schatten norm.
- The statistical results are a new contribution for tensor completion, although the assumption of incoherence (Assumption 2) and subsequent proofs are by now very standard in the M-estimation literature.
- The authors use an ADMM algorithm again for solving (6) in section 4.2. It is not clear this would have a better advantage as compared to just the simpler alternating minimization over the TT factors of the tensor. It is not clear if the convergence of ADMM to a local minima for non-convex problems with multiple factors is proven in the literature. The authors don’t cite a paper on this either. On the other hand, it is easy to see that the simpler alternating minimization (keeping other factors fixed)
converges to a local optimum or a critical point. The authors don’t discuss any of these details in 4.2. 
- The number of samples needed for statistical convergence is in the worst-case the the product of the dimensions of the tensor. Perhaps a better result would be to show that the sample complexity is proportional to the true TT rank of the tensor? The authors don’t mention any discussion on the lower-bound on the sample complexity in the paper, making the goodness of the sample complexity bound in Theorem 5 questionable.

Overall, the paper presents an interesting investigation of the TT decomposition for tensors and uses it in the context of tensor completion with statistical guarantees. It is not clear if the algorithm used for alternating optimization (ADMM) is actually  guaranteed to converge though it might work in practice. Also, the authors don’t discuss the goodness (what’s a good lower bound?) of the sample complexity bounds derived for their algorithm for tensor completion. The approach to modeling, algorithm development, scalability and statistical convergence mimicks that taken for matrix completion - However, the authors do a good job of combining it all together and putting their approach in perspective with respective to existing algorithms in the literature.
","This paper studies tensor completion problem under tensor-train(TT) format. It offers scalable algorithms as well as theoretical guarantees on recovery performance. However, I wish to hear some justifications from the authors on below problems. Currently, its quality is not good enough for acceptance to NIPS.

1). How about the comparison on recovery guarantee with other tensor formats? Such as Turker/CP decomposition, overlapping/latent tensor nuclear norm and t-SVD. From my point, with so many tensor formats at hand, it is not good to limit the scope of the submission only to TT format. Since the authors claim they are the first to give the statistical guarantee, thus I wish to see comparisons on statistical guarantees offered by different tensor formats.

2). While the paper targets at large scale problems, it can not really handle large tensor with lots of missing values. This is summarised in Table 1, where both time and space depend on the size of the tensor rather than the number of observations. It is not a problem for matrix completion problems [a], not a problem for tensor completion using Turker decomposition [d] or latent nuclear norm [e].

3). A better method than random projection provided in Section 4.1 is power method. It has been successfully applied to matrix completion problem with nuclear norm regularisation [b]. So, Theorem 1 is not useful here.

4). There is no convergence guarantee for TT-RALS from optimisation's perspective. It is hard to check Assumption 2 and 4 in practice, this may suggestion that TT-RALS algorithm is not useful. 

5). Besides, we can drop the TT-norm and directly optimise each factor in TT-format, this leads to a coordinate descent/alternative minimization algorithm (see [c,6,30]). For such algorithms, we always have convergence guarantee. The authors argue that for those methods, determination of TT-rank is a problem. However, the proposed method suffers similar problems, as they need to determine D and lambda_i. The author should include such algorithms in experiments for a comparison. 

6). I would like to see a comparison of proposed method and other state-of-the-art methods. e.g. [d,e], on 3-order tensor. Then, it is more clear for readers to understand the difference on recovery performance induced by different tensor format. It is more interesting to see TT-format with proposed method beat those state-of-the-arts.

7). Could the authors perform other experiments in Section 7.2? The results in Table 2 suggests higher order tensor is not useful, the performance significantly deteriorates as the tensor order gets higher.

8). The authors may want to explain more on the novelty inside the proof.

Reference
[a]. Spectral Regularization Algorithms for Learning Large Incomplete Matrices. JMLR 2010
[b]. Nuclear Norm Minimization via Active Subspace Selection. ICML 2014
[c]. Tensorizing Neural Networks. NIPS 2015
[d]. Low-rank tensor completion: A Riemannian manifold preconditioning approach. ICML 2016
[e]. Efficient Sparse Low-Rank Tensor Completion using the Frank-Wolfe Algorithm. AAAI 2017"
EX2: Exploration with Exemplar Models for Deep Reinforcement Learning,"Justin Fu, John Co-Reyes, Sergey Levine",https://proceedings.neurips.cc/paper/2017/hash/1baff70e2669e8376347efd3a874a341-Abstract.html,"I really enjoyed the paper. It tackles the main limitation of the count-based exploration with a novel idea, i.e., by making it possible to get the density without generative modeling. This is done by implicitly modeling the observation density from a discriminator. It is based on the intuition that if the current state has not been visited in the past, it should be easy for a discriminator to distinguish it from other states visited in the past (and thus getting high reward). To make this idea based on the exemplar model practical, the authors propose to learn the noise distribution in latent space and discriminator sharing. 

Regarding the possibility of the powerful discriminator memorizing perfectly (and thus classifying perfectly in continuous domain), it would be interesting to discuss in relation to the ""rethinking generalization"" paper (ICLR17).

Discussion and experiment on the storage complexity would be helpful, e.g., in the case where the replay buffer size is limited. A related work by D. Pathak et. al. (ICML 17) which takes the prediction error in the latent space as the exploration reward seems to suffer less from the storage complexity.

It would also be interesting to see if the authors provide some coverage heatmap for VizDoom experiment.

","This paper presents EX2, a method for novelty-based exploration. Unlike previous methods, EX2 does not require prediction of future observations, and instead relies on a discriminative model to classify which states are novel (exemplars) and then provide reward bonuses to the visitation of novel states. Results show that EX2 outperforms competing novelty-based exploration methods on a VizDoom navigation task and performs as well as others on a variety of simpler, less graphically intense tasks.

I believe this paper makes a novel algorithmic contribution to the body of intelligent exploration literature and has solid experimental results to back up the claims. The paper is well written, experimentally thorough, and a pleasure to read. 

After reading the rebuttal and other reviews, I still think the paper should be accepted.","Review of submission 1489:
EX2: Exploration with Exemplar Models for Deep Reinforcement Learning

Summary:
A discriminative novelty detection algorithm is proposed to improve exploration for policy gradient based reinforcement learning algorithms. The implicitly-estimated density by the discriminative novelty detection of a state is then used to produce a reward bonus added to the original reward for down-stream policy optimization algorithms (TRPO). Two techniques are discussed to improve the computation efficiency. 

Comments
- One motivation of the paper is to utilize implicit density estimation to approximate classic count based exploration. The discriminative novelty detection only maintains a density estimation over the states, but not state-action pairs. To the best of my knowledge, most theoretical results on explorations are built on the state-action visit count, not state visit count. Generally, exploration strategies for RL algorithms investigate reducing the RL agent’s uncertainty over the MDP’s state transition and reward functions. The agent’s uncertainty (in terms of confidence intervals or posterior distributions over environment parameters) decreases as the inverse square root of the state-action visit count. 

- It is not clear from the texts how the added reward bonus is sufficient to guarantee the novel state action pairs to be visited for policy gradient based RL methods. Similar reward bonuses have been shown to improve exploration for value-based reinforcement learning methods, such as MBIE-EB (Strehl & Littman, 2009) and BEB (Kolter & Ng, 2009), but not policy gradient based RL methods. Value-based and policy gradient based RL methods differ a lot on the technical details. 

- Even we assume the added reward bonuses could result in novel state action pairs to be visited as desired, it is not clear why policy gradient methods could benefit from such exploration strategy. The paper, State-Dependent Exploration for Policy Gradient Methods (Thomas Ruckstieß, Martin Felder, and Jurgen Schmidhuber, ECML 2008), showed that limiting the exploration within an episode (i.e., returning the same action for any given state within an episode) could be helpful for policy gradient RL methods by reducing the variance in the gradients and improving the credit assignment. The soundness of the proposed techniques would be improved if necessary explanations were provided. 

- There is one closely related recent paper on improving exploration for policy gradient methods, IMPROVING POLICY GRADIENT BY EXPLORING UNDER-APPRECIATED REWARDS (Ofir Nachum et al. ICLR 2017). It would be good for the paper to discuss the relationship and even compare against it empirically. 
"
Training Quantized Nets: A Deeper Understanding,"Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein",https://proceedings.neurips.cc/paper/2017/hash/1c303b0eed3133200cf715285011b4e4-Abstract.html,"This papers investigates theoretically and numerically why the recent BinaryConnect (BC) works better in comparison to more traditional rounding schemes, such as Stochastic Rounding (SR). It proves that for convex functions, (the continuous weights in) BC can converge to the global minimum, while SR methods fair less well. Also, it is proven that, below a certain value, learning in SR is unaffected by decreasing the learning rate, except that the learning process is slowed down.

The paper is, to the best of my understanding:
1) Clear, modulo the issues below.
2) Technically correct, except some typos.
3) Deals with a significant topic, which has practical implications: understanding how to train neural in networks with low precision.
4) Novel and original, especially considering that most papers on this subject do not contain much theory.
5) Has interesting results. Specifically, I think it helps clarify why is it so hard to train with SR over BC (it would be extremely useful if one could use SR, since then there would be any need to store the full precision weights during training).


Some issues:
1) It is confusing that w_r and w_b are both denoted w in section 4. For example, since the BC bounds are on w_r, it should be clarified that the F(w_b) behaves differently (e.g., it should have an ""accuracy floor), and what are the implications (e.g., it seems a somewhat unfair to compare this bound with the SR bound on w_b). 
2) It is not clear how tight are these bounds, and especially the accuracy floor. The paper would have been stronger if you it had a lower bound on the error in SR. Also, I would suggest doing a (simple) simulation of a convex/strongly-convex functions to check the tightness of these results.
3) Percentage of weight change graphs do not look very convincing. In figure 3 the linear layers actually decrease in SR in comparison to BC. Also, in figure 4(b) both batch sizes arrive to almost the same value: the different convergence speed could be related to the fact that with smaller batch sizes we do more iterations per epoch.

Minor issues:
* The separation of the references to various advantages in lines [26-27] seems wrong. For example, [1] actually accelerated inference throughput (using a xnor kenrel), while [3,4] only discussed this.
* line 34: ""on"" -> ""to""
* It should be mentioned on the description of the methods (R SR BC) that the weights are typically restricted to a finite domain.
* lines 100-101: I guess the authors refer to the binary weights here, not w_r, since w_r was restricted to [-1,1] not {-1,1}.
* lines 102-103: not clear if this true. See follow-up of [1]: ""Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"".
* Lemma 1: I don't think it was previously defined that d is the dimension of w. Also, in the proof, the authors should consider keeping the L1 norm on the gradients instead of \sqrt(d) times the L2 norm, which can give a much higher ""accuracy floor"".
* Line 122: confusing sentence: When alpha -> 0 the rounding actually becomes more destructive, as shown in section 5. The authors should consider replacing ""rounding effect"" with ""rounding error per step"", or removing sentence. 
* First equation in Supplementary Material (SM): change +1 to -1
* line 334 in SM: (11) -> (10)

%% After author feedback %%
The authors have addressed my concerns.","his paper presents theoretical analysis for understanding the quantized neural networks. Particularly, the convergence analysis is performed for two types of quantizers, including the stochastic rounding (SR) and binary connect (BC), with different assumptions. Empirical evaluation and results are also provided to justify the theoretical results.

Pros.
1. In general this paper is well written. Quantized nets have lots of potential applications, especially for the low-power embedded devices. Although many quantized nets have shown promising performance in practice, a rigorous analysis of these models is very essential. This paper presents some interesting theoretical results along this direction.

2. This paper shows the convergence results in both convex and non-convex settings, although certain assumptions are imposed.
     
Cons.
1. Experimental results from Table 1 suggest that BC-ADAM outperforms SR-ADAM and R-ADAM in every case. An additional comparison of runtime behavior would be very helpful in evaluating the efficiency of these methods in practice.

2. The authors claimed that, a smaller batch size leads to an lower error for BC-ADAM, while a larger batch size is preferred for SR-ADAM. However, only two batch sizes (i.e., 128 and 2014) are employed in the evaluation. More batch sizes such as 256 and 512 should be adopted to testify if the conclusion consistently holds across multiple sizes. 
","Summary
---------
This paper analyzes the performance of quantized networks at a theoretical level. It proofs convergence results for two types of quantization (stochastic rounding and binary connect) in a convex setting and provides theoretical insights into the superiority of binary connect over stochastic rounding in non-convex applications.

General comments
—-----------------
The paper states that the motivation for quantization is training of neural networks on embedded devices with limited power, memory and/or support of floating-point arithmetic. I do not agree with this argumentation, because we can easily train a neural network on a powerful GPU grid and still do the inexpensive forward-pass on a mobile device. Furthermore, the vast majority of devices does support floating-point arithmetic. And finally, binary connect, which the paper finds to be superior to stochastic rounding, still needs floating-point arithmetic.

Presentation
-------------
The paper is very well written and structured. The discussion of related work is comprehensive. The notation is clean and consistent. All proofs can be found in the (extensive) supplemental material, which is nice because the reader can appreciate the main results without getting lost in too much detail.

Technical section
------------------
The paper’s theoretical contributions can be divided into convex and non-convex results: For convex problems, the paper provides ergodic convergence results for quantization through stochastic rounding (SR) and binary connect (BC). In particular, it shows that both methods behave fundamentally different for strongly convex problems. While I appreciate the importance of those results at a theoretical level, they are limited in their applicability because today’s optimization problems, and neural networks in particular, are rarely convex. For non-convex problems, the paper proofs that SR does not enter an exploitation phase for small learning rates (Theorem 5) but just gets slow (Theorem 6), because its behavior is essentially independent of the learning rate below a certain threshold. The paper claims that this is in contrast to BC, although no formal proof supporting this claim is provided for the non-convex case.

Experiments
-------------
The experimental evaluation compares the effect of different types of quantization on the performance of a computer vision task. The main purpose of the experiments is to understand the impact of the theoretical results in practice.

- First of all, the paper should clearly state the task (image classification) and quantization step (binary). Experimental results for more challenging tasks and finer quantization steps would have been interesting.

- Standard SGD with learning rates following a Robbins-Monro sequence should have been used instead of Adam. After all, the theoretical results were not derived for Adam’s adaptive learning rate, which I am concerned might conceal some of the effects of standard SGD.

- Table 1 feels a little incoherent, because different network architectures were used for different datasets.

- The test error on ImageNet is very high. Is this top-1 accuracy?

- Why only quantize convolutional weights (l.222)? The vast majority of weights are in the fully-connected layers, so it is not surprising that the quantized performance is close to the unquantized networks if most weights are the same.

- A second experiment computes the percentage of changed weights to give insights into the exploration-exploitation behaviour of SR and BC. I am not sure this is a meaningful experiment. Due to the redundancy in neural networks, changing a large numbers of weights can still result in a very similar function. Also, it seems that the curves in Fig. 3(c) do saturate, meaning that only very few weights are changed between epochs 80 and 100 and indicating exploitation even with SR.

Minor comments
-----------------
l.76: Citation missing.
Theorem 1: w^* not defined (probably global minimizer).
l.137: F_i not defined.
l.242: The percentage of changed weights is around 40%, not 50%.

Conclusion
-----------
The paper makes important contributions to the theoretical understanding of quantized nets. Although I am not convinced by the experiments and concerned about the practical relevance, this paper should be accepted due to its strong theoretical contributions."
Convolutional Gaussian Processes,"Mark van der Wilk, Carl Edward Rasmussen, James Hensman",https://proceedings.neurips.cc/paper/2017/hash/1c54985e4f95b7819ca0357c0cb9a09f-Abstract.html,"The paper presents a method to incorporate convolutional structure in GPs, in order to make them more competitive for image classification problems. This is achieved by constructing a convolutional kernel through a latent function g() shared by different patches in the image. 
Approximate inference is based on variational methods and very promising results are given in MNIST and CIFAR datasets. 

The paper is clearly written and  it presents a quite extensive set of experimental results. Regarding the main construction of the convolutional kernel (see eq. 13 and 14) I am not sure if 
the concept of inter-domain inducing variables is really necessary or useful here. The latent function of the GP model is essentially g() which is used inside the likelihood through this patch-sharing, i.e. through the definition of f(x). Under this viewpoint the inducing variables are just points of the latent function, i.e. points of g(), and you don't really need to refer to inter-domains in order to describe the variational sparse approximation. Also having 
presented the method based on g() (and discussing f(x) only as a complementary viewpoint) will probably make it easier for the reader to understand the weighted and colour extensions.   

Can you clarify further the arguments in lines 215-220? In particular, it is not clear why you avoid the inversion of a 2 M × 2 M matrix given that q(u1,u2) is coupled. Section 3 in Supplementary material, that tries to clarify this point, is very unclear and it contains also mistakes (e.g. in equation 13). 

Overall I believe that this is a good paper since it improves a lot on GP classification by incorporating convolutional structure. Still of course GPs cannot compete with Deep NNs, but this paper introduces some new ideas and it makes good steps forward to making GP classifiers more competitive for image classification. 
","The authors propose an approach to construct convolutional kernel functions for Gaussian processes. That way Gaussian process classification can learn and use non-local features similar to a convolutional neural network. Results for image classification show that convolutional kernels have advantages compared to a standard RBF kernel.

The paper is well written and shows both the strengths and weaknesses of the approach. The construction of the convolutional kernels is documented clearly. And section 2.1 contains a good description of the inference method used for the experiments. The original contribution of this paper is the method for constructing convolutional kernels based on patch response function modeled as Gaussian process. As this approach enables non-local features for Gaussian process classification, I expect that it will be used widely and developed further.

I have read the author feedback. In my opinion presenting convolutional Gaussian processes as ""just another kernel"" is a good choice, as algorithms and software frameworks for inference with Gaussian processes typically can use custom kernels but only a limited set of likelihood functions.","This paper proposes a new formulation that provides Gaussian processes with convolutional nature, aiming at exploiting some of the advantages of this type of models, so well-known in deep neural networks. The contribution is technically solid, and a worthy contribution to the literature and toolbox of Gaussian processes. However, the paper is not very clearly written, as it seems to have been written with an expert on Gaussian processes as the target reader. The experimental results are a bit disappointing, especially those on the CIFAR-10 dataset.
"
Best Response Regression,"Omer Ben-Porat, Moshe Tennenholtz",https://proceedings.neurips.cc/paper/2017/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html,"Setting: There are two regression learners, and ""opponent"" and ""agent"". The opponent has some fixed but unknown hypothesis. The agent observes samples consisting of data pairs x,y along with the absolute loss of the opponent's strategy. The agent will produce a hypothesis. On a given data point, the agent ""wins"" if that hypothesis has smaller absolute loss than the opponent's. The goal is to win with highest probability on the underlying distribution.

Results: Proposes the problem. Reduces the problem to PAC learning, working out the linear regression case. Some experiments for this case.

My opinion: I think this is an interesting paper raising an interesting and possibly important question. The results make at least a good first step in answering the question. I slightly worry about overlap with the dueling algorithms literature, but think the learning problem proposed here is in the end quite different.


Comments for authors: I find several things interesting about your approach and questions raised by the paper.

1. Extensions to other goals than minimum absolute error could be interesting. For example, if h(x) is a ranking of alternatives, then we could say the agent ""wins"" if her ranking has some property compared to the opponents'. I think this would touch on the dueling algorithms literature too. One could also not have a ""winner take all"" reward but instead some continuous reward between 0 and 1 as a function of the two algorithms' predictions.

2. I think the approach of [treating the agent's learning problem as binary classification] is very interesting. On one hand, it makes perfect sense to pair with 0-1 loss. On the other hand, it seems to throw away some useful information. Intriguing.

3. The game theory/equilibrium extensions sounds difficult because of the machine-learning setting formulation where the underlying distribution is initially unknown and accessible via samples. Usually to formalize equilibrium one needs the agents to form consistent Bayesian beliefs about the unknown and best-respond to those beliefs. Would be very interesting if this modeling challenge could be overcome.","The paper introduces a 'duel' version of linear regression, where the learner tries to out-predict the opponent on the most data points, in a PAC sense, as opposed to simply minimizing expected loss overall.  The authors give an algorithm to compute the best linear response against any competitor, show that the problem of finding such a best response can be NP-hard, and demonstrate their algorithm empirically.

Overall, I think this paper is interesting, and the results thorough.  I appreciated the attention to exposition.

I was confused about two points, which I would like the authors to address in the rebuttal:

1. It is assumed that the best-responder knows the discrepancies but not the hypothesis \bar h, nor the signed discrepancies.  Why is this reasonable?  In the real-estate example, the actual predictions of the opponent are known, and this seems to be a more typical case.  Perhaps the hardness could be circumvented in this more powerful model (though I somewhat doubt it).

2. I did not find the authors' justification for why one would not worry about ""embarrassing predictions"" to be very satisfying.  The authors say that predicting poorly on a handful of data points is not a concern in ""most"" practical scenarios, but only a vague example about ""demographics"" is given, leaving one to conjure up countless scenarios where this is indeed a problem (consider the backlash toward xbox for failing to recognize gestures from minority users, ditto for speech recognition, etc etc).  In spite of this seemingly ""straw man"" argument, however, it seems that the algorithm can be made to avoid embarrassing predictions, at least relative to a linear opponent (Lemma 3).  Hence, I would view that result as satisfying, and the lack of such a result for a non-linear opponent concerning.

line 119: Make this a full sentence
line 123: In bullet 3 of the model, it is said that \bar h is not known to the best-responder, but of course something must be known, and later in that paragraph it is mentioned that p = |\bar h - y| is known.  It would be much clearer to state up front what IS assumed to be known, before assuring the reader what is not assumed to be known.","STRENGTHS:

On some ""abstract"" level, this paper pursues an interesting direction:
- I fully agree that ""prediction is not done in isolation"" (l24), and though this is a vague observation, it is worth pursuing this beyond established work (although I'm not an expert with a full overview over this field).
- Making explicit the tradeoff between good-often and not-bad-always in deciding on a loss function is interesting.

Overall, the writing and mathematical quality of the paper is good to very good, with some uncertainty regarding proofs though:
- Well written and structured.
- The paper is very strong in that the algorithmic and mathemtaical results are well connected (combine the VC theorems and lemmas to reach corollary 1 which shows sample bounds for their EPM algo) and cover the usual questions (consistency, runtime).
- In terms of correctness of the mathematical results, it seems that the authors know what they are doing. I only checked one proof, that of Theorem 1 (via the second proof of Lemma 5), and it seems correct although I couldn't follow all steps.


WEAKNESSES:

For me the main issue with this paper is that the relevance of the *specific* problem that they study -- maximizing the ""best response"" payoff (l127) on test data -- remains unclear. I don't see a substantial motivation in terms of a link to settings (real or theoretical) that are relevant:
- In which real scenarios is the objective given by the adverserial prediction accuracy they propose, in contrast to classical prediction accuracy?
- In l32-45 they pretend to give a real example but for me this is too vague. I do see that in some scenarios the loss/objective they consider (high accuracy on majority) kind of makes sense. But I imagine that such losses already have been studied, without necessarily referring to ""strategic"" settings. In particular, how is this related to robust statistics, Huber loss, precision, recall, etc.?
- In l50 they claim that ""pershaps even in most [...] practical scenarios"" predicting accurate on the majority is most important. I contradict: in many areas with safety issues such as robotics and self-driving cars (generally: control), the models are allowed to have small errors, but by no means may have large errors (imagine a self-driving car to significantly overestimate the distance to the next car in 1% of the situations).

Related to this, in my view they fall short of what they claim as their contribution in the introduction and in l79-87:
- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (""strategic predictions"", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player). 
- In particular, in the experiments, it doesn't come as a complete surprise that the opponent can be outperformed w.r.t. the multi-agent payoff proposed by the authors, because the opponent simply doesn't aim at maximizing it (e.g. in the experiments he maximizes classical SE and AE).
- Related to this, in the experiments it would be interesting to see the comparison of the classical squared/absolute error on the test set as well (since this is what LSE claims to optimize).
- I agree that ""prediction is not done in isolation"", but I don't see the ""main"" contribution of showing that the ""task of prediction may have strategic aspects"" yet.


REMARKS:

What's ""true"" payoff in Table 1? I would have expected to see the test set payoff in that column. Or is it the population (complete sample) empirical payoff?

Have you looked into the work by Vapnik about teaching a learner with side information? This looks a bit similar as having your discrapency p alongside x,y."
Elementary Symmetric Polynomials for Optimal Experimental Design,"Zelda E. Mariet, Suvrit Sra",https://proceedings.neurips.cc/paper/2017/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,"The authors present new results in the area of optimal experimental design (OED). My main concern about the paper is that the writing is very dense and appears to require a thorough understanding of OED. I am not sure that fits well with the audience for NIPS. So, at best the paper would be targeting a small subset of the audience. My question is whether there is a way to make the paper better suited for a broader audience. (E.g., a more detailed discussion of the setup and results on real data could help introduce key concepts from OED.)

Based on author feedback and the other reviewers' comments, it appears the results will be of interest to a subset of the NIPS audience. I'm therefore revising my score. I would still encourage the authors to write a few intro paragraphs to draw in a broader audience.
","		  The authors present a novel objective for experiment design that interpolates A- and D-optimality. They present a convex relaxation for the problem which includes novel results on elementary symmetric polynomials, and an efficient greedy algorithm based on the convex relaxation with performance guarantees for the original task. They present results on synthetic data, and present results on a concrete compressive strength task.
		  I am not an expert in this area but I believe I understand the contribution and that it is significant.
		  The paper is extremely well-presented, with minor points of clarification needed that I have identified below.
		  
		  The main suggestion I have is that the authors more explicitly explain the benefits of the new method in terms of the properties of solutions that interpolate between A and D -- In 6.2, the authors present the ability to use their method to get a spars(er) design matrix as an important feature. This should be stated at the beginning of the paper if it is a true aim (and not simply an interesting side-effect) and the authors should review other methods for obtaining a sparse design matrix. (I presume that there are e.g. L1 penalized methods that can achieve sparsity in this area but I do not know.) In general, a more explicit statement of the practical benefits of the contributions in the paper would be useful for the reader.
		  
		  l21. is a bit confusing regarding use of ""vector"" -- \epsilon_i is a scalar, yes?
		  l22. Gauss-Markov doesn't require \epsilon_i be Gaussian but it *does* require homoskedasticity. Clarify.
		  l45. ""deeply study"" - studied
		  l77. I think (2.1) is confusing with v as vectors unless the product is element-wise. Is this intended?
		  l107. ""Clearly..."" - Make a more precise statement. Is an obvious special case hard? What problem of known hardness reduces to this problem? If you mean ""no method for finding an exact solution is known"" or ""we do not believe an efficient exact solution is possible"" then say that.
	  ","The paper proposes more general formulations of optimal experiment design. More specifically, given a set of feature vectors, optimal experiment design tries to choose which of these to get labels for so that the variance of the learned model can be minimized. While previous approaches have tried to minimize the trace (A-optimality) or the determinant (D-optimality) of the covariance matrix, in this paper, the authors propose a more general formulation whose extremes include these two cases. Based on a convex formulation of this problem, the authors then go on to propose 2 algorithms to perform optimal experiment design.

The connection between elementary symmetric polynomials and its special cases was an interesting revelation. I am not enough of an expert in the area to know if this connection has been made previously or in other contexts. In the paper, the authors introduce this notion in a very matter-of-fact kind of way, without much fanfare (I had difficulty opening the supplementary material pdf which looked blank). The reduction to the elegant log det minimization immediately lets the authors utilize convex relaxations. As is pointed out in the paper, the algorithms proposed in the paper have a lot of similarity with the ones for trace minimization in reference 44. One issue/question I had is whether these algorithms are of practical importance since I haven't seen much use of A-optimality or D-optimality in the field, and now with this generalization, the practitioner will need to figure the value of the extra l parameter (this question is not addressed in the paper). Another point that may improve the paper is to report errors from models learned without choosing the rows to show whether we are losing any accuracy by minimizing the number of examples used for training.
"
Learning from Complementary Labels,"Takashi Ishida, Gang Niu, Weihua Hu, Masashi Sugiyama",https://proceedings.neurips.cc/paper/2017/hash/1dba5eed8838571e1c80af145184e515-Abstract.html,"Collecting labeled data is costly and the paper considers a new setting, where only complementary label that specifies a class where a pattern does not belong to, is collected. It is obviously that the collection of complementary label is much more easy than that of precise label. The paper presents to learn a model that minimizes the loss in terms of the complementary label. The paper then extensively studies the estimation error bound of the proposal, by using unbiased estimator given that a condition of loss function hold. Experiments are conducted on a number of data sets, and results show encouraging performance. 

The paper is clearly written. The studied problem is interesting and is of importance for several applications. The proposed method is sound, as it is based on statistical learning. The theoretical studies are extensive. To my knowledge, it is new. Experiment results are rather good.

Two minor comments:

1)	The theoretical analysis shows that the estimator error bound is related to the number of $k$. It would be interesting to see how $k$ affects the empirical performance of the proposal. 
2)	If the space is not sufficient for new empirical studies, the author could compress the theoretical part and make it more compact, such as, putting some proofs in supplementary files. 

","This paper proposes a novel problem setting and algorithm for learning from complementary labels. It shows an unbiased estimator of the classification risk can be obtained only from complementary labels, if a loss function satisfies a particular symmetric condition. The paper also provides estimation error bounds for the proposed method. Experiments show the approach can produce results similar to standard learning with ordinary labels on some datasets.

The idea and the theoretical results are interesting. But I question the usefulness of the proposed learning problem.  Though using complementary labels can be less laborious than ordinary labels, complementary labels are less informative than ordinary labels. If using N training instances with complementary labels cannot perform better (this is almost certain) than learning with N/(K-1) training instances with ordinary labels,  why should one use complementary labels? Moreover, based on the theoretical results, learning with complementary labels are restricted to a limited number of loss functions, while there are many state-of-the-art effective learning methods with ordinary labels. 

The paper only suggests in the conclusion section that such complementary label learning can be useful in the context of privacy-aware machine learning. Then why not put this study within the privacy-aware learning setting and compare to the privacy-aware learning techniques?

For section 3: (1) On line 51, it says “ that incurs a large loss for large z”.  Shouldn’t be “that incurs a large loss for small z”?  (2) It is not clear how the derivation on Line 79-80 was conducted.  Why should \sum_{y\not= \bar{y}}\ell(g_y(x))/(K-1) =  \ell(g_{\bar{y}}(x)) ?

The experiments are very basic. In the benchmark experiments, it only compares to a self-constructed ML formulation and a one-hidden-layer neural network with ordinary labels. The way of translating the complementary labels to multi-labels is not proper since it actually brings much wrong information into the labels. The one-hidden-layer neural network is far from being a state-of-the-art multi-label classification model.  It is better to just compare with the standard learning with ordinary labels using the same PC with sigmoid loss. 

Another issue is: why not use the whole datasets instead of part of them? It would also be interesting to see the comparison between complementary label learning with ordinary label learning with a range of different number of classes on the same datasets.
"
Dynamic Importance Sampling for Anytime Bounds of the Partition Function,"Qi Lou, Rina Dechter, Alexander T. Ihler",https://proceedings.neurips.cc/paper/2017/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html,"This paper presents a new anytime algorithm for estimating the value of the partition function. The proposed algorithm balances between search and sampling and it seems to give good empirical results. 

The paper is well-motivated and the proposed algorithm may have practical significance. The paper was quite technical and hard to follow. There was no high-level intuitive description of the procedure but it had to be inferred from the technical details. 

Algorithm 1 should probably have also other input such as the factors. I guess that in step 4, one expands S.

In section 2.3, it is unclear what is a mini-bucket. 

What is h^+_N? On line 115 it seems to be an upper bound initialized using a precomputed heuristics but on line 147 it is the WMB bound.","The authors present a method for estimating the partition function that alternates between performing heuristic search and importance sampling.  The estimated value of the partition function is confidence bounded and improves with additional computation time.  Experimental evaluation is performed on problems from 2006 and 2008 UAI competitions by comparing confidence bounds of the proposed method against previous work that uses only sampling [15] or search [16].  The proposed method significantly outperforms sampling on certain problems and search on others, while maintaining performance roughly comparable to or better than either sampling or search across all problems.

The originality of the work is a bit limited, as it is a fairly straightforward (but novel as far as I know) combination of two recent papers, references [15] and [16].

To review past work, [15] proposes a method that combines importance sampling with variational optimization.  The authors chose a proposal distribution that is the solution to a variational optimization problem and show this choice can be interpreted as picking the proposal distribution that minimizes an upper bound on the value of importance weights.  The importance weight bound is then used to construct a (probabilistic, but with confidence intervals) bound on the estimate of the partition function.

[16] describes a search algorithm for computing deterministic bounds on the partition function.  Unlike deterministic bounds given by variational methods, the search algorithm proposed in [16] can continue to improve its estimate in an anytime manner, even when given limited memory resources.  

This paper combines the methods of approaches of [15] and [16] by alternating between importance sampling and search.  Search is performed using a weighted mini-bucket variational heuristic.  The same mini-buckets are used to improve the proposal distribution over time, generating better samples.  The authors provide a method for weighting later samples more heavily as the proposal distribution improves.  

The authors demonstrate that the proposed method improves estimation of the partition function over state of the art sampling and search based methods ([15] and [16]).  There is room for researchers to build on this paper in the future, particularly in the areas of weighting samples generated by a set of proposal distributions that improve with time and exploring when computation should be focused more on sampling or search.

The paper is clear and well organized.  Previous work the paper builds on is well cited.  

Minor:
- Line 243 states that DIS remains nearly as fast as AOBFS in figures 3(d) and (f), but it appears that the lower bound of DIS converges roughly half an order of magnitude slower.  Line 233 states, ""we let AOBFS access a lower bound heuristic for no cost.""  Does the lower bound converge more slowly for DIS than AOBFS in these instances or is the lower bound shown for AOBFS unfair?
- Line 190, properites is a typo.
- Line 146 states that, ""In [15], this property was used to give finite-sample bounds on Z which depended on the WMB bound, h_{\emptyset}^{+}.""  It appears that in [15] the bound was only explicitly written in terms of Z_{trw}.  Mentioning Z_{trw} may make it slightly easier to follow the derivation from [15].","This paper presents a method -- dynamic importance sampling (DIS) -- for computing bounds on the partition function of a probabilistic graphical model in an anytime fashion. DIS combines and extends two existing methods for estimating the partition function -- sampling and search -- which it interleaves in order to gain the benefits of each. 

A very well-written paper that clearly lays out its thesis, contributions, and results. The paper is technically sound, with claims that are well-supported both empirically and theoretically. The writing is clear and concise. The paper nicely introduces and builds on existing work before introducing its novel contributions, which look quite promising. My only comment is that I'd like to see empirical results on applications that seem less toy and where anytime estimation of (bounds on) the partition function is directly beneficial towards some final goal, such as cost reduction, accuracy improvement, etc."
Process-constrained batch Bayesian optimisation,"Pratibha Vellanki, Santu Rana, Sunil Gupta, David Rubin, Alessandra Sutti, Thomas Dorin, Murray Height, Paul Sanders, Svetha Venkatesh",https://proceedings.neurips.cc/paper/2017/hash/1f71e393b3809197ed66df836fe833e5-Abstract.html,"The paper proposes a batch Bayesia Optimization algorithm for problems in which the queried  parameters in a batch need to be the same. The authors first proposed a straight forward algorithm where the constrained parameters are chosen according to the first element in a batch. Then, the authors propose an nested algorithm where the outer stage searches for the constrained parameters and the inner stage generates the batch for the unconstrained parameters. The authors proof the average regret of the nested algorithm vanishes superlinearly. The proposed algorithms are evaluated on both benchmark functions and real problems.

The process-constrained BO is a practical problem that people may face for real world problems. The authors propose an intuitive algorithm that can nicely address this BO problem, which produces good performance for both benchmark functions and real world problems.
","The authors propose a new method for Bayesian optimization that allows to fix some of the variables of the input domain before computing a batch. This is practical in real experiments and the authors mentions some real cases like the optimization of heat treatments  in the metallurgy industry or nano fibre production. 

The paper is very well motivated and written, the hypothesis and challenges  of the problem  are clear and the applicability of the proposed methodology in real-world scenarios is evident. In the theoretical analysis I didn't see any major flag.  The experimental section is not very extensive but this is reasonable in this case as this is the first method that allow to freeze some variables before constructing a batch.  

I think that this paper can be a good contribution to NIPS as it correctly formalizes a solution to a practical problem that hasn't been addressed before. I enjoyed reading this work. My only recommendation for the authors to improve the impact of their work is the publication of the code needed to run their method,  as this would increase the impact and visibility of this paper. Also, I would like to see if the authors think that this method can be extended to other acquisitions and if they have plans to do so.","The paper presents a bayesian optimization framework in which samples can be collected in similarly constrained batches. In short, rather than selecting a single point to sample a label from (as in usual bayesian optimization) or selecting multiple points simultaneously (as in batch bayesian optimization), multiple samples in a batch can be collected in which one or more variables need to remain constant throughout the batch. This is motivated from experiments in which some parameter (e.g., temperature in an oven, geometry of an instrument) can be designed prior to sampling, and be reused in multiple experiments in a batch, but cannot be altered modified during a batch experiment.

The authors (a) propose this problem for the first time (b) propose two algorithms for solving it, (b) provide bounds on the regret for the most sophisticated of the two algorithms, and (c) present a comparison of the two algorithms on a variety of datasets. One experiment actually involves a metallurgical setup, in which the authors collaborated with researchers in their institution to design a metal alloy, an interdisciplinary effort that is commendable and should be encouraged.

My only concern is that the paper is somewhat dense. It is unlikely that someone unfamiliar with Bayesian optimization would be able to parse the paper. Even knowing about bayesian optimization, notation is not intoduced clearly and the algorithms are hard to follow. In particular:

-what is sigma in Algorithms 1, and 2. Is it the variance of the GP used in the UCB? 
-The activation fuction alpha^GB-UCB should be clearly and explicitly defined somewhere.

-The description of the inner loop and the outer loop in Algorithm 1 are not consistent with the exposition in the text: h and g are nowhere to be found in Algorithm 1. This seems to be because your replace the optimization with the GP-UCB-PE. It would be better if the link is made explicit.

-There is absolutely no description of how any optimization involving an activation function is performed. Do you use sampling? Gradient descent? Some combination of both?   These should be described, especially in how they where instantiated in the experiments.

-Is alpha different from alpha^GP-UCB in Alg. 1? how?

-In both algorithms 1 and 2, it is not clear how previously selected samples in a batch (k'\< k) are used to select the k-th sample in a batch, other than that they share a constrained variable.
 

All of these are central to understanding the algorithm. This is a pity, as the exposition is otherwise quite lucid."
Uprooting and Rerooting Higher-Order Graphical Models,"Mark Rowland, Adrian Weller",https://proceedings.neurips.cc/paper/2017/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html,"This paper presents a reparametrization method for inference in undirected graphical models. The method generalizes the uprooting and rerooting approach from binary pairwise graphical models to binary graphical models with factors of arbitrary arity. At the heart of the method is the observation that all non-unary potentials can be made symmetric, and once this has been done, the unary potentials can be changed to pairwise potentials to obtain a completely symmetric probability distribution, one where x and its complement have the exact same probability. This introduces one extra variable, making the inference problem harder. However, we can now ""clamp"" a different variable from the original distribution, and if we choose the right variable, approximate inference might perform better in the new model than in the original. Inference results in the reparametrized model easily translate to inference results in the original model.

The idea is supported by a number of theorems. I believe the results, but I did not study the proofs in the supplementary material. In addition to the main result, which is that inference results in the new model can be mapped back to inference results in the original model, the authors also discuss how the reparametrized problems relate to each other in the marginal polytope and how factors can be decomposed into sums of ""even"" factors -- basically, parity over k variables.

There are also some basic experiments on small, synthetic datasets which demonstrate that rerooting can yield better approximations in some cases.

Given that the negligible cost of reparametrization and mapping the results, this seems like a promising way to improve inference. There's no guarantee that the results will be better, of course -- in general, rerooting could lead to better or worse results. Furthermore, we can clamp variables without rerooting, at the cost of doubling the inference time for each clamped variable. (Just run inference twice, once with each value clamped.) If rerooting is only done once, then you're getting at best a factor-of-two speedup compared to clamping. A factor of two is a small constant, and I didn't see any discussion of how rerooting could be extended to yield better results. So the practical benefit from the current work may be marginal, but the ideas are interesting and could be worth developing further in future work.

Quality: I believe the quality is high, though as I mentioned, I have not studied the proofs in the supplementary material. The limitations are not discussed in detail, but this is typical for conference papers.

Clarity: The writing was fairly clear. Figure 2 has very small text and is uninterpretable in a black and white printout.

Originality: I believe this is original, but it's a natural evolution of previous work on rerooting. This is a meaningful increment, supported by theoretical results and token empirical results, but not transformative.

Significance: As stated earlier, it's not clear that these methods will yield any improvements on real-world inference tasks (yet). And if they do, it's at best a factor-of-two speedup. Thus, this work will mainly be relevant to people working in the area of graphical models and probabilistic inference. It's still significant to this group, though, since the results are interesting and worthy of further exploration.","The authors present improvements over recent work on up/rerooting undirected graphical models; the novel step here is to consider potentials that involve more than two variables. The novelty seems to be the introduction of pure potentials, and the study of the consequences of using such pure potentials when dealing with uprooting/rerooting. The results show that up/rerooting increase the accuracy of approximate inference methods; this is a welcome result that is nicely studied empirically. Overall the paper brings an incremental but valuable contribution, and is very competent in its presentation. 

Small point: in Definition 1, what exactly happens if the edge consists only of a a pair of nodes? It becomes a single-node edge of some sort?

A question: in Expression (3), perhaps it is 1[x_i NOT= x_j]? I do not see how the indicator can use the event [x_i = x_j] in this expression.

A few suggestions concerning the text:
- Introduction, line 3: I guess it should be ""computing marginal probabilities"", not ""estimating marginal probabilities"", right?
- When there are several references together, please order them.
- After Expression (3), do not start the sentence just with ""(3) has"", but perhaps ""Expression (3) has"".
- I find that \mu_{V|U} is an awful symbol for marginal distributions; it looks like a conditional probability measure. Please use some other symbol (sometimes people use $V \downarrow U$ as a superscript to mean marginalisation).
- Please remove ""Proof in the Appedix"" from the STATEMENT of Theorem 20. "
Learned in Translation: Contextualized Word Vectors,"Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher",https://proceedings.neurips.cc/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html,"This paper proposes to pretrain sentence encoders for various NLP tasks using machine translation data. In particular, the authors propose to share the whole pretrained sentence encoding model (an LSTM with attention), not just the word embeddings as have been done to great success over the last few years. Evaluations are carried out on sentence classification tasks (sentiment, entailment & question classification) as well as question answering. In all evaluations, the pretrained model outperforms a randomly initialized model with pretrained GloVe embeddings only.

This is a good paper that presents a simple idea in a clear, easy to understand manner. The fact that the pretraining helps all tasks across the board is an important finding, and therefore I recommend this paper for publication.

Are the authors going to release their pretrained models?

The authors should cite https://arxiv.org/pdf/1611.02683.pdf, where the encoders are pretrained as language models.","This paper explores the use of attention based sequence to sequence models for machine translation to pretrain models for other tasks. The author evaluated their methods on a wide range of tasks (entailment, sentiment, question classification and question answering). Their results showed consistent improvements over the baselines. 
Overall, this paper is well written and the idea is interesting. In my opinion, the contributions of this paper are expected (moving from pretrained word embeddings to pretrained sentence embeddings) but important for sentence classification.
I have the following suggestions to improve the paper:
- The authors should have a better explaination why the performance could not be improved for the entailment task while increasing the amount of MT training pairs.
- From my experiences, I always expected to see improvements using character n-gram embeddings. Why is that not the case in this paper?
","The paper proposes uses to reuse the weights of an LSTM that was trained as a part of a neural machine translator to initialize LSTMs trained for downstream tasks. The approach brings improvement on classification, semantic entailment recognition and question answering tasks.

The paper is clearly written and easy to understand. The evaluation is exhaustive and convincing. The paper is solid and should be accepted.

A few suggestion for improving the paper even further:
- I think it would be better to use smaller subsets of the largest MT dataset available instead of using three different datasets. This way the influence of domain mismatch and the amount of data available would be disentangled.
- it would be great to see a comparison with a skip-thought-trained LSTM and with an LSTM that was pretrained as a language model.
- I encourage the authors to release their evaluation code to fascilitate the research on trasfer learning for NLP.
"
"Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding","Arya Mazumdar, Soumyabrata Pal",https://proceedings.neurips.cc/paper/2017/hash/2131f8ecf18db66a758f718dc729e00e-Abstract.html,"The authors study a clustering model where the learner is allowed to ask queries from an oracle and the goal is to find the target clustering. The queries can be same-cluster queries [or more generally, a fixed binary-valued function of the label of the input instances]. The authors study the query complexity of such models. 

The query-based clustering model is a well-motivated and relatively recent research area, with applications in record-deduplication, crowdsourcing, etc. The authors study the problem from the information theoretic perspective. In particular, they provide upper and lower bounds on the number of required queries to perfectly (or approximately) recover the target clustering. They also consider the case that the oracle is noisy. The information-theoretic tools that they use are interesting and new to ML community.

Because no structural assumption is made about the problem (e.g., having a hypothesis class, etc.) the analysis is limited. In fact, the marginal distribution of the labels is the main parameter that the bounds are based on (aside from the noise level). Therefore, the studied setting is a bit far from the usual settings studied in learning theory. However, this is a first step, and can help in understanding the information-theoretic aspects of query-based clustering, especially the related lower-bounds.","This paper explores a clustering problem of which the goal is to reconstruct the true labels of elements from possibly noisy & non-adaptive query-answers, each associating only a few elements. It derives lower bounds on the number of queries required to reconstruct the ground-truth labels exactly or partially in a variety of scenarios. It also proposes a so-called ""AND-queries"" algorithm that applies to arbitrary k-cluster case, and evaluates the performance of the algorithm under a real data set.

While the paper attempts to provide theoretical analyses for various scenarios as well as develops an algorithm together with a real-data simulation, it comes with limitations in presentation, clarity of the claimed results, and comparison to related works. Perhaps significant changes and possibly new developments are to be made for publication. See below for detailed comments.

1. (Repeating the same questions is not allowed): This constraint looks a bit restricted. Also it may degrade the performance compared to the no-restriction case, as allowing the same questions is one natural way to combat the noise effect. One proper way to justify the restricted model might be providing an extensive set of real-data simulations with a comparison to an algorithm tailored for the no-restriction case, and possibly showing a better performance of the proposed algorithm. If that is not the case, the no-restriction case is more plausible to investigate instead. The authors seem to concern about independence-vs-dependence issue for multiple answers w.r.t the same query, which might make analysis challenging. But this issue can be resolved focusing on a setting in which the same query is given to distinct annotators and therefore the answers from the annotators are assumed to be independent.   

2. (Theorem 1): The noiseless setting with exact recovery was also studied in a recent work [Ahn-Lee-Suh] which also considers the XOR-operation ('same-cluster' queries) under a non-adaptive measurement setting. But the result in [Ahn-Lee-Suh] does not coincide with the one claimed in Theorem 1. For instance, when p=1/2 and \Delta is a constant, the query complexity in Theorem 1 reads m=n/log2, while the one in [Ahn-Lee-Suh] reads m=nlogn/\Delta. This needs to be clarified in details. Moreover, the proof of Theorem 1 is hard to follow.  

3. (Theorem 3): The expression of the lower bound on \delta is difficult to interpret. In particular, it is difficult to infer the bound on m in terms of n and \delta, from which one can directly see the performance improvement due to the relaxed constraint, reflected in \delta. 

4. (Theorem 4): The same comment given w.r.t. Theorem 3 applies here. In addition, how different is the upper bound from the lower bound in Theorem 3? Can be the gap unbounded?  Again the proof is hard to follow.

5. (Figure 2):  No comparison is made w.r.t. the same cluster query method – there is a comparison only to the same-cluster-query 'lower' bound. Wondering if there is a crossing point also when compared to the same-cluster-query 'method'. If that is the case, any intuition behind that? Actually the paper claims that the proposed approach outperforms the same-cluster query scheme, which cannot be supported from Figure 2. 

6. (Algorithm 1): The proposed algorithm requires the prior knowledge on p or the relative sizes of clusters (in view of Theorem 6). How to obtain such information in practice? A proper justification might validate the practicality of the algorithm. Also the pseudo-code is hard to follow without relying on the detailed proof of Theorem 5, which is also difficult to grasp. 

7. (Figure 3 & 4 & 5): No comparison is made w.r.t. the state of the arts such as the same-cluster-query method, which is crucial to support the claim made in several places: the AND-queries algorithm outperforms the same-cluster-query scheme. 

[Ahn-Lee-Suh] K. Ahn, K. Lee, and C. Suh, “Community recovery in hypergraphs,” Proceedings of Allerton Conference on Communication, Control, and Computing, 2016.

------------ 
(Additional comments)
I have rebuttal. I found some comments properly addressed. For further clarification:
1. Still not clear as to whether repeating the same question is not helpful. Did the Nature paper show that it is not useful? Perhaps this can be clarified in a revision.
2. Re. the comparison to the same-query method: it would be clearer to include the detailed discussion as addressed in the response.","This is an interesting paper. I enjoyed reading it. In the paper, the authors consider the task of clustering data points via crowdsourcing wherein the problem is of recovering original labels of data points based on the answers to similar cluster queries. The authors propose bounds on the number of queries required to recover the labels considering noisy as well as noiseless answers for the queries. The main contribution of the paper is theoretical, along with some empirical simulations. The paper is well written, making it easy to understand the theoretical contributions for a general machine learning audience. Especially, the connection w.r.t. Information theory for deriving the bounds is explained nicely. 

I have the following concerns, in regards to improving the paper.

(1) While the idea of AND queries is interesting theoretically, it seems to conflict with the original motivation for crowdsourcing. In the case of an AND query, effectively, the label of the cluster is also given as part of the answer rather than just answering if the data points of interest belong to same cluster or not. If this is the case indeed, as I interpret from the paper, why would one even need to ask those similarity queries ? Why not ask the label for a data point itself ? This is my primary confusion w.r.t. the contributions in this paper.

(2) Is it possible to simplify the expressions for the bounds in Theorems 1,2,3,4, while keeping the essence.

(3) In the experiments, it seems that only AND queries are considered. If so, the experimental study is in question unless the above point (1) is clarified.


I am satisfied with the response of the authors for my questions above. So, voting for the acceptance of this paper."
Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization,"Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han",https://proceedings.neurips.cc/paper/2017/hash/217e342fc01668b10cb1188d40d3370e-Abstract.html,"[UPDATE AFTER AUTHOR RESPONSE]

I am happy with the authors' response. It's a valuable paper if the contributions are worked out more precisely and the relationship to Burda et al. [1] clarified.


[ORIGINAL REVIEW]

The authors propose a training procedure for networks with dropout or other forms of noise for regularization. Their approach is an importance weighted stochastic gradient descent algorithm that was originally proposed in the context of variational autoencoders by [1]. The contributions of the paper are (1) to use this approach in a new context (dropout) and (2) to show that it improves performance of a number of networks regularized with dropout.

The paper is well written and straightforward to follow, and I think it is a valuable contribution. However, I do have a number of concerns that I will list in decreasing order of importance below:

- Novelty. Unlike the authors’ repeated claims (e.g. lines 9–10, 41–44), their approach is not novel. Indeed, they do cite the original paper [1] repeatedly and even refer to results therein. Therefore, I suggest toning down the novelty claims a bit and working out more precisely the contribution of the paper.

- Section 3.3. Why not run the test empirically and show the results?

- Language. There are a couple of language mistakes, so please carefully proof-read the manuscript again. Some examples include line 108: *the* reparameterization trick; 156: *the* Bernoulli distribution; 164: within *each* mini-batch; 228: *the* MSCOCO dataset; 242f: *a* spatial stream, *a* temporal stream, *a* fusion network; 247: we observe *an* improvement.


References:

[1] Burda, Grosse & Salakhutdinov: Importance weighted auto encoders. ICLR 2016. https://arxiv.org/abs/1509.00519","
This paper introduces a method for regularizing deep neural network by noise. The core of the approach is to draw connections between applying a random perturbation to layer activations and the optimization of a lower-bound objective function. Experiments for four visual tasks are carried out, and show a slight improvement of the proposed method compared to dropout.

On the positive side:
- The problem of regularization for training deep neural network is a crucial issue, which has a huge potential practical and theoretical impact.
- The connection between regularization by noise and the derivation of a lower bound of the training objective is appealing to provide a formal understanding of the impact of noise regularization techniques, e.g. dropout.

On the negative side:
- The aforementioned connection between regularization by noise and training objective lower bounding seems to be a straightforward adaptation of [9] in the case of deep neural networks. For the most important result given in Eq (6), i.e. the fact that using several noise sampling operations gives a tighter bound on the objective function than using a single random sampling (as done in dropout), the authors refer to the derivation in [9]. Therefore, the improvement and positioning with respect to ref [9] is not clear to me.
- Beyond this connection, the implementation of the model given in Section 4 for the case of dropout is also straightforward: several (random) forward passes are performed and a weighted average of the resulting gradients is computed using Eq (8), before applying the backward pass.
- Experiments are carried out in various vision tasks, which choice could be better justified. It is not clear why these tasks are highly regularization demanding: the noise seems to be applied in a few (fully connected layers). In addition, the improvement brought out by the proposed method is contrasted across tasks (e.g. small improvement in UCF-101 and below the published baseline for image captioning).

As a conclusion, although the approach is interesting, its novelty level seems to be low and the experiments should be more targeted to better illustrate the benefit of the multiple sampling operations. In their rebuttal, I would like the authors to answer my concern about positioning with respect to [9].
","The paper interprets  noise injection, especially dropout, in deep neural networks training as stochastic nodes in the network. This interpretation naturally leads to the observation that dropout, when trained with SGD, is a special case of training a stochastic model with one sample per mini-batch.

The authors then proposed the increase the number of samples per mini-batch. Because dropout is usually applied to the top of the network, more samples lowers the variance of gradient estimation without increasing the computation cost by too much. This improves training efficiency.

The authors performed experiments on multiple tasks including classification (cifar10), VQA, and image captioning. The proposed method consistently improved accuracy."
Few-Shot Adversarial Domain Adaptation,"Saeid Motiian, Quinn Jones, Seyed Iranmanesh, Gianfranco Doretto",https://proceedings.neurips.cc/paper/2017/hash/21c5bba1dd6aed9ab48c2b34c1a0adde-Abstract.html,"This paper addressed the problem of supervised adversarial domain adaption with deep models. The application scenario is that there is a small amount of labeled target data.

This paper is not well written, and some claims/descriptions are not convincing/clear. Below are detailed comments.

(1) Why are there 4 groups (section 3.1) ? More analysis should be given. Given the current description, it is very confusing and the reviewer cannot get the insight why this will work.

(2) Lines 114-122: Is it really correct? Source domain and target domain use the same classification function?

(3) Experimental justification is not strong: Table 2 shows that proposed approach achieves slight improvement or even the performance is lower than [23].
","Overall
The paper tackles the problem of domain adaptation when a few (up to 4) unlabeled samples exist in the target domain. An approach is presented based on the idea of adversarial training. The main contribution of the paper is in proposing an augmentation approach to alleviate the scarcity of target domain data, that is by taking pairs of source-target samples as input to combinatorially increase the number of available training samples. The approach proves to be very effective on two standard domain adaptation benchmarks, achieving new state-of-the-art results using only 4 labelled samples. 
The problem is very relevant in general and the achieved results are substantial. This by itself warrants the paper to be published in NIPS. However, several important experiments are missing for a better and in-depth understanding of the proposed method. Should it have included those experiments, I’d have rated it as top 15% papers.

Related Works
+ A structured and thorough coverage of the related works regarding domain adaptation.
- related works are missing which have GAN’s discriminator to also predict class labels as in this work
- related works are missing on using pairs of samples as for augmenting low number of available data, especially for learning common embedding space

Approach
+ The incremental novelty of the augmentation technique along with the new loss for the context of deep adversarial domain adaptation is a plausible and effective approach.
- Many details are missing: batchsize, learning rate, number of general iterations, number of iterations to train DCD and g_t/h, number of trials in adversarial training of the embedding space before convergence, etc.
- introduction refers to the case where zero samples from a class is available, however this case is not discussed/studied neither in the approach section nor in the experiments.

Experiments
+ the results using only a few labelled samples are impressive. Specially, when compared with prior and concurrent works using similar approaches.
- Why not go above 4? It would be informative to see the diminishing return
- A systematic study of the components of the architecture is missing, specially comparing when g_t and g_s are shared and not, but also regarding many other architectural choices. 
- In sec 3.1 it is mentioned a classification loss is necessary for the better working of the adaptation model. It seems conceptually redundant, except with the added loss more weight can be put on classification as opposed to domain confusion. In that respect,  the experimental setup lacks demonstrating the importance of this factor in the loss function.
- It seems the most basic configuration of an adversarial training is used in this work, plots and figures showing the stability of convergence using the introduced losses and architectures is important.
- An experiment where for some classes no target sample is available?
","This paper proposed an adversarial framework for the supervised domain adaptation. This framework learns an embedded subspace that maximizes the confusion between two domains while aligning embedded versions through adversarial learning. To deal with the lack of data in target domain, the authors proposed an augmentation method by creating pairs in source and target domains to make the framework work with low number of labeled target samples. There are some problems:

1.	The idea is quite simple and intuitive as data augmentation has been widely used in deep learning for small sample problems. As expected, the results are reasonably good.
2.	Figures and tables in this paper should be better organized. The figure and corresponding description is far away with each other. For example, figure1 is illustrated in page2. But it is first mentioned in page 4.  
3.	The pipeline of framework in figure2 is inconsistency with the paper. For example, Φ is a function that concatenates the results of g_t or g_s. But in figure 2(b), Φ also includes the convolution operation. As mentioned in algorithm 1, the g_t and h should be updated by minimizing (5). However, in figure 2(c), it is updated using data in source domain not target domain.
4.	To make the experiments consistent, the authors should also run the experiments for several times and give the average accuracy and variation on MNIST-USPS-SVHN datasets.
The authors should pay attention to some details in the paper. For example, in figure 4, the vertical axis descriptions on figure4(a)(c) are inconsistent with figure4(b)(d). The horizontal axis descriptions on figure 4(b)(d) is not clear. 
"
Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes,"Taylor W. Killian, Samuel Daulton, George Konidaris, Finale Doshi-Velez",https://proceedings.neurips.cc/paper/2017/hash/2227d753dc18505031869d44673728e2-Abstract.html,"Summary: This paper presents a new transfer learning approach using Bayesian Neural Network in MDPs. They are building on the existing framework of Hidden Parameter MDPs, and replace the Gaussian process with BNNs, thereby also modeling the joint uncertainty in the latent weights and the state space. Overall, this proposed approach is sound, well developed and seems to help scale the inference. The authors have also shown that it works well by applying it to multiple domains. The paper is extremely well written. This paper would make an important addition to the literature of transfer learning within the deep reinforcement learning. 

Things I liked about the paper:

- The paper is extremely well written and easy to understand even for someone not completely familiar with the area. 
- The use of BNNs is well justified and really seem to be helping to expand the scope of applications to more complex applications. 
- The experiments use multiple strong baselines and show clear improvements over them in multiple domains, which include both toy and real world domains.

Concerns:

- There isn't enough analysis of the space requirements for the proposed method, including for the global reply buffer.
- The inference procedure proposed in this paper seems to have too many moving parts and possibly things to tune. It's not clear how much effort would be required to apply this method in a new domain
- The experiments description is a bit short for each of the domain due to space constraints. Making it slightly harder to understand them. For example, it was not clear what the Wb's and rewards corresponded to in each of the application domains. 

Minor Edits:
- Appendix references are missing in multiple places: line 114,198, and 215
- A stylistic suggestion: In the beginning of section 3, use present tense consistently instead of switching between past and present tense. 
- 79: allows us to model
- 88: wall that is blocking 
- 97: model these both classes?
- 99: depends on interactions 
- 100: a linear function 
- 122: stored in both 
- 146: developing a control policy 



","SUMMARY

The authors consider the problem of learning multiple related tasks. It is assumed each tasks can be described by a low-dimensional vector. A multi-task neural network model can than be learned by treating this low-dimensional vector as additional input (in addition to state and action). During optimization, both these low-dimensional inputs and the network weights are optimized. The network weights are shared over all tasks. This procedure allows a new task to be modeled very quickly. Subsequently, 'virtual samples' taken from the model can be used to train a policy (using DDQN). The method is evaluated on three different task families. 

REACTION TO AUTHOR'S REPLY

Thanks for the response. It cleared up many questions I had. I changed my rating accordingly. I think it would be good to show the standard deviation over independently trained models to show the variance induced (or absence of such variance) by the (random) choice of initial tasks and possibly the model training procedure. In the toy task, it seems there is always one 'red' and one 'blue' tasks in the initial data. Does that mean the initial tasks are hand-picked (rather than randomly drawn?). 

TECHNICAL QUALITY

The used approach seems sensible for the task at hand. The development of the method section seems mostly unproblematic. Two issues / suggestions are 1) when tuning the model, shouldn't the update for the network weight take samples from the global buffer into account to avoid catastrophic forgetting for the next task? 2) I'm not sure why you need prioritization for the model update, if you are looking to minimize the mean squared error wouldn't you want to see all samples equally often?

Support for the method is given by experimental evaluation. The method is mostly compared to ablated versions, and not to alternatives from the literature (although one of the methods is supposed to be analogous to the method in [11], and there might not be many other appropriate methods to compare to - would a comparison to [2] be possible?). In any case, the selection of methods to compare to includes different aspects, such as model-free vs model based, and independent models vs. multitask vs. single model for all tasks. It is not clear whether multiple independent runs were performed for each methods. Please specify this, and specify what the error bars represent (standard deviation or standard error, one or two deviations, spread between roll-outs or between independent trials, how many trials).

Dependent on the experiment, the proposed method seems at least as good as most baselines, and (at least numerically) better than all baselines on the most challenging task. The additional material in the appendix also shows the runtime to be much faster compared to the GP-based method in [11].


NOVELTY

The general framework is relatively close to [11]. However, the use of neural networks and the possibility to model non-linear dependence and covariance between states and hidden parameters seems like an important and interesting contribution. 

SIGNIFICANCE AND RELEVANCE

Multi-task and transfer learning seem important components for RL approaches in application domains where rapid adaptation or sample efficiency are important. The proposed method is limited to domains where the difference between tasks can be captured in a low dimensional vector.


CLARITY

There are a couple of issues with the paper's clarity. If found Figure (1) hard to understand, given its early position in the text and the absense of axis labels. I also found Algorithm 1 hard to understand. The 'real' roll-out only seemed to be used in the first episode, what is done with the data after the first episode (apart from storing in global memory for next task?). Maybe split move entire if (i=0) out of the for loop (and remove the condition), than the forloop can just contain (15) if I'm not mistaken. 

The description of the experiments is missing too many details. I couldn't find in either appendix or paper how many source instances the method was trained on before evaulation on the target task for the second or third experiment. I also couldn't find how many times you independently trained the model or what error bars represent. Two source tasks for the toy tasks seems very little given that you have differences between instances across multiple axis - with just two examples would it only learn 'red' vs 'blue' since this is the most important difference?

Furthermore there are quite a few spelling errors and similar issues in the text. I'll mention some of them below under minor comments.




MINOR COMMENTS
- Typesetting: subscripts GP and BNN are typeset as products of single-letter variables
- You might want to explain the subscripts k,b,a,d in (1) briefly.
- 79: model -> to model
- since your model's epsilon distribution is not dependent on the current state, I don't think this could model hereskedasticyity (line 31). If so, please explain. (Uncertainty in weights lead to different predictive uncertainty in parts of the state-space, like a regurlar GP. But, like a regular GP this doesn't necessarily mean you model different 'noise distribution' or risk in different part of your state space )
- 'bounded number of parameters' seems somewhat ambiguous (do you mean a finite-dimensional vector, or finitely many values for this parameter?). 
- 81: GP's don't really care about high-d input representation (linear scaling in input dimensionality), tractability issues mainly concern number of samples (cubic scaling for naive implementations).
- 92-93: are delta_x and delta_y really the same weather a=E or a=W as it appears from this equation?
- 100: a linear of function -> a linear function
- 114: Sec ??
- 123: is develop -> is to develop
- 169 from which learn a policy -> please rephrase
- 174 on trained two previous -> please rephrase
- 188 ""of average model""-> of the average model
- 198 Appendix ??
- 230 - I'm not sure if embedded is the right word here
- 247-249 - it would be interesting to see a comparison
- 255 - ""our work""
- references: It would be good to go over all of these ones as there are many small issues. E.g. the IEEE conferences mention the year and 'IEEE' twice. Names like 'Bayesian' or 'Markov' and abbreviations like STI, HIV, POMDP, CSPBVI should be capitalized. Q-learning too. For [21] I would cite the published (ICLR) version. Publication venue is missing for [35].
- appendices: just wanted to make sure you really meant exp(-10), not 1E-10. Does the prior need to be so small? Why? What do you mean when you say the prior variance changes over training? I think with |w| you mean the number of elements, but you used notation of a norm (w is a vector, not a set). 
 
","This paper proposes a new formulation of Hidden Parameter Markov Decision Pro-
cess (HiP-MDP) which is a model with a structure to adopt subtle change in tasks by using a latent task parametrization that is combined with state and action to define system dynamics. Differently from the previous HiP-MDP, the proposed method models the interaction between states and the latent parameters for HiP-MDP by using a Bayesian neural network that also improves scaling.


The paper explains the motivation and contributions clearly.  Integrating the interaction between latent parameters and states in HiP-MDP is an interesting motivation but authors need to demonstrate better that this extension yields a significant improvement in other different settings than toy data and HIV data.  Rather than running linear version, direct comparison to Doshi-Velez and Konidaris would be better to show that the extension is significantly better by integrating the interaction properly . 

Authors state that closer work is Bai et al.  however they do not compare to them. 
There are some typos to be fixed: The paragraph related to health care is incomplete:“diagnoses
for each subgroup. Our work” and “Appendix ??”

The paper proposes a novel approach for transfer learning among tasks with subtle changes by adopting HiP-MDP.  In order to be convinced that the extension is significant, having more results demonstrating the accurate modeling of the interaction in HiP-MDP would be helpful."
Multi-View Decision Processes: The Helper-AI Problem,"Christos Dimitrakakis, David C. Parkes, Goran Radanovic, Paul Tylkin",https://proceedings.neurips.cc/paper/2017/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html,"This article presents a Markov decision model with two independent controllers, who have different beliefs about the transition probabilities of the model. The model is designed to fit the situation of an autonomous vehicle with both a machine and a human controller. As such the presentation is very much focused on a situation in which one player (the machine) has a correct model of state transitions whereas the other player (the human) has an incorrect model. The example of interest is where the human can, at any time, decide to intervene and seize control the game. The model allows us to consider the implications of this dichotomy, and in particular whether the ""correct"" controller should use a different strategy if it knows about the failures of the ""incorrect"" controller. Efforts are also made to calculate how much lower the system payoffs will be under this scenario.

I like the concept of the paper, and think it's quite well put together. However the paper is insufficiently discursive for us to appreciate the meaning of the concepts provided. We are told about the influence of an agent, about policy distance, and sub optimality levels, but I gleaned no intuition for how tight these concepts are, or how to use them in an example setting. Furthermore the model results in non-Markovian and complicated solution concepts, and the presented solution method is only approximate with no guarantees on its level of sub optimality. Some experiments are carried out, but they are cursory and it's nigh on impossible to work out what is going on.

In summary, each component is (to my mind) interesting, but the presentation of all of these concepts in such a tight package means that we do not learn enough about any of them to ascertain if they are important.

Furthermore, a discrete state and action space is so distant from the application area that it is difficult to see that this is actually sufficiently along the lines towards the application to merit giving up on complete theory.","The paper introduces multi-view decision processes (MVDP) as a cooperative game between two agents, where one agent may have an imperfect model of the transition probabilities of the other agent.

Generally, the work is motivated well, the problem is interesting and clearly relevant to AI and the NIPS community. The theoretical results are nicely complemented by experiments. Presentation is mostly smooth: the main goals are consistent with the narrative throughout the paper, but the some details from the experiments seem less clear (seem points below.)

Some points are confusing and need further clarification:

1. In the second footnote on page 3, what is a ""worst-case best response""? This is not defined, seems like they should all share the property that they attain the maximum value against some policy.

2. The authors talk about solving using backward induction and make reference to Bozansky et al 2016 and Littman 1994, but those works treat exclsively the zero-sum case. MVDP is a cooperative game that can be at least as hard as general-sum games (by Lemma 1), and backward induction generally cannot solve these due to equilibrium selection problems leading to non-unique values for subgames, also shown in Zinkevich et al 2005. How is it possible to apply backward induction by propagating single Q and V values here then? The authors claim to find an approximate joint optimal policy, which is stronger than a Nash equilibrium, and it seems by Zinkevich et al 2005 this should at least require cyclic (nonstationary) strategies.

3. From the start the authors talked about stochastic policies, but this is a cooperative identical payoff game. It's not clear later (seeing the proof of Lemma 1) that mixed strategies are necessary due to the different views leading to an effectively general-sum game. How then could a the determinstic version of the algorithm that computes a pure stationary policy be approximately optimal? Even in the Stackelberg case, mixing could be necessary.

Minor:

The role of c in the c-inervention games is not clear. On page 6, the authors write a cost c(s) is subtracted from the reward r_t when P2 takes an action other. But then in Remark 1 gives the equation rho(s, a) = [rho(s) + cI{a_{t,2} != a^0}] / (1+c) does not subtract c. This can be easily fixed by rewording the text at bottom of page 6.
","Summary:
Motivated by settings in which a human and an artificial intelligence agent coordinate to complete a task (e.g in autonomous driving of a vehicle), authors consider a Stackleberg game in which the leader (AI) has perfect knowledge of the transition probabilities, but the follower’s (human) model (his/her subjective probabilities) might be inaccurate. Authors analyze the impact of the inaccuracy in the follower’s subjective transition probabilities on policy planning by the leader and present a dynamic programming approximation algorithm for computing the optimal policy. 

I find the problem interesting, relevant, and well-motivated. But I find it hard to interpret the results authors present. In particular, section 2 of the paper quantifies how far the optimal utility can be from that of the jointly optimal policy and the naive optimal policy, but these bounds are not elaborated on and is hard to understand whether they say anything non-trivial and/or interesting. 

The algorithm proposed (which is a form of backward induction) does not solve the planning problem exactly (that is shown to be computationally hard as one may expect), but it additionally suffers from multiple limitation (as outlined by authors themselves in paragraph “optimality”, line 235).That calls for substantial expansion of the empirical section of the paper, possibly evaluating on real-world data and settings."
Maximum Margin Interval Trees,"Alexandre Drouin, Toby Hocking, Francois Laviolette",https://proceedings.neurips.cc/paper/2017/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html,"The paper introduces a computationally efficient decision tree algorithm for learning interval data outcomes. The paper is well written and provides useful illustrates to document the method. The method shows fine performance on simulated data and several real data sets.

The paper compares against other interval based prediction methods. However, several of them are not contrasted in the related work. Adding differentiating factors from those would strengthen the paper in identifying when to use MMIT.  

While the paper is motivated heavily by survival analysis, the algorithm was not demonstrated on classic survival data sets (e.g. UCI thoracic surgery, PBC, etc.). It is not clear why not, given the motivation and that the algorithm appears to work with intervals defined as y_= y-.
In survival analysis one is typically interested in rate estimation and risk attribution. In this case, the method would predict a time of event. I imagine risk attribution would be done in the same way as, e.g. random survival forests.

Decision trees can be limited in their performance. Food for thought: how might you extend this to MMI forests?","
The authors of this paper present a new decision tree algorithm for the interval regression problem.  Leaves are partitioned using a  margin based hinge loss  similar to the L1-regularized hinge loss in Rigaill et al, Proc ICML 2013. However, the regression tree algorithm presented in this work is not limited to modeling linear patterns as the L1-regularized linear models in Rigaill et al.  For training the non linear tree model, a sequence of convex optimization subproblems are optimally solved in log-linear time by  Dynamic Programming (DP). The new maximum margin interval tree (MMIT) algorithm is compared with state-of-the-art margin-based and non-margin-based methods in several real and simulated datasets.
-	In terms of originality, the proposed margin based hinge loss is similar to the L1-regularized hinge loss in Rigaill et al, Proc ICML 2013. However, the regression tree algorithm presented in this work is not limited to modeling linear patterns as the L1-regularized linear models in Rigaill et al. MMIT achieves low test errors on nonlinear datasets and yields accurate models on simulated linear datasets as well. 
-	In terms of significance: interval regression is a fundamental problem in fields such as survival analysis and computational biology. There are very few algorithms designed for this task, and most of them are linear models. A  method learning nonlinear tree models like Maximum Margin Interval Trees (MMIT) could be helpful for practitioners in the field.
-	It terms of quality, the paper is fairly executed, the authors compare MMIT with state-of-the-art margin-based and non-margin-based methods in several real and simulated datasets. The 11-page supplementary material contains proofs, pseudocode, details of the open source implementation (link was hidden for anonymity) and of the experiments.
-	In terms of clarity, this is a well written paper.
In summary, my opinion is that this is a well written and nicely organized work; the proposed method would be useful in real-world applications, and the novelty of the work satisfies the NIPS standards. Thus I recommend this work for publication.
","Maximum Margin Interval Trees
---------------------------------

In this paper the authors study interval regression problems, where
each example is associated with a range output instead of a single
point. Specifically, the authors investigate how to modify trees to
produce such output by minimizing a modified sum of hinge losses. The
key contribution of the paper is a dynamic programming algorithm that
efficiently constructs these trees.  The authors provide experimental
results on a variety of simulated and real datasets.

This is a generally well written paper, though it gets a little
subscriptitis in 4.1-4.2. The algorithm seems sound and the
experiments do a good job of comparing the approach to a reasonable
set of baselines on a variety of datasets along both accuracy and time
complexity metrics (which is the key selling point of the paper). 

I did have some questions that the authors could clarify:
1. What is epsilon in the experiments? How was it chosen? What is the
effect of varying it?
2. What changes if the breakpoints are not all different (line 117)?
3. Does the algorithm work for trees with nonconstant leaves? (a model
tree of some sort) What would need to change?
4. The choice of the examples with CART in the experiments seems
rather strange. I think a baseline with CART where the examples are
(x, (y_low+y_high)/2) would make more sense.
5. Along the same lines, would just building two trees one for y_low
and the other for y_high work? I'm not very convinced we need a separate
method just for this kind of problem.

To summarize, this is a nice paper that proposes and studies an
algorithm for learning interval trees and supports it with
experimental results. Some key clarifications and experimental
modifications would make the paper stronger.

After feedback: The authors have clarified several questions I had, and I have upgraded my score to take this into account."
Online Learning with a Hint,"Ofer Dekel, arthur flajolet, Nika Haghtalab, Patrick Jaillet",https://proceedings.neurips.cc/paper/2017/hash/22b1f2e0983160db6f7bb9f62f4dbb39-Abstract.html,"This paper introduces a new setting for online linear optimization where the learner receives a signal at the beginning of each round about the loss function going to be played by adversary. Under strongly convexity assumption on the player’s decision set, the paper shows an improved O(log n) regret bound. The authors also provide nice results on lower bounding the regret under different notions of convexity/uniformity on decision set.
	
The problem being dealt with is interesting and has proper motivation, though previous papers have dealt it with in the past, in one way or another. The presentation of the paper was mostly clear. The claimed contributions are discussed in the light of existing results and the paper does survey related work appropriately. Regarding the quality of the writing,  the paper is reasonably well written, the structure and language are good. The paper is technically sound and the proofs seem to be correct as far as I checked. 

To conclude, the paper is overall well written and well organized. The problem dealt with consists of a new view to the online linear optimization that requires a new type of analysis. The exact setup of the feedback lacks motivation, but overall the idea of an analysis aimed to exploiting side information in online learning is sufficiently interesting and motivated.
","The paper considers a model of online linear optimization in a convex and compact set K where, at the beginning of each time step t, the learner receives an adversarially chosen vector v_t such that the cosine of the angle between v_t and the actual loss vector at time t is at least alpha > 0. The main result shows that the hint can be used to improve the regret by optimally changing the component parallel to the hint of a prediction x. This corresponds to measuring the linear loss of x using a convex ""virtual"" loss function whose curvature is related to the curvature of K. In particular, when K is (essentially) a L_2 ball (i.e., modulus of convexity exponent q=2), then the virtual loss is exp-concave. As the virtual loss regret is always smaller than the linear loss regret, this implies a logarithmic regret bound on the original problem.

The scenario for q > 2 is not so nice anymore (the problem gets harder because for q large K tends to a L_infinity ball for which hints provably don't help). Indeed, improvements over the T^{1/2} standard regret rate are obtained only when q < 3 and hints are taken from a finite set. Also, the algorithm is completely different from the one for the case q = 2.

Instead, here one would like to have a single algorithm covering all q larger or equal than 2, with a regret attaining T^{1/2} as q grows large, and no need for a finite set of hints.

The lower bounds are interesting and informative.

Overall, an interesting paper with nice results for the case q=2. The virtual loss functions are a good trick that works well. On the other hand, the setting is not really motivated and the results for q > 2 are not that convincing.

In ""Competing with wild prediction rules"" Vovk studied a possibly related problem where the learner competes with a Banach space of functions with curvature q \ge 2 and uses no hints. Perhaps this setting can be reduced to that one.","The paper concerns online linear optimization where at each trial, the player, prior to prediction, receives a hint about the loss function. The hint has a form of a unit vector which is weakly correlated with the loss vector (its angle's cosine with loss vector is at least alpha). The paper shows that:
- When the set of feasible actions is strongly convex, there exists an algorithm which gets logarithmic regret (in T). The algorithm is obtained by a reduction to the online learning problem with exp-concave losses. The bound is unimprovable in general, as shown in the Lower Bounds section.
- When the set of actions is (C,q)-uniformly convex (the modulus of uniform convexity scales as C eps^q), and the hint is constant among trials, a simple follow-the-leader algorithm (FTL) achieves a regret bound which improves upon O(sqrt(T)) when q is between 2 and 3. This is further extended to the case when the hint comes from a finite set of vectors.
- When the set of actions is a polyhedron, the worst case regret has the same rate as without hints, i.e. Omega(sqrt(T)), even when the hint is constant among trials.

The first result is obtained by a clever substitution of the original loss function by a virtual loss which exploits the hint vector and is shown to be exp-concave. Then, an algorithm is proposed which as subroutine uses any algorithm A_exp for exp-concave online optimization, feeding A_exp with virtual losses, and playing an action for which the original loss is equal to the virtual loss (by exploiting the hint) of A_exp. Since the virtual loss of the adversary is no larger then its original loss, the algorithm's regret is no more than the regret of A_exp.

Interestingly, the second result uses FTL, which does not exploit the hint at all (and thus does not even need to know the hint). So I think this result is more about the constraint on the losses -- there is a direction v, along which the loss is positive in each round. This result resembles results from [15] on FTL with curved constraint sets. I think the results from [15] are incomparable to those here, contrary to what discussion in the appendix say. On the one hand, here the authors extended the analysis to uniformly convex sets (while [15] only work with strongly-convex sets). On the other hand, [15] assume that the cumulative loss vector at each trial is separated from the origin, and this is sufficient to get logarithmic regret; whereas here logarithmic regret is achieved under a different assumption that each individual loss is positive along v (with constant alpha). 

The extension of Theorem 4.2 to arbitrary hints (Appendix C) gives a bound which is exponential in dimension, while for q=2 Section 3 gives a much better algorithm which regret is only linear in the dimension. Do you think the exponential dependence on d for q > 2 is unimprovable? 

The paper is very well written, and the exposition of the main results is clear. For each result, the authors give an extended discussion, including illustrative examples and figures, which builds up intuition on why the result holds. I checked all the proofs in the main part of the paper and did not find any technical problems. I found the contribution to online learning theory strong and significant. What the paper lacks, however, is a motivating example(s) of practical learning problem(s) in which the considered version of hint would be available.

---------------

I have read the rebuttal. The authors clarified comparison to [15], which I asked about in my review.
"
DPSCREEN: Dynamic Personalized Screening,"Kartik Ahuja, William Zame, Mihaela van der Schaar",https://proceedings.neurips.cc/paper/2017/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html,"This paper presents DPSCREEN, a method for determining optimal screening policies for large classes of diseases. Compared to other work in the area, DPSCREEN takes into account both the patient's clinical (screening) history, in addition to patient features pertinent to the specific disease model. As a result, DPSCREEN is able to achieve personalization in the screening process, which yields efficiency gains over existing screening policies without negatively impacting the timeliness of disease detection.

Overall I found this paper to be well-written and technically sound. The approach is well-motivated, considers pertinent related work, and provides a concise but thorough discussion of the mathematical basis of the algorithm, including computational complexity. DPScreen is validated on a large (45k patient) dataset from the Athena Health Network. This sufficiently demonstrates the improvements provided by personalization. The model is also validated across disease model types. The result is a paper that makes a clear technical contribution that also has the potential for practical impact as well.","The objective of the paper is to find the best policy for patient screening given the pertinent information. To provide the policy, a disease should be modeled as a finite state stochastic process. The policy is trying to minimize screening costs and delays. The authors propose an approximate solution that is computationally efficient. The experiments on simulated data related to breast cancer indicate that the proposed algorithm could reduce delays while keeping the same cost when compared to a trivial baseline
  Positives:
+ personalized screening is an important topic of research
+ the proposed algorithm is reasonable and well-developed
+ the results are promising
+ the paper is well written
  Negatives:
- the proposed algorithm is purely an academic exercise. It assumes that the disease model of a given patient is known, which could be never assumed in practice. The main obstacle in personalized screening is not coming up with a policy when the model is known, but inferring the model from data. 
- the methodological novelty is not large
- the experiments only compare the proposed algorithm to a baseline doing annual screening. Given the previous point about the disease model, an important question that should be studies experimentally is the robustness of the proposed method to inaccuracies in the disease model. In other words, how sensitive is the policy to uncertainties in the disease model.
  Overall, the topic is interesting and the proposed algorithm is reasonable. However, the methodological novelty is not large and the paper would need to do a better work to convince readers that this work is practically relevant.",The paper suggests a method for predicting appropriate screening time for breast cancer patients to minimize screening cost and delay cost. This is done in a personalized fashion by taking into account the personal medical history of the patient along with external information. The paper is generally well written and the proposed method shows significant improvement over existing methods.
Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions,"M. Sevi Baltaoglu, Lang Tong, Qing Zhao",https://proceedings.neurips.cc/paper/2017/hash/23af4b45f1e166141a790d1a3126e77a-Abstract.html,"The paper presents a simple yet interesting problem. That of optimizing a bidder payoff in repeated auctions. The setup considered by the paper is one where payouts and spot prices are random. The learner must choose his bidding strategy under a budget constrained.  While in principle this problem can be solved doing empirical payoff maximization, as the authors show, this is in fact NP hard. 

The contribution of the paper is to provide a dynamic program solution to the empirical optimization problem under the reasonable assumption of payoffs being Lipchitz in expectation as a function of the bids.  The algorithm is simple and straightforward, while the proofs are fairly standard. Nevertheless the authors are still the first to propose this solution to the problem. The empirical results are also compelling. 

On the down side,  since the authors are considering a stochastic setup it would be interesting to measure the performance of this algorithm in terms of a notion of pseudo-regret in which case one would expect it to be constant.
Finally, the main issue with this paper is its lower bound. Indeed, the lower bound provided here corresponds to an adversarial setup.  It is the usual random distribution that depends on the number of rounds chosen to prove lower bounds in bandits and expert learning algorithms in an adversarial scenario. ","*I have read the rebuttal.*

Summary:
This paper studies online repeated bidding strategy for a specific multi-commodity auction, where at each time the bidder needs to decide how to allocate a fixed budget among K goods with individual clearing price and utility. The environment is assumed to be iid so that a simple follow the leader approach will admit sublinear regret. The difficulty is that computing ERM is NP-hard for this problem and the paper thus proposes to discretize the possible bids so that a simple dynamic programming approach can efficiently compute the ERM. A regret bound of order \sqrt{T} is proven under some Lipschitz continuity assumption, and a matching lower bound is also provided. The paper is concluded with an experiment on real data comparing several algorithms.

Major Comments:
Overall the problem is quite interesting and the experimental results are promising. The technical contributions of the paper is not very strong though. Everything appears to be straightforward: the environment is stochastic so follow the leader works already; computing exact ERM is NP-hard, so discretization is a natural idea; the dynamic programming formula is also not difficult to come up with.

One suggestion I have is to see whether techniques from a recent work of Dudik et al. ""Oracle-Efficient Online Learning and Auction Design"", FOCS 2017 can be used here to deal with adversarial setting. Their approach assumes an ERM oracle, which is exactly provided by this work.

Minor Comments:
1. The paper keeps mentioning related works on MAB, but the setting here is full information instead of bandit. I think the related works discussion could be improved in this sense.

2. Eq.(1), maybe define what \geq mean for vectors.

3. L146, the solution of the example needs more explanation.

4. L172, r'_{k,i}, maybe swap the two subscripts to make notation more consistent.

5. In the definition of V_n(b), I think \hat{r}_t should be \hat{r}_{t,i}.","This paper  studies the online learning (stochastic and full-information) problem of bidding in multi commodity first price auctions. The paper introduces a polynomial time algorithm that achieves a regret of \sqrt{T log(T)} that has a near optimal dependence on T.

The main challenge that the paper has to deal with is to find a computationally efficient algorithm for computing the best biding strategy given a known distribution.The authors first demonstrate that natural approaches for solving this problem exactly are not computationally efficient (this is not a formal np-hardness proof). Then, they provide a FPTAS for solving the problem using dynamic programming. Once they have a FPTAS for the offline problem, their results hold for the stochastic online setting using existing reductions. I haven’t carefully looked in to the details of their analysis of the dynamic programming, but I think the effectiveness of it here is interesting and surprising — specially given that the variation of this problem for the second price auctions is hard to approximate. 

As for the weaknesses, I find the theorems to be hard to interpret. Particularly because there are a lot of variables in them. The authors should provide one simple variation of theorem 1 with fewer parameters. For example, when l = 0, and u = 1, the lipschitz conditions holds with L = 1 and p = 1, and B = K. Assigning \alpha = \sqrt{t} gives a regret of \tilde O(\sqrt{t \log(T)}).

As mentioned above the results mentioned here hold for the full-information online stochastic setting, that is, the valuation and the clearing price are drawn from a the same distribution at every step. I find this to be a weird choice of a setting. In most cases, full information settings are accompanied by adversarial arrival of the actions, rather than stochastic. This is indeed due to the fact that the statistical and computational aspects of online stochastic full-information setting is very similar to the offline stochastic full-information setting. So, I think a much more natural setting would have been to consider the adversarial online setting.

In this regard, there are two very relevant works on no-regret learning in auctions that should be cited here. Daskalakis and Syrgkanis FOCS’16 and later on Dudik et al. FOCS’17 study the problem of online learning in Simultaneous Second Price auctions (SiSPA). This is also the problem of optimal bidding in a repeated multi-commodity auction where each item is assigned based on a second price auctions, rather than a first price auction that is considered in this paper. In particular, the latter paper discusses computationally efficient mechanisms for obtaining a no-regret algorithm in the adversarial setting when one can solve the offline problem efficiently. Their methods seem to apply to the problem discussed in this paper and might be specially useful since this paper provides an approximation method for solving the offline problem. It is worth seeing whether the machinery used in their paper can directly strengthen your result so that it holds for the adversarial online setting. 

After rebuttal:
I suggest that the authors take a closer look at the works of Dudik et al. and Daskalakis and Syrgkanis. In their response, the authors mention that those works are inherently for discrete spaces. From a closer look at the work of Dudik et al. it seems that they also work with some auctions specifically in the continuous space by first showing that they can discretize the auction space. This discretization step is quite common in no-regret learning in auctions. Given that the current submission also shows that one can discretize the auctions space first, I think the results of Dudik et al. can be readily applied here. This would strengthen the results of this paper and make the setting more robust. I suggest that the authors take a closer look at these works when preparing their discussion."
A-NICE-MC: Adversarial Training for MCMC,"Jiaming Song, Shengjia Zhao, Stefano Ermon",https://proceedings.neurips.cc/paper/2017/hash/2417dc8af8570f274e6775d4d60496da-Abstract.html,"This paper describes a novel adversarial training procedure to fit a generative model described by a Markov chain to sampled data. The Markov chain transitions are based on NICE, and so are reversible and volume preserving. It is therefore straightforward to use these as proposals in a Metropolis MCMC method to sample from arbitrary distributions. By repeatedly fitting the Markov chain model to samples from preliminary runs, we can hope that we'll end up with an MCMC method that mixes well on an arbitrary target distribution. Like HMC, the Markov chain is actually on a joint distribution of the parameters of interest, and some auxiliary random draws used to make a deterministic proposal.

It's a promising idea. The MCMC examples in the paper aren't particularly compelling. They're ok, but limited. However, this is the sort of algorithm that could work and be useful in some realistic situations.


Quality

The paper appears to be technically sound. It describes a valid MCMC method. It's unclear when a bootstrapping procedure (a precise procedure isn't given in the paper) will converge to a working proposal mechanism. The paper ends alluding to this issue. Variational inference would find a feasible region of parameters, but (like all MCMC methods), the method could be trapped there.

There is a danger that parts of the distribution are ignored. The cost function doesn't ensure that the proposal mechanism will get everywhere. Future work could investigate that further. Mixing updates with a more standard MCMC mechanism may reveal blind spots.

The toy experiments indicate that the method can learn to hop modes with similar densities in 2D. But don't reflect statistical models where MCMC inference is used. Of course section 3 shows some interesting transitions can be learned in high dimensions. However, the interest is in application to MCMC, as there is plenty of work on generating synthetic images using a variety of procedures.

The experiments on Bayesian logistic regression are a starting point for looking at real statistical analyses. Personally, if forced to use HMC on logistic regression, I would estimate and factor the covariance of the posterior S = LL', using a preliminary run, or other approximate inference. Then reparameterize the space z = L^{-1}x, and run HMC on z. Equivalently set the mass matrix to be S^{-1}. Learning a linear transformation is simpler and more standard than fitting neural nets -- would the added complication still be worth it?

Projects like Stan come with demonstrations. Are there more complicated distributions from real problems where A-NICE-MC would readily out-perform current practice?


Clarity

The paper is fairly clearly written. The MCMC algorithm isn't fully spelled out, or made concrete (what's p(v)?). It would only be reproducible (to some extent) for those familiar with both HMC and NICE.


Originality

While the general idea of fitting models and adapting proposals has appeared periodically in the MCMC literature, this work is novel. The adversarial method proposed seems a good fit for learning proposals, rather than boldly trying to learn an explicit representation of the global distribution.


Significance

Statisticians are unlikely to adopt this work in its current form. It's a rather heavier dependency than ""black-box variational inference"", and I would argue that fitting neural nets is not generically black-box yet. The work is promising, but more compelling statistical examples would help.

Volume preserving proposals won't solve everything. One might naively think that the only challenge for MCMC is to come up with proposal operators that can make proposals to all the modes. However, typical samples from the posterior of a rich model can potentially vary in density a lot. There could be narrow tall modes, or short large basins of density, potentially with comparable posterior mass. Hierarchical models in particular can have explanations with quite different densities. A volume-preserving proposal can usually only change the log-density by ~1, or the Metropolis rule will reject too often.


Minor

The discussion of HMC and rules that return to (x,v) after two updates are missing the idea of negating the velocity. In HMC, the dynamics only reverse if we deterministically set
    v' <- -v'
after the first update.

Please don't use the term ""Exact Sampling"" in the header to section 4.1. In the MCMC literature that term usually means generating perfect independent samples from a target distribution, a much stronger requirement than creating updates that leave the target distribution invariant: http://dbwilson.com/exact/

The filesize of this paper is larger than it should be, making the paper slow to download and display. Figure 2 seems to be stored as a .jpeg, but should be a vector graphic -- or if you can't manage that, a .png that hasn't passed through being a .jpeg. Figure 3 doesn't seem to be stored as a .jpeg, and I think would be ~10x smaller if it were(?).

Several bibtex entries aren't capitalized properly. Protect capital letters with {.}.","I have read the author feedback; thanks for the comments and clarifications.

----

This is a fun paper, which proposes a new way to train Markov operators whose stationary distribution matches a desired target. Two applications are considered: one, as a manner for generating synthetic data with a Markov chain whose stationary distribution is a given training data distribution, and two, as a means for learning efficient MCMC kernels for Bayesian posterior inference. The basic idea is to train a generative model and a discriminator according to a GAN objective, where the generative model is parameterized by repeated application of a transition kernel to samples from some initial distribution.

I think that this is a great way to leverage Markov chain properties for constructing generative networks. I expect the choice of objective function in eq (3) is critical to making this work in practice: as the paper states, backpropagation through a long Markov chain, while technically possible, would be prohibitively expensive. Instead, the objective function is chosen to roughly encourage both fast convergence to the ergodic regime, and that the target distribution remains invariant to repeated sampling from the transition distribution. The results here look visually quite good, and this is a much cleaner approach overall than e.g. the earlier (cited) generative stochastic networks.

I find the application to sampling from Bayesian posterior distributions slightly less convincing, but I think the results are nice and there are some good ideas presented. In particular, the connection between volume-preserving flows and volume preservation required for symmetric MH proposals is quite clever, and I think the NICE proposal (or something quite similar) could find other application.

My skepticism stems from:

• What is the real computational cost of training? I find it hard to believe that in real-world problems it is more efficient to both train the NICE proposal as well as run the MCMC algorithm, than it would be to simply go run the HMC algorithm for longer. Relatedly,

• how well does the bootstrap procedure in section 4.5 actually work? How do you choose the initial parameters \theta_0, and how sensitive is this to poor initialization? A pathological choice of initial operator could prevent ever finding anything resembling “real” samples from p_d. In general this sort of “adaptive” MCMC algorithm which updates the parameter simultaneously while performing inference is delicate, and is not necessarily ergodic, losing the guarantees that come with static MCMC algorithms. See e.g. Roberts & Rosenthal “Ergodicity of Adaptive MCMC” for algorithms, theory, and a discussion of the challenges.

• While HMC certainly would fail to move between the modes of mog2, mog6, and ring5, I would point out that these are not really good examples of the sort of distributions which actually arise as Bayesian posteriors. (The logistic regression example is better on that account, but I would be cautious using Bayesian logistic regression as a benchmark for sampler performance; see the paper by Chopin and Ridgeway “Leave Pima Indians alone: binary regression as a benchmark for Bayesian computation”.)

Minor comments: 

• The phrase NICE is used throughout, starting with the title and abstract, but the acronym is not finally defined until the middle of page 5!

• Consider using additional sampling diagnostics than ESS, and consider an HMC benchmark which averages over multiple chains. Using multiple chains (and looking at Gelman’s R-hat diagnostic criteria) should yield better results for HMC on the synthetic data examples, as the algorithm would not collapse into a single mode."
Question Asking as Program Generation,"Anselm Rothe, Brenden M. Lake, Todd Gureckis",https://proceedings.neurips.cc/paper/2017/hash/24681928425f5a9133504de568f5f6df-Abstract.html,"This paper attempts to reproduce user questions in a game of battleship where asking about individual squares is replaced by asking questions essentially over an artificial language with compositional logical (Montagovian) semantics.  A log-linear model is used over a four simple features of the questions --- expected information gain (informativeness), answer type (Boolean or numerical), and whether the question involves features of the board at all.

At a high level the motivation in terms of active learning is reasonable but the ideas in this paper are all rather obvious and the task is very simple, highly constrained, and logically unambiguous.  I do not believe that this paper will be of interest to the NIPS community.","The authors examine human question asking where answers are K-ary. They define by hand a PCFG for a “battleship” domain, where there are hidden colored shapes in a partially observable grid (i.e., some tiles are revealed as containing part of a ship of a specific color or being empty). The task of the agent is to ask a question with a single word answer that provides as much information about the state of the board. The PCFG served as a prior over questions, which were defined as statements in lambda calculus. Question goodness was defined as a linear function of its informativeness (expected information gain), its complexity (in terms of its length or negative log probability under of it being generated by the PCFG with a uniform distribution over rewrite rules), and “answer type” (e.g., whether it provides a true/false or color as an answer). The human data came from a previously published paper and was hand coded into their grammar. The authors compared lesioned versions of the model via leave one out cross-validation and found that all factors provided a non-negligible contribution (although complexity was clearly the most important). Finally, they presented a series of novel questions.

Question-asking is an important domain within linguistics and hypothesis testing. The authors provide an innovative perspective unifying modern linguistic and cognitive theory to examine how people ask questions, which has the potential to foster novel research in the active learning literature with models asking more sophisticated questions. The paper is extremely clear and well-written. The model is appropriately evaluated from both quantitative and qualitative perspectives.

From the perspective of a computational cognitive scientist, this paper advances the field of self-directed learning. Previous work found that question informativeness was a poor predictor of human question asking. This followup paper resolves part of why this is the case (it is secondary to complexity and may in part be due to the size of possible questions available to the model). Thus, the paper contributes to the cognitive science literature, which may be sufficient to justify its publication in NIPS.

However, there are serious limitations to their approach, which I can see limiting its interest to the more general NIPS audience. Although it may be possible to automate the human question text to statements in some semantic logic, it is unclear how the approach could scale or extend to domains of interest to the broader community. The battleship domain is small with only a few different features and their method is already computationally strained. That being said, the ideas are interesting enough (active learning with more rich questions) to be of interest, though it may be that the authors will need to provide a clear scenario of interest to the broader community where their technique is likely to be applicable and provide novel insights. 

Minor comments:
1)	It may not be revealing, but I believe you can do a nested model comparison with the LLs of the lesioned models and get tests of statistical significance.
2)	Moving forward, for sequential question asking, there are other models within the linguistics literature of question-asking in formal semantics and pragmatics. A lot of the work comes out of the Questions under Discussion literature. I listed a few references as pointers to this literature.
Ginzburg, J. (1995). Resolving questions, part I. Linguistics and Philosophy, 18(5), 459-527.

Rojas-Esponda, T. (2013). The roadsigns of communication. Proceedings of Semdial (DialDam), 17, 131-139.
","The paper presents a cognitive model that is intended to predict when certain questions are asked in certain contexts (belief states). Under this model, the unnormalized log probability of a question is defined as the weighted sum of informativeness (expected information gain), complexity (roughly, question length), answer type, and ""relevance"" (whether the question refers to the belief state or just world-independent logical facts). This model is applied to the Battleship domain where the task is to figure out where within a grid a number of ships (clusters of grid elements) are located. The full model with all of the features mentioned above did better at predicting held-out human data (in terms of log-likelihood) than lesioned versions with fewer features. In terms of correlation with human judgments, the model that didn't take into account EIG did about as well as the full model.

The paper seems generally technically sound.

I think it succeeds more as an AI project than as a cognitive science project. The paper states that ""We find that our model predicts what question people will ask"" and ""from a CogSci standpoint, our results show how people balance informativeness and complexity."" However, I'm not convinced that we learn much about how people balance these factors. In the Battleship domain, it seems that people are either influenced by EIG a little (if we believe the log-likelihood results, which according to bootstrap estimates have a 19% chance of being wrong) or not at all (if we believe the correlation results, which are also pretty noisy due to sparse human data). I don't think these results allow us to conclude much about how people balance these factors in general.

The paper is well-written and clear. I could probably reproduce the results.

The paper is missing some prior work. One of the apparent innovations of the paper is to ""model questions as programs that, when executed on the state of a possible world, output an answer."" This is the same as the notion of a ""goal"" used in Hawkins 2015 (""Why do you ask? Good questions provoke informative answers.""), and is based on the notion of a question-under-discussion (QUD) introduced by Roberts 1996 (""Information structure in discourse: Towards an integrated formal theory of pragmatics.""). The questioner model by Hawkins also uses expected information gain and trades it off with the complexity of questions, but is applied to a much simpler domain.

I think this sort of model (that has a compositional prior over questions and chooses based on EIG and other factors) makes a lot of sense as a component for cognitive and AI models, and I expect it to be used more in the future. I view the main contribution of this paper as showing that you can in fact generate interesting compositional questions for a non-trivial domain using this approach.
"
Gradient Methods for Submodular Maximization,"Hamed Hassani, Mahdi Soltanolkotabi, Amin Karbasi",https://proceedings.neurips.cc/paper/2017/hash/24b43fb034a10d78bec71274033b4096-Abstract.html,"This paper study the problem of continuous submodular maximization subject to a convex constraint. The authors show that for monotone weakly DR-submodular functions, all the stationary points of this problem are actually good enough approximations (gamma^2 / (1 + gamma^2), where gamma is the DR factor) of the optimal value. They also show that stochastic projected gradient descent and mirror descent converge to a stationary point in O(1/eps^2) iterations.

The paper is clear and well presented, with few typos. The results presented can handle more general constraints than prior work and can address the stochastic setting, but at the cost of a worst approximation factor (1/2 for DR-submodular functions as opposed to 1 - 1/e).

Some comments/questions:
-It is worth mentioning that stochastic projected gradient descent was also used recently for submodular minimization in ""Chakrabarty, D., Lee, Y. T., Sidford, A., & Wong, S. C. W. Subquadratic submodular function minimization. STOC 2017.""
- For the deficiency example of FW in A.2, it would be clearer to present the example with a minibatch setting, with m_i,n = 1/(b+1) batch size, the same example should follow through, but it would make it clearer that a slightly larger minibatch size won't fix the problem.
- Would this deficiency example hold also for the algorithm ""Measured continuous greedy"" in ""Feldman, Moran, Joseph Naor, and Roy Schwartz. ""A unified continuous greedy algorithm for submodular maximization."" FOCS, 2011, whose updates are slightly different from FW?
- You reference the work [20] on p.2, but I did not find this work online?
- Can you add what are other examples of L-smooth continuous submodular functions, other than the multilinear extension?
- For a more convincing numerical comparison, it would be interesting to see the time comparison (CPU time not just iterations) against FW. 
- What is the y-axis in the figures?

Some typos: 
-line 453: f_ij - f_i - f_j (flipped)
-lemma B.1: right term in the inner product should be proj(y) - x.
-lemma B.2: first term after equality should be D_phi(x,y)
- Eq after line 514 and line 515: grad phi(x+) (nabla missing)
- In proof of lemma B.5: general norm instead of ell_2
- Eq after line 527 and 528: x_t and x^* flipped in inner product.","Summary: The paper proves a 1/2-approximation guarantee for fixed points of monotone continuous DR-submodular functions. It also proves guarantees for stochastic gradient and mirror methods. Overall, I think the theoretical results in the paper are interesting. However, the presentation of the paper is not good and confusing, and it reduces the quality of the whole paper.

Some specific comments:
1. The paper contains so many typos and grammar mistakes that are really distracting.
2. From line 94, the definition says that the cross-second-derivatives are non-negative, but in Equation (2.3) they are <= 0.
3. It is not clear to me what the L_2 mentioned on line 109 is? And what does that sentence mean?
4. In Theorem 4.2, the function should be weakly DR-submodular.
5. The definition of the projection in Definition 4.5 is confusing. I think this sentence needs to be rewritten.
6. Theorems 4.2, 4.3, and 4.6 seem to prove results for different types of functions. For instance, Theorem 4.2 is for weakly DR-submodular functions, Theorem 4.3 is only for DR-submodular functions, and Theorem 4.6 is for submodular functions. The paper should make them clear and discuss this more.
7. For Theorem 4.6, does the function F need to be monotone? If not, why?
8. In the experiment, since FW was proposed for the non-stochastic case, the paper should also compare the methods in this setting (i.e. using full gradients instead of mini-batches).
9. Since the main contributions of the paper are the theoretical results, it would be useful to include some proof sketches of the theorems. The paper can save more space by, for example, reducing the repeated description of the mirror ascent algorithm in Theorem 4.6.","This paper considers algorithms for the maximization of continuous (weakly) submodular functions in the stochastic setting over a bounded convex set, and proposes using projected SGD. SGD and projected SGD methods have been widely studied for convex stochastic problems, but the problem considered here is not convex. These methods have also been studied recently for nonconvex L-smooth stochastic optimization problems where rates of convergence to stationary points are obtained. In my view, what is novel in this paper is to combine this existing results with a continuous DR-submodular assumption, which allows the authors to show that all stationary points have values of at least OPT/2, so they can get a 1/2 approximation guarantee (Theorem 4.2). This is a nice approach that brings continuous optimization ideas to a field that is often studied from a discrete viewpoint, which I think is timely and relevant.

My vote is a ""clear accept"", but I feel addressing the comments/questions below can indeed make it a ""strong accept"":

1. I suggest the authors give a brief proof outline of teh main theorems in the main text; I did look at the proofs in the supplement, but it is better to have a sense of what part of proofs follow from similar proofs of convergnce of nonconvex SGD and what parts are specific to using the submodularity structure.

2. I enjoyed reading the paper, and I like the approach taken. However I was left with the question of why the continuous greedy algorithm can't be adapted to the stochastic setting. What is the bottleneck here? Also, in comparison to FW methods, starting at zero is cited as a weakness, but this seems not to be a problem in most examples. 

3. Equation 2.6 is wrong as written; as it does not make sense to divide by a vector.  (easy to fix, but I surprised at the sloppiness here given that the paper well written overall).

4. Just for clarity, in eq 1.1, state clearly that F_\theta(x) is submodular in x for every \theta.

5. Can some nonconvex constraint sets which have an easy projection be handled as well?

6. What if  the projection onto set K can be computed only approximately?



"
Recycling Privileged Learning and Distribution Matching for Fairness,"Novi Quadrianto, Viktoriia Sharmanska",https://proceedings.neurips.cc/paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html,"This paper proposes a framework which can learn classifiers that satisfy multiple notions of fairness such as fairness through unawareness, demographic parity, equalized odds etc. The proposed framework leverages ideas from two different lines of existing research namely, distribution matching and privileged learning, in order to accommodate multiple notions of fairness.  This work builds on two prior papers on fairness - Hardt et. al. and Zafar et. al. in order to create a more generalized framework for learning fair classifiers. The proposed method seems interesting and novel, and the ideas from privileged learning and distribution matching have not been employed in designing fair classifiers so far. The idea of proposing a generalized framework which can handle multiple notions of fairness is quite appealing. 
The paper, however, has the following weaknesses: 
1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification
2) the optimization procedure used to solve the multi-objective optimization problem is not discussed in adequate detail

Detailed comments below: 

Methods and Evaluation: The proposed objective is interesting and utilizes ideas from two well studied lines of research, namely, privileged learning and distribution matching to build classifiers that can incorporate multiple notions of fairness. The authors also demonstrate how some of the existing methods for learning fair classifiers are special cases of their framework. It would have been good to discuss the goal of each of the terms in the objective in more detail in Section 3.3. The part that is probably the most weakest in the entire discussion of the approach is the discussion of the optimization procedure. The authors state that there are different ways to optimize the multi-objective optimization problem they formulate without mentioning clearly which is the procedure they employ and why (in Section 3). There seems to be some discussion about the same in experiments section (first paragraph) and I think what was done is that the objective was first converted into unconstrained optimization problem and then an optimal solution from the pareto set was found using BFGS. This discussion is still quite rudimentary and it would be good to explain the pros and cons of this procedure w.r.t. other possible optimization procedures that could have been employed to optimize the objective. 
The baselines used to compare the proposed approach and the evaluation in general seems a bit weak to me. Ideally, it would be good to employ baselines that learn fair classifiers based on different notions (E.g., Hardt et. al. and Zafar et. al.) and compare how well the proposed approach performs on each notion of fairness in comparison with the corresponding baseline that is designed to optimize for that notion. Furthermore, I am curious as to why k-fold cross validation was not used in generating the results. Also, was the split between train and test set done randomly? And, why are the proportions of train and test different for different datasets?

Clarity of Presentation: 
The presentation is clear in general and the paper is readable. However, there are certain cases where the writing gets a bit choppy. Comments:
1. Lines 145-147 provide the reason behind x*_n being the concatenation of x_n and z_n. This is not very clear. 
2. In Section 3.3, it would be good to discuss the goal of including each of the terms in the objective in the text clearly. 
3. In Section 4, more details about the choice of train/test splits need to be provided (see above). 

While this paper proposes a useful framework that can handle multiple notions of fairness, there is scope for improving it quite a bit in terms of its experimental evaluation and discussion of some of the technical details. ","
          The author(s) of this paper take an interesting perspective on the problem of enforcing fairness in learning via the use of the privileged learning framework. Roughly speaking, they allow the protected attributes to be used at training time, but do not allow them to be used at testing time. The way in which this idea prevents proxy variables from affecting fairness is by enforcing a distribution similarity constraint on the outcomes as appropriate for the fairness measure.
          
          The authors compare their results to using standard SVMs and show that they can obtain better accuracy with respect to group discrepancies.
          
          I think this idea of viewing protected attributes as privileged is interesting. It supports some arguments in the field that the learner should NOT ignore protected attributes when training. And the use of distribution constraints helps block proxy variables, which is important.
          
          However, it seems to me that the correct comparison to show that this method is effective is against *other* fairness preserving methods that don't use protected data (or eliminate it) at training time, so as to argue that this approach is better. I suspect that this method might yield better fairness-accuracy tradeoffs as well, but one needs some evidence for this. Comparing to a vanilla SVM seems a little too easy. This is my main hesitation with this paper.
          
          The authors are somewhat selective in their citing of prior work. Beyond one stray reference to the Ruggieri et al paper, they ignore a fair amount of work on demographic parity in the pre-2015 era (!), mostly by Calders, Kamishima, and others. The authors also mention disparate impact and the 80% rule, but don't mention the Zafar et al paper in this context (although they cite it for other reasons). They don't cite the Feldman et al KDD 2015 paper which first studied the disparate impact measure in a learning context.
          
          Finally, while this is not directly related to the authors' work, they might want to look at the literature on auditing black box models. In particular, the work by Adler et al from ICDM 2016 starts with a classifier that's built in a black box way presumably using protected data, and looks at how to modify the *test* data so as to ensure fairness. This is again an example of protected data being used at training time but not at test time.
      ","This paper proposes a unified framework for balancing three studied aspects of fairness in machine learning: fair treatment, where decisions are not based on protected attributes; fair impact, where a certain protected attribute does not end up positively or negatively affecting a data point; and fair supervised performance, where (roughly speaking) every protected group is harmed or helped in the same way.  The crux of the paper is putting each of these three competing aspects into a multi-objective optimization problem (Eq 7), either in the objective or as a hard constraint.  The authors (I think rightfully for this application area) recommend returning a Pareto frontier of options, instead of setting exogenously parameters to balance the multiple chunks of the objective.  The second major contribution of the paper is in determining a better method for dealing with equalized odds/equalized opportunity.

Overall, I like the focus of the paper -- the fairness in ML literature is a bit fractured when it comes to who likes which criterion (or criteria), and I'm not convinced even the three aspects from the literature used in this paper are the end-all-be-all of fairness concerns in classification.  In terms of core ML / core optimization contributions, the paper is weak.  However, Eq 7 seems eminently usable in practice to me, and while I would've liked to see substantially heavier experimental evaluation (because, when it comes down to it, those will make a break the usefulness of any ""unification"" like this via multi-objective optimization), I think the paper is well-written and has promise."
Collecting Telemetry Data Privately,"Bolin Ding, Janardhan Kulkarni, Sergey Yekhanin",https://proceedings.neurips.cc/paper/2017/hash/253614bbac999b38b5b60cae531c4969-Abstract.html," Line 122
The authors claim that the 1-bit mechanism for mean estimation is tuned for efficient communication. Is there any difference between the basic randomizer in [3] with one item and the 1-bit mechanism? If the protocol is equivalent to the basic randomizer in [3] in a specific setting, it should be clearly stated.

Theorem 2
Please compare the utility in terms of n and d with existing histogram protocols.

Sec 3.1
It was unclear to me what privacy is guaranteed after alpha-point rounding. If a data collector reports her data repeatedly (say N times), the privacy budget should be set following the composition theorem. The alpha-point rounding introduces another random source into the responses and it certainly improves privacy to some extent, but the composition theorem still is applied as long as she responds to the data collector multiple times. 





 

","The authors propose a more efficient mechanism for local differential privacy. In particular, they propose a 1 bit local differential privacy method that enables mean estimation in a distributed and effective way.  Also a d-bit method is proposed for histogram estimation. It is argued that these two goals are of partical relevance in collection of telemetry data. The main advance is in the communication efficiency - which is key to large scale deployment. The authors also note that this mechanism is indeed deployed at large scale in industry and likely consistutes the largest deployment of differential privacy to date. While this on its own does not add to the value of the scientific contribution - it does reinforce the point of an efficient, effective and practical approach.

The authors submitted another version of their paper as supplementary material that contains the proofs. But it also contains more figures and extended main text. This is borderline. The supplementary material should only contain the additional material - while the main paper should be self-contained and not repeated. It is also weird, that the reviewer has to hunt for proofs in provided text. There should be a clear separation between submission and supplementary material.

Overall, the presentation is clear - partially due to outsourcing parts of the submission to the supplementary. However, alpha-rounding part remains partially unclear. Can the authors please elaborate on the claimed guarantees?

While the experimental study is interesting - there is a question about reproducibility of the results.As it looks this is real user and company data - how would follow up work compare here? It would be beneficial to provide an alternative solution -- potential simulated data -- so that comparison remain open source.
However, the experiments are considered convincing and support the main claim."
Parallel Streaming Wasserstein Barycenters,"Matthew Staib, Sebastian Claici, Justin M. Solomon, Stefanie Jegelka",https://proceedings.neurips.cc/paper/2017/hash/253f7b5d921338af34da817c00f42753-Abstract.html,"The paper ‘Parallel streaming Wasserstein Barycenters’ presents a method for 
scaling the computation of Wasserstein barycenters in a distributed and principled
way. Through a semi-discrete formulation of the problem allowing to handle continuous and
nonstationary distributions. Bounds are provided on the recovery of the true barycenter
wrt. discretization and its effectivity is demonstrated on a toy example and a large scale Bayesian inference problem.

The paper is very well-written and contributes strongly to the use of Wasserstein distance 
in large scale settings. The methodological and theoretical contributions are strong. The use 
of the method in a WASP setting [47] is particularly appealing and promising. For those reasons 
I will recommend a strong accept.

I have some small questions/remarks that coud be addressed in a revised version provided that the
paper is accepted:
 - it is not clear if problem (10) and (11) are strictly equivalent. If yes, could you discuss why ?
Can you comment on the ‘sparsity of gradients’ as discussed above ? Why is it an issue ?  
 - as in [22], computation of i_W can be done in closed-form if the W distance is regularized 
 by entropy. What is the impact of the overall complexity of the parallel implementation ? Did the authors
 experiment this ?
 - what is an InfiniBand cluster ?
 - in table 1, with the LP solver from [47], why is the W_2 distance is not decreasing monodically 
wrt. n ?
 

After rebuttal. I wish to thank the authors for their explanations and clarifications","Title: Parallel Streaming Wasserstein Barycenters

Comments:

- This paper presents a new method for performing low-communication parallel inference via computing the Wasserstein barycenter of a set of distributions. Unlike previous work, this method aims to reduce certain approximations incurred by discretization. Theoretically, this paper gives results involving the rate of the convergence of the barycenter distance. Empirically, this paper shows results on a synthetic task involving a Von Mises distribution and on a logistic regression task.

- I feel that the clarity of writing in this paper is not great. It would be better to clearly (and near the beginning of the paper) give an intuition behind the methodology improvements that this paper aims to provide, relative to previous work on computing the Wasserstein barycenter. This paper quickly dives into the algorithm and theory details (“Background”, “Mathematical preliminaries”, “Deriving the optimization problem”), without giving a clear treatment of previous work, and how this methods of this paper differ from this work. It is therefore is hard to see where the material developed in previous work ends and the new methodology of this paper begins. A simple description (early on) of this method, how it differs from existing methods, and why it solves the problem inherent in these existing methods, would greatly increase the clarity of this paper. 

- Furthermore, it would be nice to include more motivation behind the main theoretical results that are proved in this paper. It is hard to get a grasp on the usefulness and contribution of these results without some discussion or reasoning on why one would like to prove these results (e.g. the benefits of proving this theory).

- Finally, I do not think that the empirical results in this paper are particularly thorough. The results in Table 1 are straightforward, but these seem to be the only empirical argument of the paper, and they are quite minimal.","This paper applies optimal transport algorithm to the barycenter communication problem. It approximates continuous distributions with discrete distributions with finite support and use stochastic projected gradient decent to solve the optimization problem. It shows the consistency of this algorithm and parallelizability of this algorithm because the variables can be decoupled. The authors show the speedup and improved memory consumption of this new algorithm. Also it works with nonstationary data.

Overall a nice paper. It could be interesting to see how the accuracy of this new algorithm compared to LP. How much do we lose in adopting this new algorithm? Or do we ever lose anything?

----------------------
After author rebuttal:

Many thanks for the rebuttal from the authors. They do clarify. My scores remain the same though. Thanks!"
"Adaptive Accelerated Gradient Converging Method under H\""{o}lderian Error Bound Condition","Mingrui Liu, Tianbao Yang",https://proceedings.neurips.cc/paper/2017/hash/2612aa892d962d6f8056b195ca6e550d-Abstract.html,"The paper extends the linear convergence result for PG and rAPG from QGC to a more broader condition termed here as HEB. An adaptive version of APG is also presented that does not insist on knowing the ""condition number"" apriori.

The extension to HEB and the convergence results presented are all interesting (did not fully check correctness of the proofs).

As an example of ML application a case where \theta=0.5 was shown (Cor 1). However, since it is \theta strictly less than 0.5 that is more interesting, are there examples of ML set-ups where this is the case?

I have read the rebuttal and would like to stay with the score.","Summary: This paper studies the convergence rate of the proximal gradient and its accelerated version under the more general Holderian error bound condition, further extending a line of recent efforts that has weakened the usual strong convexity assumption to the quadratic growth condition. The main contribution is a restarting scheme that eliminates the need of knowing the Holderian constant. The authors complement their theoretical claim with some numerical experiments, which seem to verify the efficiency of the proposed adaptive algorithm.

As far as I can tell, this work is a fine synthesis of some recent developments around semialgebraic functions, error bound, and the proximal gradient algorithm. The results are not too surprising given the existing known results. Nevertheless, it is a very welcome addition to the field. And given its practical nature, the proposed algorithm might be useful for a variety of applications (some toy demonstrations in the experiment). 


Some comments:

While the technical proofs involve many pieces from relevant references, the general idea is in fact quite simple (and dates back at least to FISTA): one first derives some condition (e.g. Theorem 5) that is guaranteed to hold if the correct parameter is used, then in the algorithm one periodically checks this condition; if it ever falls short then we know our parameter of choice is too small and we increase it (the familiar doubling trick) and rerun the algorithm. This high level description of the adaptive algorithm perhaps worth explicitly mentioning in the revision (to give the less familiar readers a clear guidance). 

Line 85: no need to mention that ""g(x) > -\infty for all x"" since the range is assumed to be (-\infty, \infty].

Theorem 1: the option I and II are only defined in the appendix. better mention them in the main text (or at least say ""see appendix""). The calculations and notations in the proofs of Theorem 1 and 2 are a little sloppy: when theta > 1/2, the rate does not depend on epsilon (the target accuracy) at all? Some more details between say line 68 and line 69 are needed. 

Section 4: Can one develop a similar proof for the objective value gap? Is it because of the DUAL algorithm you use to solve Eq (8) that limits the guarantee to the minimal gradient?

Can we say anything about the dependence of the constants theta and c on the dimension and the number of samples? It could be possible that a linear rate is in fact worse than a sublinear rate if such dependence is not carefully balanced.

Experiments: On some datasets (cpusmall and bodyfat), why proximal gradient takes so many more iterations? This looks a little unusual. How many of the proximal mappings are spent on searching for the step size (for each method)? Also, to give a fuller picture, maybe also include the result for the decrease of the objective values. Another observation here: is it really meaningful to consider an absolute epsilon = 10^{-6} say? For optimization per se, sure. But what if say we only care about the classification accuracy? Would it be possible that an solution at epsilon = 10^{-3} already gives us comparable classification accuracy? (This should be easy to verify since the authors did experiment with a classification dataset.)

"
What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,"Alex Kendall, Yarin Gal",https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html,"This paper proposes a new Bayesian approach to deal with both Aleatoric and Epistemic uncertainties at the same time for deep neural network models. The basic idea is very simple. Basically, the variance of the output is also predicted by the model in addition to the output for aleatoric uncertainty while marginalizing the parameters for epistemic uncertainty. Methods to deal with each uncertainty already exist, but this is the first method to deal with both. The proposed method yields the best results on the two benchmarks they used. The analysis shows the interesting properties of the variances coming from the two uncertainties.

This paper is well-written and easy to follow. It has relevant references for previous work.

The novelty of the proposed method itself is not very high given that it just additionally predicts the variance of the output for aleatoric uncertainty while using Bayesian Deep Learning for epistemic uncertainty. However, the combination is valuable and this paper gives interesting analysis on the two uncertainties. It is important to understand what uncertainties the current deep learning methods can and cannot deal with. This paper provides insight on that.","I have read the other reviews and the rebuttal.

Additional comment: The authors asked what I mean by 'numerical uncertainty'. I mean what is called 'algorithmic uncertainty' in this Wikipedia article: https://en.wikipedia.org/wiki/Uncertainty_quantification . Deep learning models are very high dimensional optimisation problems and the optimisers does not converge in almost all cases. Hence, this numerical error, i.e. the difference between the true minimum and the output of the optimiser, is a very significant source of error, which we are uncertain about (because we do not know the solution). Additionally, even rounding errors can add to the uncertainty of deep learning, as is e.g. discussed in this paper: http://proceedings.mlr.press/v37/gupta15.pdf . 

Hence, I think that the question in the title ""What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"" is not really answered. Although the paper is apart from this very good, this limits my rating at 'good paper, accept'. In my opinion, the authors should make clearer which kinds of uncertainty they consider (and which they ignore) or why they believe that the uncertainties considered are the dominant ones (which I find completely unclear).

% Brief Summary

The paper is concerned with evaluating which types of uncertainty are needed for computer vision. The authors restrict their analysis to aleatoric and epistemic uncertainty (leaving out numerical uncertainty). Aleatoric uncertainty includes the uncertainty from statistical noise in data. Epistemic uncertainty is usually another term for ignorance, i.e. things one could in theory know but doesn't know in practice. In this paper however, the authors use epistemic uncertainty as a synonym for model uncertainty (or structural uncertainty, i.e. ignorance over the true physical model which created the data). The paper raises the question how these two sources of uncertainty can be jointly quantified, and when which source of uncertainty is dominant.  In particular, this is discussed in the setting of computer vision with deep learning. While classically it was not possible to capture uncertainty in deep learning, Bayesian neural networks (BNN) have made it possible to capture at least epistemic uncertainty. The authors aim to jointly treat epistemic and aleatoric uncertainty in BNNs.

To expand the existing literature, the authors propose to include aleatoric uncertainty over pixel output sigma by fixing a Laplace likelihood which contains sigma, which is to be minimised with respect to the parameter vector theta. They also observe that this inclusion of aleatoric uncertainty can be interpreted as loss attenuation.

This novel method is then applied to pixel-wise depth regression and semantic segmentation. For semantic segmentation performance, a notable increase in performance is achieved by modelling both aleatoric and epistemic uncertainty.

In Chapter 5, an analysis of the effects of epistemic and aleatoric uncertainty follows. Notably, the authors experimentally validate the well-known claims that aleatoric uncertainty cannot be reduced with more data, and that only epistemic uncertainty increases for out-of-data inputs. The paper concludes by claiming that aleatoric uncertainty is particularly important for large data situations and real-time applications, while epistemic uncertainty is particularly important for safety-critical applications and small datasets. 

% Comment

I am skeptical about the use of the classifications of uncertainties which can arise in this setting in this paper. While I agree with the definition of aleatoric uncertainty, I do not agree that epistemic uncertainty is the same as model uncertainty. For me, model uncertainty is the ignorance about the true physical model which creates the data. Epistemic uncertainty is supposed to capture everything which is unknown, but is deterministic and could in theory be known. So, I think that the numerical uncertainty is missing here. In large dimensional optimisation problems (as they arise in deep learning), it is completely unclear whether the optimisation algorithm has converged. I would even expect that the numerical uncertainty could be a dominant source of error, in particular when the computation has too be performed online, e.g. when the self-driving car learns while it's driving. Hence, I would like to ask the authors on their thoughts on this.

The second issue I want to arise is the following: The authors convincingly argue that epistemic uncertainty is reduced when more data is available. In statistics, people talk about the statistical consistency in model. A model is statistically consistent when its output is a Dirac measure on the ground truth, in the limit of infinite data. It would be very interesting to see whether statistical consistency results for deep learning (if they exist) can be connected to the paper's pitch on the relative unimportance of epistemic uncertainty in big data situations.

Since I think (as explained above) that the classifications of uncertainties is missing the numerical uncertainty of the optimiser, I would argue that the paper does not completely live up to comprehensively answer the question asked in the title. While it is fine, that this is not part in the paper; I think it should be discussed in the introduction. Since, the paper is---a part from this---good, I vote for 'accept'.","The paper provides an analysis of the aleatoric and epistemic uncertainty in the Bayesian neural network. It proposes a method that models both types of uncertainty and shows that by doing that the performance of the network could be further improved in real applications such as depth estimation and semantic segmentation.

General comment:
The problem of uncertainty modeling in representation learning is of great importance, especially to real world safety-critical applications. The paper is well written, technically sound and carefully evaluated. So I recommend accepting the paper.

Pros:
1. The paper works on a very important topic of uncertainty modeling in deep learning.
2. The paper is very well written in both text and equations, and easy to understand.
3. The paper introduces a way to combine aleatoric and epistemic uncertainty in Bayesian deep learning, which is shown to outperform state-of-the-art deterministic deep learning models in different real datasets.
4. The paper provides an interesting analysis of aleatoric and epistemic uncertainty based on the observations from the experimental results.

Cons:
1. The paper could be further improved by clarifying some of the details: 

(a) In Equation 6, are the estimations of mean and variance based on the same neural network architecture with some of the parameters shared or independent ones? It seems the prediction and aleatoric uncertainty should be completely isolated. In that case, should they be estimated using completely independent networks?

(b) In Table 3(a), the aleatoric variance is getting 20% lower when doing transfer evaluation in NYUv2 while in Table 3(b), the aleatoric variance is about twice of the original value. I wonder if the authors have an explanation of such phenomenon. 

2. The paper mentions that the epistemic uncertainty can be explained away by getting a larger training data set. However, the biased (or long-tail) data distribution is an issue in large scale learning. Models learned from long-tail distribution may perform worse in categories with small sample. Also, in that case, the estimation of epistemic uncertainty could be in large variance because the computation of Equation 9 would need a sufficient number of samples. Would the proposed model address such issues?
"
Reconstruct & Crush Network,"Erinc Merdivan, Mohammad Reza Loghmani, Matthieu Geist",https://proceedings.neurips.cc/paper/2017/hash/269d837afada308dd4aeab28ca2d57e4-Abstract.html,"The paper proposes a variant of learning for binary classification with a score that characterizes the difference between an auto-encoder's reconstruction and the original sample. The idea seems interesting and has some novelty.

One concern is that binary classification is a very limited setting and there are very few binary problems in the real world. Based on the formulation, it looks like it should be easy to give a discussion how it can be applied to multi-class classification or other problems. Experiments should also be conducted.

Meanwhile, the experiments are relatively weak and on relatively small datasets. The reconstruction result does not look sharp either. Considering the tremendous progress the community has made in recent years on GANs, there should be many ideas that on can borrow to produce better results.

In summary, the idea in the paper looks interesting, but the discussion and experiments in the paper may not be sufficient for publication yet. I encourage the authors to further the idea a bit and submit again.","The paper proposes a classification technique using deep nets to deal with:
(a) covariate shift (i.e., when the train and test data do not share the same distribution)
(b) PU settings (i.e., when there are only positive and unlabeled datapoints are available).
The key idea is to train an auto-encoder that reconstruct the positive instances with small error and the remaining instances (negative or unlabeled) with a large error (above a desired threshold). This structure forces the network to learn patterns that are intrinsic to the positive class (as opposed to features that are discriminative across different classes). The experiments highlight that the proposed method outperforms baselines across different tasks with different data types (image, short text, and dialogues). 

Overall, I enjoyed reading the paper. It is honest, concise and well-written. The idea of considering the reconstruction error of auto-encoders as an energy function is indeed interesting. While my overall recommendation is accept, there are a few points that I am not confident about:
(1) The baselines; For the image classification experiments, the authors use a standard CNN as a baseline and I'm not quite certain if these count as a competitive baseline (specially for the ""not-in-training"" setup). My question is that are there other baselines that are more suitable for such settings?
(2) The design of auto-encoders are discussed only briefly. The suggested structures are not trivial (e.g., (32)3c1s-(32)3c1s-(64)3c2s-(64)3c2-(32)3c1s-512f-1024f). The only discussion on the structure is the following sentence: ""The choice of architectures for standard classifier and auto-encoder is driven by necessity of fair comparison."" which is frankly not sufficient. At the same time, It seems to me this is the general trend in research on Neural Nets. Thus, I'm not confident how much this concern is valid.

Overall, my recommendation is accept however with not so great confidence in my review.","This paper describes an autoencoder model which will assign low construction error for positive sample and high construction error for negative samples. 

For image binary classification problem, seems this autoencoder outperforms traditional standard classifier under a high inbalance data setting. 

For Amazon review classification problem, I notice for the given samples, positive examples are significantly shorter than negative examples, which looks like a bias.

For the QA experiment, in this paper only checked 2 out of 1000 tasks and announce it achieves same performance to memory network. 

Overall I think this paper is ok but the technique depth and impression are not meeting the standard of NIPS.  "
Permutation-based Causal Inference Algorithms with Interventions,"Yuhao Wang, Liam Solus, Karren Yang, Caroline Uhler",https://proceedings.neurips.cc/paper/2017/hash/275d7fb2fd45098ad5c3ece2ed4a2824-Abstract.html,"Update after the authors' response.
Thanks for your answer. Very happy about the scientific discussion, this is a really nice piece of work, and you convinced me you know the subject very well and your contributions are now much clearer to me.
I really encourage you to consider all the points raised by all reviewers (reading the reviews by others also helped) to make the best possible paper in the end!
Good luck with your paper.

#################################
This work is anchored in a Bayesian network (BN) framework for causal inference from interventional and observational data. The authors built from existing works in several directions: 2 algorithms are presented (one showing consistency only under conditions which are difficult to control) with their respective theoretical guarantees. Simulated and real world data are used to demonstrate the method performance.

Remarks:
 - l18: careful, all BN are not causal; distribution-based BN have a different interpretation of their edges in terms of (in)dependence relationships. I know you are not fooled, bu try making it clearer.
 - section 2: you missed at least Rau et al. BMC System Biol 2013.
 - the covered arrow concept is not 100% clear to me, can you give a little bit more simple interpretation on its meaning if possible? Usefulness?
 - l91: intervention DAG, why don't you simply say that it consists of the initial DAG without edges pointing at the nodes which was intervened?
 - l115: interesting example. Perhaps explaining it in details in the paper only once (it is back on p4 avfter Th4.1) would be a very good idea.
 - Greedy SP essentially seeks an ordering of the variables. If the ordering is fixed, the problem is 'easy'. If not, an adequate way to browse graph configurations is needed. E.g. see https://arxiv.org/abs/1507.02018
 - Eqn after l132 looks like a global structural Lasso Equation. Can you comment a bit please?
 - Th4.1: is the result 'almost surely'? As nj's \to \infty?
 - l165: skeleta?
 - simulations seem a bit too simple and not very high-dimensional...when the real data is complex and high-dimensional?!","In this paper the authors propose two permutation based algorithms for causal structure learning from a mix of observational and interventional data. 
The authors assume that their data does not contain hidden variables and that the interventional data comes from a do-type intervention, where it is known which variables were intervened on.
Their second algorithm, called IGSP, is fully non-parametric and relies primarily on conditional independence tests.

Previous work on this topic is the GIES algorithm from Hauser and Buhlmann (2012). The authors show that GIES is in fact inconsistent, whereas IGSP is consistent under the faithfulness assumption. 
Additionally, for a linear Gaussian setting, the authors show that IGSP and GIES perform comparably, whereas it is expected that IGSP performs better in non-linear or non-Gaussian settings as it is non-parametric.

This is very nice and comprehensive work on a very important topic.
 
Since their IGSP algorithm is non-parametric and relies on conditional independence tests, I was wondering if the authors consider extending it to the hidden variable case?

As for specific comments on the paper: 

It would be beneficial to make the implementation of the proposed algorithms available. Additionally, it would be interesting to know the computational complexity of IGSP or at least empirically compare the runtime of IGSP and GIES.

As for the example in analysis of protein signaling data: You use the data from Sachs et al. (2005) which is purely interventional, but it is my understanding that your method specifically assumes it also has access to observational data? Do you do anything to remedy that, or is this just an example where the model assumptions are violated? 

In line 204: \alpha_{ij} should be a_{ij} 
","The paper describes two permutation-based causal inference algorithms that can deal with interventional data. These algorithms extend a recent hybrid causal inference algorithm, Greedy SP, which performs a greedy search on the intervention equivalence classes, similar to GIES [8], but based on permutations. Both of the proposed algorithms are shown to be consistent under faithfulness, while GIES is shown not to be consistent in general. 

Algorithm1 is computationally inefficient, so the evaluation is performed only with Algorithm 2 (IGSP) with two different types of independence criteria (linear Gaussian and kernel based). The evaluation on simulated data shows that IGSP is more consistent than GIES. Moreover, the authors apply the algorithm also to two biological datasets, a well-known benchmark dataset (Sachs et al.) and a recent dataset with gene expressions.

Quality
Pros:
- I didn’t check in depth the proofs, but to the best of my knowledge the paper seems to be technically sound. 
- It seems to provide a good theoretical analysis in the form of consistency guarantees, as well as an empirical evaluation on both simulated and real-world datasets.
- The evaluation on the perturb-seq seems to be methodologically quite good.

Cons:
- The paper doesn’t seem to discuss a possibly obvious drawback: the potentially higher order conditional independence tests may be very brittle in the finite data case.
- The evaluation compares only with GIES, while there are other related methods, e.g. some of the ones mentioned below.
- The empirical evaluation on real-world data is not very convincing, although reconstructing causal networks in these settings reliably is known to be very difficult.
- For the Sachs data, the paper seems to suggest that the consensus network proposed in the original paper is the true causal graph in the data, while it is not clear if this is the case, so I think Figure 4 is a bit misleading. Many methods have been evaluated on this dataset, so the authors could include some more comparisons, at least in the Supplement.

Clarity
Pros:
- The paper is well-organized and clearly written.
- It is quite clear how the approach is different from previous work.
- From a superficial point of view, the proofs in the Appendix seem to be quite well-written, which seems to be quite rare among the other submissions.

Cons:
- Some parts of the background could be expanded to make it more self-contained, for example the description of Greedy SP. 
- It probably sounds obvious to the authors, but maybe it would be good to point out in the beginning of the paper that the paper considers the causal sufficient case, and the case in which the intervention target is known.
- For the Sachs data, I would possibly use a Table of causal relations similar to the one in the Supplement of [13] to compare also with other methods.

Original
Pros:
- The paper seems to provide an interesting extension of Greedy SP to include interventional datasets, which allows it to be applied on some interesting real-world datasets.

Cons:
- The related work section could refer to the several constraint-based (most of them in practice hybrid) methods that have been developed recently to reconstruct causal graphs from observational and interventional datasets (even when we cannot assume causal sufficiency), for example:
-- A. Hyttinen, F. Eberhardt, and M. Järvisalo: Constraint-based Causal Discovery: Conflict Resolution with Answer Set Programming, Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, 2014.
-- Triantafillou, Sofia, and Ioannis Tsamardinos. ""Constraint-based causal discovery from multiple interventions over overlapping variable sets."" Journal of Machine Learning Research 16 (2015): 2147-2205.
-- Magliacane, Sara, Tom Claassen, and Joris M. Mooij. ""Ancestral causal inference."" Advances in Neural Information Processing Systems. 2016.
- The last paper also offers some (possibly limited) asymptotic consistency guarantees, maybe the authors could discuss its limitations with respect to their guarantees.

Significance
Pros:
- This paper and other permutation-based causal discovery approaches seem to provide a novel and interesting alternative to previous methods

Cons:
- The applicability of these methods to real-world data is reduced by the assumption that there are no latent confounders.

Typos:

- References: [13] N. Meinshausen, A. Hauser, J. M. Mooijc, -> Mooij
"
Deep Dynamic Poisson Factorization Model,"Chengyue Gong, win-bin huang",https://proceedings.neurips.cc/paper/2017/hash/27ed0fb950b856b06e1273989422e7d3-Abstract.html,"This papers introduces the deep dynamic Poisson factorization model, a model that builds on PF to allow for temporal dependencies. In contrast to previous works on dynamic PF, this paper uses a simplified version of a recurrent neural network to allow for long-term dependencies. Inference is carried out via variational inference, with an extra step to maximize over the neural network parameters. The paper reports experiments on 5 real-world datasets.

Overall, I found the idea potentially interesting, but I still think the paper needs some improvements in its execution before it can be accepted in a conference like NIPS. Please find my comments below.

+The formulation in Eq. 4 doesn't correspond to a RNN structure, because h_t^{(n)} depends only on the top-layer memory vectors, and it does not depend on previous time steps for the same layer, as in the RNN setup. Is there any reason for this simplified structure?

+In Section 2, some of the model equations are missing (e.g., it is not clear how to parameterize the Gamma in Eq. 4, or what's the distribution that generates theta_t in Eq. 3).

+I disagree with the discussion in lines 110-118 regarding ""implicit distributions"". Eqs. 9 and 10 are actually explicit: they are Gammas with given shapes and rates. Since the distribution of theta_tk (Eq. 9) can be computed in closed form conditioned on h^{(0)}, it cannot be called implicit. The same comment applies to h in Eq. 10.

+Lines 30-31: The sentence ""This method may have weak points in analyzing other data with different pattern long-time dependence, such as fanatical data and disaster data"" wasn't clear to me; can you elaborate?

+The subsection in lines 133-155 isn't clear to me. What is exactly the ""loss function"" in Eq. 15?

+MSE/PMSE is typically not a good metric for PF-like models. The results should additionally include at least predictive log-likelihood.

+The value of K in the experiments (line 207) seems too small to me. I suggest the authors use larger values, such as K=100 (at least).

+I couldn't connect the discussion in lines 224-232 with Fig. 3: it is not clear.

+The update equations for the variational inference procedure (Eqs. 11-14) can safely be moved to the supplement.

+The caption in Fig. 4 cannot be read clearly.

+I *strongly* recommend to follow general writing advice about the use of the passive voice (specially for the abstract).

+The writing quality is poor, and the paper contains many typos. Here are some examples: 
  -Line 15, ""high-dimensional""
  -Line 22, ""factorize""
  -Line 24, ""property of the""
  -Lines 29, 34, ""Dirichlet""
  -Line 34, ""nested""
  -Fig. 1, ""visual representation""
  -Fig.1, ""transmission"" (not sure if that's an appropriate word anyways)
  -Incomplete sentences (e.g., line 49)
  -Many missing articles (e.g., ""Although Dirichlet distribution is often used as prior distribution"")
","The authors propose a novel method to capture the complex long-time dependence over the count vectors by exploiting recurrent neural networks to represent implicit distributions, which is realized by the gamma distribution. The shape parameters of those gamma distributions, capturing the time dependence, are approximated by neural networks, which try to cover the information from prior (from higher layer) and likelihood (from low layer). The parameters in neural network are updated based on the expectation of latent variables, which is helpful to reduce the variance caused by sampling and increase the computational efficiency. The model provides an interesting and smart way to handle the long dependency problem in the dynamic model. I like it, but this manuscript may be written a little rush and I I have some comments as below:
1. Some critical typos, i.e. the equation 10.
2. The authors consider the scale parameter as fixed and only update the shape parameter in those gamma distributions, which need some detailed discussion and analysis.
3. In the experiments, the authors only show the performance of the proposed model with two layers and do not compare it with the model with the single layer and more layers, which is not enough to understand the influence of the layers on the performance. Some other parameters also need discussing, i.e. the size of the window, the number of factors.
4. For better understanding, I suggest the authors display the detailed graphical illustrations of the whole model. According to the problem, the author can refer to the figure 1 in [1].
5. Recently some new deep Poisson factor analysis models have been proposed, such as [2] and [3], which need to discuss in Introduction.
 
[1] Chung J, Kastner K, Dinh L, et al. A Recurrent Latent Variable Model for Sequential Data. NIPS, 2015.
[2] Mingyuan Zhou, Yulai Cong, and Bo Chen, Augmentable gamma belief networks, Journal of Machine Learning Research,17(163), 1-44, 2016.
[3] Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou, Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC, ICML 2017.
","The paper deals with incorporating long range temporal dependence in a Poisson factorization model with deep structure. As opposed to the previous works on dynamic PF, this paper employs a recurrent neural network like structure for modeling the long-term dependency. Finally, the inference is carried out using variational inference with some optimization routines for finding the parameters of the neural network.  The paper has good technical contents but is also plagued with some grammatical and factual errors. These are listed below:

1. The statement made about modeling nonnegative numbers on lines 86-87 is incorrect. It's the Poisson Randomized Gamma distribution that is used for modeling the nonnegative numbers.
2. Please make the best results bold in Table 2. 
3. Held-out perplexity should have been a stronger indicator in the experiments in Section 4.2, though MSE/PMSE are okay.
4. The limitations in the existing works for modeling the long-range dependency (reference 1,2) are adequately addressed in this paper.  However, to illustrate the utility of the proposed method with multiple levels of hierarchy, a comparison with dynamic PF with one level deep architecture could have been useful. The comparison with LSTM is useful, but the learning algorithms are different. Therefore, this comparison does not truly justify the need for multiple levels of hierarchy in the proposed method. 
5. There are several grammatical errors and typos as listed below that make the paper little difficult to read:
-- ""unit the data"" 
-- ""Noticing the lack of capturing ..""
-- ""PFA factorized a count matrix"" (the tense of this sentence is incongruent with the tense in other sentences in the same paragraph)
-- ""other data with different pattern long-time dependence ..""
-- ""Examples for this including ..""
-- ""shows excellent results on predict and ..""
-- ""However, it is rough for them ..""
-- ""V-dimesnion sequentially ..""
-- ""K-dimension latent variable ..""
-- ""due to the including of feature-wise""
-- ""recommend systems"" etc."
Scalable Generalized Linear Bandits: Online Computation and Hashing,"Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, Rebecca Willett",https://proceedings.neurips.cc/paper/2017/hash/28dd2c7955ce926456240b2ff0100bde-Abstract.html,"The paper identifies two problems with existing Generalized Linear Model Bandit algorithms: their per-step computational complexity is linear in t (time-steps) and N (number of arms). The first problem is solved by the proposed GLOC algorithm (first that achieves constant-in-t runtime and O(d T^½ polylog(T) ) regret) and the second by a faster approximate variant (QGLOC) based on hashing. Minor contributions are a (claimed non-trivial) generalization of linear online-to-confidence bounds to GLM, an exploration-exploitation tradeoff parameter for QGLOC that finally justifies a common heuristic used by practitioners and a novel simpler hashing method for computing QGLOC solutions.

I think the paper is interesting and proposes a novel idea, but the presentation is sometimes confused and makes it hard to evaluate the impact of the contribution w.r.t. the existing literature.
the authors claim that all existing GLMB algorithms have a per-step complexity of t. This is clearly not the case in the linear setting when \mu is the identity, and closed forms for \hat{theta}_t allow the solution to be computed incrementally. This cannot be done in general in the GLM where \hat{\theta}_t is the solution of a convex optimization problem that requires O(t) time to be created and solved. Indeed, GLOC itself uses ONS to estimate \theta, and then obtain a closed form solution for \hat{\theta}, sidestepping the O(t) convex problem. The authors should improve the presentation and comparison with existing methods, highlighting when and where this dependence on t comes from and why other methods (e.g., why hot restarting the convex solver with \hat{\theta}_{t-1}) cannot provide the same guarantees, or other examples in which closed forms can be computed.
The authors should expand the discussion on line 208 to justify the non-triviality: what original technical results were necessary to extend Abbasi-Yadkori [2] to GLMs?
While the road to solving the O(t) problem is somewhat clear, the approach to reduce the O(N) complexity is not well justified. In particular (as i understood):
GLOC-TS is hash-amenable but has d^3/2 regret (when solved exactly)
QGLOC is hash-amenable and has d^5/4 regret, but only when solved exactly
GLOC-TS has no know regret bound when solved inexactly (using hashing)
QGLOC has a fixed-budget regret guarantee (presented only in the appendix, d^5/4 or d^3/2?) that holds when solved inexactly (using hashing), but reintroduces a O(t) space complexity (no O(t) time complexity?)
This raises a number of questions: does GLOC-TS have a regret bound when solved inexactly? Does any of these algorithms (GLOC, QGLOC, GLOC-TS) guarantee O(d^{1,5/4,3/2}\sqrt{T}) regret while using sublinear space/time per step BOTH in t and N at the same time?
The presentation should also be improved (e.g., L1-sampling requires choosing m, but QGLOC's theorem makes no reference to how to choose m)
The experiments report regret vs. t (time), and I take t here is the number of iterations of the algorithm. In this metric, UCB-GLM (as expected) outperforms the proposed method. It is important to report metrics (runtime) that were the reason for the introduction of GLOC, namely per-step time and space complexity, as t and N change.

Minor things:
L230 ""certain form"" is a vague term
L119 S must be known in advance, it is very hard to estimate, and has a linear impact on the regret. How hard it is to modify the algorithm to adapt to unknown S?
L109+L127 The loss fed to the online learner is \ell(x,y) = -yz +m(z) and m(z) is \kappa strongly convex under Ass. 1. For strongly convex functions, ONS is not necessary and simple gradient descent achieves logarithmic regret. Under appropriate conditions on \kappa and y, the problem might become simpler.
L125 Ass. 1 is common in the literature, but no intuition for a value of \kappa is provided. Since the regret scales linearly with \kappa, it would be useful to show simple examples of known \kappa e.g. in the probit model and 0/1 rewards used in the experiments. It would also be useful to show that it does not satisfy the previous remark.
L44 3 is not included in the ""existing algorithm"". Typo, or TS does not suffer from both scalability issues?
L158 S should be an input of Alg. 1 as well as Alg. 2

AFTER REBUTTAL
	I have read the author rebuttal, and I confirm my accept. I believe the paper is interesting, but needs to make a significant effort to clarify the message on what it does and what it does not achieve.

","--- Paper ---
The paper focuses on computationally efficient versions of Generalized Linear Bandits. It builds upon online learning to get rid of complexity increasing with time, it proposes two ways to deal with the number of arms, and it discusses its own flavor of approximate inner product.

The price to pay is about the regret of the proposed approaches, which ranges from the same as state of the art to a regret with an additional $d$ factor depending on the algorithm and on the setting (infinite of finite number of arms). As most applications involve a finite set of arms, we may argue that the proposed approches suffer at least a $\sqrt(d)$ additional term.

--- Review ---
The aim of the paper deserves interest and the proposed meta-algorithm path the way to several applications. Its a pleasure to see computational limitations studied in any corner.

Compared to previous attempts concerning Generalized Linear Bandits [41], the proposed approach is generic: Theorems 1 and 2 apply to any couple (Generalized Linear Model, Online Learning algorithm).

The only missing result could be a problem-dependent bound of the regret, which anyway loses its interest when the number of arms is big.

--- Typos ---
L50: a specific 0/1 reward[s] with the logistic
L319: where we [we] scale down 
","Overview: The paper proposes new, scalable algorithms for generalized linear bandits. The existing GLB algorithms have scalability issues (i) under a large time horizon, and (ii) under a large number of arms: the per-time-step space and time complexity of existing GLB algorithms grow at least linearly with time t, and existing algorithms have linear time complexities in the number of arms N. 
  To deal with the first issue, the paper proposes to use any online learning algorithm as a black box, turn the outputs of the online learner into a confidence set of the true parameter, and finally apply the principle of optimism in the face of uncertainty. The paper provides a regret bound for the proposed algorithm. 
  To deal with the second problem, the paper proposes to use hashing as a solution. The paper gives two GLB algorithms that are hash-amenable. The paper also proposes a sampling-based method to reduce the overhead of hashing. 

Significance: The paper contributes to making GLB algorithms more scalable and more practical for real-world applications that require fast decisions. 

Originality: The paper mostly extends existing tools and techniques to GLB problems.

Quality: The paper provides mathematical proofs for all its claims. The paper also includes a section of computational studies for its proposed algorithms. The part on GLOC looks solid, while the part on hash-amenable GLBs seems insufficient. The authors may want to compare QGLOC-Hash and GLOC-TS-Hash with, say, GLOC and UCB-GLM to show how performance deteriorates with the use of hashing. 

Clarity: While the content of this paper may be of great practical interests, this paper is not very well-written. The authors should clearly define the notations and symbols before including them in the theorems. For example, the theta_t_hat in Theorem 1 is not defined anywhere in the text. Also, in Theorem 2, it is unclear what beta_t_bar does. There is also a typo in Theorem 3, etc. 
  Further, the authors may want to give a formal definition of hash-amenability in section 4 and explains how hashing works with the rest of the bandit algorithm. 
  Besides stating all the theorems, the authors may want to better explain their approaches in words. For example, when constructing the confidence set, why do you choose the center of the ellipsoid to be the ridge regression estimator on the natural parameters predicted by the online learner? The authors may want to give more justifications (in words) for their approaches so that the reader can better understand their algorithms. 
"
Experimental Design for Learning Causal Graphs with Latent Variables,"Murat Kocaoglu, Karthikeyan Shanmugam, Elias Bareinboim",t,"The authors propose theory and algorithms for identifying ancestral relations, causal edges and latent confounders using hard interventions. Their algorithms assume that it is possible to perform multiple interventions on any set of variables of interest, and the existence of an independence oracle, and thus is mostly of theoretical value. In contrast to previous methods, the proposed algorithms do not assume causal sufficiency, and thus can handle confounded systems. The writing quality of the paper is good, but some parts of it could be changed to improve clarity. 

General Comments
Several parts of the paper are hard to follow (see below). Addressing the below comments should improve clarity.  In addition, it would be very helpful if further examples are included, especially for Algorithms 1-3. Furthermore, results are stated but no proof is given. Due to lack of space, this may be challenging. For the latter, it would suffice to point to the appendix provided in the supplementary material. Some suggestions to save space: (1) remove the end if/for/function parts of the algorithms, as using appropriate indentation suffices, (2) the “results and outline of the paper” section could be moved to the end of the introduction, and several parts in the introduction and background could be removed or reduced.
Although existing work is mentioned, the connections to previous methods are not always clear. For instance, how do the proposed methods relate to existing methods that assume causal sufficiency? How about methods that do not assume causal sufficiency, but instead make other assumptions? Also, the paper [1] (and references therein) are related to the work but not mentioned.
In the abstract, the authors mention that some experiments may not be technically feasible or unethical, yet this is not addressed by the proposed algorithms. In contrast, the algorithms assume that any set of interventions is possible. This should be mentioned as a limitation of the algorithms. Furthermore, some discussion regarding this should be included. An important question is if and how such restrictions (i.e. some variables can’t be intervened upon) affect the soundness/completeness of the algorithms.
Lines 47-50: The statement regarding identifiability of ancestral relations is incorrect. For some edges in MAGs it can be shown that they are direct, or whether the direct relation is also confounded; see visible edges in [2] and direct causal edges in [3]. If shown to be direct, they are so only in the context of the measured variables; naturally, multiple variables may mediate the direct relation. 
Line 58: [4] is able to handle interventional data with latent variables. Other algorithms that are able to handle experimental data with latent variables also exist [5-9].

Definition 1: How is the strong separating system computed? (point to appendix from the supplementary material)

Lemma 3: Shouldn’t also X_i \not\in S?

Algorithm 2, line 7: What is Y? This point is crucial for a correct understanding of the algorithm. I assume there should be an iteration over all Y that are potential children of X, but are not indirect descendants of X.

Lemma 4: For sparse graphs, the transitive closure can be trivially computed in O(n * (n + m)) time (n: number of nodes, m: number of edges) which is faster than O(n^\omega). I mention this as the graphs considered in this paper are sparse (of constant degree).

Line 179: The fact that Tr(D) = Tr(D_{tc}) is key to understanding Lemma 5 and Algorithm 3 and should be further emphasized.
Algorithm 3: It is not clear from the algorithm how S is used, especially in conjunction with Algorithm 1. One possibility is for Algorithm 1 to also always intervene on S whenever intervening on set S_i (that is, intervene on S_i \cup S). Is this correct? This part should be improved.
Algorithm 3: Using S to intervene on a random variable set, and then performing additional interventions using Algorithm 1 raises the question whether a more efficient approach exists which can take into consideration the fact that S has been intervened upon. Any comments on this?
Algorithm 3: It would be clearer if the purpose of c is mentioned (i.e. that it is a parameter which controls the worst case probability of recovering the observable graph).
Line 258: Induced matching is not defined. Although this is known in graph theory, it should be mentioned in the paper as it may confuse the reader.
Do-see test: Although not necessary, some comment regarding the practical application of the do-see test would be helpful, if space permits.
Typos / Corrections / Minor Suggestions
Pa_i is used multiple times but not defined
The longest directed path is sometimes denoted as r and sometimes as l. Use only one of them.
Abstract: O(log^2(n)) -> O(d*log^2(n))
Line 30: … recovering these relations … -> … recovering some relations … (or something along that lines, as they do not identify all causal relations)
Line 59: international -> interventional
Line 105: Did you mean directed acyclic graph?

Line 114: (a) -> (b)

Line 114: … computed only transitive closures … -> … computing using transitive closured …

Line 120: (a) -> (c)
Line 159: Define T_i … -> something wrong here

Line 161: Using a different font type for T_i is confusing. I would recommend using different notation.

Lemma 5, Line 185: V_i \in S^c would be clearer if written as V_i \not in S (unless this is not the same and I missed something)
Line 191: We will show in Lemma 5 -> Shouldn’t it be Theorem 3?
Algorithm 3: \hat{D}(Tr(\hat{D}_S)) looks weird. Consider removing Tr(\hat{D}_S) (unless I misunderstood something).
Lines 226-227: To see the effect of latent path
Line 233: Figure 4.2 -> Figure 2
Line 240: … which shows that when … -> remove that
[1] Hyttinen et al, Experiment Selection for Causal Discovery, JMLR 2013
[2] Zhang, Causal Reasoning with Ancestral Graphs, JMLR 2008
[3] Borboudakis et al, Tools and Algorithms for Causally Interpreting Directed Edges in Maximal Ancestral Graphs, PGM 2012
[4] Triantafillou and Tsamardinos. Constraint-based causal discovery from multiple interventions over overlapping variable sets. JMLR 2015
[5] Hyttinen et al, Causal Discovery of Linear Cyclic Models from Multiple Experimental Data Sets with Overlapping Variables, UAI 2012
[6] Hyttinen et al, Discovering Cyclic Causal Models with Latent Variables: A General SAT-Based Procedure, UAI 2013
[7] Borboudakis and Tsamardinos, Towards Robust and Versatile Causal Discovery for Business Applications, KDD 2016
[8] Magliacane et al, Ancestral Causal Inference, NIPS 2016
[9] Magliacane et al, Joint Causal Inference from Observational and Experimental Datasets, Arxiv



"
Lower bounds on the robustness to adversarial perturbations,"Jonathan Peck, Joris Roels, Bart Goossens, Yvan Saeys",https://proceedings.neurips.cc/paper/2017/hash/298f95e1bf9136124592c8d4825a06fc-Abstract.html,"This paper introduces lower bounds on the minimum adversarial perturbations that can be efficiently computed through layer-wise composition.

The idea and the approach is timely, and addresses one of the most pressing problems in Machine Learning. That being said, my main criticism is that the bounds are too loose: the minimum adversarials found through FGSM are several orders of magnitude larger then the estimated lower bounds. That might have two reasons: for one the lower bounds per layer might not be tight enough, or the adversarials found with FGSM are simply to large and not a good approximation for the real minimum adversarials perturbation. To test the second point, I’d encourage the authors to use better adversarial attacks like LBFGS or DeepFool (e.g. using the recently released Python package Foolbox which implements many different adversarial attacks). Also, the histograms in Figure 2&3 are difficult to compare. A histogram with the per-sample ratio between adversarial perturbation and lower bound would be more enlightening (especially once the bounds get tighter).","The authors of this manuscript derive the lower bounds on the robustness to adversarial perturbations on feed-foreword neural networks, particularly for CNNs. The theory is validated on MNIST and CIFAR-10 data sets. Overall, the paper is well structured; and the idea is presented well. One major question is how this theory can be extended to GAN network? Moreover, the references at the end of the paper should be rigorously reformatted. I can identify various errors/typos, such as Iclr, mnist, lowercase-uppercase inconsistency, etc.","This proposes analysis of the network to bound the size of the adversarial perturbations for deep convolutional networks.

The exact shape and reason for adversarial examples is an increasingly more studied domain of neural networks. Robustness/sensitivity to adversarial examples can have various security implications. This paper gives an iterative method for computing the robustness (minimum perturbation necessary to change the output class label) by precomputing the bound on the most commonly occurring layer types (fully connected, convolutional + ReLU, max-pooling) and describing how to back-propagate the estimate.

The paper has two main uses: theoretical analyses of networks can be useful for comparing the robustness of different models. Also the bounds could be useful for optimizing the networks for robustness. Although the work is relatively straightforward, it could present a significant milestone toward improved analysis and construction of networks for adversarial robustness.
"
Reliable Decision Support using Counterfactual Models,"Peter Schulam, Suchi Saria",https://proceedings.neurips.cc/paper/2017/hash/299a23a2291e2126b91d54f3601ec162-Abstract.html,"Comments:

- formula (3). There are misprints in the formula. E.g. in the second term other arguments for lambda^*(t) should be used; in the third term lambda depends on theta, however theta are parameters of the CGP process and lambda in the second term does not depend on theta. Then is this a misprint?

- Figure 2. The figure is pale, names of axes are too small

- page 7, section ""Model"". 
1) How did the authors combine these two covariance functions?
2) Used covariance functions depend only on time. How did the authors model depedence of outcome distribution on a history, actions, etc.? 
3) In this section the authors introduced response functions for the first time. Why do they need them? How are they related with model (2)?

- page 8. Discussion section. I would say that the idea to combine MPP and GP is not new in the sense that there already exist some attempts to use similar models in practice, see e.g. https://stat.columbia.edu/~cunningham/pdf/CunninghamAISTATS2012.pdf

Conclusions:
- the topic of the paper is important
- the paper is very well written
- a new applied model is proposed for a particular class of applications
- although the idea is very simple, experimental results clearly show efficiency of the developed method
- thus, this paper can be interesting for a corresponding ML sub-community
","Thanks to the authors for a very interesting paper. The main contribution that I see is the proposal of a counterfactual GP model, which can take into account discrete actions, events, and outcomes, and as the authors argue allows for a much better model of discrete event data, e.g. electronic medical records, by explicitly accounting for future actions. 

A minor point: the citation for Neyman shouldn't be  to 1990, as the paper is from 1923. Yes, the accessible translated version is from 1990, but the citation should include the fact that it's of an article from 1923, seeing as Neyman died in 1981. ;) 

I have some suggestions for strengthening the presentation and outlined some of what confused me. Hopefully addressing this will improve the paper.

The framework adopted, of counterfactual predictions, is a very reasonable way to think about causal inference. I believe that the two main assumptions are standard in some of the literature, but, following Pearl, I do not think that they are the best way to think about what is going on. Overall, I would very much like to see a graphical model in Section 2. 

Regarding Assumption 1, I would like an account following or at least engaging with Pearl's critique of VanderWeele's claim about the consistency rule as an assumption rather than a theorem. 

Regarding Assumption 2, you cite Pearl so I am sure you are aware of his viewpoint, but I think it is very important to make clear what the no unmeasured confounders assumption is, and I believe that a graphical model is the best way to do this. It is worth pointing out as well that, following Pearl, we can go beyond the (often unrealistic) assumption of no unmeasured confounders (see e.g. the back-door and front-door criteria) as long as we're willing to state clearly what our structural assumptions are. Far from a criticism of how you've set things up, I believe that this viewpoint will mean you can cover more cases, as long as there is a way to setup the appropriate regression using a GP (which there should be!)

Having I think understood Assumptions 1 and 2, it now seems like the claim is that we can estimate P(Y[a]) by marginalizing P(Y | A = a, X = x) with respect to X. But I'm having trouble connecting this with the claim that the CGP will do better than the RGP. The end of the ""Model."" section has the key insight, which I think you ought to expand a bit on and move earlier. It'd be helpful, I think, to write down the likelihood of the RGP so that it can be compared to Eq (3). It would also help to refer to Figure 1 (which is indeed best viewed in color, but could be made quite a bit more legible in grayscale!)---what would RGP do for Figure 1, for example? 

Now having read the experiments I'm confused again. In the ""Experimental"" scenario it is the case that after 12 hours there is a structural break, and all patients are assigned to a control condition, since no actions (treatments) are taken. But why are the RGP and CGP equivalent here?

The Observational scenario makes sense, and the fact that the CGP performs well is reassuring, but why does this support your claim that the CGP is able to ""learn a counterfactual predictive model""? Shouldn't you ask the CGP to make predictions under different treatment regimens---to take the example from Figure 1, shouldn't you ask the CGP to predict what will happen if Drug A is administered at hour 13, if Drug B is administered at hour 13, or if no drug is administered? It seems to me that the CGP is a better predictive model than the RGP, and thus does a better job predicting. I think it's true that this means that under your assumptions it will also do a better job on counterfactuals, but you should evaluate this.

A question: how did you fit your models? I guess you maximized Equation 3, but there's some mixture modeling stuff going on here, and the model is highly non-convex. Were there issues with optimization? Did you need to do random restarts? How did you pick initial values? Where did the confidence intervals on parameters come from?","
      The paper presented a counterfactual model for time-continuous data building upon Gaussian processes. The literature review is extensive and thorough, and the experimental results are convincing. I have the following questions/comments:
      1. To what extend one has to use the counterfactual arguments in order to get the final Bayesian model, which under assumptions 1 and 2 is sufficient for inference?
      2. In (3), what is the model for $\lambda_\theta^*(s)$. Is it a parametric form? How is the integration done?
      3. In all the examples, the observations are constrained to be non-negative. What is the likelihood model for the observation model that allows this to be done? Or is this restriction ignored and a Guasssian likelihood used?
      4. How is the mixture of three GPs done? Is it via variational inference of MCMC?
      5. In lines 329 and 330, it will be clearer if expressions for the two alternatives are given. Is the first of these the so called RGP?
      "
Group Additive Structure Identification for Kernel Nonparametric Regression,"Chao Pan, Michael Zhu",https://proceedings.neurips.cc/paper/2017/hash/2aedcba61ca55ceb62d785c6b7f10a83-Abstract.html,"The paper considers kernel regression in the high dimensional setting. The starting point of the paper is the ""additive model"", which consider regressors on the form

  f = sum_p f_p(X_p)

i.e. correlations between different dimensions are ignored. Obviously it is problematic to ignore such correlations so the authors instead consider the ""group additive model"", where different dimensions are grouped, such that correlations between some dimensions can be modeled. The authors provide a formal treatment of this setting, and provide some (somewhat) ad hoc algorithms for finding optimal groupings. Results are presented for small synthetic and real-world cases.

For me, the key issue with the paper is that it does a rather poor job of describing related work, so it is difficult for me (as a non-expert) to determine the true novelty of the work. The task of finding optimal groups in data is e.g. also considered in ""structured sparsity"" and when learning graphical models/Bayesian networks. Yet, the authors make no mention of these fields. Likewise, the problem can be seen as an instance of ""feature selection"", but again this is not mentioned. Effectively, the related work described in the paper boils down to a single KDD-2015 paper [8], which is not enough for me to determine the actual novelty of paper.

That being said, the theoretical treatment does strike me as novel. The authors end up defining a simple complexity measure on a grouping (this measure favors many small groups). This is derived as an upper bound on the covering number. This seems sensible enough. The downside, is that this measure does not lend itself to easy optimization, so the authors either propose an exhaustive search (which scales poorly) or a greedy method (akin to forward feature selection). While I acknowledge that discrete functions (such as one describing optimal groupings) are generally difficult to optimize, I must say I was somewhat disappointed that the authors hadn't arrived at a measure that could be optimized more efficiently than just using cross-validation. The authors mention that they have been ""investigating and comparing different measures"" (line 174), but they provide no further insights, so it is hard for me to determine the suitability of the chosen measure.

Experimentally, the authors provide simple synthetic validation of their framework along with a simple example on the Boston Housing data. They provide no baseline and do not compare with other methods. Again, it is difficult for me to determine if the presented results are good as I have nothing to compare with.

== Post rebuttal ==

The authors clarified some of my concerns in their rebuttal. I still think the experiments are thin, and that the authors did an unacceptably poor job of relating their work to ideas in machine learning. However, I do acknowledge that the paper has valid contributions and have improved my score somewhat. I strongly encourage the authors to (at least) improve their discussion of related work when the paper is published.
","Note: Since the supplement appears to include the main paper, I simply reviewed that, and all line numbers below correspond to the supplement.

Summary:
This studies group-additive nonparametric regression models, in which, for some partitioning of the predictor variables, the regression function is additive between groups of variables; this model interpolates between the fully nonparametric model, which is difficult to fit, and the additive model, which is sometimes too restrictive. Specifically, the paper studies the problem where the group structure is not known in advance and must be learned from the data. To do this, the paper proposes a novel penalty function, based on the covering numbers of RKHS balls, which is then added to the kernel ridge regression objective. This results in an objective that can be optimized over both the group structure (which, together with the kernel determines a function space via direct sum of RKHSs over each group of variables) and the regression estimate within each group. Two algorithms are presented for approximately solving this compound optimization problem, and then theoretical results are presented showing (a) the rate at which the empirical risk of the estimate approaches the true risk of the true optimum, and (b) consistency of the group structure estimate, in that the probability it matches the true group structure approaches 1 as n -> infinity. Finally, experimental results are presented on both synthetic and real data.



Main Comments:

The key innovation of the paper appears to be recognizing that the complexity of a particular group structure can be quantified in terms of covering numbers of the direct sum space. The paper is comprehensive, including a well-motivated and novel method, and reasonably solid theoretical and empirical results. I'm not too familiar with other approaches to fitting models between the additive and nonparametric models, but, assuming the discussion in Lines 31-50 is fairly, complete, this paper seems like a potentially significant advance. As noted in the Discussion section, the main issue with the method appears to be difficulty solving the optimization problem over group structure when the number of variables in large. The paper is also fairly clearly written, aside from a lot of typos.


Minor Comments/Questions:
Just curious: is there any simple characterization of ""interaction"" between two variables that doesn't rely on writing the whole model in Equation (1)?
Line 169: I don't quite understand the use of almost surely here. Is this meant as n -> infinity?
Equation (3) is missing a summation (over i).
Line 205: Perhaps ""translation invariant kernel"" is a more common term for this than ""convolutional kernel""


Typos:
Line 145: ""on RHS"" should be ""on the RHS""
Line 152: ""not only can the bias of \hat f_{\lambda,G} reduces"" should be ""not only can the bias of \hat f_{\lambda,G} reduce""
Line 160: ""G^* exists and unique"" should be ""G^* exists and is unique""
Line 247: ""turning parameters"" should be ""tuning parameters""
Algorithm 2: Line 1: ""State with"" should be ""start with""
Line 251: ""by compare"" should be ""by comparing""."
A multi-agent reinforcement learning model of common-pool resource appropriation,"Julien Pérolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel",https://proceedings.neurips.cc/paper/2017/hash/2b0f658cbffd284984fb11d90254081f-Abstract.html,"An interesting take on modelling the CPR dilemma. From a social science perspective and purely as future work, it would be interesting to see the following:

1. How the agents behave if they are aware of the type of opponents in their local view (zapper/non-zapper). Does it lead them to form a (temporary?) alliance or do they tend to avoid each other.

2. The effects of varying the rate at which resources are harvested for a part of the agent population.","* Summary
Authors introduce a novel multi-agent problem (from machine learning's perspective) of common-pool resource appropriation and study how independently learning agents, while trying to optimize certain incentives, can learn to avoid ""tragedy of the commons"". The authors find that in certain environments, the learned strategies are consistent with predictions from economics.

* Strengths
The paper is well written and argued paper introducing a novel problem applying techniques of multi-agent reinforcement learning to model possible mechanisms by which intelligent agents can learn to avoid ""tragedy of the commons"". It shows how modern machine learning methods, when applied appropriately can add more fidelity to the model under study, while still maintaining some tractability. The authors clearly mention the limitations of the model and avoid making any comments about the social-economical implications. 

* Weaknesses
- The paper is not self contained
Understandable given the NIPS format, but the supplementary is necessary to understand large parts of the main paper and allow reproducibility. 
I also hereby request the authors to release the source code of their experiments to allow reproduction of their results.

- Use of deep-reinforcement learning is not well motivated
The problem domain seems simple enough that a linear approximation would have likely sufficed? The network is fairly small and isn't ""deep"" either.

- > We argue that such a mechanism is more realistic because it has an effect within the game itself, not just on the scores
This is probably the most unclear part. It's not clear to me why the paper considers one to be more realistic than the other rather than just modeling different incentives? Probably not enough space in the paper but actual comparison of learning dynamics when the opportunity costs are modeled as penalties instead. As economists say: incentives matter. However, if the intention was to explicitly avoid such explicit incentives, as they _would_ affect the model-free reinforcement learning algorithm, then those reasons should be clearly stated.

- Unclear whether bringing connections to human cognition makes sense
As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"". 

* Minor comments
** Line 16: 
  > [18] found them...
  
  Consider using \citeauthor{} ?

** Line 167:
  > be the N -th agent’s
  should be i-th agent?

** Figure 3:
Clarify what the `fillcolor` implies and how many runs were the results averaged over?

** Figure 4:
Is not self contained and refers to Fig. 6 which is in the supplementary. The figure is understandably large and hard to fit in the main paper, but at least consider clarifying that it's in the supplementary (as you have clarified for other figures from the supplementary mentioned in the main paper).

** Figure 5:
- Consider increasing the axes margins? Markers at 0 and 12 are cut off.
- Increase space between the main caption and sub-caption.

** Line 299:
From Fig 5b, it's not clear that |R|=7  is the maximum. To my eyes, 6 seems higher.
","General comment:

The paper is well structured, the research well carried and was pleasant to read. The contribution of the paper is made clear and lies in two aspects: 
-	Development of a game like environment to model the CPR appropriation model.
-	Description of a learned policy by multiple deep reinforcement learning agents. The authors make the descriptive goal very clear and I think the description work is rigorous and well achieved.
The authors are trying to answer a relevant problem of CPR appropriation. The goal is to understand and describe possible emerging behavior in common-pool resource appropriation.

The environment:

On the first part, I must admit that my background in economy and social sciences is not enough to gage if the environment is relevant to model the CPR problem. However, the authors cite many sources from the field and claim that their model is more realistic. 
Some quantitative results to compare the new POMDP environment to previous work should be added. If it is not the main concern of the author it should be at least in the supplementary material.

The description of the POMDP environment is well explained, the supplementary material is necessary to understand the environment, especially the observation space. I think the caption of Figure 1 should be changed to make more obvious the difference between states and observations.

One of the most interesting aspect of the paper is the definition of the social-outcome metrics (section 2.3). I think these are very interesting and strongly help the interpretation of the resulting policies. To the best of my knowledge, drawing conclusions on the behavior of the agents based on these metrics is a novel approach and a good contribution.

The results:

The conclusions drawn are corrects and reasonable. A lot of quantitative graphs are present to justify the conclusions. I found the videos extremely helpful in understanding the behaviors. They designed an interesting set of experiments that allowed the emergence of non-trivial behaviors. In a RL point of view I think this is a key aspect of the work.
The analysis of the emerging behaviors from the learning curve is well carried thanks to the relevant metrics. 
The authors compare their results with previous work qualitatively and make rather clear the advantages and limitations of their approach. I am not sure if it is possible but having a table or a graph with quantitative metrics would have been even more assertive.

Conclusions:

Regarding the social science aspect, the authors qualitatively compare their work to the existing literature. In my opinion, a quantitative result would have helped making the contribution to the social science domain clearer. The authors are aware of the limitations of the model and raise relevant open questions that could be addressed as future work.
On the reinforcement learning aspect, they propose a novel approach to describe multi-agent behaviors and use novel metrics that help both the description work and the design of new experiments. I think this approach could be extended to other domains as future work. 

Minor comments:
-	A link to the code should have been added to the supplementary material for reproducibility of the results.
-	Figure 2 could be closer to the paragraph referring to it (Section 3)
-	The explanation of the Schelling diagram could have been clearer.
-	Rename discussion section to conclusion?
"
Decoding with Value Networks for Neural Machine Translation,"Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, Tie-Yan Liu",https://proceedings.neurips.cc/paper/2017/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html,"This paper uses a reinforcement learning approach to equip a sequence-to-sequence lstm NMT model with an objective prediction model so that the greedy beam search that optimizes maximum likelihood is tempered by a prediction of the objective based on the current state of the search.

The paper is well written and clearly outlines the model design and experimental procedure. The data and comparisons are all apt, the baselines are chosen wisely and are not weak, and the result is quite significant; I will probably incorporate these results in my own NMT research.

I would, however, like to see a comparison between this work and the work of the cited Shen et al. paper that directly trains NMT to maximize BLEU instead of maximum likelihood. It seems that the same information is being learned in this work and, if so, that would be a more efficient way to learn. A difference between the two would tease apart the impact of the RL vs the different objective. That work does evaluate on 2 of the sets shown here; I did not carefully check to see that the entire experimental setup is the same, but if so, then you could easily use those results to strengthen your case.

I was confused about one aspect of training: sentence-level BLEU is used as an objective to train the Value NN, but that metric, without any modifications, can be quite erratic when used to evaluate a single sentence, since 0 4-gram matches will render the score 0 and not take into account partial matches. Various tricks have been employed to get around this; BLEU+1 is the most common (see http://www3.nd.edu/~dchiang/papers/mira.pdf or http://www.aclweb.org/anthology/D07-1080). How was this handled?

51&52: outputted->output
81: proceeding->preceding
eq 3 and 4: g and q functions not defined
96: 30k^50 candidates. it's sum_{i=1}^50 30k^i candidate *states*
104-113: long winded. You could just say, essentially, ""Beam decoding makes search errors"" if you need to make space.","This paper addresses one of the limitation of NMT, the so-called
exposure bias, that results from the fact that each word is chosen
greedily. For this, the authors build on standard technique of
reinforcement learning and try to predict, for each outgoing
transition of a given state, the expected reward that will be achieved
if the system take this transition.

The article is overall very clear and the proposed ideas quite
appealing, even if many of the decisions seem quite ad hoc (e.g. why
doing a linear combination between the (predicted) expected reward and
the prediction of the NMT system and not decoding directly with the
former) and it is not clear whether the proposed approach can be
applied to other NLP tasks.

More importantly, several implementation ""details"" are not specified.
For instance, in Equation (6), the BLEU function is defined at the
sentence level while in the actual BLEU metric is defined at the
corpus level. The authors should have specified which approximation of
BLEU that have used.

The connection between structure prediction (especially in NLP) and
reinforcement learning has been made since 2005 (see for instance [1],
[2] or, more recently, [3]). It is, today, at the heart of most
dependency parsers (see, among others, the work of Goldberg) that have
deal with similar issues for a long time. The related work section
should not (and most not) be only about neural network works.

The MT system used in the experiments is far behind the performance
achieved by the best systems of the WMT campaign: the best system (a
""simple"" phrase-base system as I remember) achieves a BLEU score of
~20 for En -> De and 35 for En -> Fr while the scores reported by the
authors are respectively ~15 and ~30. I wonder if the authors would
have observed similar gains had they considered a better baseline
system.

More generally, most of the reported experimental results are quite
disturbing. Figure 3.a  shows that increasing the beam actually
degrades translation quality. This counter-intuitive results should
have been analyzed. Figure 3.b indicates that taking into account the
predicted value of the expected BLEU actually hurts translation
performance: the less weight the predicted score gets in the linear
combination, the better the translation.

[1] Learning as Search Optimization: Approximate Large Margin Methods for Structured
Prediction, Hal Daumé III and Daniel Marcu, ICML'05

[2] Searched-based structured prediction, Hal Daumé III, John Langford
and Daniel Marcu, ML'09

[3] Learning to Search Better than Your Teacher, Kai-Wei Chang, Akshay
Krishnamurthy, Alekh Agarwal, Hal Daumé III and John Langford","Decoding with Value Networks for Neural Machine Translation



At various places, you mention the similarity between your approach and AlphaGo, which also scores actions using a linear combination of a scores from conditional probability model and scores that estimate the future reward that an action will lead to. However, I wish this discussion was a bit more precise. Namely, you should mention Monte Carlo Tree Search and how it is different than your approach. MCTS has various methods for balancing exploration vs. exploitation that you ignore when you sample your trajectories from your policy. These may have been useful. 
Your discussion of related work should include the broader selection of RL methods that have been used in NMT over the past couple of years. For example, it would be helpful for readers if you mentioned actor-critic approaches and described how your work is different. 
You definitely need to improve the exposition regarding the semantics for the policy that the value network is computed with respect to. You say that it estimates the expected reward under a policy. However, you never state formally what this distribution is. Since you use the avg bleu score, it seems like you’re defining it as the distribution defined by running beam search to get a set of candidate predictions and then uniformly sampling from these. Why is this the right distribution to be using? You could, for example, just sample from the conditional distribution of y_t given the y's before it.
I was very confused why you chose avg bleu and not max bleu in line 165. Shouldn’t the score of a word be the maximum possible reward achievable if we choose that word? There is a nice line of related work in the parsing literature on defining what these look-ahead oracles should be. See Goldberg and Nivre [1] and follow-on work. 


[1] Goldberg and Nivre. A Dynamic Oracle for Arc-Eager Dependency Parsing. 2012. 




Minor comments

line 156: ""Temple Difference""

line 149: it would be helpful to remind the reader what h is (same for c in line 142)

line 152: by softmax do you mean sigmoid? Also, why is this a good idea?

line 163: it’s not really an expectation: it’s the deterministic set output by beam search

citations: you should use citet in the natbib package to reference papers as elements in the sentence. For example, rather than saying ‘[1] do xyz’ you should say ‘author1 et al. [1] do xyz'

references section: often you cite the arxiv version of a paper that has been accepted to a conference
"
Population Matching Discrepancy and Applications in Deep Learning,"Jianfei Chen, Chongxuan LI, Yizhong Ru, Jun Zhu",https://proceedings.neurips.cc/paper/2017/hash/2b45e8d6abf59038a975faeeb6dc0782-Abstract.html,"The paper defines Population Matching Discrepancy between two distributions
as the Wasserstein distance between two minibatches from the distributions.
The Wasserstein distance is computed by an exact O(N^3) or an approximate O(N^2) algorithm.

Pros:
- It is interesting to see the experiments with this computation of the Wasserstein distance.
The generated images are not as good as from Wasserstein GAN.

Cons:
- The proposed distance would need large N to estimate the Wasserstein distance between two diverse multimodal distributions.
I suspect that problems would be already visible, if trying to match a mixture of Gaussians (including learning the variances).
- If N is not large enough, the optimization may have the global minimum at a point different from the true distribution.
For example, the learned distribution may have less entropy.
The SVHN samples in Figure 4 seem to have low diversity. Digit 8 appears frequently there.


Minor typos:
- Line 141: s/usally/usually/

Update:
I have read the rebuttal. Thanks for the extra experiments.

The authors should clarity the limitation of PMD and MMD.
MMD is OK with batch_size=2.
MMD can be trained with SGD, 
if using the unbiased estimator of MMD from the original ""A Kernel Two-Sample Test"" paper.
So MMD can converge to the right distribution if using small minibatches.
On the other hand, PMD does not have a known unbiased estimator of the gradient.

","This paper presents the population matching discrepancy (PMD) as a better alternative to MMD for distribution matching applications.  It is shown that PMD is a sampled version of Wasserstein metric or earth mover’s distance, and it has a few advantages over MMD, most notably stronger gradients and the applicability of smaller mini-batch sizes, and fewer hyperparameters.

For training generative models at least, the MMD metric does suffer from weak gradients and the requirement of large mini-batches, the proposals in this paper therefore provides a nice solution to both of these problems.  The small mini-batch claim is verified quite nicely in the empirical results.  The verification of the stronger gradients claim is less satisfactory, since the MMD metric depends on the scale parameter sigma, it is essential to consider either the best sigma or a range of sigmas when making such a claim.

In terms of having fewer hyper-parameters, I feel this claim is less well-supported, because PMD depends on a distance metric, and this distance metric might contain extra hyperparameters as well as in the MMD case.  Moreover, it is hard to get a reliable distance metric in a high dimensional space, therefore PMD may suffer from the same issue of relying on a distance metric as MMD.  On the other hand, there are some standard heuristics for MMDs about how to choose the bandwidth parameter, it would be good to compare against such heuristics and treat MMD as a hyperparameter-free metric as well.

Overall I think the proposed method has the nice property of permitting small minibatch sizes therefore fast training.  It seems like a valid improvement over large batch MMD methods.  But the it still has the problem of relying on a distance metric, which may limit its success on modeling higher dimensional data.","The authors present PMD a population based divergence between probability distributions and show it is a consistent estimator of the Wasserstein distance.

The estimator presented is conceptually simple and differentiable, which is a clear alllows training NN based models.

The authors thoroughly compare PMD to MMD, which is the most prominent population based divergence in machine learning.

The authors comment on the drawbacks of their method: exact calculation has cubic complexity, but propose the use of an approximation which has quadratic complexity, and show in their empirical results that this does not degrade statistical performance too much.

The paper is well structured and written and includes references to previous work where due.

The theoretical results seem correct.

The experimental analysis is adequate. They compare PMD to MMD and other methods for domain adaptation and compare to MMD for generative modelling.

I would have liked to see the method being used for generative modelling in domains with many modes. I wonder if PMD works when N is smaller than the number of modes.

All things considered I think this is a good paper, that presents a possibly very useful method for comparing distributions."
Predictive State Recurrent Neural Networks,"Carlton Downey, Ahmed Hefny, Byron Boots, Geoffrey J. Gordon, Boyue Li",https://proceedings.neurips.cc/paper/2017/hash/2bb0502c80b7432eee4c5847a5fd077b-Abstract.html,"This paper proposes a new model for dynamical systems (called PSRNN), which combines the frameworks of PSR and RNN non-trivially. The model is learned from data in two steps: The first step initialize the model parameters using two-stage regression (2SR), a method previously proposed by Hefny et al for learning PSRs. The second step use Back-propagation-through-time to refine the parameters. The learned model can then be used for filtering and prediction. The model has an appealing bi-linear gating mechanism, resembling the non-linear gating mechanisms used in LSTM and other models and enjoys rich functional form via kernel embedding and/or multilayer stacking. A factorized version is also discussed.

The empirical study in this paper shows a clear advantage of PSRNN over previously proposed methods and looks quite promising. Unfortunately, the paper is not well written, not self-contained and hard to follow. There is no chance I could implement this algorithm given the description in the paper. I often find this very frustrating in practice.

Most prominently, the introduction of 2SR in Section 4.1 is highly unclear. First, I could not make mathematical sense out of equations (4) and (5). How does a multiplication of 3-mode tensor and a matrix is defined? Going over the referred paper of Hefny et al (briefly) I could not readily determine how their approach can be adapted to PSRNN in this paper. This should be discussed in detail. In particular, I didn't understand how W,Z in (4) and (5) are used to produce a PSRNN, since Z is not used in a PSRNN model. So why one need to learn Z?

In line 109, the definition of f_{t+k} is ambiguous. How is it precisely defined in terms of o_{1:T}?

Although Section 6 is named ""Theoretical discussion"", no theory on the performance of PSRNN is given in this paper. The last paragraph of this section is unclear to me.

To conclude, the model and results in this paper should be definitely published, but after a major revision of the manuscript, in particular making the learning procedure clearer to the non-expert reader.","This paper extends predictive state representations (PSRs) to a multilayer RNN architecture. A training algorithm for this model works in two stages: first the model is initialized by two-stage regression, where the output of each layer serves as the input for the next layer, next the model is refined by back propagation through time. The number of parameters can be controlled by factorizing the model. Finally, the performance of this model is compared to LSTMs and GRUs on four datasets.

The proposed extension is non-trivial and seem to be useful, as demonstrated by the experimental results. However, the initialization algorithm, which is a crucial learning stage, should be described more clearly. Specifically, the relation of the estimation equations (3-5) to Heffny et al. is not straightforward and requires a few intermediate explanations. On the other hand, Section 3 can be shortened. The introduction of the normalization in Equation (1) is a bit confusing as it takes a slightly different form later on.

Two strong aspects of the paper are that initialization algorithm exploits a powerful estimation procedure and the Hilbert embedding provides a rich representation. More generally, exploiting existing approaches to learn dynamical systems is an interesting avenue of research. For example, [Haarjona etal. 2016] proposed construction of RNNs by combining neural networks with Kalman Filter computation graphs. 

In the experimental section, can the authors specify what kernels have been used?

It would be interesting to elaborate on the added value of additional layers. The experiments show the added value of two-layer models with respect to a one-layer in terms of test performance measure. Is this added value achieved immediately after initialization, or only after applying BPTT? Does the second layer improve by making the features richer? Does the second stage regression learn the residuals of the first one.

Minor comments and typos:
- Figures are too small
- Line 206: delete one ‘the’

Ref: 
Haarjona etal. 2016, Backprop KF: Learning Discriminative Deterministic State Estimators
","The authors study the well known Predictive State Representation problem and introduce a new neural network based architecture for tackling this problem. They discuss pros and cons of Bayesian approaches while mentioning the power and drawbacks of RNN based setting. Later, they propose a new procedure which tries to exploit the power of both settings. 
The paper is fairly readable and managed in a good way while covering many pieces. 
The used techniques are theoretically well studied in the literature and the authors provide a good intuition for their proposal. 

There are 3 minor things I like to see in the paper. It would be interesting to have the computation complexity of the initialization. It would be good to cite this paper ""Tensor Regression Networks"" https://arxiv.org/pdf/1707.08308.pdf which uses the tensor contraction as a core of deep learning process. 
You use ""can be "" two times in the abstract."
Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes,"Jeremiah Liu, Brent Coull",https://proceedings.neurips.cc/paper/2017/hash/2bb232c0b13c774965ef8558f0fbd615-Abstract.html,"The paper proposes a statistical test for particular non-linear effects in a linear mixed model (LMM).
The problem of testing non-linear effects is relevant, especially in the natural sciences. The novelty is significant. The experimental validation has its flaws, but may be considered acceptable for a conference paper.

The method consists of multiple parts:
1) The main new idea introduced in the paper is to introduce a kernel parameter (garotte) that interpolates between a null model and the desired alternative model and to perform a score test on this parameter. This elegant new idea is combined with several established steps to obtain the final testing procedure:
2) Defining a score statistic and deriving an approximate null distribution for the statistic based on the Satterthwaite approximation.
3) The use of cross-validation to chose appropriate kernel parameters for the null model.
4) An application, where multiplicative interactions are being tested.

The title is somewhat ambiguous, as ""Functional Effect"" is not clear.
Initially I assumed it to mean multivariate, or time-series, as in the related https://www.ncbi.nlm.nih.gov/pubmed/28056190 .
Consequently, the authors may want to change the term Functional.

Also, the title claims that the test is ""robust"".
While the authors discuss the effets of kernel choice and model missmatch on robustness (power and T1 error) and propose the use of cross validation to alleviate this, the experiments suggest that robustness is not achieved in practice. The experiments rather show that the choice of the kernel has a great influence on the results.

It is hard to disentangle power and T1 error in the same experiment, as done by the authors. This may yield misinterpretation of the results.
Instead of arguing that simple models yield inflated T1 error, one could also argue that the Satterthwaite approximation does not yield an accurate null distribution, leading to inflated or deflated T1 error, depending on the model.
A better way to assess T1 error would be a set of T1 simulations with varying alpha levels than 0.05.

Also, from an applied perspective it should be argued that the linear model does not have inflated T1 error, but rather does exactly what it is supposed to do, namely detect multiplicative effects in the data. Given the kernel functions used (Matern and RBF) the functions involve multiplicative effects in the input variables, even under the null simulation.

These issues should at least be discussed.
","The authors provide a mechanism for establishing hypothesis tests of functional forms based on Gaussian process descriptions. First, following [12] the Gaussian process predictor is represented as a linear mixed effects model. Second, using [11], the authors use the classical variance component test to develop a testing procedure for hypothesis involving GPs. Third, following [13], the kernel function is replaced by a garrote kernel function which simplifies the application of the test. The test is illustrated with a simulated data example. 

I found the reading of the paper extremely involved. There is a lack of explanations of the basic methods, and the reader needs to refer constantly to [11], [12] and [13] to follow the general ideas of the paper. It would be nice if the paper could be made self-contained or at least, the key ideas provided in a supplemental material. 

I have two criticisms: (1) why doing this is more convenient/better than doing model selection for GPs, i.e. using ARD priors to select which features are more relevant? (2)  how could a practitioner use the test? The application of the test could be better described so that it could actually be used. Examples with real data can also help to better understand the method. 

My feeling is as the paper stands, it can hardly have an impact on the NIPS community. ","This paper considers an interesting issue of hypothesis testing with complex black models. I will say from the outset that I am skeptical of the NHST framework, especially as it is currently applied in classical statistical settings, where I think it is clear the ongoing harm that it has done to applied science (see, e.g. the ASA's statement on p-values). Whether I should be more or less skeptical of it in nonparametric settings is an interesting questions and I'd be happy to be convinced otherwise. I guess the standard arguments aren't worth rehashing in great detail but just so you're aware of where I'm coming from, if I care about the (possibly non-linear) interaction effect then I want a kernel parameterized as:

beta1 * k1a(x1,x1') + beta2 * k2c(x2,x2') + beta12 * k1b(x1,x1')*k2b(x2,x2')

I want to fit that model using all of my expensive data, and then say something about beta12. Bayesian methods would give me a posterior credible interval for beta12---given limited data this is going to be my approach for sure and I'm going to spend time constructing priors based on previous studies and expert knowledge. But if you want to take a frequentist approach that's fine, you can just bootstrap to obtain a confidence interval for beta12, which I'm going to be happier with than the asymptotics-based approach you take for small samples.

OK, now that I've gotten that disclaimer out of the way (apologies!), taking your paper on its own terms, here are some areas that I think could be improved.

On line 29 I'm not sure why the null hypothesis is additive; shouldn't this be a multiplicative interaction between air pollutants and nutrient intake?

Be careful in Section 2: the fact that h ~ GP with covariance kernel k means that the probability that h is in the RKHS H_k is 0! This is a surprising fact (see Wahba 1990 and Lukic and Beder 2001) and I don't think it invalidates your approach (e.g. E[h] is in the RKHS), but it's worth fixing this point.

I like the GP-LMM connection and think it needs more explanation as I am not very familiar with REML. Could I, for example, posit the following model:

y = h1 + h2 + epsilon, h1 ~ GP(0,K1) and h2 ~ GP(0,K2) and then use your approach to decide between these models:
y = h1 + epsilon
y = h2 + epsilon
y = h1 + h2 + epsilon?

I think the answer is yes, and I'm wondering if it'd be clearer to explain things this way before introducing the garrote parameter delta. 

Turning the test statistic into a model residual also helped me think about what was going on, so maybe it'd be good to move that earlier than Section 4. It also might be nice to show some training/testing curves (to illustrate bias/variance) in Section 4 to explain what's going on.

It wasn't obvious to me why the additive main effects domain H1 \osum H12 is orthogonal to the product domain H1 \otimes H2 (section 5).

For the experiments, if this were real data I could imagine trying all of the difference kernel parameterizations and choosing the one that the fits the best (based on marginal likelihood I guess) as the model for the null hypothesis. Of the different models you considered, which one fit the data best? And did this choice end up giving good type 1 and type 2 error?

A real data experiment would be nice."
"Sharpness, Restart and Acceleration","Vincent Roulet, Alexandre d'Aspremont",https://proceedings.neurips.cc/paper/2017/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html,"This paper consider first-order algorithms for Holder-smooth convex optimization in the oracle model with an additional sharpness assumption, guaranteeing that, within a neighborhood of optimum, a reduction in objective value yields a reduction in distance from optimum. Recently, there has been growing interest in the algorithmic consequences of the presence of sharpness, particularly in the setting of alternating minimization and of compressed sensing.

Sharpness can be exploited to speed up the convergence of first-order methods, such as Nesterov's accelerated gradient descent, by appropriately restarting the algorithm after a certain number of iterations, possibly changing with the number of rounds. First, the authors provide asymptotically optimal restart schedules for this class of problem for given sharpness parameters mu and r. While this is interesting, the result is essentially the same as that appearing, in more obscure terms, in Nemirovski and Nesterov's original 1985 paper ""Optimal methods of smooth convex optimization"". See paragraph 5 of that paper.

More importantly, the authors show that a log-scale grid search can be performed to construct adaptive methods that work in settings when mu and r are unknown, which is typical in sharpness applications. This appears to be the main novel idea of the paper. From a theoretical point of view, I find this is to be a fairly straightforward observation. On the other hand, such observation may be important in practice. Indeed, the authors also show a small number of practical examples in the context of classification, in which the restart schedules significantly improve performance. At the same time, the fact that restarts can greatly help the convergence of accelerated methods has already been observed before (see O'Donoghue and Candes, as cited in the paper). 

In conclusion, I find the paper interesting from a practical point of view and I wish that the authors had focused more on the empirical comparison of their restart schedule vs that of Nemirovski and Nesterov and others. From a theoretical point of view, my feeling is that the contribution is good but probably not good enough for NIPS. It might help if the authors, in their rebuttal, explained more clearly the relation of their non-adaptive bounds with those of Nemirovski and Nesterov. 



","Summary of the paper
====================
This paper considers restarting schemes which allow one to explicitly incorporate growth properties (namely, sharpness) of convex functions into algorithms which do not necessarily exploit this additional favorable assumptions. First, the number of inner iterations per epoch is scheduled based on the parameters of the growth condition. As these parameters are hard to approximate, an adaptive scheduling is devised based on parameters grid search. Finally, it is shown that one can obtain a near-optimal rate only by knowing the value of the minimizer (omitting the requirement for knowing the sharpness parameters).


Evaluation
==========
The main contribution of of the paper is combining the mechanism of restarting schemes with the growth conditions of convex functions. The actual rate obtained by this technique seem to be of a somewhat narrow practical value (requires strong prior knowledge or grid search). However, from theoretical standpoint, it is an interesting general approach of exploiting sharpness. That said, the paper seems to contribute to the study of restarting mechanisms schemes only incrementally. The paper is well-written and easy to follow.



General Comments
================
- A more through discussion regarding other existing algorithms which obtain the same optimal rate is missing.  
- Related Work which may be worth mentioning:  
     - similar upper bound: https://arxiv.org/pdf/1609.07358.pdf
     - lower bound using restarting scheme 
               http://proceedings.mlr.press/v48/arjevani16.pdf


Minor Comments
==============
- L16: Might worth emphasizing that f^* is taken over K (and not, e.g., the domain over which f is defined).
- L153: In what sense should we expect convergence?
- L217: Didn't find the definition for Q before this line (I did find a definition in the next section).
- L285 (appendix): broken reference ??

"
Dynamic Routing Between Capsules,"Sara Sabour, Nicholas Frosst, Geoffrey E. Hinton",https://proceedings.neurips.cc/paper/2017/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html,"Overview :this paper introduces a dynamic routing process for connecting layers in a feedforward neural net, as described in Procedure 1 on p 3. The key idea here is that the coupling coeff c_ij between unit i and unit j is computed dynamically (layerwise), taking into account the agreement between the output v_j of unit j, and the prediction from unit i \hat{u}_{{j|i}. This process is iterates between each layer l and l+1, but does not (as far as I can tell) spread further back.

Another innovation used in the paper is a form of nonlinearity as in eq 1 for units which uses the length of the capsule output v_j to encode strength of activity, and the direction of v_j to encode the values of the capsule parameters.

A shallow CapsNet model is trained on MNIST, and obtains very good performance (a check of the MNIST leaderboard shows best performance of 0.23 obtained with a committee of deep conv nets), cf performance in Table 1.

I regard this paper as very interesting, as it has successfully married the capsules idea with conv nets, and makes use of the dynamic routing capabilities. The results on highly overlapping digits (sec 6) are also impressive. This idea could rally take off and be heavily used.

One major question about the paper is the convergence of the iteration in Proc 1. It would be highly desirable to prove that this converges (if it does so). This issue is as far as I can tell not discussed in the paper.

It would also be very nice to see an example of the dynamic routing process in action, especially to see how a unit whose \hat{u}_{j|i} is out of agreement with the others gets suppressed.

This process of assessing agreement of bottom-up contributions is reminiscent of the work in Rao and Ballard (Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects, Nature Neurosci. 2(1) 79-87, 1999), esp the example where local bar detectors all predict the same longer bar. One could e.g. look at how activations of 3 units in a line --- compare to one that is disoriented, e.g. |-- in your network.

I would encourage the authors to publish their code (this can be following acceptance). This would greatly facilitate uptake by the community.

Quality: As far as I can tell this is technically sound.
Clarity: The writing is clear but my detailed comments give a number of suggestions to make the presentation less idiosyncratic.

Originality: The idea of capsules is not new, but this paper seems to have achieved a step change in their performance and moved beyond the ""transforming autoencoder"" framework in which they were trained.

Significance: The level of performance that can be achieved with a relatively shallow network is impressive. Given the interest in object recognition etc it is possible that this dynamic routing idea could really take off.

Other points:

p 1 Need to ref Hinton et al (20111) when you first mention ""capsules"" in the text.

p2 and Procedure 1:I strongly recommend ""squashing"" to ""squidging"" as it is a term that has been used in the NN literature (and sounds more mathematical).

p 2 The constraint that the sum_j c_{ij} = 1 is reminiscent of the ""single parent constraint"" in Hinton et al (2000). It would be good to discuss this more.

p 3. The 0.5 in eq (4) seems like a hack, although it would look better if you introduced a weighting parameter alpha and set it to 0.5. Also I don't understand why you are not just using softmax loss (for a single digit classifier) on the ||v_j||'s (as they lie in 0,1). (For the multiclass case you can use multiple independent sigmoids.)

p 3 I recommend ""PrimaryCapsules"" rather than ""PrimaryCaps"" as Caps sounds like CAPS.

p 5 Lee et al [2016] baseline -- justify why this is an appropriate baseline.

p 5 sec 5.1 Tab. 4 -> Fig. 4

p 6 Figure 4 caption: ""Each column"" -> ""Each row"".

p 6 Sec 6 first para -- it is confusing to talk about your performance level before you have introduced the task -- leave this for later.

p 6 sec 6: note a heavily overlapping digits task was used in Hinton et al (2000).

p 8. It would be better to put the related work earlier, e.g. after sec 4.

p 9. The ordering of the references (as they appear in the paper) is unusual and makes them hard to find. Suggest alphabetical or numbered in order of use. These are standard bibtex options.


","This paper presents CapsNet, a multi-layer capsule system (similar to a deep network with multiple layers) and demonstrate the effectiveness of their approach on the MNIST dataset and demonstrate ability to handle multiple overlapping digits as well as affine perturbations.

A capsule, as proposed by ""Transforming Auto-encoders"" 2011 by Hinton, is a set of neurons whose activity (represented as a vector) suggests the presence of a visual object (measured by the magnitude of activity vector) and the object parameters (represented by the orientation of the vector). A multi-layer capsule system consists of several layers, where each layer is a set of capsules, and hence architecturally it bears significantly resemblance to a deep network with several convolution and pooling layers. A key difference is that instead of a scalar output of a convolution, capsules have a vector output, and instead of max-pooling to route information from low level layers to high level layers, it uses dynamic routing based on the activation of the capsules (magnitude of the activity vector). For training, they use a 'margin loss' over all digits classes as well as a reconstruction loss (as a regularity to reconstruct the image) from the last activity layer.

This paper uses a 3 layered capsule system trained on MNIST datasets with translation perturbations, and significantly outperformed a similarly sized CNN with max-pooling and dropout on affNIST data (MNIST with affine perturbations). A multiMNIST dataset is presented with images of overlapping digits with the task of identifying both the digits; the proposed capsule system successfully segmented the images into the images of both digits. Evaluation on CIFAR10 dataset achieved performance similar to early CNN evaluations (15.7% error). 

Strengths
* Paper presents significant refinements to the 2011's capsule system (Transforming Auto-encoders), achieving strong performance with shallow networks.
* The trained network demonstrated greater robustness to affine perturbations compared to CNNs with max-pooling and dropout, suggesting possibility of better generalization. This was attributed to the dynamic routing of information in capsule system; this line of research would indeed be useful to the NIPS community.

Weaknesses
* (Primary concern) Paper is too dense and is not very easy to follow; multiple reads were required to grasp the concepts and contribution. I would strongly recommend simplifying the description and explaining the architecture and computations better; Figure 7, Section 8 as well as lines 39-64 can be reduced to gain more space.
* While the MNIST and CIFAR experiments is promising but they are not close to the state-of-art methods. It is not obvious if such explicitly dynamic routing is required to address the problem OR if recent advances such as residual units that have enabled significantly deeper networks can implicitly capture routing even with simple schemes such as a max-pooling. It would be good if authors can share their insights on this.
","Quality

The presented work is technically sound, though ideas as such are not novel. However, the particular implementation presented is.
The authors discuss strengths and weaknesses of their presented approach, showing promising results on MNIST variants and drawbacks on more realistic tasks like Cifar10.

Clarity

The paper is well-organized, but some details are confusing or unclear.

- Discuss difference to original capsule work.
- Why are 1-3 refinement iterations chosen, what happens after more iterations?
- How many iterations were necessary for Cifar10?
- Compare the computational cost of baseline and capsules, as well as the cost of the refinement steps.
- What happens when the invariant sum over the coupled prediction vectors in equation (2) and the associated non-linearity are replaced by a simple linear layer and standard non-linearity?
- line 135: ""We test using a single model with no ... data augmentation"". A couple of lines before, the authors mention they do moderately augment data with shifts. Why do shifts improve performance, given that the authors claim capsules are designed to be robust to such variations?

Originality

The presented work is original as it introduces a new routing principle for capsule networks. 
However, novelty with respect to classical capsules should be discussed more clearly. 

Relevant related work, either dealing with separating filter response into magnitude and orientation, estimating keypoints or doing locally adaptive filtering, as opposed to global normalization of STNs:

https://arxiv.org/abs/1701.01833
https://arxiv.org/abs/1703.06211
https://arxiv.org/abs/1612.04642
https://arxiv.org/abs/1706.00598
https://arxiv.org/abs/1605.01224
https://arxiv.org/abs/1605.09673

Significance

The presented results are not very strong and it is hard to say how significant the findings are, as the authors do not thoroughly investigate more interesting domains than digits.

Performance on MNIST is a very limited metric, given that:

i) Saturated at this point
ii) Scattering has shown that local invariance wrt deformation, translation and rotation is enough to achieve very good performance
iii) It lacks ambiguity and many other properties that make natural images so challenging, especially the assumption of only one entity per location becomes questionable under clutter

The robustness results on affine and overlapping MNIST are promising, but should be validated on more realistic tasks with more challenging local statistics.

It would be great if the authors would provide the reader with insight into strengths and weaknesses on more realistic problems. 

Some suggestions:

i) More thorough evaluation + visualisations on Cifar10. The results seem weak for now, but might shed some light on failure modes and work to be accomplished by follow-up papers
ii) Check if affine robustness holds for Cifar10 as well to similar degree, this would change my vote on the paper
iii) The iterative Tagger (Graeff et al.) might give some inspiration for additional experiments with more natural ambiguity and should be discussed in related work as well

A strong analysis on the drawbacks of the presented method and open problems would make this a very useful paper, but in its current form, it is hard to tell what the suggested method achieves beyond being potentially more representationally efficient and robust on variants of MNIST."
InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations,"Yunzhu Li, Jiaming Song, Stefano Ermon",https://proceedings.neurips.cc/paper/2017/hash/2cd4e8a2ce081c3d7c32c3cde4312ef7-Abstract.html,"Paper Summary:

This paper focuses on using GANs for imitation learning using trajectories from
an expert. The authors extend the GAIL (Generative Adversarial Imitation
Learning) framework by including a term in the objective function to incorporate latent structure (similar to InfoGAN). The authors then proceed to show that using their framework,
which they call InfoGAIL, they are able to learn interpretable latent structure
when the expert policy has multiple modes and that in some setting this robustness allows them to outperform current methods.

Paper Overview:

The paper is generally well written. I appreciated that the authors first demon-
started how the mechanism works on a toy 2D plane example before moving
onto more complex driving simulation environment. This helped illustrate the
core concepts of allowing the learned policy to be conditioned on a latent variable
in a minimalistic setting before moving on to a more complex 3D driving simulation.
My main concern with the paper is that, in integrating many different ideas, 
it was hard to distil the core contribution of the paper which, as I understood, 
was the inclusion of latent variables to learn modes within an overall policy when using GAIL.

Major Comments:

1. In integrating many different ideas, it was difficult to distil the core 
contribution of this paper (the inclusion of latent variables to learn modes 
within a policy). As an illustration of this problem, In subsection 3.1 the authors refer 
to ""InfoGAIL"" in its simplest form, but the supplementary material they present a more 
complex algorithm which includes weight clipping, Wasserstein GAN objective as well as an
optional term for reward augmentation and this is also called ""InfoGAIL"". I felt the paper 
could beneft from concretely defining InfoGAIL in its most simple form, and then when 
adding additional enhancements later and making it clear this is InfoGail with these extra enhancements.

2. Linked to my first point, I felt the paper would benefit from an algorithm
box for the simplest form of InfoGAIL in the paper itself (not supplement-
tary). This would add to the readability of the paper and
allow a reader not too familiar with all the details of GANs to quickly
grasp the main ideas of the algorithm. I'd suggest adding this before the
section 4 on optimization. Then keeping the InfoGAIL with extensions in
supplementary material (and renaming it accordingly there).

3. The authors show how InfoGAIL works in an idealized setting where the number of modes are known. 
It would be useful to see how InfoGAIL behaves when the number of modes is not known and the number of latent variables 
are set incorrectly. For example, in the 2D plane environment there are 3 modes. What happens if we use InfoGail 
but set the number of latent variables to 2, 4, 5, 10 etc. Can it still infer the modes of the expert policy?
Is the method robust enough to handle these cases?

Minor Comments:

1. subsection 4.1 refers to reward augmentation. But the experiment in sub-
section 5.2 don't mention if reward augmentation is used to obtain results
(reward augmentation is only mentioned for experiments in subsection
5.3). It's not clear if the authors use reward augmentation in this experiment 
and if so how it is defined.

2. The heading of the paper is ""Inferring The Latent Structure of Human
Decision-Making from Raw Visual Inputs"". Does InfoGAIL only work
in domains where we have raw visual inputs. Also we may assume the method would work
if the expert trajectories do not come from a human but some other noisy source.
The authors may want to revise the title to make the generality of their framework more
clear.","
The paper tackles the problem of learning by demonstration from raw visual inputs. As demonstrations may notably be provided by different experts, the authors propose to learn latent factors to disentangle the different demonstrations. The proposed technique builds on several previous works, namely GAIL (Generative Adversarial Imitation Learning) as general framework for learning from demonstration, InfoGAN to take into account latent factors, reward augmentation for taking into account a priori knowledge about the task, Wasserstein GAN as an extension of GAN to regression, use of a baseline and replay buffer for variance reduction, and finally initialization of the policy with behavior cloning. The method is evaluated in the TORCS environment on two tasks: turn and pass, where in both two different behaviors can be observed in the demonstrations.

** Strengths
The authors tackle the important and hard problem of learning from demonstration directly from raw visual inputs. The proposed approach combines several previous techniques in a sound way as far as I understand. As demonstrated, it can distinguish two sets of trajectories in the TORCS environment.

** Weaknesses
It seems the proposed method requires the a priori knowledge of the number of latent modes. How would the performance change if this number were incorrectly chosen?

The clarity of the paper could be improved. Notably, it would help the reader if the overall objective function of the proposed were stated once. Otherwise it is a bit too implicit in my opinion.
Besides, I think more emphasis should be put on the fact that the method needs auxiliary information (velocity, previous two actions and car damage). Currently I don’t think the method is fully end-to-end as claimed in the conclusion.
Moreover I feel the abstract overstates the achievement of the proposed method. I wouldn’t  qualify what the method achieves in the two tasks, turn and pass, as producing different driving styles.

Other comments:
- The authors should recall the definition of the notation E_{\pi_\theta}[f(s, a)] for a given function f.
- In (2), Q -> Q_\psi
- l.191: D -> D_\omega
- l.232: output that indicates
- l.317: it -> It

","The overall problem is interesting, showcasing combination of several recently proposed methods (Wasserstein GAN, InfoGAIL etc). However, only two latent states considered in the experiment. Given that the experiment was only conducted in the driving domain, I'm not certain if the method would apply as well in other domains with more complicated latent structure "
A Regularized Framework for Sparse and Structured Neural Attention,"Vlad Niculae, Mathieu Blondel",https://proceedings.neurips.cc/paper/2017/hash/2d1b2a5ff364606ff041650887723470-Abstract.html,"The authors investigate different mechanisms for attention in neural networks. Classical techniques are based on the softmax function where all elements in the input always make at least a small contribution to the decision. To this end, sparse-max [29] was recently shown to work well. It focuses on a subset of the domain, treating the remaining part as `not contributing’. Here, the authors generalize this concept by applying a general smoothing technique which is based on conjugate functions of the max operator. The authors then show how to compute the required derivatives for training and how to employ structured regularization variants.

The approach is validated on textual entailment (Stanford Natural Language Inference dataset), machine translation and sentence summarization (DUC 2004 and Gigaword dataset).

The proposed regularization terms are explored on those tasks and advantages are demonstrated compared to classical softmax regularization.

Strength:
The paper investigates interesting techniques and convincingly evaluates them on a variety of tasks. 

Weaknesses:
The authors argue that the proposed techniques `often lead to more interpretable attention alignments’. I wonder whether this is quantifiable? Using computer vision based models, attention mechanisms don’t necessarily correlate well with human attention. Can the authors comment on those effects for NLP?

Although mentioned in the paper, the authors don’t discuss the computational implications of the more complex regularization mechanisms explicitly. I think a dedicated section could provide additional insights for a reader.","This paper presents a number of sparsifying alternatives to the softmax operator that can be straightforwardly integrated in the backpropagation of neural networks. These techniques can be used in general to approximate categorical inference in neural networks, and in particular to implement attention. The paper extends work such as sparsemax from its reference [29]. 

I believe this is a strong paper with significant contributions and thorough justifications. The paper presents:

1. The gradient of a general regularization of the max operator (Section 2.2).

2. Softmax and sparsemax as examples (2.3).

3. An original derivation of the Jacobian of any differentiable regularizer, and an example with the squared p-norm (3.1).

4. Two examples with structured regularizers, with algorithms for their computation and an original derivation of their Jacobians (3.2). One of them (fusedmax) is a novel use of the fused Lasso [39] as an attention mechanism.

One may find a limitation of this paper in the fact that the experiments are carried out using networks that are not anymore the state of the art. As such, the paper does not claim any improvements over state-of-the-art accuracies. However, in my opinion this is not important since the aim is to replace the softmax operator in a variety of plausible architectures.

The related work section is very clear and current, including a reference ([21]) on structured attention from ICLR 2017 that had immediately come to my mind. The structure discussed in this reference is more general; however, the contributions are very different. The paper also contains rich and descriptive supplemental material.","Summary
=======

  This paper presents a framework for implementing different sparse
  attention mechanisms by regularizing the max operator using convex
  functions. As a result, softmax and sparsemax are derived as special
  cases of this framework. Furthermore, two new sparse attention
  mechanisms are introduced that allow the model to learn to pay the
  same attention to contiguous spans. My concerns are regarding to the
  motivation of interpretability, as well as the baseline attention
  models. However, the paper is very well presented and the framework is
  a notable contribution that I believe will be useful for researchers
  working with attention mechanisms.


Strengths
=========

  - The provided framework encompasses softmax and sparsemax as special
    cases.
  - It is a drop-in replacement for softmax, so it can be readily used
    for extending a variety of existing neural networks that employ a
    softmax attention mechanism.
  - There seems to only be a small computational overhead (~25% increase
    in runtime).
  - While the attention weights produced by the sparse attention models
    are slightly more interpretable, using such sparse attention didn't
    hurt performance on RTE, MT and sentence summarization tasks.


Weaknesses
==========

  - Looking at Fig 1 and playing Devil's advocate, I would argue that
    interpretability doesn't really seem to improve that much with the
    variants of sparse attention presented here -- I could just
    threshold the weights of a normal softmax attention mechanism (or
    increase the contrast in Fig 1) and I would get a similar outcome. I
    do see however that with span-based sparse attention things start to
    look more interpretable but I have the impression that the three
    tasks are not the best fit for it. For instance, Q&A on SQuAD might
    have been a more interesting task as it asks for a span in a
    paragraph as the answer for a given question. This might have
    alleviated another weakness of the evaluation, namely that we don't
    see strong improvements when using span-based sparse attention. So
    right now the main takeaway is that we can obtain similar
    performance with slightly improved interpretability -- where I am
    still unsure how practically relevant the latter is.
  - Related to the point above, I do believe it is important to point
    out that interpretability of attention weights is problematic as the
    attentive RTE and MT (and maybe the summarization model too?) are
    not solely relying on the context vector that is obtained from the
    attention mechanism. Hence, any conclusions drawn from looking at
    the attention weights should be taken with a big grain of salt.
  - Moreover, the RTE attention baseline seems weak (though I understand
    that model is inherited from sparsemax). [35] report a test set
    accuracy of 83.5% whereas the baseline attentive RTE model here only
    achieves 81.66%, and the best performance for fusedmax reported here
    is 82.41%. In fact, what I would have liked to see is a comparison
    of a state-of-the-art attentive RTE/MT model with a drop-in
    replacement of fusedmax/oscarmax and an improvement in
    accuracy/BLEU.


Minor Comments
==============

  - I think it would be great if the limitations of this framework could
    be highlighted a bit more. For instance, I am wondering whether your
    framework could also support more global structured penalties such
    as the prior in MT that most of the attention should be along the
    diagonal and my hunch is that this would be difficult. See Cohn,
    Trevor, et al. ""Incorporating structural alignment biases into an
    attentional neural translation model."" arXiv preprint
    arXiv:1601.01085 (2016).
  - I believe in section 2 it could be made clearer when gradients are
    calculated by a solver and when not.
"
Style Transfer from Non-Parallel Text by Cross-Alignment,"Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola",https://proceedings.neurips.cc/paper/2017/hash/2d2c8394e31101a261abf1784302bf75-Abstract.html,"This paper develops a method for performing style transfer between two linguistic domains using only un-paired examples. The authors separate out style and content within a probabilistic adversarial auto-encoder framework, implemented as RNN encoders/generators with feed-forward (convolutional) discriminators. Evaluations on sentiment reversal, word substition decipherment and word order recovery experiments show that the methods outperform simple baselines as well as a stronger VAE baseline, but still fall short of training on actual parallel text (as is to be expected).

This is a good paper. The modifications are well-motivated and the model exposition is very clearly presented. The experimental results seem believable and show that the methods clearly outperform variational auto-encoders. Overall I think this work represents a good step in the right direction for linguistic style transfer research.

Nit: Choose one spelling for {cypher, cipher}.","This paper proposes a method for style transfer between natural language sentences that does not require any actual examples of style-to-style translation, only unmatched sets of examples in each style. This builds on the high-level idea of the Zhu et al. CycleGAN, but is technically quite different to accommodate the different requirements of discrete sequence generation. The paper is clear, includes some clever applications of an adversarial objective, and I see no serious technical issues. The resulting style-transferred sentences are short and a bit odd, but the task is ambitious enough that even this strikes me as a substantial accomplishment, and the analytic results suggest that this approach to modeling is basically on the right track. 

The paper's comparison with the Hu et al. Controllable Text Generation paper, which attempts to solve roughly the same problem, is a bit weak. The two papers are chronologically close enough that I don't think this should be grounds for rejection, but it could be done better. The VAE baseline in this paper is never fully described, so it's hard to tell how well it can be used as a proxy to understand how well the Hu et al. model would do. If that baseline is not closely comparable to Hu et al.'s model, it would be helpful to include their as a baseline.

26-34: The initial description of the model here took me a while to understand, and didn't completely make sense until I'd seen more of the paper. I suspect a visual depiction of the model at this point could help quite a bit.

Figure 1 could be clearer. The lines without arrowheads are a bit confusing.","This paper presents a method for learning style transfer models based on non-parallel corpora. The premise of the work is that it is possible to disentangle the style from the content and that when there are two different corpora on the same content but in distinctly different styles, then it is possible to induce the content and the style components. While part of me is somewhat skeptical whether it is truly possible to separate out the style from the content of natural language text, and that I tend to think sentiment and word-reordering presented in this work as applications correspond more to the content of an article than the style, I do believe that this paper presents a very creative and interesting exploration that makes both theoretical and empirical contributions. 

— (relatively minor) questions:

- In the aligned auto-encoder setting, the discriminator is based on a simple feedforward NN, while in the cross-aligned auto-encoder setting, the discriminators are based on convolution NNs. I imagine ConvNets make stronger discriminators, thus it’d be helpful if the paper can shed lights on how much the quality of the discriminators influence the overall performance of the generators.

- Some details on the implementation and experiments are missing, which might matter for reproducibility. For example, what kind of RNNs are used for the encoder and the generator? What kind of convolution networks are used for the discriminators? What are the vocabulary sizes (and the corresponding UNK scheme) for different datasets? The description for the word reordering dataset is missing. If the authors will share their code and the dataset, it’d help resolving most these questions, but still it’d be good to have some of these detailed specified in the final manuscript."
Unsupervised Learning of Disentangled Representations from Video,"Emily L. Denton, vighnesh Birodkar",https://proceedings.neurips.cc/paper/2017/hash/2d2ca7eedf739ef4c3800713ec482e1a-Abstract.html,"This paper presents a neural network architecture and video-based objective function formulation for the disentanglement of pose and content features in each frame. The proposed neural network consists of encoder CNNs and a decoder CNN. The encoder CNNs are trained to decompose frame inputs into contents features (i.e. background, colors of present structures, etc), and pose features (i.e. current configuration of the structures in the video). A decoder CNN is then used to combine the content features of one frame and pose features of a different frame to reconstruct the frame that corresponds to the pose features. This paper presents a new loss for the disentanglement of pose features from input images using a discriminator network (similar to adversarial loss). They train a discriminator network find similarities between features from the pose encoder of images from the same video but different time steps. The pose encoder is then trained to find features that the discriminator cannot distinguish as being similar. At convergence, the pose encoder features will only contain difference between the frames which should reflect the pose change of the person in time. After the encoder-decoder CNN has been trained to identify the content and pose features, an LSTM is trained to take the content features of the last frame and previous pose features to predict the next pose features. The disentangled content and pose features result in high quality video prediction. The experimental results backup the advantage of this method in terms of feature quality for classification tasks, and shows quantitative and qualitative performance boost over the state-of-the-art in video prediction.

Pros:
Novel video-based loss functions for feature disentanglement.
Excellent results on video prediction using the learned pose and content features.

Cons:
Although this paper claims to be an unsupervised method, the proposed objective function actually needs some weak supervision that different videos should have different content, e.g. object instances; otherwise the second term in equation 3 could be negatively affected (classifying images from different videos as different when they may look the same).
The pose LSTM is trained separately from the encoder-decoder CNN. Can the authors comment on why joint end-to-end training was not performed? In fact, similar networks for end-to-end video prediction have been experimented in Oh et al, NIPS 2015. 
Some typos in equations and text.
Should equation 3 be L_{adversarial}(C) instead of L_{adversarial}(D)?
h^t_c in LSTM input could be denoted as h^n where n is the last observed frame index, t makes it seem as the current step in generation.
Lines 204 and 219 point to Fig. 4.3 which is not in the main paper.
Figure 4 should have matching test sequences since it expect us to compare the “left”, “middle”, and “right” results.
Action classification experiments are a little confusing:
It is not clear what features are used to determine action class and how they are used (i.e. concatenated features? Single features and average for each video? Classification on clips?). The paper states: “In either case, we train a two layer classifier network S on top of either hc or hp, with its output predicting the class label y.”
In KTH, the content is not a good indicator about the action that is happening in the video since the same people perform all actions in similar backgrounds. Thus it’s not clear how the content features can be useful for action classification.
Classification results of network trained without pose disentanglement loss can make classification results stronger if previous issues are clarified.
Pose estimation results using the disentangled pose features are strongly recommended that will significantly strengthen the paper.
For the SUNCG dataset (3D object renderings), comparison with the weakly-supervised pose disentangling work by Yang et al, NIPS 2015 is suggested to highlight the performance of the proposed method.
The use of the first term in L_{adversarial}(E_p) is not clear to me, it seems that the first term and second term in this loss are pulling the features in completely opposite directions which, to my understanding, is helping D instead of confusing it. Isn’t confusing D the whole purpose of  L_{adversarial}(E_p) to confuse D?
Title can be more specific to the contribution (adversarial-like loss for content-pose feature disentangling)
","This paper proposes a new video prediction model called DrNet that achieves very compelling long-term video prediction (on the order of hundreds of frames) by learning a disentangled feature space using both pixel prediction and adversarial losses on the latent space.

The proposed model uses a similar convolutional LSTM setup with content and pose factors as in previous video prediction models. The novel aspect is the introduction of a scene discriminator and a pose encoder network. The scene discriminator is trained to distinguish whether two pose encodings come from the same video or from different videos. The pose encoder network is trained to produce encoding pairs from the same video that maximize the *entropy* of the scene discriminator.

In addition to the adversarial loss, there are reconstruction and similarity losses that exploit notions of temporal slowness in the content features. The model can do forward prediction in latent space simply by fixing the content features and predicting the next pose features conditioned on the fixed content and previous pose features.

Questions/comments:
- In figure 2, from steps t+2 onward it looks like two h_c vectors are fed into the LSTM instead of an h_c and an h_p. Is this intentional? Based on the model description it seems that predicted h_p should be fed from one state to the next, not h_c.
- You may also want to cite “Deep Visual Analogy Making” from NIPS 2015, which uses a factorization of the latent representation into style and content to improve the time horizon of next-frame prediction.

The experimental results demonstrate compelling disentangling and impressive long term video prediction across a wide variety of datasets: bouncing digits, NORB, 3D chairs, and KTH videos. ","This paper propose a model and a loss function that combined together is able to disentangle video frame representations to content and pose. And they shows impressive results in long sequence generation and content and action classification. 

Some detail comments:
-The generated samples looks very sharp, and I am very interested if there were more detailed analyses as what is the cause? Would it be possible to generate samples with wrong pose that still look as sharp?
-Table 1: what happens if the concatenated (h_p, h_c) is used for classification?
-E3: Shouldn’t it be L(C) ?
-E5: Shouldn’t it be L(E_p) and not L(E_c)?
-L:219 I’m a bit confused where overfitting is being discussed and referring to Fig 4.3, isn’t the Fig4.3 from Mathieu et al ?

Quality: Except some minor probably typos, the paper seems to be technically sound. And has good range of experiments analyzing different aspect of the learned representation.

Clarity: It’s well organized, with good review of recent work, model description and results. Minor comment: due to space limit the final part of result section looks a bit cramped, the paper clarity could be improved if larger portion is dedicated to results and analysis section.

Originality: The core model and it’s component are simple but  the way they have posed the problem and combined those components and the loss function combination is novel. 

Significance: I think it’s a simple model with very impressive results. Many of the current video generation models degenerate after only few time steps where as this model has sharp samples for longer sequence and could be a work that other people working on video generation could build upon it."
Countering Feedback Delays in Multi-Agent Learning,"Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter W. Glynn, Claire Tomlin",https://proceedings.neurips.cc/paper/2017/hash/2e0aca891f2a8aedf265edf533a6d9a8-Abstract.html,"If we accept that distributed learning is interesting, then this article presents a nice treatment of distributed mirror descent in which feedback may be asynchronous and delayed. Indeed, we are presented with a provably convergent learning algorithm for continuous action sets (in classes of games) even when individual players' feedback are received with differing levels of delay; further more the regret at time T is controlled as a function of the total delay to time T.  This is a strong result, achieved by using a suite of very current proof techniques - lambda-Fenchel couplings serving as primula-dual Bregman divergences and associated tools.

I have some concerns, but overall I think this is a good paper.
- (very minor) In the first para of Section 2.2, ""following learning scheme"" actually refers to Algorithm 1, over the page.
- Lemma 3.2. If the concept of variational stability implies that all Nash equilibria of a game are in a closed and convex set, to me this is a major restriction on the class of games for which the result is relevant. Two example classes are given in the supplementary material, but I think the main paper should be more upfront about what kinds of games we expect the results to hold in.
- At the bottom of page 5, we are presented with an assumption (buried in the middle of a paragraph) which is in some sense the converse to Lemma 4.2. While the paper claims the assumption is very weak, I would still prefer it to be made more explicitly, and some more efforts made to explain why it's weak, and what might make it fail
- In Algorithm 2, the division by |G^t_i| is very natural. Why is something similar not done in Algorithm 1?
- (Minor) Why do we talk about ""Last iterate convergence""?  This is a term I'm unfamiliar with. I'm more used to ""convergence of intended play"" or ""convergence of actual play"".
- References 15 and 16 are, I think, repeats?
- You should probably be referencing recent works e.g. [Mertikopoulos] and [Bervoets, S., Bravo, M., and Faure, M]","The authors show that in continuous games that satisfy a global variational stability property, Online Mirror Descent converges pointwise to the equilibrium set, even under delayed feedback (as long as it is synchronous across players). They also give a variant of OMD that converges even under asynchronous feedback. 

I think the authors present interesting results, in particular their delayed feedback results are quite interesting and their Fenchel based Lyapunov potential is an interesting technique. For these reasons it deserves acceptance. 

On the negative side I do feel that games that satisfy global variational stability are fairly limited and as expected, the authors fail to portray many example application (only convex potential games and linear Cournot games, which are classes that are sort of ""easy"" in terms of convergence of decentralized dynamics). So this limits the scope of these results. ","The paper considers a repeated multi-player game with convex compact action sets. Each player is assumed to play an Online Mirror Descent policy. The main question is the convergence of this global discrete-time dynamic to an equilibrium. A new notion of stability (for equilibria), called lambda-variational stability, is introduced. It is proved that if the set of Nash equilibria of the game is variationaly stable, then the dynamic converges to the latter set. An additional sophistication is considered: the feedback of each player may be delayed.

The main issue that bothers me is the motivation: on p2, ""it is natural to assume that players adopt some variant of online mirror descent when faced with such online decision process"". Why not some other regret minimizing policy or some random policy ? The only motivation I can think of, for the study of the convergence of such discrete-time game dynamic to equilibra, is the computation of an equilibirum (similarly to optimization problems where the goal is the computation of some minimizer). But then, the derivation of some explicit rate of convergence would be desirable, as well as a discussion about the computational efficiency of the procedure.

Besides, the main point of the paper seems to be the notion of lambda-variational stability and the convergence analysis. The introduction of delayed feedbacks feels secondary and a bit distracting.

Another issue is the great similarity of the notion of variational stability and the tools used in the convergence analysis (like Fenchel coupling) with e.g. the paper P. Mertikopoulos, Learning in concave games with impecfect information, arxiv, 2016. A precise comparison with (and of course a reference to) the previous paper should have been included in order for the reader to measure the real contribution of the present paper."
Affinity Clustering: Hierarchical Clustering at Scale,"Mohammadhossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, Vahab Mirrokni",https://proceedings.neurips.cc/paper/2017/hash/2e1b24a664f5e9c18f407b2f9c73e821-Abstract.html,"The paper focuses on the development of the field of distributed hierarchical clustering. The authors propose a novel class of algorithms tagged 'affinity clustering' that operate on the basis of Boruvka's seminal work on minimal spanning trees and contrast those to linkage clustering algorithms (which are based on Kruskal's work). 

The authors systematically introduce the theoretical underpinnings of affinity clustering, before proposing 'certificates' as a metric to characterise clustering algorithm solutions more generally by assessing the clustered edge weights (cost). 

Following the theoretical analysis and operationalisation of MapReduce variants of affinity clustering for distributed operation, the quality is assessed empirically using standard datasets with variants of linkage- and affinity-based algorithms, as well as k-means. In addition to the Rand index (as metric for clustering accuracy) the quality of algorithms is assessed based on the ratio of the detected clusters (with balanced cluster sizes considered favourable). 

Affinity-based algorithms emerge favourably for nearly all datasets, with in parts significant improvements (with the flat clustering algorithm k-means as closest contestant).

Finally, the scalability of affinity clustering is assessed using private and public corpi, with near-linear scalability for the best performing case.

Overall, the paper proposes a wide range of concepts that extend the field of hierarchical clustering (affinity-based clustering, certificates as QA metric). As far as I could retrace, the contributions are systematically developed and analysed, which warrants visibility.

One (minor) comment includes the experimental evaluation. In cases where the affinity-based algorithms did not perform as well (an example is the cluster size ratio for the Digits dataset), it would have been great to elaborate (or even hypothesise) why this is the case. This would potentially give the reader a better understanding of the applicability of the algorithm and potential limitations.

Minor typos/omissions:

- Page 8, Table 1: ""Numbered for ImageGraph are approximate.""

- Page 8, Subsection Scalability: Missing reference for public graphs.  

","The paper provides a demonstration of MapReduce based hierarchical clusterings. The basic premise is that the edge-removal based algorithms (namely, the heaviest edge in a cycle cannot be in an MST) can be parallelized in MapReduce, specially if we can use certificates (of primal-dual optimality) to compare across different graphs. 

The experiment scales are corresponding improvements are noteworthy and that is the strong point of the paper.

The low point of the paper is the obfuscation/confoundedness in theory. One wishes that the paper said the summary that was laid out above - the two sentences likely convey more information that the authors wrote. First of all, the authors should refer to disc paintings and certificates by their original notions such as region growing and primal-dual algorithms. It may be the case that [9,24] managed to get papers published without mentioning that these ideas were invented elsewhere, but the authors should realize that getting a paper accepted does not avoid the issue of being laughed at by posterity. And in case the authors do not actually know what is going on (a low probability event) in MST computation and certificates, please read

Michel X. Goemans, David P. Williamson:
A General Approximation Technique for Constrained Forest Problems. SIAM J. Comput. 24(2): 296-317 (1995)

Incidentally, Definition 2 should not use the word ""Steiner"". Steiner trees are not defined in the manuscript, and nor are necessary. 


"
Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,"Federico Monti, Michael Bronstein, Xavier Bresson",https://proceedings.neurips.cc/paper/2017/hash/2eace51d8f796d04991c831a07059758-Abstract.html,"This paper generalizes a recent NIPS 2016 paper [10] by allowing convolutional neural network (CNN) models to work on multiple graphs.  It extracts local stationary patterns from signals defined on the graphs simultaneously.  In particular, it is applied to recommender systems by considering the graphs defined between users and between items.  The multi-graph CNN model is followed by a recurrent neural network (RNN) with long short-term memory (LSTM) cells to complete the score matrix.

Strengths of the paper:
* The proposed deep learning architecture is novel for solving the matrix completion problem in recommender systems with the relationships between users and the relationships between items represented as two graphs.
* The proposed method has relatively low computational complexity.
* The proposed method has good performance in terms of accuracy.

Weaknesses of the paper:
* The multi-graph CNN part of the proposed method is explained well, but the RNN part leaves open many questions.  Apart from illustrating the process using an example in Figure 3, there is not much discussion to explain why it can be regarded as a matrix diffusion process.  Does the process correspond to optimizing with respect to some criterion?  What is the effect of changing the number of diffusion steps on the completed matrix and how is the number determined?  This needs more detailed elaboration.

In addition, to demonstrate that incorporating the pairwise relationships between users or items helps, you may add some degenerate versions of RMGCNN or sRMGCNN by removing one or both graphs while keeping the rest of the architecture unchanged (including the RNN part).

Minor comments (just some examples of the language errors):
#18: “Two major approach”
#32: “makes well-defined”
#66: “Long-Short Term Memory”
#76: “turns out an NP-hard combinatorial problem”
#83-84: “to constraint the space of solutions”
#104-105: “this representation … often assumes”
#159: “in details”
Table 1 & Table 2: strictly speaking you should show the number of parameters and complexity using the big-O notation.

The reference list contains many arXiv papers.  For those papers that have already been accepted, the proper sources should be listed instead.
","This paper studies matrix completion problem when side information about users and items is given that can be exploited in recommendation. Unlike existing methods such as induced matrix completion or geometric matrix completion that tries to explicitly exploit the auxiliary information in factorization methods to reduce the sample complexity to mitigate for data sparsity, this paper introduces a multi-graph convolutional neural network to complete the partially observed matrix. The authors conducted through experiments  on synthetic and real datasets and compared the proposed algorithm to existing methods that demonstrates the effectiveness of proposed method.  

The presentation of the paper was mostly clear. The claimed contributions are discussed in the light of existing results and the paper does survey related work appropriately. Regarding the quality of the writing,  the paper is reasonably well written, the structure and language are good. 

Overall, on the positive side, the paper has nice results, proposes an interesting algorithm, and experiments are thorough and compelling. But the paper lacks enough novelty, and in my opinion the main contribution of this paper is the generalization of recently proposed convolutional neural networks on a single graph to multiple graphs, not particularly interesting. 


","In short

--- Paper ---
The paper introduces a Neural Network (NN) architecture which targets matrix completion with graph-based constraints on rows and columns of the matrix. The proposed architecture builds upon graph convolutional NN (which are extended to multi-graph setting) and recurrent NN. The efficiency of that architecture is demonstrated on standard data-sets.


--- Review ---
The paper is clear and complete, and the proposed architecture is interesting and efficient on the considered datasets."
Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification,"Jinseok Nam, Eneldo Loza Mencía, Hyunwoo J. Kim, Johannes Fürnkranz",https://proceedings.neurips.cc/paper/2017/hash/2eb5657d37f474e4c4cf01e4882b8962-Abstract.html,"I have been reviewing this paper for another conference, so in my review I mostly repeat my comments sent earlier. Already at that time I was for accepting the paper. It is worth to underline that the paper has been further improved by the authors since then.

The paper considers a problem of solving multi-label classification (MLC) with recurrent neural networks (RNNs). MLC is converted into a sequential prediction problem: the model predicts a sequence of relevant labels. The prediction is made in a sequential manner taking into account all previous labels. This approach is similar to classifier chains, with two key differences: firstly, only single model is used, and its parameters are shared during prediction; secondly, RNN predicts ""positive"" labels only. In this way, the algorithm can be much more efficient than the standard classifier chains. The authors discuss also the problem of the right ordering of labels to be used during training and testing.

The paper is definitely very investing. The idea of reducing MLC to sequential prediction problem is a natural extension of chaining commonly used for solving MLC under 0/1 loss. I suppose, however, that some similar ideas have been already considered. The authors could, for example, discuss a link between their approach and the learn-to-search paradigm used, for example, in ""HC-Search for Multi-Label Prediction: An Empirical Study"". I suppose that the algorithm presented in that paper could also be used for predicting sequences of ""positive"" labels.

Minor comments:
- The authors write that ""Our first observation is that PCCs are no longer competitive to the sequence prediction approaches w.r.t. finding the mode of the joint distribution (ACC and maF1)"". It seems however that PCC performs better than RNN under the ACC measure. 
- FastXML can be efficiently tuned for macro-F (see, ""Extreme F-measure Maximization using Sparse Probability Estimates""). It is not clear, however, how much this tuning will improve the result on BioASQ.

After rebuttal:

I thank the authors for their response.
","This paper presents a novel approach to learning the joint probability of labels in multi-label classification problems. The authors introduce a formulation where the joint probability is computed over a sequence of positive labels. The novel approach is analyzed empirically on three benchmark datasets. 

As claimed by the authors, the novel approach has a number of advantages compared to existing methods such as PCCs. One important advantage is that the length of the chain decreases by considering positive labels only, resulting in a more compact model that suffers less from error propagation. In addition, the use of  seq-to-seq neural nets instead of more traditional base learners in PCC can also result in performance gains in areas where deep learning is typically successful (images, text, speech, etc.). 

In general I find the method proposed in this paper interesting. However, I do see a few possibilities for further improving the paper:

-	In the beginning of the paper the authors indicate that they are interested in minimizing the subset zero-one loss. This might be a natural choice, given that a joint probability distribution over labels is constructed. However, in the experiments, the authors start to focus on other loss functions for which the proposed framework is not suited. It is clear that a BR-style method should be preferred for Hamming loss, whereas more complicated approaches are needed to optimize the different variants of the F-measure. The analyzed methods, neither the PCC baseline, nor the method of the authors, optimize the F-measure. More complicated algorithms are needed for that, see e.g. Waegeman et al. On the Bayes optimality of F-measure maximizers, JMLR 2014. 

-	In light of this discussion, it is also a bit strange to optimize subset zero-one loss in MLC applications with hundreds of labels. Subset zero-one loss is for sure not the measure of interest in extreme MLC applications.    
"
f-GANs in an Information Geometric Nutshell,"Richard Nock, Zac Cranko, Aditya K. Menon, Lizhen Qu, Robert C. Williamson",https://proceedings.neurips.cc/paper/2017/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html,"This paper considers the f-GAN principle generalized to f-divergences. Generalizing the connection between the KL divergence and regular exponential families by using the chi-logarithm, the authors prove the variational information geometric f-GAN (vig-f-GAN) identity, which provides an interpretation of the objective function of the f-GAN. The authors also prove a sufficient condition on activation functions to obtain factored generative distributions. 

The main theoretical result, vig-f-GAN identity is interesting and its applicability is promising. However, the meanings of main theorems are hard to follow since they are written in quite general forms.

Section 6: It is hard to understand how the observations in experiments are related to theoretical results. 

p.7, l.258, ``For eq.(7), is means when C …’’: This clause has a wrong structure. 

","Thank you for an interesting read.

This paper proposed an information geometric (IG) view of the f-GAN algorithm. It first showed that f-GAN converges in parameter space using the 1-1 mapping of f-divergence and chi-divergence and a Bregman divergence result. Then it also discussed a proper way to implement f-GAN. Finally in the main text it provided a factorisation result of the deep neural network representation and discussed the choice of activation functions which has 1-1 mapping to the chi (or f) function. I didn't check every detail in the appendix but it seems to me that the proofs (except for Thm. 8 which I don't have time to read before due) are correct.

I think this paper is very dense and contains many new results that could be of interest for machine learning and information theory. Especially I'm impressed to see the exposition of Thm 4 which tells the IG part of the story for f-GAN. So I think this paper is a clear accept. 

However, I do think this paper could be very difficult to understand for those who don't know too much about the connections between IT and IG (at least the simplest one that KL for an exponential family can be mapped to Bregman divergence, and the Fenchel duality stuff). Many deep learning engineers could be this kind of person and also they're the main audience for a GAN paper. So the following are my suggestions that could potentially make the paper clearer:

1. I like Fig 1 in the appendix which states the connections between IT and IG in f-GAN context. Might consider moving it to the main text.

2. Might be helpful to make a clear statement why considering the IG view could be helpful at the beginning. My understanding is that you can use the geometry in the parameter space to discuss the behaviour of optimisation. You mentioned that briefly in page 7 which I found is a bit too late. 

3. I'm not sure if I understand how section 5 connects to f-GAN. It seems to me that you just used the deformed exponential family to explain the distribution a deep neural net can represent, thus not a consequence of f-GAN optimisation results. Yes I buy the point that v, f, and chi have 1-1 correspondence, but then you didn't say anything about how this result could help design the f-GAN game, e.g. which f divergence we should pick, or given an activation function, which f-GAN objective works the best in terms of say convergence.

4. Why for Thm 6 the phi_l can be viewed as ""deep sufficient statistics""? I don't think eq. (13) is of the form of a deformed exponential family?

5. As said, might be helpful to consider moving line 257-269 to other places. Also it seems to me that the utility theory part is not directly related to the IG view, so might be good to delete that paragraph (you can keep it in the appendix) and free some spaces to explain your main results.

6. I feel the experiments are not directly related to the main points claimed in the paper. For example, you can discuss (A) by only having the results from section 5, i.e. I don't really need to know the IG view of f-GAN to apply these new activation functions. Also for (B) I only need to understand section 4, which is not that closely related to the IG view of f-GAN. Especially your results of WGAN could be distractive and confusing, since this paper is mainly about f-GANs, and I actually spent some time to find the sentence (line 227-228) about WGAN.

In summary, while this paper provides many useful results and dense derivations, I have a feeling that the material is not organised in a crystal clear way. Instead, it looks like squeezing results from multiple papers to an 8-page NIPS submission. So while I am supportive for acceptance, I do think this paper needs editing to make the claims clearer and more coherent. 
","The authors identify several interesting GAN-related questions, such as to what extend solving the GAN problem implies convergence in parameter space, what the generator is actually fitting when convergence occurs and (perhaps most relevant from a network architectural point of view) how to choose the output activation function of the discriminator so as to ensure proper compositeness of the loss function.
The authors set out to address these questions within the (information theoretic) framework of deformed exponential distributions, from which they derive among other things the following theoretical results:
They present a variational generalization (amenable to f-GAN formulation) of a known theorem that relates an f-divergence between distributions to a corresponding Bregman divergence between the parameters of such distributions. As such, this theorem provides an interesting connection between the information-theoretic view point of measuring dissimilarity between probability distributions and the information-geometric perspective of measuring dissimilarity between the corresponding parameters. They show that under a reversibility assumption, deep generative networks factor as so called escorts of deformed exponential distributions. Their theoretical investigation furthermore suggests that a careful choice of the hidden activation functions of the generator as well as a proper selection of the output activation function of the discriminator could potentially help to further improve GANs. They also briefly mention an alternative interpretation of the GAN game in the context of expected utility theory.

Overall, I find that the authors present some interesting theoretical results, however, I do have several concerns regarding the practical relevance and usefulness of their results in the present form.

Questions & concerns:

To begin with, many of their theorems and resulting insights hold for the specific case that the data and model distributions P and Q are members of the deformed exponential family. Can the authors justify this assumption, i.e. elaborate on whether it is met in practice and explain whether similar results (such as Theorems 4 & 6) could be derived without this assumption?

One of the author’s main contributions, the information-geometric f-GAN identity in Eq.(7), relates the well-known variational f-divergence formulation over distributions to an information-geometric optimization problem over parameters. Can the authors explain what we gain from this parameter-based point of view? Is it possible to implement GANs in terms of this parameter-based optimization and what would be the benefits? I would really have liked to see experimental results comparing the two approaches to optimization of GANs. Somewhat surprising, the authors don’t seem to make use of the right hand side of this identity, other than to assert that solving the GAN game implies convergence in parameter space (provided the residual J(Q) is small). And why is this implication not obvious? Can the authors give a realistic scenario where convergence in the variational f-divergence formulation over distributions does not imply convergence in parameter space?

As an interesting practical implication of their theoretical investigation, the authors show that the choice of the output activation function g_f (see section 2.4 in Ref.[34]) matters in order for the GAN loss to have the desirable proper compositeness property. I found this result particularly interesting and would certainly have liked to see how their theoretically derived g_f (as the composition of f′ and the link function) compares experimentally against the heuristic choice provided in Ref. [34].

Consequences for deep learning and experimental results: Under the assumption that the generator network is reversible, the authors show that there exists an activation function v such that the generator’s hidden layers factor exactly as escorts for the deformed exponential family. In Table 2 (left), the authors compare several generator architectures against different choices of activation functions (that may or may not comply with this escort factorization). The plot for the GAN MLP, for instance, indicates that the “theoretically superior” μ-ReLU performs actually worse than the baseline ReLU (limiting case μ → 1). From their analysis, I would however expect the invertible μ-ReLU (satisfying reversibility assumption) to perform better than the non-invertible baseline (not satisfying reversibility assumption). Can the authors explain why the μ-ReLU performs worse? Can they comment on whether the DCGAN architecture satisfies the reversibility assumption and how these results compare against the Wasserstein GAN which is not based on f-divergences (and for which I therefore do not expect their theoretical analysis to hold)? Finally, as far as I can tell, their results on whether one activation (Table 2 center) or link function (Table 2 right) performs better than the others are all within error bars. I don’t think this provides sufficient support for their theoretical results to be relevant in practice.

Minor concerns:

Many of their results are derived within the framework of deformed exponential densities. As the general ML community might not yet be familiar with this, I would welcome it if the authors could provide more intuition as to what a deformation (or signature) and an escort is for instance.

In Theorem 8, the authors derive several upper bounds for J(Q). How do these bounds compare against each other and what are the implications for which activation functions to use? They also mention that this upper bound on J(Q) is decreasing with the normalization parameter of the escort Z. Can they elaborate a bit more on why this is a good thing and how we can leverage this?

In light of these comments, I believe the insights gained from the theoretical analysis are not substantial enough from the point of view of a GAN practitioner.
"
Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples,"Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum",https://proceedings.neurips.cc/paper/2017/hash/2f37d10131f2a483a8dd005b3d14b0d9-Abstract.html,"This paper proposes a couple of alternatives to self paced learning and hard example mining: weighting or sampling data points based on a) the variance in the prediction probabilities across iterations or b) closeness of the prediction probability to the decision threshold. The paper was well written and easy to read.

I like the fact that the authors test their methods on a variety of datasets and models. It seems like different methods win in different experiments though. Is there some intuition on how to choose which method to use for a given task?  

Have you tried or do you have any intuitions about how your methods compare against Bayesian methods for computing predictive uncertainty? E.g. Dropout at test time as proposed by Gal and Ghahramani can be used (although in some limited models) to compute the variance of predictions and could be used as an alternate to variance computed from history.

Overall, this is a nice paper and the results are interesting, but not groundbreakingly so.
","
The paper proposes a novel way of sampling (or weighing) data-points during training of neural networks. The idea is, that one would like to sample data-point more often which could be potentially classified well but are hard to learn (in contrast to outliers or wrongly labeled ones). To `find' them the authors propose two (four if split into sampling and weighing) schemes: The first one (SGD-*PV) proposes to weigh data-points according to the variance of the predictive probability of the true label plus its confidence interval under the assumption that the prediction probability is Gaussian distributed. The second one (SGD-*TC), as far as I understand, encodes if the probability of choosing the correct label given past prediction probabilities is close to the decision threshold. 
The statistics needed (means and variances of p) can be computed on-the-fly during a burn-in phase of the optimizer; they can be obtained from a forward pass of the network which is computed anyways. The authors test their methods on various architectures and dataset and compare to a range of baselines.

The paper motivates the problem well and the introduction is clearly written (Figure 1 is very helpful, too). The technical sections 3.2 (up to the logReg example) and 3.3 are very dense;  moving table 1 (and or 2) to the appendix and elaborating a bit more (especially on 3.3) would be very helpful.

Major Questions:
- I do not understand how SGD-*PV prefers uncertain example over hard ones, i.e. why should hard samples have lower predictive variance than uncertain ones? Hard ones might be labeled incorrectly, but the probability might still fluctuate.
- In 3.3 P(i|H, S_e, D) is largest around \bar{p}=0.5. Does that mean the method implicitly assumes 2 output classes only? If not, I would like to know how P encodes closeness to a threshold for multiple classes.
- In 3.2 the authors assume that p is Gaussian distributed. Is this meant across occurrences in the mini-batch? I would also like the authors to comment a bit more on this assumption, also since clearly p is not Gaussian distributed (it ranges between 0 and 1). Would it be possible to impose this assumption on log(p) instead to get a better fit?
- how many samples do you typically collect for p? as many as epochs? is this the reason you need a burn in time? or is there another reason why it is good to first burn-in and then start the proposed sampling/weighing schemes?

Further comments:
- since the authors propose a practical algorithm it would be nice to include a sentence about memory requirement or other non-trivial implementation details (in case there are some). 
- As mentioned above, I encourage the authors to expand the technical section (3.2 before the logReg example and especially 3.3) since it is very dense.
- It is very nice that the authors compare against many baselines
- It is nice that the authors are transparent about how the learning rates etc. are chosen.

(*after rebuttal*: The rebuttal clarified most of my questions. If the paper gets accepted I encourage the authors to also clarity the points in the paper (to eases the reading process) as they promise in the rebuttal, especially i) the presentation above the logistic regression example in Section 3.2 (maybe a pointer that the assumptions will be discussed below). At that point they are quite ad-hoc and it is hard (and not beneficial for the flow of thoughts) to trace back later ii) closeness to threshold: recap shortly what \bar{p} was (it took me a while to find it again) iii) the meaning of the burn-in phase. iv) A sentence about implementation which points to the Appendix. In good faith that this will be done I increased the score.)","This paper mainly introduces two methods in the SGD framework to emphasize high variance samples in training. One is to measures the variance prediction probabilities and the other is to estimate the closeness between the prediction probabilities and the decision threshold.

I think the methods proposed by this paper is simple and clean. They also make sense intuitively. In the experiment sections, the authors did a solid analysis (in 10 datasets) to test the performance of the proposed methods. These give clear evidence that the proposed methods based on the prediction variance and the threshold closeness, despite mathematically quite simple, achieve consistently better results than reasonable baselines (SGD-SCAN, SGD-WD and SGD-WE).

They also consider the situation where the training dataset is more/less noisy and the results given by the experiments matches the intuition of the proposed algorithms."
SchNet: A continuous-filter convolutional neural network for modeling quantum interactions,"Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, Klaus-Robert Müller",https://proceedings.neurips.cc/paper/2017/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html,"Summary: 
In order to design new molecules, one needs to predict if the newly designed molecules could reach an equilibrium state. All possible configurations of atoms do not lead to such state. To evaluate that, the authors propose to learn predictors of such equilibrium from annotated datasets, using convolutional networks. Different to previous works, that might lead to inaccurate predictions from convolutions computed in a discretized space, the authors propose continuous filter convolution layers. They compare their approach with two previous ones on 2 datasets and also introduce a third dataset. Their model works better than the others especially when using more training data. 

I don't have a very strong background in physics so I'm not the best person to assess the quality of this work, but I found that the proposed solution made sense, the paper is clearly written and seems technically correct.  

Minor: don't forget to replace the XXX
Points 3,4 should not be listed as contributions in my opinion, they are results
l. 26 : the both the -> both the
Ref 14: remove first names 


","I have no expertise in DL applied to chemistry. I have seen other papers on that topic, but I cannot evaluate what is novel, what is trivial, and what is based on sound principles.

I get the impression from the paper that the authors have a good understanding of topic.

I would like to ask the authors for a clarification about what they do with their ""continuous convolution"". It seems to me that they are simply using a kind of radial basis function, with a finite number of locations. Then they use a matrix multiplication followed by element-wise multiplication. It feels more like good old interpolation in the space of locations with a finite number of basis elements.


Seems like a nice application, but I don't know much about the chemistry part.

Also, I think the authors should mention Residual Networks at some place because their design from Figure 2 really looks like it features Resnet blocks.
"," 
This paper proposes a new deep learning architecture for modeling the quantum structure of small molecules, including a particular type of layer with a continuous response in the location of the atom nuclei. The authors also combine a number of different ideas to address the particular challenges of this application: “dynamic filters,” graph-convolutions, and supervised targets for the gradient at the input layer. 
 
I think this is an interesting paper, both for its significance to chemical physics and its significance to the deep learning community. It combines a number of recently-proposed ideas into a single deep learning architecture.
 
The first two sections could be better organized. For a machine learning audience, it might be useful to clearly list the idiosyncrasies of this application and how they are being modeled in the proposed approach. In particular, the first two paragraphs of Section 2 reference other work without clearly explaining the relationship. 
"
GibbsNet: Iterative Adversarial Inference for Deep Graphical Models,"Alex M. Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron C. Courville, Yoshua Bengio",https://proceedings.neurips.cc/paper/2017/hash/30f8f6b940d1073d8b6a5eebc46dd6e5-Abstract.html,"This paper presents GibbsNet, a deep generative model formulated as transition operators.  The transition operators are learned in an adversarial way, similar to that of the adversarially learned inference (ALI).  However instead of using a fixed prior p(z), GibbsNet does not require the specification of a particular prior, but rather learn a prior implicitly.  Training is done by unrolling the sampling process multiple times and doing adversarial learning to match the sampling distribution to the one clamped from data and doing posterior only once.  When unrolling for only one step GibbsNet becomes equivalent to ALI.

I found the idea of having an implicit prior quite interesting, and making it implicit does allow much more sophisticated priors to be used.  Not requiring the prior to be Gaussian lifted some restrictions on the model and provided an opportunity to make the model more expressive.  Compared to other transition operator-based models, GibbsNet has the appealing property that the transition process passes through a latent layer therefore may enable faster mixing, however this aspect is not explored in this paper.

In terms of more technical details, I found the assumptions in Proposition 1 to be a little unrealistic, in particular assumption (a) will be hardly satisfied by any typical model we use.  Usually, in latent variable models our latent variables z live in a much lower dimensionality space than visible variables x, therefore the mapping from z to x will only cover a small subspace of x.  It is possible to make the probability of any x non-zero by using a noise distribution over x but purely relying on that is not going to be very effective.  However on the other side the z-to-z transitions should be able to cover all the possibilities and hence learning complicated q(z) distributions is possible.

This paper is mostly clear but has some writing issues, in particular almost none of the figures and therefore visualization results are directly referenced or discussed in the main text.  Since the paper is not that tight on space, some more discussion about the experiment results should be included.","
Summary-
This paper proposes a novel and interesting way of training and doing inference
in encoder-decoder style deep generative models. The transition operator,
parametrized by encoder and decoder neural nets, starts from latent variables
(z) that are noise, and produces a pair (x, z) by repeatedly applying itself.
This generator model is trained to fool a discriminator that is trying to
distinguish between generated samples, and (x, z) pairs computed on real data.
In other words, instead of using the model's own Langevin dynamics to run a
Markov Chain, a transition operator is trained to directly produce samples.
This helps avoid the problems inherent in trying to sample from directed deep
generative models (such as chains not mixing well), at the cost of not having a
proper energy function, or probability distribution associated with the learned
transition operator.

Strengths-
- The model is novel and interesting. It provides a way of training deep
generative models that are amenable to feed-forward inference, feed-forward
decoding, and can also generate samples in a few steps.
- The paper is well-written and easy to follow.
- The experiments are well-designed and help highlight the properties of
the learned joint distribution. Relevant baselines are compared to.

Weaknesses-
- No major weaknesses. It would be good to have a quantitative evaluation of the
generative performance, but that is understandably hard to do.


Minor commenpt and typos-
- ""For the transition operator to produce smooth mixing, it is necessary for
the latent variable q(z|x)."" : Something missing here ?
- In Fig 5, it might help compare results better if the same half-images were
used for both ALI and GibbsNet.

Overall
The proposed model is novel and has many desirable properties. It provides a
good example of using adversarial training to learn an interesting generative
model. This idea seems quite powerful and can in future be applied on more
challenging problems.","
        The authors proposed an extension over the Adverserially Learnt Inference(ALI) GAN that cycles between the latent and visible space for a few steps. The model iteratively refines the latent distribution by alternating the generator and approximate inference model in a chain computation. A joint distribution of both latent and visible data is then learnt by backpropagating through the iterative refinement process. The authors empirically demonstrated their model on a few imprint tasks. 

        Strength:
           - The paper is well-organized and is easy to follow.
           - The experimental results on semi-supervised learning are encouraging. (more on that see the comment below. )

        Weakness:
           - The main objection I have with the paper is that the authors did not put in any effort to quantitatively evaluate their newly proposed GAN training method. Comparing the inception score on CIFAR-10 with ALI and other benchmark GAN methods should be a must. The authors should also consider estimating the actual log likelihood of their GAN model by running the evaluation method proposed:""On the Quantitative Analysis of Decoder-Based Generative Models"", Wu et al. The bottom line is that without appropriate quantitative analysis, it is hard to evaluate how well the proposed method does in general. What should help is to see a plot where an x-axis is the number of Gibbs steps and y-axis as one of the quantitative measures. 
           - The improvement of the proposed method seems to be very marginal compared to ALI. The appropriate baseline comparison should be a deeper ALI model that has 2N number of layers. The ""Gibbs chain"" used throughout this paper is almost like a structured recurrent neural network with some of the intermediate hidden layers actually represents x and z. So, it is unfair to compare a 3-step GibbsNet with a 2-layer feedforward ALI model.

===================
After I have read the rebuttal from the author, I have increased my score to reflect the new experiments conducted by the authors. The inception score results and architecture comparisons have addressed my previous concerns on evaluation.  

I am still concerned regarding the experimental protocols. The exact experimental setup for the semi-supervised learning results was not explained in detail. I suspect the GibbsNet uses a very different experimental protocol for SVHN and MNIST than the original ALI paper. It is hard to evaluate the relative improvement over ALI if the protocols are totally different. It is necessary to include all the experimental details in a future revision for reproducibility."
Bayesian GAN,"Yunus Saatci, Andrew G. Wilson",https://proceedings.neurips.cc/paper/2017/hash/312351bff07989769097660a56395065-Abstract.html,"Summary:
The paper introduces a Bayesian type of GAN algorithms, where the generator G and discriminator D do not have any fixed initial set of weights that gets gradually optimised. Instead, the weights for G and for D get sampled from two distributions (one for each), and it is those distributions that get iteratively updated. Different weight realisations of G may thus generate images with different styles, corresponding to different modes in the dataset. This, and the regularisation effect of the priors on the weights, promotes diversity and alleviates the mode collapse issue. The many experiments conducted in the paper support these claims.

Quality, Originality, Clarity:
While most GAN papers today mainly focus on slight variations of the original net architecture or of the original GAN objective function, this paper really presents a new, original and very interesting algorithm that combines features of GANs with a strongly Bayesian viewpoint of Neural Nets (similar in spirit, for example, to the viewpoint conveyed by Radford Neal in his PhD thesis), where the weights are parameters that should be sampled or integrated out, like any other parameter in the Bayesian framework. This leads to a very elegant algorithm. The paper contains many good experiments that prove that the algorithm is highly effective in practice (see Fig 1 and Table 1), and the produced images (though not always state of the art it seems) indeed reflect a high diversity of the samples. Moreover, the paper is very clearly written, in very good english, with almost no typos, and the algorithm is well explained. Overall: a very good paper!

Other remarks:
- Am I right in understanding that at each \theta_g sampling step, you are actually not really sampling from p(\theta_g | \theta_d), but rather from something more like p(\theta_g | \theta_d) * p(theta_d | theta_g^old) (and similar for p(\theta_d | theta_g)) ? In that case, this should be stressed and explained more clearly. In particular, I got confused at first, because I did not understand how you could sample from p(\theta_g | \theta_d) if \theta_d was not fixed.
- Algorithm 1: the inner loop over M does not stick out clearly. The 'append \theta_g^k to sample set' should be inside this loop, but seems outside. Please ensure that both inner- and outer loop are clearly visible from first eye sight.
- l.189 & l.192: clash of notations between z (of dimension d, to generate the synthetic dataset) and z (of dimension 10, as random input to the generator G). Use different variable for the first z
- Figure 1: What is p_{ML-GAN} ? I guess that the 'ML' stays for 'Maximum-Likelihood' (like the maximum-likelihood gan mentioned on l.267 without further introduction.) and that the MLGAN is just the regular GAN. In that case, please call it simply p_{GAN}, or clearly introduce the term Maximum-Likelihood GAN and its abbreviation MLGAN (with a short justification of why it is a maximum likelihood, which would actually be an interesting remark).
- l.162-171: Describe the architectures more carefully. Anyone should be able to implement *exactly* your architecture to verify the experiments. So sentences like ('The networs we use are slightly different from [10] (e.g. we have 4 hidden layers and fewer filters per layer)', without saying exactly how many filters, is not ok. Also, the architecture for the synthetic dataset is described partly at l.162-163 and partly at l.191. Please group the description of the network in one place. The authors may consider describing roughly their architectures in the main part, and put a detailed description in the appendix.

Response to rebuttal:
We thank the authors for answering our questions. We are happy to hear that the code will be released (as there is usually no better way to actually reproduce the experiments with the same parameter settings). Please enclose the short explanations of the rebuttal on GANs being the Maximum Likelihood solution with improper priors into your final version.","This paper provides a Bayesian formulation for GAN. The BayesGAN marginalizes the posteriors over the weights of the generator and discriminator using SGHMC. The quantitive experiment result is state-of-art, although the sample quality is not impressive. 

The paper is well written and easy to follow. My concern is under relative low dimension output, the model seems is able to avoid model collapse. It is not clear for higher dimension output, whether Bayes GAN is still able to capture the distribution of data. 

Another issue is to my knowledge, SGHMC converges much slower than normal SGD with momentum. In senthetic dataset, BayesGAN converges faster, but it is not clear for complex input distribution, the BayesGAN is still keeping advantages in converge. 

I also suggest to change the legends of figures, because “ML GAN” is quite misleading. 
","Overview


This work proposes a fully Bayesian approach to Generative Adversarial Nets (GANs) by defining priors over network weights (both the discriminator and the generator) and uses stochastic Gradient Hamiltonian Monte Carlo to sample from their posteriors. A fully Bayesian approach could allow for more extensive exploration of the state space and potentially avoid the mode collapse problem afflicting other GANs. Authors study this approach within the scope of semi-supervised learning and show that the proposed technique can achieve state of the art classification performance on several benchmark data sets but more importantly, can generate a more diversified artificial samples implying multimodal  sampling behaviour for network weights.



Quality

The paper makes an important contribution toward closing the gap between purely data driven GANs susceptible to mode collapse problem and more interpretable Bayesian generative models that can produce diverse representations of data. Results of experiments with several benchmark and a synthetic data set are quite encouraging. Samples generated by the proposed model seem to be more diverse and expressive than those generated by competing GANs.

Clarity

It was a pleasure reading this paper. Very well written and organized.


Originality

Bayesian approach to GANs is taken by assigning Bayesian priors (albeit vague) over network weights and using SGHMC to sample from their posteriors. To the best of my knowledge this work is original.


Significance
The proposed work has the potential to open a new avenue for research in GANs and as such can be considered highly significant.


Comments:

- The posteriors for \theta_d and \theta_g are empirical approximations. Do we really know the sampler can still converge to the invariant distributions given that the conditional distributions are only crude approximations of the true distributions? Any analysis (theoretical or experimental) done?

- Algorithm 1 runs for 5000 iterations and iterations are run sequential. What parts of this approach are parallelizable? How does the run time compares to standard GANs?

- There has been some recent work in GANs that use finite Gaussian mixture models to generate samples in the code space (produced by a stacked denoising autoencoder). These approaches have similar motivation (to be able to generate multi mode and more representative data sets) as the current work but they approach the problem from a visualization perspective. It may be worthwhile to mention them.


Typo:

Line 188: a set generators should be a set of generators.

"
Alternating minimization for dictionary learning with random initialization,"Niladri Chatterji, Peter L. Bartlett",https://proceedings.neurips.cc/paper/2017/hash/3210ddbeaa16948a702b6049b8d9a202-Abstract.html,"This paper proposes and analyzes an alternating minimization-based algorithm to recover the dictionary matrix and sparse coefficient matrix in a dictionary learning setting.  A primary component of the contribution here comes in the form of an alternate analysis of the matrix uncertainty (MU) selector of Belloni, Rosenbaum, and Tsybakov, to account for worst-case rather than probabilistic corruptions.

Pros:
+ The flavor of the contribution here seems to improve (i.e., relax) the conditions under which methods like this will succeed, relative to existing works.  Specifically, the motivation and result of this work amounts to specifying sufficient conditions on the vectorized infinity norm of the unknown dictionary matrix, rather than its operator norm, under which provable recovery is possible.  This has the effect of making the method potentially less dependent on ambient dimensions, especially for ""typical"" constructions of the (incoherent) dictionaries such as certain random generations.
+ The alternate analysis of the MU selector is independently interesting.

Cons:
- It would be interesting to see some experimental validation of the proposed method, especially ones that investigate the claimed improvements in the conditions on the unknown dictionary relative to prior efforts.  In other words, do the other efforts that state results in terms of the operator norm fail in settings where this method succeeds?  Or are the methods all viable, but just a more refined analysis here?  This is hard to determine here, and should be explored a bit, I think.
- The paper is hard to digest, partly because of notation, and partly because of some pervasive grammatical and formatting issues:
    - Line 8 of algorithm 1, as written, seems to require knowledge of the true A,x quantities to compute.  In reality, it seems this should be related somehow to the samples {y} themselves.  Can this be written a bit more clearly?
    - Condition (c4) on page 6, line 206 is confusing as written.  The dimension of x is r, and it is s-sparse, so there are more than r options for *sets* of size s; this should be r-choose-s, I guess.  The subsequent conditions are apparently based on this kind of model, and seem to be correct.
    - Why include the under brace in the first equation of line 251 on page 7?  Also, repeating the LHS is a little non-standard.
    - The ""infinite samples"" analysis is a little strange to me, too.  Why not simply present and analyze the algorithm (in the main body of the paper) in terms of the finite sample case?  The infinite case seems to be an analytical intermediate step, not a main contribution in itself.
    - There are many sentence fragments that are hard to parse, e.g., ""Whereas..."" on line 36 page 2, ""While..."" on line 77 page 2, and ""Given..."" on line 131 page 4.

","This paper give theoretical guarantees for an alternating minimization algorithm for the dictionary learning and sparse coding problem. The analysis is based on the infinity norm of the dictionary and gives linear convergence rate for sparsity level s = O(sqrt(d)). The sparse regression estimator is the {l_1, l_2, l_\infty}-MU selector and the dictionary learning step is an one-step steepest descent method.

The idea of the alternating minimization algorithm is not new. The new aspect of the algorithm in this paper is that {l_1, l_2, l_\infty}-MU selector is used for sparse regression. The main interesting result in this paper is its theoretical analysis. It uses infinity norm instead of 2 norm for the dictionary and gives a better sparsity level when compared to the best previous results in the overcomplete setting. However, this paper is not easy to follow for non-experts. The authors suppose readers have sufficient background in this areas and do not give a complete introduction. See comments below.

*), The title claims the proposed method is fast. However, no experiments are given in this paper. Since this new algorithm has a better bound of sparsity level theoretically, it also would be good to have numerical comparisons to show the probability of successful recovery versus sparsity level.

*), P3, L88: 
Algorithm 1 is not clear at the current stage. For example, y is in R^d from L18. Therefore, by L77 in paragraph of notation, y_k in Step 3 is supposed to be a scalar. However, from L110 (2), y_k should be a vector in R^d. I suppose the samples should be a matrix Y and each column is denoted by y_k \in R^d. 
\mathbb{I} in Step 5 is used without introduction.
The motivation of Step 8 is not given. It is a steepest descent update, right? If yes, the cost function should be given.
A^* is used in Step 8, but A^* is unknown. The authors could add a intermediate step which use the given samples y instead of A^*. In summary, Algorithm 1 needs be clarified.

*), P7, L238. What cost function is the gradient \hat{g}_{i j} with respect to?

","The paper proposes an alternating minimization algorithm for dictionary learning, and theoretical guarantees are also given. In each step the algorithm first uses an l1, l2 and l_infty algorithm with thresholding to get an estimate of the coefficients, and then use another gradient step to update the dictionary. 

To me two shining points of the paper:
1. Guarantee holds for the overcomplete dictionary.
2. Improved the sparsity level requirement by a factor of log d.

Obviously the NIPS format is too short for the arguments the authors are making, and a lot of details are moved to the appendix. Due to time limit I cannot read all the details of the proof. Below are some questions:
1. In A1 you have a mu-incoherence assumption, but mu is not shown in your theorem 3. Is it hidden somewhere?
2. In assumption B1 you mentioned, and I agree that there is a fast random initialization so that the condition holds. Can you give some details about your initialization procedure and guarantees?
3. How do you handle the permutation invariance of A? 
4. In your algorithm 1, line 3, the MUS algorithm has a return, but in your definition (equation 2), the return is not specified. Actually the returned should be theta instead of (theta, t, u).
5. In your algorithm 1, line 3, can you give some explanation about “(w_k^t is the k^th covariate at step t)”? Why w_k^t is called the k^th covariate?
6. Any simulation result verifying your convergence rate?
"
Sparse Embedded $k$-Means Clustering,"Weiwei Liu, Xiaobo Shen, Ivor Tsang",https://proceedings.neurips.cc/paper/2017/hash/3214a6d842cc69597f9edf26df552e43-Abstract.html,"This paper proposes a dimension reduction method for k-means clustering, and provides a theoretical guarantee of the proposal.
This paper is overall well written. In particular, the theoretical contribution is interesting and solid.
However, although the proposed method is empirically shown to be superior to other advanced clustering methods, the results are not reliable due to the following reasons:
- To what values \epsilon and \delta are set in experiments? Since they work as parameters in practice, such information is crucial in empirical evaluation.
- Related to the above point, how to set them in real applications? Is there any guideline?
- In addition, the sensitivity of the clustering performance of the proposed method with respect to changes in such parameters should be empirically examined.
- What is the measure of clustering results? Accuracy is not appropriate and variation of information should be used instead.
","The authors proposed a simply but effective  k-means based clustering algorithm with a new random projection dimensionality reduction technique. Both theoretical and empirical results show the effectiveness and efficiency of the proposed method compared to the baseline k-means and comparison dimensionality reduction techniques. The paper would be a good fit to NIPS and proposed method would be useful for the public community.

Here are some minor points.
1. Line 100 and 101 are confusing, local optimal or global optimal?
2. Practically, how to set epsilon and delta? Does it effect the results?
3. Line 217 and 218 the authors said that SE outperforms RP regarding accuracy, which can not be seen from Figure 1.
4. The runtime for the proposed method is O(nnz(X)), but for the evaluated image datasets nnz(X) is closer to n *d, right? However, the proposed method is still much faster than RP with complexity of O(nd). Maybe the authors can explain more about this.
5. Is that possible to extend the method to the kernel space?","The authors proposed a sparse embedded k-means clustering algorithm to improve the running time of matrix multiplication of current proposed randomization projection method under sparse setting of data matrix in the literature. In particular, they demonstrated that their algorithms achieve the calculation time of matrix multiplication of order proportional to the number of non-zeroes entries of data matrix. 

I think the sparse embedded k-means clustering algorithm is rather interesting; however, it is not a very surprising improvement given the current results from the paper of C. Boutsidis et al. (2015). More specifically, both papers try to approximate low dimension solution for the original solution of K-means problem. To do that, C. Boutsidis et al. (2015) proposed to multiply the data matrix X with a random matrix having entries $1/\sqrt{d'}$ or $-1/\sqrt{d'}$ where $d'$ is the dimension of the approximation solution. However, their proposed solution does not work work well with sparse setting of data matrix as the calculation of matrix multiplication from their method greatly depends on the number of data points and the number of clusters. To account for that drawback, the authors of current paper proposed to multiply the data matrix with sparse random matrix inspired from diagonal Rademacher matrix. At the high level, it is unsurprisingly will improve the computation of matrix multiplication from C. Boutsidis et al.'s work under sparse setting of data matrix. At the setting of rather dense data matrix, I think both methods may differ slightly in terms of accuracy and matrix multiplication time. In summary, even though the paper is rather interesting, I think the contribution of the paper is not very surprising.

Some minor comments:

(1) Will the condition in Definition 2 hold for any matrix $D$?

(2) It may be better for readers if the authors replace notation $\widehat{D}$ in Lemma 1 with less confusing notation. At the moments, the notations in Lemma 1 look rather similar.

(3) I do not see the clustering accuracy with Random projection method from C. Boutsidis et al.'s work with RCV1 data in the experiment section. I wonder what may happen with that accuracy result?

"
Reducing Reparameterization Gradient Variance,"Andrew Miller, Nick Foti, Alexander D'Amour, Ryan P. Adams",https://proceedings.neurips.cc/paper/2017/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html,"Summary

This paper proposes a control variate (CV) for the reparametrization gradient by exploiting a linearization of the data model score.  For Gaussian random variables, such a linearization has a distribution with a known mean, allowing its use as a CV.  Experiments show using the CV results in faster (according to wall clock time) ELBO optimization for a GLM and Bayesian NN.  Furthermore, the paper reports 100 fold (+) variance decreases during optimization of the GLM.  


Evaluation

Method:  The CV proposed is clever; the observation that the linearization of the data score has a known distribution is non-obvious and interesting.  This is a contribution that can easily be incorporated when using the reparametrization trick.  The only deficiencies of the method are (1) it requires the model’s Hessian and (2) application is limited to Gaussian random variables.  It also does not extend to amortized inference (i.e. variational autoencoders), but this limitation is less important.  I think the draft could better detail why it’s limited to Gaussian variables (which is a point on which I elaborate below under ‘Presentation’).  Some discussion of the hurdles blocking extension to other distributions would improve the draft and be appreciated by readers.  For instance, it looks like the linearized form might have a known distribution for other members of the location-scale family (since they would all have the form f(m) + H(m)(s*epsilon))?  Thoughts on this?    

Experiments:  Experiments are limited to small models (by modern standards)---GLM and one-layer / 50 hidden unit Bayesian NN---but are adequate to demonstrate the correctness and utility of the method.  I would have liked to see what held-out performance gains can be had by using the method---you must have looked at held-out LL?---but I realize this is not crucial for assessing the method as a CV.  Still I think the paper is strong enough that even a negative or neutral result would not hurt the paper’s quality.

Presentation:  The paper’s presentation of the method’s derivation is clear and detailed.  I had no trouble seeing the logical flow from equation to equation.  However, I think the draft’s description is somewhat ‘shallow’---by that I mean, while the algebra is clear, more discussion and intuition behind the equations could be included.  Specifically, after the method is derived, I think a section discussing why it's limited to Gaussian rv’s and why it can’t be extended to amortized inference would be beneficial to readers.  It took me two-to-three reads through the draft before I fully understood these limitations.  To make room for this discussion, I would cut from the introduction and VI background since, if a person is interested in implementing your CV, they likely already understand VI, the ELBO, etc. 

Minutiae: Table 1 is misreferenced as ‘Table 3’ in text.

Conclusion:  This paper was a pleasure to read, and I recommend its acceptance.  The methodology is novel and interesting, and the experiments adequately support the theory.  The draft could be improved with more exposition (on limitations, for instance), but this is not a crucial flaw.  ","This paper shows how to build a control variate to reduce the variance of the reparameterization gradient estimator in variational inference. The main idea is to linearize the ""data term"" (the gradient of the log-joint) around the mean of the variational distribution, and use this linearized term to build the control variate. Results show faster convergence of the ELBO and significant variance reduction.

Overall, the idea is interesting and the writing quality is good. However, I have some concerns, specially with computational complexity:

1. The proposed approach relies on the Hessian information from the model. The paper mentions efficient ways to compute Hessian-vector products (ref. [15]), but it should include further analysis on the resulting complexity of the inference for each of the approaches that are described (full Hessian, diagonal Hessian, HVP+Local).

2. Related to the point above, the experiments are done on relatively small models (D=37 and D=653). That suggests that scalability is an issue with this method. The paper would be significantly improved with experiments on more realistic scenarios, with at least thousands or tens of thousands latent variables.

3. Table I should also include vanilla reparameterization gradients (i.e., with no control variates), not only comparisons of methods described in the paper.

4. The paper focuses on the Gaussian case. This is sensible because it is the most common use for reparameterization gradients. I wonder if the proposed approach can be extended beyond the Gaussian case, as it would become harder to fully define the distribution of tilde{g}, similarly to Eqs. 21-22. The papers states (lines 124, 138) that for some transformations T we can compute this distribution; is there any other example besides the Gaussian?

Here are also some minor comments:

1. The discussion about high-dimensional models (lines 176-186) is not clear to me.

2. In line 239, it reads ""as the Hessian diagonal only reduces mean parameter variance by a factor of 2-5"". That doesn't seem to be the case according to Table 1.

3. In line 157, the text reads ""The data term [...] is all that remains to be approximated"". This is misleading, as it seems to imply that the Eqs. before (Eqs. (15)-(18)) are approximations, when they are not.

4. I think that ""Doubly Stochastic Variational Bayes for non-Conjugate Inference"", which was developed in parallel to [19], should be cited in line 36.

5. Eqs. (15)-(18) are standard in reparameterization trick for VI; I think these can be moved to the appendix to gain some space.

6. There are some typos: ""parameter"" (151), ""complements"" (216), ""Table 1"" (230), ""L=2"" (253).
","I have read the author feedback and have adjusted my rating and comments accordingly.

Summary: The paper proposes a control variate to reduce the variance of the reparameterisation gradients in Monte Carlo variational inference. The reparameterisation gradient can be decomposed into several terms and a local linearisation can be done on the data term of this gradient. This observation results in a tractable control variate which relates the structure of the variational approximation to the curvature of the unapproximated joint density. The experiment results seem to support this control variate on two models [a generalised linear model and a small Bayesian neural network] that use Gaussian variational approximations.

Details: 
I enjoyed reading this paper -- it was clearly written and the structure is well-thought-out and easy to follow/understand. 

It would be great if alternative control variates are discussed and compared against, for example, the delta control variate technique in Paisley et al.'s stochastic search for VI paper.

One disadvantage of this approach is that it requires the Hessian or Hessian vector product of the joint density of the model. These are expensive to evaluate in high dimensions [additionally, these involve a sum over the training instances]. Even though (online or diagonal or monte carlo) approximations are available they don't seem to help reduce variances and even hurt the performance. My personal opinion is that more cheap and cheerful, and *effective* approximations are needed for this method to be deployed practice to more realistic cases (bigger models + bigger datasets).

Would it help to decompose the joint density further into a sum of prior density + log likelihood, then check the gradients given by these extra terms and perform linearisation only on the log likelihood instead of on the joint?"
Min-Max Propagation,"Christopher Srinivasa, Inmar Givoni, Siamak Ravanbakhsh, Brendan J. Frey",https://proceedings.neurips.cc/paper/2017/hash/327708dd10d68b1361ad3addbaca01f2-Abstract.html,"The authors describe the implementation and application of belief propagation for the min,max semiring.  While BP is known to work in any commutative semiring, the authors derive efficient updates over the min-max semiring and provide an example application for which the algorithm is relevant.  The paper is well-written and easy to follow.

I'm a bit on the fence with this paper.  The algorithm isn't super novel, though the efficient implementation is, as far as I know, new.  I suppose that I would be more likely to vote for acceptance if the authors had compared against more standard BP implementations and explained the trade-offs.  For example, min_x max_a f_a(x_a) can be reformulated only as a minimization problem by introducing a new variable y and solving min_{x,y} y such that y >= f_a(x_a).  I feel like the message passing updates can also be simplified in this case using the same kinds of ideas that the authors propose and it doesn't even require solving a min max for each update.

","POST-REBUTTAL UPDATE
====================
I have read and considered the authors' response and stand by my initial decision.  I would heavily stress that the revised paper more clearly state prior work on message passing specific to  the min-max semiring, including Aji and McEliece and Vinyals et al.  

SUMMARY:
========
The authors propose a dynamic programming algorithm for solving min-max problems with objectives that decompose as a set of lower dimensional factors.  For cases where factors have high arity the authors further show an efficient updating scheme in cases where factors can be efficiently minimized with some variables clamped.  Further speedups are shown for cases where the domain is constrained to a feasible subset.

PROS:
=====
This reviewer found the paper to be well-rounded with a clear presentation of algorithmic components and with experiments on an interesting load balancing application.  Results on the latter applications, while not a decisive improvement, are comparable to algorithms specifically taylored to that problem.  The method of efficient updating for large clique factors is a good practical consideration.

CONS:
=====
The strongest criticism of this reviewer is that the authors could more honestly credit Vinyals et al. (2013) with the development of the min-max BP variant.  This reviewer is aware that the aforementioned work is not heavily cited and that the authors do cite it as well as compare to experimental results.  However, from reading this paper it was not clear that much of the BP formulation was provided by Vinyals et al.  Nevertheless, this reviewer feels that the present work provides a number of improvements over Vinyals et al. that merit publication, including a clearer description of min-max BP for factor graphs without specialization to junction trees and efficient updating for high arity factors.

Detailed comments:
  * (L:18) cite Kschischang et al. (2001) in reference to turbo codes for LDPC
  * A brief statement of min-max semiring existence would be useful (c.f. Vinyals et al. 2013).  Sum/max-product and max/min-sum semirings are well-known in the graphical models literature but min-max is not commonly discussed
  * (L:183-186) Seems to suggest that argmin X is not unique as some elements do not affect objective value at the minimum.  Please clarify or discuss to what extent this is a problem
  * In Sec. 5.2 numerous locations interchange maximization subscript ""a"" with ""m"", particularly in Eq. (13)
  * Fig. 5: Legend is much too small to read on printed version","As the title suggests, this paper is interested in
      solving min-max problems using message-passing algorithms. The
      starting point is Eq. 1: You want to optimize a vector x such
      that the max over a discrete set of functions is minimize. The
      set of functions should be finite.

      It's well known that changing the semiring can change the message-passing algorithm from a joint optimization
      (max product) algorithm to a marginalization algorithm (sum
      product). So, just like the (+,*) semiring yields the sum-product
      algorithm and the (max,*) semiring yields the max-product
      algorithm, this paper uses the (min,max) semi-ring. Then,
      Eqs. 2-3 are exactly the standard BP messages on factors and
      single variables, just with the new semi-ring. (It would be good
      if the paper would emphasize more strongly that Eqs. 2 and 3 are
      exactly analogous-- I had to work out the details myself to
      verify this.)

      Since that's a semi-ring you get all the normal properties
      (exactness on trees, running times). However, you don't seem to
      inherit much of the folk theorems around using BP on loopy
      graphs when you change the semi-ring (where most applications
      probably lie) so this paper investigates
      that experimentally.

      The paper assumes that solving the minimization problem on any particular
      factor can be solved in a fixed amount of time. This is
      reasonable to make analysis tractable, but it's worth pointing
      out that in many applications, doing this might itself involve
      running another entire inference algorithm (e.g. max-product
      BP!) There is also an algorithmic specialization to speed up
      message-computation for the case
      where there is a special ""choose-one"" form for the factors,
      similar to some factor forms that are efficient with different
      semigroups.

      Experimentally, it is compared to a previous algorithm that runs
      a bisection search with a CSP solver in each iteration. This
      appears to perform slightly better, although apparently at
      higher cost.

      As far as I know, this particular semi-ring variant of belief
      propagation has not been investigated before. I don't see strong
      evidence of a huge practical contribution in the expeirments
      (though that might reflect my lack of expertise on these
      problems). Nevertheless, the algorithm is extremely simple in
      concept, and it certainly seems worth noting that there is
      another semi-ring for BP that solves another possibly
      interesting set of problems. Even if the results here aren't
      that strong in the current form, this might be a first step
      towards using some of the many improvements that have been made
      to other semi-ring versions of BP.

      (I was surprised that this particular semi-ring hasn't been
      proposed before, but I'm not aware of such a use, and a brief
      search did not find anything. If this is not actually novel, I
      would downgrade my rating of this paper.)

      - Fig. 5 is very hard to read-- illegible in print
      - In eq. 13 I believe you want a -> m?

EDIT AFTER REBUTTAL:

I've read the rebuttal and other reviews, see no reason to change my recommendation. (I remain positive.)
      "
Statistical Cost Sharing,"Eric Balkanski, Umar Syed, Sergei Vassilvitskii",https://proceedings.neurips.cc/paper/2017/hash/32b3ee0272954b956a7d1f86f76afa21-Abstract.html,"The paper studies the problem of statistical cost sharing in cooperative games where one does not observe the cost (or characteristic function) for every possible coalition. The goal is then to compute a cost sharing that approximate desired notions (core and Shapley value) while scaling well with the number of observed tuples (S, C(S)). 

The paper first gives results for the core: a bound to compute probably stable allocations where improves a previous bound (but is, according to the authors independently discovered in a paper currently on Arxiv), a better (logarithmic) bound for a weaker notion of approximate stability, and an impossibility result. Then, results for approximating the Shapley value are given and finally a new concept of statistical Shapley value is introduced that is easily approximable. 

Theorem 1 was, according to the authors, discovered independently from the reference Balcan et al 2016 currently on arxiv. It is of course impossible to check but this result is also only a small part of the overall contribution of this paper. 

I find the statistical cost sharing problem interesting and relevant and the results nice. I find the definition of the statistical Shapley value an interesting extension but I wonder how much sense it really makes in practice. It is basically tailored to be easy to approximate from the empirical average of costs observed from distribution \mathcal{D} but it is unclear to me that it still enjoy the nice properties of the classical Shapley value; for instance the fact of being thought as ""fair"" or being stable / giving incentives to users to join, etc.

Minor comments: 
- typo in abbstract, remove curvature once
- In the definition of the statistical cost sharing problem: is N necessarily one of the observed costs in the sequence?

","This paper continues the work of Balcan et al. ’15 on learning cost sharing functions in cooperative game theory from sample observations. 

Classical work in cooperative game theory considers the existence and knowledge of a cost function C(.) such that C(S) is the cost of a set S. Works in this space are interested in computing cost-sharing values \phi_i for all individuals i, that have certain properties. The first notion of interest discussed in this work is the Core. Core is a set of values \phi_i that incentivizes all individuals to form one coalition. The second cost-sharing notion is the Shapley value, that at a high level coincides with the marginal cost of a player in a random ordering of all players. Both of these concepts can be defined by properties that are discussed in section 2.1.

In this work, the authors consider a setting where the cost function C(.) is not known in advance, but samples of the form (S, C(S)) are drawn from some distribution over sets. The goal of the paper is to use these samples to learn values \phi_i that (approximately) preserve the properties associated with the definition of Core and Shapely value on sets S that are drawn from the same distribution.

I like the general premise of the paper. I agree that many game theoretic models rely on the type of knowledge that might not be available, in the case of this paper the ability to know C(.) fully. So, I find it valuable to explore how one can use a learning theoretic framework for creating robust versions of these game theoretic definitions that can be learned from an environment. This is not a new direction, however, as it has been explored by Balcan et al’15 for cooperative games, and a large body of work for other aspects of game theory, such as social choice, and mechanism design.

One concern is that a number of the results in this paper are either marginal improvements over previous works, or are results that are said to be “independently discovered by Balcan et al.’16”. I find this mention of “independently” a little awkward, given that the work has been published. I would normally not hold this against a result, specially if the result requires deep insights where independent work sheds new light. But, as far as machine learning insight go in these example, the methodology is quite simple — noticing that the properties describing a notion create an LP whose constrains are given by the samples and therefore can be learned.

There are also novel directions in this work that haven’t been explored in previous works. For example, the authors also explore how to approximate the Shapley value and also introduce a variation of shapley value that satisfies probabilistic (data-dependent) properties.

All together, I like the paper and the direction it’s taking. But, I’m not entirely convinced that the contributions of this paper constitute new interesting results that would be of special interest the community.

After rebuttal:
I have read the author's response.","This paper studies cost sharing in cooperative games. There is a ground set N of players and a cost function C that maps subsets S of N to a real-valued cost C(S). The goal is to split the entire cost of the ground set C(N) among the players in such a way that certain desirable properties are satisfied, such as the “core property” (defined in Section 2.1) or the “Shapley axioms” (defined in Section 2.2). Prior work in the cost sharing literature has designed algorithms computing the cost shares which have oracle access to the underlying cost function C. Recent work by Balcan, Procaccia, and Zick in IJCAI 2015 studies the learning theoretic setting where the algorithm does not have oracle access but receives a sample of subsets labeled by their cost {(S1, C(S1)), (S2, C(S2)), …, (Sm, C(Sm))}. In this statistical cost sharing model, the authors of the present paper design sample efficient and computationally efficient learning algorithms that compute cost shares that (approximately) satisfy various properties with high probability.

From what I understand, in Balcan et al.’s IJCAI 2015 paper, they showed that for some games, it is possible to efficiently compute a “probably approximately stable core” (Definition 3) using a polynomial number of samples and then in the arXiv paper published a year later they showed that this is true for all games with a non-empty core. This matches Theorem 1 of the present paper. The present paper builds on Balcan et al.’s work by defining two other notions of an approximate core (Definition 3). They prove various upper and lower bounds on sample complexity and computability for these other notions (Theorems 2 and 3).

Finally, the authors study cost shares that approximate the “Shapley value” which is a solution to the cost sharing problem satisfying several desirable properties. They show that if the cost function is a submodular function, then the extent to which the Shapley value can be approximated from samples from the uniform distribution/any bounded product distribution depends on the submodular function’s curvature (with upper and lower bounds). The also show that if the cost function is a coverage function, then it can’t be approximated from samples over the uniform distribution.

Overall, I think this paper makes a solid contribution to an interesting and applicable subject area. There are a few obvious ways the theorems could be improved:
- Theorem 2: It’s too bad that the sample complexity depends on the spread of C since it seems reasonable that the spread of C might be comparable to n in many settings. Whether or not the dependence can be removed is especially interesting since Theorem 1 has no dependence on the spread. Do the authors have any intuition about whether or not the dependence on the spread is necessary?
- Theorem 4: Can this theorem be generalized to other distributions? If the authors have any interesting insights into the key technical challenge here, that would be a nice addition to the discussion.

I think the axioms for the data-dependent Shapley value could be better motivated. Specifically, the symmetry and zero element axioms seem a bit unintuitive because there’s no connection to the cost function. To me, a more natural distributional parallel to the original symmetry condition would be something like, “For all i and j, if Pr_{S \sim D}[C(S \cup {i}) = C(S \cup {j})] >= ½, then \phi_i^D = \phi_j^D” (I chose ½ arbitrarily). Also, in my opinion, a more natural distributional parallel for the original zero element condition could be “For all i, if Pr_{S \sim D}[C(S \cup {i}) – C(S) = 0] >= ½, then \phi_i^D = 0.” 

Small questions/comments:
- I think Theorem 1 should say “probably approximately stable core.”
- In the proof sketch of Theorem 4, what is C_{A_{sigma < i}}(i)?

=======After author feedback=======
Thanks, it would be great to include examples where a dependence on the spread is necessary. And I see, that's a fair point about how you have limited latitude in crafting alternative characterizations."
Dilated Recurrent Neural Networks,"Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A. Hasegawa-Johnson, Thomas S. Huang",https://proceedings.neurips.cc/paper/2017/hash/32bb90e8976aab5298d5da10fe66f21d-Abstract.html,"The paper is relevant to the community, well-written and decently structured. I have no major concerns, only some interested questions:
- Why do the other networks not learn at all in some of the tasks? / What is the deeper reason for this?
- How does the proposed memory capacity measure relate to the memory capacity introduced by Jaeger in the context of Echo State Networks?

Minor comments:
-p.3: subseuqnces -> subsequences
-p.6: ""We implement two setting."" -> settings
-p.6: ""which is consistent with our argument in 3"" -> in Section 3
-p.8: ""Figure 6 demonstrates both the wall time"" -> overall time?
","The paper proposes to build RNNs out of recurrent layers with skip connections of exponentially increasing lenghts, i.e. the recurrent equivalent of stacks of dilated convolutions. Compared to previous work on RNNs with skip connections, the connections to the immediately preceding timestep are fully replaced here, allowing for increased parallelism in the computations. Dilated RNNs are compared against several other recurrent architectures designed to improve learning of long-range temporal correlations, and found to perform comparably or better for some tasks.

The idea is simple and potentially effective, and related work is adequately covered in the introduction, but the evaluation is somewhat flawed. While the idea of using dilated connections in RNNs is clearly inspired by their use in causal CNNs (e.g. WaveNet), the evaluation never includes them. Only several other recurrent architectures are compared against. I think this is especially limiting as the paper claims computational advantages for dilated RNNs, but these computational advantages could be even more outspoken for causal CNNs, which allow for training and certain forms of inference to be fully parallelised across time.

The noisy MNIST task described from L210 onwards (in Section 4.2) seems to be nonstandard and I don't really understand the point. It's nice that dilated RNNs are robust to this type of corruption, but sequences padded with uniform noise seem fairly unlikely to occur in the real world. In Section 4.3, raw audio modelling is found to be less effective than MFCC-based modelling, so this also defeats the point of the dilated RNN a bit. Also, trying dilated RNNs combined with batch normalization would be an interesting additional experiment.

Overall, I don't find the evidence quite as strong as it is claimed to be in the conclusion of the paper.

Remarks:

- Please have the manuscript proofread for spelling and grammar.

- In the introduction, it is stated that RNNs can potentially model infinitely long dependencies, but this is not actually achievable in practice (at least with the types of models that are currently popular). To date, this advantage of RNNs over CNNs is entirely theoretical.

- It is unclear whether dilated RNNs also have repeated stacks of dilation stages, like e.g. WaveNet. If not, this would mean that large-scale features (from layers with large dilation) never feed into fine-scale features (layers with small dilation). That seems like it could be a severe limitation, so this should be clarified.

- Figures 4, 5, 6 are a bit small and hard to read, removing the stars from the curves might make things more clear.


UPDATE: in the author feedback it is stated that the purpose of the paper is to propose an enhancement for RNN models, but since this enhancement is clearly inspired by dilation in CNNs, I don't think this justifies the lack of a comparison to dilated CNNs at all. Ultimately, we want to have models that perform well for some given task -- we do not want to use RNNs just for the sake of it.

That said, the additional experiments that were conducted do address my concerns somewhat (although I wish they had been done on a different task, I still don't really get the point of ""noisy MNIST"".) So I have raised the score accordingly.","The paper studies from a theoretical point of view the benefits of using dilated skip connections for RNN to improve their long-term memorization capabilities. A new neural memory measure - mean recurrent length - is introduced that shows that the dilated setting is superior to the skip connection setting, while having less parameters. Interestingly, the dilated setting allows for parallelisation leading to lower computational time.
Experiments on the copy problem and mnist show significantly better performance compared to the vanilla counterparts, while experiments on language modelling (Penn tree bank) and speaker identification yield competitive results, but with simpler settings.

Comments:
As the authors point out, a dilated LSTM setup was already proposed in Vezhnevets et al, Feudal networks. It is unclear in what way their ""fundamental basis and target tasks are very different""; as in this paper, the goal there is to improve long term memorization. Instead, it would be beneficial to discuss the results obtained here in light of their findings. E.g. they found that dilated LSTM on its own does not perform better than vanilla LSTM. Is the stacked dilated model that makes the results in this paper outperform the vanilla versions in all cases? 

In Dilated residual networks, by Yu et al, the authors discuss the potential gridding artefacts from using exponentially increasing dilations in stacked models. It would be useful to discuss such issues in the recurrent setup as well. 

Few typos: line 48 temporal; line 78 subsequences; 

    "
The Expressive Power of Neural Networks: A View from the Width,"Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, Liwei Wang",https://proceedings.neurips.cc/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html,"The paper is on width as a complexity measure for neural networks. The activation function is assumed to be ReLU, and approximation is measured by the L^1 distance
over R^n. Positive and negative results are given on approximability, the width/depth trade-off is studied, and some experimental results are given. A proof outline
is given for the positive result only (Theorem 1). Detailed proofs are given in the Appendix.

The negative results (Theorem 2 and 3) seem somewhat unusual. Theorem 2 states inapproximability of *every* nontrivial function by width n networks. One wonders whether this
is mainly caused by the fact that ReLU and L^1 distance over R^n is an unnatural combination to consider? This, of course is not a well-defined question, but certainly some
justification of this choice should be given in the paper. Also, some intuition should be given for the proof (which uses concepts from [17] and gives some kind of normal form). 

Theorem 3 is formulated in a strange way: $\epsilon$ has no role in the statement. The theorem simply says that under the stated conditions the target function and the
function computed by the network (both continuous) are different on the cube. Thus the theorem only gives a sufficient condition for a continuous function to be not computable exactly by the given class of networks. In this case the proof (given in Appendix C) is simple, but the short proof contains several typos: the limits for $i$ and $j$ are exchanged, and the argument of the span is wrong (the members of the list are not vectors). This is somewhat worrisome with respect to the longer proofs which I could not inspect in detail.

It seems that the interpretation of the experiments is somewhat problematic, as `exceeding the bound of Theorem 4 by a constant factor' for a few small sizes is hard to interpret. Also, the width-depth trade-off problem is about worst-case (called `existential' in the paper), so one should be careful to interpret the experimental results as evidence for the worst-case behavior.

Considering the network width parameter is an interesting topic. The choice of the model (activation function and approximation criterion) is not justified in the paper,
and it is not clear how much the particular results are due to this choice. The results seem interesting, and the open problem posed about the width-depth trade-off
seems interesting as well. 
","The paper studies the expressive power of width--bounded, arbitrary--depth networks. Specifically they derive universal approximation statements for such networks. The analogue result for depth--bounded width--unbounded network received considerable attention and states that depth=2 and exponential width is enough to approximate any network.

The first natural question is whether width=2 and exponential depth should be enough. Not surprisingly this is not true. But moreover the authors give a crisp threshold and show that unless width > n (n is input dimension) no universal depth theorem holds (thm 2. and 3.)
I think this result is interesting, and motivates understanding the effect of width.

On a positive note, they do show that for width= n+4, we can achieve universal depth theorem: Roughly the idea is to propagate the input layer, as well as compute additional neuron at each layer and sum all calculation so far: which roughly turn a large width network into a large depth network.

In terms of clarity and writing: In my opinion the authors put a lot of focus on theorem 1 and not enough focus on theorem 2,3: There is no overview of these statements proofs and it is very hard to verify them. The author should give an overview of the techniques and proof. Currently these theorems are simply differed to the appendix and even there the authors simply provide some list of technical Lemmas.

On a broader level, the main caveat of these universal approximation theorems is that they relate to arbitrary, very large and general classes of functions (Lebesgue integrable).  The crucial property of deep networks, in terms of expressive power, is that with bounded width and poly-depth you can approximate any efficiently computable function.

Nevertheless, it is still interesting to understand the approximation power of arbitrary functions on a theoretical basis. Also, the lower bound showing that with sublinear width there is no universal approximation even for exponential depth is an interesting result.","SUMMARY 
* This paper studies how width affects the expressive power of neural networks. It suggests an alternative line of argumentation (representation of shallow by deep narrow) to the view that depth is more effective than width in terms of expressivity. The paper offers partial theoretical results and experiments in support of this idea. 

CLARITY 
* The paper expresses the objectives clearly and is easy to follow. However, the technical quality needs attention, including confused notions of continuity and norms. 

RELEVANCE 
* The paper discusses that too narrow networks cannot be universal approximators. This is not surprising (since the local linear map computed by a ReLU layer has rank at most equal to the number of active units). Nonetheless, it highlights the fact that not only depth but also width is important. 
* The universal approximation results seem to add, but not that much to what was already known. Here it would help to comment on the differences. 
* The paper presents a polynomial lower bound on the required number of narrow layers in order to represent a shallow net, but a corresponding upper bound is not provided. 
* As an argument in favor of depth, the theoretical results do not seem to be conclusive. For comparison, other works focusing on depth efficiency show that exponentially larger shallow nets are needed to express deep nets. The paper also presents experiments indicating that a polynomial number of layers indeed allows deep narrow nets to express functions computable by a shallow net. 

OTHER COMMENTS 
* The paper claims that ``as pointed out in [2]: There is always a positive measure of network parameters such that deep nets can be realized by shallow ones without substantially larger size''.  This needs further clarification, as that paper states `` we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network.'' 
* In line 84, what is the number of input units / how do the number of parameters of the two networks compare? 
* In related works, how to the bounds from [14,18] compare with the new results presented in this paper? 
* In line 133. Lebesgue measurable functions are not generalizations of continuous functions. 
* In line 134 the argumentation for L1 makes no sense. 
* In line 163 ``if we ignore the size of the network ... efficient for universal approximation'' makes no sense. 
* The comparison with previous results could be made more clear. 
* Theorem 2 is measuring the error over all of R^n, which does not seem reasonable for a no-representability statement. 
* Theorem 3 talks about the existence of an error epsilon, which does not seem reasonable for a no-representability statement. Same in Theorem 4.  
* In line 235 the number of points should increase with the power of the dimension. 

CONCLUSION 
The paper discusses an interesting question, but does not answer it conclusively. Some technical details need attention. 
"
Inverse Reward Design,"Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J. Russell, Anca Dragan",https://proceedings.neurips.cc/paper/2017/hash/32fdab6559cdfa4f167f8c31b9199643-Abstract.html,"This paper proposes a new problem, whereby an agent must in effect select an optimal reward function, from observing a proxy (incomplete) reward function and a set of trajectories.   This is a very interesting extension of the well-studied inverse reinforcement learning, which offers useful new perspectives on the value alignment problem.

The proposed problem is clearly defined, relevant literature is discussed.  The formulation is quite elegant, and it is a pleasant surprise to see the interpretation that comes out of the bayesian IRL case, whereby the proxy reward reduces to providing a set of corresponding demonstrations.  I liked both the problem formulation and the proposed approximations to the IRD posterior.

The main aspect of the paper that could be improved is the experiments. Currently, there are simple toy MDPs that serve as proof of concept.  For illustrative purposes, they are suitable and experiments are correctly executed.  But I would be interested in seeing the method used for a real-world MDP, where reward is obtained from a human.

A few minor points that could be improved:
- Notation in Definition 3 gets confusing; the different fonts used for the reward (proxy reward vs space of possible reward) are hard to parse visually.
- It may be interesting to relate the proxy reward to a notion of ""necessary but not sufficient"" reward information.
- Sec.4: It would be useful to discuss the complexity of the approximations discussed.
- Fig.4:  Add y-axis label.
- References: Some inconsistencies in style and missing information, e.g. where was Singh et al published?

In summary, the paper is novel, thought-provoking, and tackles an important problem.  I recommend acceptance.","
        The authors develop ""Inverse Reward Design"" which is intended to help mitigate unintended consequences when designing reward functions that will guide agent behaviour. This is a highly relevant topic for the NIPS audience and a very interesting idea.
        
        The main potential contribution of this paper would be the clarity of thought it could bring to how issues of invariance can speak to the IRL problem. However, I find that the current presentation to be unclear at many points, particularly through the development of the main ideas. I have done my best to detail those points below.
        
        My main concern is that the presentation leading up to and including 4.2 is very confused.
        
        The authors define P(\tilde w | w^*). (They always condition on the same \tilde M so I am dropping that.)
        
        At l.148, the authors state that through Bayes' rule, get  P(w^* | \tilde w) \propto P(\tilde w | w^*) P(w^*)
        
        In the text, the display just after l.148 is where things start to get confused, by the use of P(w) to mean P(w^* = w)
        
        The full Bayes' rule statement is of course P(w^* | \tilde w) = P(\tilde w | w^*) P(w^*) / P(\tilde w), or 

        P(w^* | \tilde w) = P(\tilde w | w^*) P(w^*) / \int_\w^* P(\tilde w | w^*) P(w^*) dw^*
        
        I am confused because 1) the authors don't address the prior P(w^*) and 2) they state that the normalizing constant integrates ""over the space of possible proxy rewards"" and talk about how this is intractable ""if \tilde{w} lies in an infinite or large finite set."" Since w^* and \tilde{w} live in the same space (I believe) this is technically correct, but a very strange statement, since the integral in question is over w^*. I am concerned that there is substantial confusion here at the core idea of the whole paper. I suspect that this has resulted just from notational confusion, which often arises when shorthand [P(w^*)] and longhand [P(w^* = w)] notation is used for probabilities. I also suspect that whatever was implemented for the experiments was probably the right thing. However, I cannot assess this in the current state of the paper.
        
        Finally, I would suggest that the authors address two questions that I think are natural in this setting. 1) What do we do about the prior P(w^*)? (Obviously there won't be a universal answer for this, but I imagine the authors have some thoughts.) 2) 
        
        Below are additional comments:
        
        l.26 - The looping example isn't given in enough detail to help the reader. The example should stand alone from the cited paper.
        Figure 1 - I understand the podium in the figure on the right, but the meaning of the numbers is not clear. E.g. is the first number supposed to convey [2][0] or twenty?
        
        l.50 - ""Extracting the true reward is..."" - Do you mean ""We define...""
        
        Figure 2: uknown
        
        l.134 - I think somewhere *much* earlier it needs to be made clear that this is a feature-based reward function that generalizes over different kinds of states.
        
        l.138 - \xi and \phi(\xi) are not precisely defined. I don't know what it means for an agent to ""select trajectory \xi""
        
        l.145 - Notation in (1) and (2) is confusing. I find conditioning on \xi drawn from ... inside the [] strange. Subscript the expectation with it instead?
        
        l.148 - P(w) should be P(w|\tilde{M}) unless you make additional assumptions. (true also in (3)) On the other hand, since as far as I can tell everything in the paper conditions on \tilde{M}, you may as well just drop it.
        
        l.149 - Probabilities are normalized. If you're going to talk about the normalizing constant, write it explicitly rather than using \propto.
        
        l.150 - Is this the same w? Looks different.
        
        l.151 - I think you want = rather than \propto. Something is wrong in the integral in (3); check the variable of integration. There are \bar\phi and \tilde\phi and it's not clear what is happening.
                
        l.158 - ""constrained to lie on a hypercube"" - Just say this in math so that it's precise.
        
        l.166 - Can't (4) just go from 1 to N?
        
        l.228 - I wouldn't open a figure caption with ""Our proof-of-concept domains make the unrealistic assumption."" Furthermore, I have no idea how the figure caption relates to the figure.
      

=== UPDATE

The authors have clarified section 4 and I understand much better what they were after. I have updated my score accordingly.","The paper ""Inverse Reward Design"" discusses Markov decision processes (MDPs) where the reward function is designed by an expert who makes assumptions which may be specific to the dynamics of the original MDP for which the reward function was specified. However, the reward function may not directly generalize to new MDPs since the expert may not be able to take everything into account when designing the original reward function. The paper defines the inverse reward design (IRD) problem as finding a probability distribution over the true reward function for a new MDP given a reward function for the original MDP.

The investigated problem is interesting and has real-world relevance. The proposed computational approaches work in the tested problems. The paper is well written.

For computing the probability distribution over the true reward function, the paper assumes a maximum entropy distribution over trajectories given the expert's reward function and the original MDP. Using this assumption the distribution over the weights of the true reward function can be defined. To make computations tractable the paper presents three different approximation schemes and evaluates the schemes in problems designed specifically to investigate the IRD problem. The results show that the IRD approaches perform better compared to using the original reward function when using a selected MDP planning approach which avoids risks.

I would like the authors to answer following question:
- In general, the solution to IRL is non-unique and therefore often a maximum entropy assumption is used. However, here we have information about possible solutions since we are given a reward function designed by the expert. Could we here in principle compute a solution to the IRD problem without the maximum entropy (or similar) assumption? If not, a practical example would be nice?


DETAILS

For the example in Figure 1 Left a more formal definition of the example could clarify the picture?

The example in Figure 1 Right is confusing. There are two problems:
1) A clear definition of reward hacking is missing
2) The picture can mean many different things for different readers. Now the picture looks like that the designed reward should actually work but why it does not work is unclear. The caption talks about pathological trajectories and not winning the race but the picture does not address these points using concrete examples. Maybe a top down picture of a race track where the agent gets high reward for undesired behavior such as cutting corners or driving over people, or similar, could help?


RELATED WORK

In the IRD problem, the expert is required to define a reward function. How does IRD relate to other kinds of goal specifications (e.g. logic constraints)? Could the approach be extended to these kind of settings?


EVALUATION

The text ""In the proof-of-concept experiments, we selected the proxy reward function uniformly at random. With high-dimensional features, most random reward functions are uninteresting. Instead, we sample 1000 examples of the grid cell types present in the training MDP (grass, dirt, and target) and do a linear regression on to target reward values."" should be clarified. Now, the paper says that the reward function is selected uniformly at random but that is a bad idea and instead 1000 examples are sampled?

Figure 4: What do the error bars denote? Units for the y-axis should be added. How is literal-optimizer represented in Figure 4?


LANGUAGE

""we then deploy it to someone’s home"" -> ""we then deploy the robot to someone’s home""

""the bird is alive so they only bother to specify""
Who is they?

""as this is an easier reward function to learn from.""
easier than what?

""uknown feature."" -> ""unknown feature.""

""might go to the these new cells"" -> ""might go to these new cells""

""subsample the the space of proxy"" -> ""subsample the space of proxy""
"
The power of absolute discounting: all-dimensional distribution estimation,"Moein Falahatgar, Mesrob I. Ohannessian, Alon Orlitsky, Venkatadheeraj Pichapati",https://proceedings.neurips.cc/paper/2017/hash/331316d4efb44682092a006307b9ae3a-Abstract.html,"This paper presents a theoretical examination of the optimality of absolute discounting similar to the examination of optimality of Good Turing in Orlitsky and Suresh (2015).  Results for minimax optimality, adaptivity and competitiveness are presented, as well as an equivalence between absolute discounting and Good Turing in certain scenarios, which suggests a choice of discount.  Experimental results demonstrate the quality of the approach, along with some interesting results on predicting terror attacks in classes of cities (e.g., cities with zero prior attacks) given prior data.

The paper is very well written and crystal clear, and the results are quite interesting.  This is an excellent addition to the literature on these methods.  I have two minor quibbles with how absolute discounting is presented.  First, it is stated in the introduction and after equation 1 that in the NLP community absolute discounting has long been recognized as being better than Good Turing for sequence models.  That's not entirely accurate.  What has been recognized is that Kneser-Ney smoothing, which imposes marginal distribution constraints on absolute discounted language models, is superior.  Absent the marginal distribution constraints of the Kneser Ney method, absolute discounting is rarely chosen over Katz smoothing (based on Good Turing).  Due to the simplicity of absolute discounting, the imposition of these constraints becomes an easy closed-form update, hence its use as the basis for that technique.  If such constraints were easy to apply to Good-Turing then perhaps that would have been the basis of the approach.  None of this negates anything that you argue for in your paper, but you should just be a bit clearer about absolute discounting being superior to Good Turing.

Second, your presentation of absolute discounting in equation 1 really does imply that delta is between 0 and 1, even though that is not stated until later.  If you want to have a more general discounting presentation which allows discounts greater than 1 (which is often how it is presented in the literature using a max with the resulting value and 0) then you'll need to account for the case where delta is greater than the count.  As it stands, the presentation is quite clean, so you may want to just be explicit that the presentation assumes the discount is less than 1, which is the interesting scenario, and explain in a footnote what kinds of modifications would be required otherwise.

I also think you could give an easy intuition about the formula for the case when mu_j = 0 in equation 1 in a single sentence by pointing out that (1) there is D*delta discounted mass (numerator) which (2) must be shared among the k-D items with zero counts and normalized (denominator).  Obvious, yes, once you work through it, but probably could save the reader a bit of time with such an intuition.

Otherwise, I thoroughly enjoyed the paper and think it is a nice addition to the literature.","
SUMMARY: The paper discusses methods for estimating categorical distributions, in particular a method called ""absolute discounting"". The paper gives bounds for the KL-risk of absolute discounting, and discusses properties such as minimax rate-optimality, adaptivity, and its relation to the Good–Turing estimator.

CLARITY: The paper is presented in reasonably clear language, and is well-structured.

NOVELTY: The contributions of the paper seem good, but perhaps rather incremental, and not ground-breakingly novel. The paper advocates for the use of absolute discounting, and gives good arguments in favor, including theoretical properties and some experimental results. But the technique as such isn't especially novel, and more general versions of it exist that aren't referenced. The literature review might not be thorough enough: for example, there are many relevant techniques in Chen & Goodman's comprehensive and widely-cited report (1996, 1998) that aren't mentioned or compared to. The paper does cite this report, but could perhaps engage with it better.

Also, the paper does not give any insight on its relation to clearly relevant Bayesian techniques (e.g. Pitman-Yor processes and CRPs) that are similar to (and perhaps more general than) the form absolute discounting presented in this paper. These other techniques probably deserve a theoretical and/or experimental comparison. At the very least they could be mentioned, and it would be helpful to have their relationship to ""absolute discounting"" clarified.

SCORE:
Overall, I enjoyed reading the paper, and I hope the authors will feel encouraged to continue and improve their work. The main contributions seem useful, but I have a feeling that their relevance to other techniques and existing research isn't clarified enough in this version of the paper.


A few minor suggestions for improvements to the paper follow below.

SECTION 8 (Experiments):
Some questions.
Q1: Is 500 Monte-Carlo iterations enough?
Q2: If the discount value is set based on the data, is that cheating by using the data twice?

LANGUAGE:
Some of the phrasing might be a bit overconfident, e.g.:
  Line 1: ""Categorical models are the natural fit..."" -> ""Categorical models are a natural fit...""?
  Line 48: ""[we report on some experiments], which showcases perfectly the all-dimensional learning power of absolute discounting"" (really, ""perfectly""?)
  Line 291: ""This perfectly captures the importance of using structure [...]""

NOTATION:
Totally minor, but perhaps worth mentioning as it occurs a lot: ""Good-Turing"" should be written with an en-dash (""–"" in Unicode, ""--"" in LaTeX), and not with a hyphen (""-"" in Unicode, ""-"" in LaTeX).

BIBLIOGRAPHY:
In the bibliography, ""Good–Turing"" is often written in lowercase (""good-turing""), perhaps as a result of forgetting the curly brace protections in the BibTeX source file. (Correct syntax: ""{G}ood--{T}uring"".)
The final reference (Zip35, Line 364) seems incomplete: only author, title and year are mentioned, no information is given where or how the paper was published.
","Summary: The authors study the performance of absolute discounting estimator of discrete distributions from iid samples. The competitive loss performance (introduced in last year's NIPS best paper) is used primarily as the metric. The main result is to show strong guarantees of the absolute discounting method (widely used in NLP including in KN smoothing) in a large variety of adaptive distribution families (including the power law distributions). The connection to  Good-Turing performance is especially striking. 

Comments: This is an outstanding paper worthy of an oral slot. It continues the line of research introduced in [OS16]: ""Competitive Distribution Estimation: Why is Good-Turing Good"" and especially the competitive optimality setting for a large family of distributions. The main results are aptly summarized in lines 141-163, where one sees that absolute discounting inherits much of the same properties as Good Turing, especially in the context of power law distribution families. I have no comments on the proofs -- they are very nicely done, of which I found the proof Theorem 6 to be the most involved and creative. 
   One question I have is  regarding the following: in a then-seminal work Yee Whye Teh showed that the hierarchical Chinese restaurant process parameters can be naturally learnt by an absolute discounting method -- this provided a Bayesian context to KN smoothing and modified KN smoothing techniques in language modeling. The standard reference is @inproceedings{Teh2006AHB,
  title={A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes},
  author={Yee Whye Teh},
  booktitle={ACL},
  year={2006}
} but various other summaries and (even a NIPS tutorial) are available online.  How does the present work relate to this body of work? Is there a sense in which the hierarchical Pitman Yor process represent the argument to the minimax settings of this paper? "
A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,"Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel",https://proceedings.neurips.cc/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html,"Summary:
The paper tries to address the policy overfitting issue of independent RL for multiagent games. The authors proposes a conceptual algorithm for learning meta policies in two-player games by extending existing methods and designs metric to measure the overfitting effect of independent RL.

Strengths:
The literature review is thorough.
A new metric evaluating the effectiveness of policy learning for multiagent games manifests some unexplored properties of multiagent RL learning.

Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016. 
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.

Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above.

Reproducibility:
Appears to be reproducible.","The paper proposes a problem with current learning methods multi-agent settings (particularly independent learners setting), which is overfitting to the policies of other agents. A novel method to measure such overfitting is proposed - joint policy correlation. A practical parallel learning algorithm is proposed that significantly reduces overfitting under the proposed measure on gridworld and poker domains.

The paper is well-motivated and the exposition is generally clear. The methods are technically sound and I see no obvious flaws. The background material review is extensive and thorough and references are excellent.

From reading the paper, a few questions still remain:
- it seems that parts of the method rely on all agents sharing the same action space. Would the method be able to generalize to heterogeneous agent populations?
- in the details on projected replicator dynamics, it is still not clear how the described quantities are used to update meta-strategy sigma_i. An explicit question would be useful to solidify this.

The problem of overfitting to other agents is indeed an important one in multi-agent systems and I am happy to see work attempting to quantify and overcome this issue. I believe this work will be of value to the machine learning community and I would like to see it published at NIPS.","Summary:

""A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning"" presents a novel scalable algorithm that is shown to converge to better behaviours in partially-observable Multi-Agent Reinforcement Learning scenarios compared to previous methods. The paper begins with describing the problem, mainly that training reinforcement learning agents independently (i.e. each agent ignores the behaviours of the other agents and treats them as part of the environment) results in policies which can significantly overfit to only the agent behaviours observed during training time, failing to generalize when later set against new opponent behaviours. The paper then describes its solution, a generalization of the Double Oracle algorithm. The algorithm works using the following process: first, given a set of initial policies for each player, an empirical payoff tensor is created and from that a meta-strategy is learnt for each player which is the mixture over that initial policy set which achieves the highest value. Then each player i in the game is iterated, and a new policy is trained against policies sampled from the meta-strategies of the other agents not equal to i. After training, this new policy is added to player i's set of policies. After all agents have trained a new policy, the new entries within the empirical payoff tensor are computed and the meta-strategies are updated. Finally, the process repeats again, training another policy for each agent and so on. This algorithm, called ""Policy-Space Response Oracles"" (PSRO), is shown to be a generalization of several previous algorithms including independent RL, fictitious play and double oracle. An extension is then developed, called Deep Cognitive Hierarchies (DCH), which operates only on a tractable hierarchy of asynchronously updated policies instead of having meta-strategies learnt over the entire set of all previous policies. PSRO and DCH are then empirically evaluated on several 2D grid world games and Leduc poker. To identify overfitting ""joint policy correlation"" matrices are constructed on which the ""average proportional reduction"" metric is computed. This metric makes clear that independent RL does very poorly at generalizing to new opponent behaviours, even when those opponents were trained under identical conditions. Finally, it is shown that PSRO and DCH converge to close to an approximate Nash Equilibrium much faster than Neural Fictitious Self-Play, a related multi-agent algorithm which also uses deep neural network function approximators.

Qualitative Assessment:

I think the paper presents an interesting and novel solution to the important problem of learning generalizable behaviours in multi-agent environments. The algorithm developed is also scalable and lends itself to be used efficiently with modern function approximators like deep neural networks. The experimental results clearly demonstrate the problem with the naive method of independent RL, with the development of the ""joint policy correlation"" matrices showing that independent RL gets far lower reward when set against a novel behaviour. It is interesting to see how brittle RL algorithms are if other agents are treated simply as part of the environment. The connections between PSRO and previous methods such as double oracle and fictitious play are informative and give an intuition on what each method is actually computing in an easily comparable way. The related work cited is extensive and covers a wide variety of methods. While it quickly covers a lot of ground, the writing is still clear and easy to follow. The hyperparameter settings are given in the appendix to help others reproduce the work.

I have some clarifying questions:
1) Is there a strict reason for using a hierarchy of policies in DCH? Why restrict levels from only considering oracles in previous levels? I understand there is some intuition in that it resembles Camerer, Ho, & Chong’s cognitive hierarchy model, but I was wondering if there was another reason to impose that constraint.
2) In Figure 5 (a) and (b), it seems like NFSP has not plateaued completely. Does NFSP converge to roughly the same NashConv as PSRO/DCH, or a slightly better solution? It would be informative to see the long term performance (until convergence) of all methods. 
3) The DCH algorithm seems to have a large number of hyperparameters, with RL agents and decoupled meta-solvers needing to be trained asynchronously. How sensitive is the algorithm's performance to hyperparameters? Did it require a large hyperparameter search to get it working? 

Typos:
1) There is a sentence fragment at line 280.
2) At line 266, should the upper-case sigma be an upper-case delta (meta-strategy mixture)?"
Spectral Mixture Kernels for Multi-Output Gaussian Processes,"Gabriel Parra, Felipe Tobar",https://proceedings.neurips.cc/paper/2017/hash/333cb763facc6ce398ff83845f224d62-Abstract.html,"This paper presents a multi-output Gaussian process co-variance
function that is defined in the spectral domain.  The approach of
designing co-variances in the spectral domain is an important
contribution to GPs that I personally feel have not gotten the
attention that it deserves. This paper extends this to the
multi-output scenario. The paper is an extension of the work presented
in [1]. However, considering the multi-output case the challenge is
how to describe cross-covariance which is does not have to be
positive-definite. In this paper the authors describes just such an
approach in a manner that leads to the spectral mixture covariance for
multi-output GPs. 

The paper is well written with a short and clear introduction to the
problem and to GP fundamentals. The explanation of the spectral
mixture kernel is clear and the extensions proposed are to my
knowledge novel. The relationship to previous work is well explained
and I believe that there is sufficient novelty in the work to justify
publication.

The experiments are sufficient for a paper such as this, they do not
provide a lot of intuition however, especially the Jura data-set is
hard to actually reason about. But, for a paper such as this I think
the experimental evaluation is sufficient.

-[1] Wilson, A. G., & Adams, R. P. (2013). Gaussian process kernels for
  pattern discovery and extrapolation. In , Proceedings of the 30th
  International Conference on Machine Learning, {ICML} 2013, Atlanta,
  GA, USA, 16-21 June 2013 (pp. 1067–1075). : JMLR.org.","The paper formulates a new type of covariance function for multi-output Gaussian processes. Instead of directly specifying the cross-covariance functions, the authors propose to specify the cross-power spectral densities and using the inverse Fourier transform to find the corresponding cross-covariances. The paper follows a similar idea to the one proposed by [6] for single output Gaussian processes, where the power spectral density was represented through a mixture of Gaussians and then back-transformed to obtaining a powerful kernel function with extrapolation abilities. The authors applied the newly established covariance for a synthetic data example, and for two real data examples, the weather dataset and the Swiss Jura dataset for metal concentrations. 

Extending the idea from [6] sounds like a clever way to build new types of covariance functions for multiple-outputs. One of the motivations in [6] was to build a kernel function with the ability to perform extrapolation. It would have been interesting to provide this covariance function for multiple-outputs with the same ability. The experimental section only provides results with very common datasets, and the results obtained are similar to the ones obtained with other covariance functions for multiple-outputs. The impact of the paper could be increased by including experiments in datasets for which none of the other models are suitable. 

Did you test for the statistical significance of the results in Tables 1 and 2. It seems that, within the standard deviation, almost all models provide a similar performance. 
","The paper introduces a new covariance structure for multioutputs GPs, which corresponds to the generalisation of the spectral approach (Bochner Theorem) to build kernels from A. G. Adams and R. P. Adams. One particular asset of the proposed method is that the parameters of the model can be interpreted (such as delay between outputs or phase difference).

The definition of covariance structures for multioutputs GPs is a challenging topic and relatively few methods are currently available to do so. The method proposed by the authors is theoretically sound and its effectiveness is demonstrated on several datasets. 

The method is clearly described in a well written paper, and the choices for the illustrations appear relevant to me. If the parameter learning isn't too troublesome (the paper does not contain much informations regarding this), I would expect this method to become a standard for multioutput GPs.

Questions 
 * How does the method behaves when few training points are available?
 * (more a curiosity) A channel with a large length-scale cannot be highly correlated with a channel with a small length-scales. How does that appear in your model?

Comments
 * In the synthetic example (Section 4.1), the number of training points should be specified

Minor remarks: I guess a word is missing in lines 74 and 140."
Affine-Invariant Online Optimization and the Low-rank Experts Problem,"Tomer Koren, Roi Livni",https://proceedings.neurips.cc/paper/2017/hash/347665597cbfaef834886adbb848011f-Abstract.html,"Summary:
This paper proposes a new optimization algorithm, Online Lazy Newton (OLN), based on Online Newton Step (ONS) algorithm. Unlike ONS which tries to utilize curvature information within convex functions, OLN aims at optimizing general convex functions with no curvature.  Additionally, by making use of low rank structure of the conditioning matrix, the authors showed that OLN yields better regret bound under certain conditions. Overall, the problem is well-motivated and the paper is easy to follow. 
 
Major Comments:
1. The major difference between OLN and ONS is that ONS introduces a lazy evaluation step, which accumulates the negative gradients at each round.  The authors claimed in Lines 155-157 that this helps in decoupling between past and future conditioning and projections and it is better in the case when transformation matrix is changing between rounds.  It would be better to provide some explanations.
 
2. Lines 158-161, it is claimed that ONS is not invariant to affine transformation due to its initialization.  In my understanding, the regularization term is added partly because it allows for an invertible matrix and can be omitted if Moore-Penrose pseudo-inverse is used as in the FTAL.
 
3. Line 80 and Line 180, it is claimed that  O(\sqrt(r T logT)) is improved upon O(r \sqrt(T)). The statement will hold under the condition that r/logT = O(1), is this always true?
 
Minor:
1. Lines 77-79, actually both ONS and OLN utilizes first-order information to approximate second-order statistics. 
 
2. Line 95, there is no definition of the derivative on the right-hand-side of the equation prior to the equation.
 
3. Line 148-149, on the improved regret bound.  Assuming that there is a low rank structure for ONS (similar to what is assumed in OLS), would the regret bound for OLS still be better than ONS?
 
4. Line 166, 'In The...' -> 'In the...'
 
5. Line 181, better to add a reference for Hedge algorithm.
 
6. Line 199,  what is 'h' in '... if h is convex ...'?
 
7. Line 211-212,  '...all eigenvalues are equal to 1, except for r of them...', why it is the case?  For D = I + B defined in Line 209, the rank for B is at most r, then at least r of the eigenvalues of D are equal to 1.
 
The regret bound depends on the low rank structure of matrix A as in Theorem 3, another direction that would be interesting to explore is to consider the low rank approximation to the matrix A and check if similar regret bound can be derived, or under which conditions similar regret bound can be derived.  I believe the proposed methods will be applicable to more general cases along this direction.
 ","This paper considers online learning problem with linear and low-rank loss space, which is an interesting and relative new topic. The main contribution lies in proposing an online newton method with a better regret than [Hazan et. al. 2016], namely O(\sqrt(rT logT) vs. O(r\sqrt(T)). And the analysis is simple and easy to follow.

There are a few concerns listed below.
1.	The better regret bound is arguable. Assuming low rank, ‘r’ is typical small while ‘T’ could be very large in online setting. Thus, comparing with existing work, which establishes regrets of O(r\sqrt(T)) and O(\sqrt(rT)+logNlogr)), the achieved result is not very exciting. 
2.	Algorithm 1 requires a matrix inverse and solving a non-linear programming per iteration, and a sub-routine appears inevitable for most problems (e.g. the low rank expert example with domain `simplex’). Such complexity prevents the algorithm from real online applications and limits it in analysis. 
3.	The reviewer appreciates strong theoretical work without experiments. However, the presented analysis of this paper is not convincing enough under NIPS criterion. A empirical comparison with AdaGrad and [Hazan et. al. 2016] would be a nice plus.

5.	`A_t^{-1}’ in Algorithm 1 is not invertible in general. Is it a Moore–Penrose pseudoinverse? And does a pseudoinverse lead to a failure in analysis? 
6.	Though the authors claim Algorithm 1 is similar to ONS, I feel it is closer to `follow the approximate leader (ver2)’ in [Hazan et.al. 2006]. Further discussion is desirable.  

Overall, this work makes a theoretical step in special online setting and may be interesting to audiences working in this narrow direction. But it is not very exciting to general optimization community and appears too expensive in practice. 

*************
I read the rebuttal and removed the comment on comparison with recent work on arxiv.
Nevertheless, I'm still feel that O(\sqrt(rT logT) is not a strong improvement over  O(r\sqrt(T)), given that T is much larger than r. 
An ideal answer to the open problem should be O(\sqrt(rT) or a lower bound showing that O(\sqrt(rT logT) is inevitable.","The paper analyzes a particular variant of online Newton algorithm for online
linear(!) optimization. Using second-order algorithm might sound non-sensical.
However, the point of the paper is to compete with the best preconditioning of
the data. The main reason for doing this is to solve the low rank expert
problem. (In the low-rank expert problem, the whole
point is to find a basis of the loss matrix.)

The main result of the paper is an algorithm for low rank expert problem that
has regret within sqrt(log T) of the lower bound for the low rank experts
problem. In particular, it improves sqrt{r} factor on the previous algorithm.

Large parts of the analysis are the same as in Vovk-Azoury-Warmuth forecaster
for online least squares (see e.g. the book Gabor Lugosi & Nicolo Cesa-Bianchi),
the second-order Perceptron (""A second-order perceptron "" by Nicolo
Cesa-Bianchi, Alex Conconi, and Claudio Gentile), or the analysis of Hazan &
Kale for exp-concave functions. I suggest that authors reference these papers.
The second-order perceptron paper is particularly relevant since it's a
classification problem, so there is no obvious second-order information
to use, same as in the present paper.

Also, the paper ""Efficient Second Order Online Learning by Sketching""
by Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, John Langford
is worth mentioning, since it also analyzes online Newton's method
that is affine invariant i.e. the estimate of the Hessian starts
with zero (see https://arxiv.org/pdf/1602.02202.pdf Appendix D).

The paper is nicely written. I've spot-checked the proofs. They look correct. At
some places (Page 7, proof of Theorem 1), inverse of non-invertible matrix A_t
is used. It's not a big mistake, since a pseudo-inverse can be used instead.
However, this issue needs to be fixed before publication.
"
Pose Guided Person Image Generation,"Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool",https://proceedings.neurips.cc/paper/2017/hash/34ed066df378efacc9b924ec161e7639-Abstract.html,"Summary:

The paper proposes an architecture for generating a images containing a person that combines the appearance of a person in an input image and a specified input pose (using heat maps).


Strengths:

+ the proposed task is novel

+ novel two stage coarse-to-fine architecture

+ well written

+ while the results contain discernible artifacts, they are promising

+ extensive evaluation of design decisions


Weaknesses:

- the quantitative evaluation is generally weak; this is understandable (and not a major weakness) given that no suitable evaluation metrics exist


Comments:

Given the iterative refinement nature of the proposed architecture, it is suggested that some recent work on iterative refinement using cascaded networks be included in the related work, e.g., [a,b]:

[a] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. H man pose estimation with iterative error feedback. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[b] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In European Conference on Computer Vision (ECCV), 2016.

- line 110: ""we find that using residual blocks as basic component improves the generation performance""  In what way?  Please elaborate.

While the evaluation is generally extensive, I was left curious how using the Stage 2 network with the inputs for Stage 1 and trained adversarially would perform.  Is the blurring observed with Stage 1 + GAN due to the intermediate fully connected layer?

Overall rating:

Based on the novelty of the problem and approach I am confidently recommending that the paper be accepted.

Rebuttal:

While this reviewer appreciates the attempt at a user study, given its very preliminary nature, I do not find it convincing and thus am discount it in my review rating.  The authors are urged to include a substantive human evaluation in their revision.   Nonetheless, my positive review of the work still stands.","The paper proposes a human image generator conditioned on appearance and human pose. The proposed generation is based on adversarial training architecture where two-step generative networks that produces high resolution image to feed into a discriminator. In the generator part, the first generator produce a coarse image using a U-shape network given appearance and pose map, then the second generator takes the coarse input with the original appearance to predict residual to refine the coarse image. The paper utilizes the DeepFashion dataset for evaluation.

The paper proposes a few important ideas.

* Task novelty: introducing the idea of conditioning on appearance and pose map for human image generation
* Techniques: stacked architecture that predicts difference map rather than direct upsampling, and loss design

The paper can improve in terms of the following points to stand out.

* Still needs quality improvement
* Significance: the paper could be seen one of yet-another GAN architecture
* Problem domain: good vision/graphics application, but difficult to generalize to other learning problems

The paper is well organized to convey the key aspects of the proposed architecture. Conditioned on appearance and pose information, the proposed generator stacks two networks to adopt a coarse-to-fine strategy. This paper effectively utilize the generation strategy in the dressing problem. The proposed approach looks appropriate to the concerned problem scenario. The difference map generation also looks a small but nice technique in generating higher resolution images.

Probably the major complaints to the paper is that the generated results contain visible artifacts and still requires a lot of improvement for application perspective. For example, patterns in ID346 of Fig 4 results in black dots in the final result. Even though the second generator mitigates the blurry image from the first generator, it seems the model is still insufficient to recover high-frequency components in the target appearance.

Another possible but not severe concern is that some might say the proposed approach is an application of conditional GANs. Conditioning or stacking of generators for adversarial training have been proposed in the past; e.g., below, though they are arXiv papers. The paper includes application-specific challenges, but this might not appeal to large number of audiences.

* Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, Dimitris Metaxas, ""StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks"", arXiv:1612.03242.
* Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie, Stacked Generative Adversarial Networks, arXiv:1612.04357.

In overall, the paper successfully proposed a solution to the pose-conditioned image problem, and properly conducts evaluation. The proposed approach sufficiently presents technical novelty. The resulting images still needs quality improvement, but the proposed model at least generate something visually consistent images. My initial rating is accept.","This paper develops a GAN-type system to generate a novel-view person images guided by a target skeleton (represented as keypoints). The system contains mostly known techniques such as DCGANs, U-nets, so the contribution is about the specific computer vision application. 
Pros:
* The proposed idea is interesting, the writing is easy to follow, and empirical experiments show that it is able to synthesize images that are realistic-looking at some level.
Cons:
* As this work is application oriented, it is important to achieve good results. My main complain is 
	** there is no quantitative evaluation. For example, is it possible to organize user studies to evaluate how many times the generated images can trick human eyes.
	** the empirical results are not visually appealing. Take fig.1 for example, the generated images are too small to see. Even this way, there are many noticeable visual errors when comparing the results to the targets. It is thus a bit discouraging to conclude that the results so far are of good quality or close to real images. The same issue stays for almost all the results including the supplementary figures. Personally I feel the authors are at the right direction and the work is promising, while it is yet to stop and declare victory at this stage.

After rebuttal:
I am still not convinced after reading the authors' feedback. In particular, it has also been pointed by other reviewers that there are still issues regarding quantitative as well as qualitative results, all is important for such a practical oriented work. The rebuttal shows some attempts but they still do not address these concerns to a satisfied degree. I feel it may not be ready to be well presented in a venue like NIPS."
Successor Features for Transfer in Reinforcement Learning,"Andre Barreto, Will Dabney, Remi Munos, Jonathan J. Hunt, Tom Schaul, Hado P. van Hasselt, David Silver",https://proceedings.neurips.cc/paper/2017/hash/350db081a661525235354dd3e19b8c05-Abstract.html,"This paper combines ideas from Successor Representation and a generalization of the famous policy improvement step of Policy Iteration framework to propose a novel transfer learning algorithm, in the context of MDPs wherein the transition dynamics remain fixed but the reward function changes. The crux of the idea is a kind of two stage process:
Stage1: A decoupled representation based model for the Q-function is used to learn successor features and the underlying reward vector for the MDP separately. Stage2: If transfer to task n+1 is to be shown, optimal successor features and resultant Q* functions are built for tasks 1...n. The resultant optimal Q* functions for task T_1.. T_n are used to perform a generalized form of policy improvement step in task n+1. Let the resultant policy be denoted by \pi. The authors demonstrate theoretical results for how close Q_{\pi} is to Q* for the n+1 task.  The proposed method is empirically demonstrated on a gridworld task, as well as a reaching task. The use of successor representation for transfer is not novel, and this has appeared in various guises in the literature as pointed out in the nice lit review section. What appears to be novel is the particular representation mechanism chosen (SF) and the consequent applicability to continuous domains. 

The paper is very well written. The ideas are very clearly presented, and it was a pleasure to read. The empirical results were useful and the baselines used were reasonably powerful. The analysis is sufficiently detailed. The paper does make the point repeatedly, that this method is useful for building a library of skills. While one can see conceptually that this might be so, the experiments do not demonstrate this conclusively. There are many issues that need to be addressed as pointed out by the authors themselves to make this a functional option learning system. 


","This paper presents a RL optimization scheme and a theoretical analysis of its transfer performance. While the components of this work aren't novel, it combines them in an interesting, well-presented way that sheds new light.

The definition of transfer given in Lines 89–91 is nonstandard. It seems to be missing the assumption that t is not in T. The role of T' is a bit strange, making this a requirement for ""additional transfer"" rather than just transfer. It should be better clarified that this is a stronger requirement than transfer, and explained what it's good for — the paper shows this stronger property holds, but never uses it.

Among other prior work, Theorem 1 is closely related to Point-Based Value Iteration, and holds for the same reasons. In fact, w can be viewed as an unnormalized belief over which feature is important for the task (except that w can be negative).

The corollary in Lines 163–165 can be strengthened by relaxing the condition to hold for *some* s, s' that have positive occurrences.

The guarantee in (10) only comes from the closest w_j. Wouldn't it make sense to simplify and accelerate the algorithm by fixing this j at the beginning of the task? Or is there some benefit in practice beyond what the analysis shows? Empirical evidence either way would be helpful.

Very much missing is a discussion of the limitations of this approach. For example, how badly does it fail when the feature-space dimension becomes large?

Beyond the two 2D experiments presented, it's unclear how easy it is to find reward features such that similar tasks are close in feature-space. This seems to go to the heart of the challenge in RL, rather than to alleviate it.

The connection drawn to temporal abstraction and options is intriguing but lacking. While \psi is clearly related to control and w to goals, the important aspect of identifying option termination is missing here.","This paper introduces a transfer learning method for MDPs with the same dynamics but different reward functions. It achieves this by using a representation that decouples rewards from MDP dynamics and combining this representation with a simple, but novel way of combining policies.

The paper describes 2 main contributions: successor features and generalized policy improvement. Both ideas are extensions of existing work (successor states and policy improvement). Nonetheless, I feel both constitute a significant contribution.  Successor features offer a principled way to generalize over families of tasks and their value functions. Though similar ideas have been explored in universal option models and UFVAs (as the authors also mention), the authors demonstrate that SF provide a very method for the transfer learning setting. The generalized policy improvement (Theorem 1) extends Bellman’s policy improvement and seems likely to find more applications in both multi-task and parallel learning settings. The paper combines both methods into an elegant approach to multi-task learning, which is supported with both theoretical and empirical results. 

The paper is very well written. It clearly introduces background and provides intuitive explanations as well as formal results for the presented ideas. The experiments provide thorough evaluations on a simple and a more involved testbed. The methods are compared both to a base learner and a simple transfer method. Overall, the experiments convincingly demonstrate the merit of the proposed method.
"
On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning,"Xingguo Li, Lin Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao",https://proceedings.neurips.cc/paper/2017/hash/351b33587c5fdd93bd42ef7ac9995a28-Abstract.html,"This paper discusses a difference of convex proximal Newton algorithm for solving sparse non-convex problems.The authors claim that this is the first work in second order approaches for high-dimensional sparse learning for both convex and non-convex regularizers. They also provide statistical and computational guarantees.

However, the paper has certain weaknesses that are addressed below:

* Most of the theoretical work presented here are built upon prior work, it is not clear what is the novelty and research contribution of the paper. 

* The figures are small and almost unreadable

* It doesn't clearly state how equation 5, follows from equation 4

* It is not clear how \theta^{t+1/2} come into the picture. Explain

* S^{*} and S^{~} are very important parameters in the paper, yet they were not properly defined. In line 163 it is claimed that S^{~} is defined in Assumption 1, but one can see the definition is not proper, it is rather cyclic.

* Since the comparison metric here is wall-clock time, it is imperative that the implementation of the algorithms be the same. It is not clear that it is guaranteed. Also, the size of the experimental data is quite small. 

* If we look into the run-times of DCPN for sim_1k, sim_5k, and sim_10k and compare with DC+ACD, we see that DCPN is performing better, which is good. But the trend line tells us a different story; between 1k and 10k data the DCPN run-time is about 8x while the competitor grows by only 2x. From this trend it looks like the proposed algorithm will perform inferior to the competitor when the data size is larger e.g., 100K. 

* Typo in line 106
 ","Summary:
This paper considers a DC (difference of convex) approach to solve *sparse* ERM problems with a non-convex regularization (in high dimensions). The specific non-convex regularizer studied in this paper is the capped l1 penalty. Now, given a particular convex relaxation via the DC approach, this paper proposes to solve the problem using proximal Newton. 

The paper analyses the l2 convergence of their solution (at any step of the convex relaxation) to the truth, and shows that it enjoys quadratic convergence to stastical rates. The analysis is contingent on a Sparse Eigenvalue (similar to RSC) condition and a restricted Hessian smoothness condition. 

Comments:

Overall, this is a well written paper and the results presented seem sound. I do however have the following questions/comments:

1. What is T in the warm initialization step of the proposed algorithm ? What does it scale as i.e. what is its dependence on other constants involved in the proof ?
2. It would be good if the authors could provide a more detailed comparison with [36] in the related work. It seems that [36] also considers proximal Newton. However, [36] only considers a convex regularizer. Is another consequence of this work also that the analysis of [36] can be improved with weaker conditions of this paper ?
3. Theorem 6 shows that for large enough K, and small s' (i.e. none or few weak signals), the algo in this paper can yield a l2-rate of O(\sqrt{s*/n}). However, if I am not wrong, there is also a minimax lower bound of O(\sqrt{s* log d / n}) i.e. an extra log d factor (for example, see Theorem 1(b) in https://arxiv.org/abs/0910.2042). Am I missing something here ? How can these two rates be reconciled ?
4. In the experiments, the authors should specify how \beta (the tuning parameter for capped l1) was chosen.

EDIT: I have seen the authors' response, and my rating stands.","The authors present a DC proximal Newton algorithm for nonconvex regularized sparse learning problems in high dimensions. The method is shown to obtain local quadratic convergence at each stage of convex relaxation while maintaining solution sparsity by exploiting characteristic structures of sparse modelling (i.e., restricted strong convexity and Hessian smoothness). 

This paper is very clear and I find it very interesting. To the best of my knowledge, the details of the analysis are sound and this is a significant contribution towards our understanding of how second-order methods in high dimensions have superior performance (both empirically and computationally) despite the lack of theoretical analysis in the past for nonconvex regularized sparse modelling approaches.

Questions: 
- In the numerical results, how much of the time in the Table 1 results was the Warm Initialization step? As I understand, Figure 5 shows the convex relaxation stages after warm initialization. Is that correct? Do the approaches the authors compare against (DC + APG and DC + ACD) also use a warm initialization step?
- Can the authors comment on if there are non-additive L(\theta) functions that might be of interest and if so, how the analysis in this work might extend to this setting?

==============
POST REBUTTAL:
==============
I have read the author rebuttal and the other reviews. I thank the authors for addressing my comments and questions. My recommendation is still for acceptance."
Hypothesis Transfer Learning via Transformation Functions,"Simon S. Du, Jayanth Koushik, Aarti Singh, Barnabas Poczos",https://proceedings.neurips.cc/paper/2017/hash/352fe25daf686bdb4edca223c921acea-Abstract.html,"The papers tackles the issue of domain adaptation regression some target labels are available. They focus on non-parametric regression when the source and target regression functions are different. Basically, they assume there is a transformation function between the source and target regression functions. Given this transformation, the algorithm proposed works as follows.
1. train the source domain regression function (with an algo)
2 construct new ""target"" data thanks to the transformation function G(.,.), the source function and the target sample
3 train an auxiliary function with this new dataset (with another algo)
4 the final regression target model returned by the algorithm is defined as G(source model(x),auxiliary function(x))
They also provide a strong theoretical study and experiments.

PROS:
- The theoretical results (based on stability) seem solid
- The experiments provided are sound

CONS:
- My main concern is related to the fact that the method depends on the transformation function G which can be a disadvantage of the algorithm in practice. (However it is not hidden by the authors and they discuss the definition of such a function.)

","The paper presents a supervised non-parametric hypothesis transfer learning (HTL) approach for regression and its analysis, aimed at the cases where one has plenty of training data coming from the source task and few examples from the target one. The paper makes an assumption that the source and the target regression functions are related through so called transformation function (TF). The TF is assumed to have some parametric form (e.g. linking regressions functions linearly) and the goal of an algorithm is to recover its parameters. Once these parameters are learned, the hypothesis trained on the source task can be transformed to the hypothesis designated for the target task. The paper proposes two ways for estimation of these parameters, that is through kernel smoothing and kernel ridge regression. For both cases the paper presents consistency bounds, showing that one can have an improvement in the exponent of the non-parametric rate once the TF is smoother, in Holder sense, than the target regression function. In addition, the paper argues that one can run model selection over the class of TF through cross-validation and shows some bounds to justify this, which is not particularly surprising. Finally, the paper concludes with numerical experiments on two regression tasks on the real-world small-scale datasets.

The rates stated in main theorems, 2 and 3 appear to make sense. One can see this from theorem 1, which is used to prove both, and essentially states an excess risk as a decomposition into the sum of excess risk bounds for source/TF regression functions. The later are bounded through well established excess risk bounds from the literature.

I do not think this is a breakthrough paper, but it has a novel point as it addresses the theory of HTL in the non-parametric setting because it studies the Bayes risk rates rather than generalization bounds as was done previously. 

The paper has a novel bit as it studies non-parametric consistency rates of HTL, while previous theoretical works focused on the risk bounds. As one should expect, the improvement then occurs in the right place, that is in the exponent of the rate, rather than in some additive or multiplicative term. At the same time the algorithm appears to be simple and easy to implement. This makes this paper a good contribution to the literature on HTL. Also, the paper is quite well positioned in the related work. Unfortunately, it doesn't compare to previous HTL approaches experimentally and experimental evaluation is, frankly, quite modest, despite the simplicity of the algorithm.","This paper considers the hypothesis transfer learning problem, which tries to incorporate a hypothesis trained on the source domain into the learning procedure of the target domain. A unified framework for HTL is proposed by introducing the transformation function, and a theoretical study on the excess risk is given. Moreover, two case studies on kernel smoothing and kernel ridge regression show the faster convergence rates of excess risk.

The paper is well written, the unified framework and the theoretical studies are novel in this field.

My comments are listed as follows:
1) Most examples are simple linear transformation functions. What about nonlinear cases? 
2) In the proposed algorithm, training the auxiliary function from T^{WG} is involved. However, when the number of labeled data in target data is small, this may fail to obtain a good auxiliary function.
3) In Section 5, finding the best transformation function with a specific type is discussed. In practice, it would be also desirable to know how to select the type of transformation function.
4) In the experiments, Figure 2 (a) is redundant, since the results are provided in Table 1, I think the space should be saved to present more details of results on fMRI dataset. 
"
Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting,"Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, Tie-Yan Liu",https://proceedings.neurips.cc/paper/2017/hash/353de26971b93af88da102641069b440-Abstract.html,"This paper extends the finite sample analysis of GTD algorithm in Liu et al.(2015) to the following perspectives:

1) change from i.i.d sampling to Markov sampling condition, which is more realistic
2) change the stepsize setting
3) discussed the impact of experience replay as a way of accelerating the mixing time of the Markov chain

Here are some comments:
1) In proposition 1, it requires to know the smallest eigenvalue of $C$ and ${A^T}{M^{ - 1}}A$, however, in realistic settings, neither $C$ and ${A^T}{M^{ - 1}}A$ are accessible, and only there empirical estimation $\hat{C}$ and ${\hat{A}^T}{\hat{M}^{ - 1}}\hat{A}$ are available. This becomes a weak-point of such analysis.

2) It should be noted that stepsize is not a big contribution compared with Liu et al.(2015). There is sample complexity analysis before by Nemirovski of convex-concave saddle-point with diminishing stepsize (such as Robbins-Monro stepsize). Using constant stepsize with iterative averaging does not imply the analysis can not be applied to diminishing stepsize cases but can provide better convergence rate than using the diminishing stepsize. 

3) Since the paper talks about sampling complexity in Markov settings, an expectation bound seems not necessary in the main part of the paper. The reviewer suggests moving Sec. 3.1 to the Appendix, and enrich Sec. 3.2 from at least the following two:
1. proof sketch part.
2. result compared with the high-prob bound in Liu et al.(2015).

4 The reviewer is especially interested to see the sample complexity comparison between the result in the paper and this paper's analysis method (both applied to Markov sampling settings):
https://arxiv.org/pdf/1703.05376.pdf
It should be noted that GTD/GTD2 can be either single-time-scale or two-time-scale, and TDC is strictly two-time-scale. ","The paper addresses an existing limitation with finite-sample bounds for GTD style algorithms. Most existing results assume that samples come from an i.i.d process instead of the true Markov process. This paper shows that finite-sample (sublinear) convergence can be achieved even when the samples indeed come from a Markov process. The results apply both in expectation and with high probability.

In terms of significance, the paper closes an important gap in the analysis of the GTD style algorithms. It lifts some limitations of previous work. The results unfortunately depend on a number of complex assumptions that are difficult to verify in practice (particularly Assumptions 5 and 6). The analysis also does not seem to offer new insights into how the methods can be improved and or used in practice. The practical impact of this analysis is likely to be limited.

The results are clearly relevant to the NIPS community.

The approach relies mostly on existing techniques but uses them in a novel way to solve a complex problem. Proving the results requires a number of auxiliary random variables and it is difficult to check  its correctness given the space limitations of conference paper. 

The paper is well written in general. Given the technical nature of the results, the authors provide ample intuition behind the approach, challenges, and the significance of the results. 

The presentation does suffer from many language and editing issues that would have to be addressed before publication. There are too many grammatical issues to list them, often words with a different meaning than intended are used,  there are many missing or extra articles, and spaces before commas.

Minor issues:
- line 144: The reference here is oddly placed, making the statement confusing
- line 157: the references are incorrectly formatted
- reply trick instead of replay trick
- theoretical founding instead of theoretical finding
- para 60-68 is particularly rife with problem
- ","It is well known that the standard TD algorithm widely used in reinforcement learning does not correspond to the gradient of any objective function, and consequently is unstable when combined with any type of function approximation. Despite the success of methods like deep RL, which combines vanilla TD with deep learning, theoretically TD with nonlinear function approximation is demonstrably unstable. Much work on fixing this fundamental flaw in RL has been in vain, till the work on gradient TD methods by Sutton et al. Unfortunately, these methods work, but their analysis was flawed, based on a heuristic derivation of the method. 

A recent breakthrough by Liu et al. (UAI 2015) showed that gradient TD methods are essentially saddle point methods that are pure gradient methods that optimize not the original gradient TD loss function (which they do not), but rather the saddle point loss function that arises when converting the original loss function into the dual space. This neat trick opened the door to a rigorous finite sample analysis of TD methods, and has finally surmounted the obstacles in the last three decades of work on this problem. Liu et al. developed several variants of gradient TD, including proximal methods that have improved theoretical and experimental performance. 

This paper is an incremental improvement to Liu et al., in that it undertakes a similar analysis of saddle point TD methods under a more realistic assumption that the samples are derived from a Markov chain (unlike Liu et al., which assumed IID samples). This paper provides an important next step in our understanding of how to build a more rigorous foundation for the currently shaky world of gradient TD methods. The analysis is done thoroughly and it is supplemented with some simple experiments.

The work does not address the nonlinear function approximation case at all. All the current work on Atari video games and deep RL is based on nonlinear function approximations. Is there any hope that gradient TD methods will be shown someday to be robust (at least locally) with nonlinear function approximation?
"
Variational Inference via $\chi$ Upper Bound Minimization,"Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David Blei",https://proceedings.neurips.cc/paper/2017/hash/35464c848f410e55a13bb9d78e7fddd0-Abstract.html,"- The paper proposes an upper bound minimization to be added to the LBO of variational inference (VI), resulting in an optimization problem similar in principle to the Squeeze or the sandwich theorem optimizations. 

- All in all, the idea is interesting and sound. I reckon it would benefit better from comparing to state-of-the-art VI and BBVI algorithms.

- The experiments section needs strenghtening, both in terms of the datasets and problems, and (possibly more importantly) in terms of comparing to state-of-the-art VI algorithms.

- The variance problem mentioned in the last two lines deserves much more than that, I think. It might be the reason why the method has not been applied to more real-world problems.

- Throughout the paper, the note of the fact this algorithm being able to capture advantages of both EP and VI have been thrown a lot. I do not see an enough evidence that the opposite (having problems from both) wouldn't happen at times. Recall that overdispersen in an abstract sense is not a desirable characteristic. It is only desirable when such overdispersion represents the true p with high fidelity. As much as underdispersion is a problem of VI, overdispersion is aslo at many times a problem of EP and an advantage of VI. Maybe a more rigorous study of such cases (at least empricially) would provide further clarification.

- Which VI version was used in the first two experiments? Why isn't it one of the recently proposed VI algorithms that have low variance, etc? It would be beneficial to see how the proposed algorithm fares against more refined VI versions.

- line 29: ""For example, KLVI for Gaussian process classification ..."": Is that an example of the phrase right before it?

- line 74: ""and also revisits [19]"": This format of citation is not the best since you're now considering [19] a part of the text without adding up the author' names.

- line 198: ""probit classification"": I reckon it is regression, ain't it?

- Refs 21 and 22 belong to the same conference, yet the venue was written differently. More consistence is needed at a NIPS level.","Summary

The paper presents a chi-divergence based stochastic black-box variational approach (motivated as an alternative to EP), which also enjoys zero-avoiding behavior but with an explicit optimization objective function. As opposed to ELBO based approaches, the proposed method is based on minimizing the chi-divergence upper bound of marginal likelihood, termed as CUBO. The proposed optimization scheme is based on exponentiated CUBO such that the Monte Carlo estimation of the upper bound and gradients are unbiased with preserved upper bound guarantee. Experimental evaluation illustrates sandwich plots on both synthetic and real world datasets, and demonstrate improved test error in classification and posterior uncertainty estimates in Cox process, with comparison against Black-box VI and EP.  

Qualitative Assessment

Variational inference approaches based on maximizing ELBO typically underestimate posterior uncertainty. The chi-divergence based approach proposed in this paper favors over-dispersion by minimizing a stochastic approximation of the upper bound of model evidence. 

The proposed approach is closely related to the work of Li and Turner, 2016, as is mentioned and differentiated in this paper. The author drew a nice parallel with the relationship between EP (alpha = 1) and KLVI (alpha = 0), and made the flip of arguments as one distinction against [Li and Turner, 2016]. It is interesting to motivate in this way. However, I found it to be a relatively smaller distinction than claimed. Within the alpha-divergence family, the reverse order of p and q is controlled by the parameter alpha or 1-alpha.  (1) Setting n = 2 in this paper corresponds to set alpha = -1 in [Li and Turner, 2016], both yield exactly the same form of Chi-upper bound. (2) The flip argument in EP and KLVI leads to completely different optimization algorithms, while here the optimization algorithms are both stochastic variational algorithms, with Monte Carlo approximation, and the “reparametrization trick”.

That said, this paper takes a considerably different approach in Monte Carlo approximation. Li and Turner, 2016 uses a naive Monte Carlo approximation on the objective. The estimation is biased. While it works fine for the lower bound when alpha > 0, it becomes troublesome when alpha < 0. First, due to the concavity of logarithm and Jensen inequality, the upper bound guarantee is broken. Second, the bias increases with increasing sample size. This paper avoids these issues by taking the exponential form of the objective and therefore provides unbiased updates. This scheme provides a valuable addition to [Li and Turner, 2016] by being capable of handling upper bound cases in a principled way. But I do have some reservations about whether the exponential form would induce high variance or possible numerical instability as an expense in practice. 

The sandwich theorem looks very vague and informal. Better specify the range of n in the first two bullets of Theorem 1. The sandwich plots are very interesting. This might be useful for better model selection or hyperparameter optimization in future work. 

Another suggestion is the presentation of posterior variance estimation results in Section 3.3. The heat maps might be of practical interest, but they do not offer a direct visual clue about whether or not over-or under-estimation occurs and if so, to what extent. Perhaps, a more informative choice would be scatterplots with BBVI, CHIVI, and EP against HMC, respectively. Also, why are the EP results in Table 3 missing? 

Minor issues: 
- Line 83, no positive value of alpha … leads to the chi-divergence that we minimizing in this paper. What about negative values? 
- Line 85: should clarify monotonicity in what? Order?
- Line 109-120: the zero-avoiding behavior is described in a mixed context of KL and chi-divergence. Better focused on chi-divergence as the main concern of this paper. 
- Line 125-126: typo, the last p(z|x) should be p(x, z)
- Line 171: to be more clear, consider change ""hold"" to ""exist/matter""

Overall, this paper is closely related to the work [Li and Turner, 2016] with a few considerable distinctions made. The proposed chi-divergence method is mainly motivated as an alternative to EP, and the sandwich property of CUBO is illustrated in sandwich plots on both synthetic and real datasets. The exponential objective with unbiased Monte Carlo approximation addresses several issues of [Li and Turner, 2016], and could be useful in handling other upper bound cases as well. There is still some room for improvement in the presentation of results.  
","Continuing on the recent research trend of proposing alternative divergences to perform variational inference, the author proposed a new variational objective CUBO for VI. Inference is equivalent to minimizing CUBO with respect to the variational distributions. CUBO is a proxy to the Chi-divergence between true posteriors and variational distributions, and is also an upper bound of the model's log-evidence.

The author claimed that the key advantages of CHIVI over standard VI is the EP-like mass covering property of the resulted variational approximations that tend to over-estimate posterior variance instead of under-estimation as with standard VI. Compared to EP, CHIVI optimizes a global objective function that is guaranteed to converge. The author also claimed that CUBO and ELBO together form a sandwich bound of the marginal likelihood that can be used for model selection. The technical content of the paper appears to be correct albeit some small careless mistakes that I believe are typos instead of technical flaw (see #4 below).

The idea of having a sandwich bound for the log-marginal likelihood is certainly good. While the author did demonstrate that the bound does indeed contains the log-marginal likelihood as expected, it is not entirely clear that the sandwich bound will be useful for model selection. This is not demonstrated in the experiment despite being one of the selling point of the paper. It's important to back up this claim using simulated data in experiment.

Another key feature of CHIVI is the over-dispersed approximation that it produces, which the author claimed lead to better estimates of the posterior. I believe a more accurate claim would be 'lead to more conservative estimates of the posterior uncertainty', as the goodness of approximation is fundamentally limited by the richness of the variational distributions, which is orthogonal to the choice of the variational objective function. The author provided some evidence for the claim in the Bayesian probit regression, GPC and the basketball player experiments. However, while it is certainly believable that the 'goodness of posterior approximations' plays a role in the test classification error of the first two experiments, the evidence is a little weak because of the discrete nature of the models' outputs. The author can perhaps make a stronger case by demonstrating the superior posterior uncertainty approximation through Bayesian neural network regression, in which the test LL would be more sensitive to the goodness of approximation, or through active learning experiment in which good posterior uncertainty estimates is crucial. There are examples in both https://arxiv.org/pdf/1502.05336.pdf and https://arxiv.org/pdf/1602.02311.pdf.
The basketball player example provides pretty good visualisations of the posterior uncertainty, but I'm not entirely sure how to interpret the quantitative results in Table 3 confidently. (e.g., Is the difference of 0.006 between CHIVI and BBVI for 'Curry' a big difference? What is the scale of the numbers?)

One aspect that I find lacking in the paper is how computationally demanding CHIVI is compared to standard VI/BBVI? A useful result to present would be side-by-side comparisons of wall clock time required for CHIVI, BBVI and EP in the first two experiments. The variance of the stochastic gradients would certainly play a role in the speed of convergence, and as the author mentioned in the conclusion, the gradients can have high variance. I am quite curious if the reparameterization trick would actually result in lower variance in the stochastic gradients compared to score function gradient when optimizing CUBO?

One important use of the ELBO in standard VI is to learn the model's parameters/hyper-parameters by maximizing ELBO wrt the parameters. This appears to be impossible with CUBO as it's an upper bound of the log-marginal likelihood. The author used grid search to find suitable hyper-parameters for the GPC example. However, this is clearly problematic even for a moderately large number of hyper-parameters. How can this be addressed under the CUBO framework? What is the relationship between the grid search metric (I assume this is cross-validation error?) and the sandwich bound for the parameters on the grid?

While the paper is pretty readable, there is certainly room for improvements in the clarity of the paper. I find paragraphs in section 1 and 2 to be repetitive. It is clear enough from the Introduction that the key advantages of CHIVI are the zero avoiding approximations and the sandwich bound. I don't find it necessary to be stressing that much more in section 2. Other than that, many equations in the paper do not have numbers. The references to the appendices are also wrong (There is no Appendix D or F). There is an extra period in line 188.

The Related Work section is well-written. Good job!

Other concerns/problems:
1. In the paper, a general CUBO_n is proposed where n appears to be a hyper-parameter of the bound. What was the choice of n in the experiments? How was it selected? How sensitive are the results to the choice of n? What property should one expect in the resulted approximation given a specific n? Is there any general strategy that one should adopt to select the 'right' n?

2. In line 60, what exactly is an 'inclusive divergence'?

3. In line 118 and 120, the author used the word 'support' to describe the spread of the distributions. I think a more suitable word here would be 'uncertainty'.

4. The equation in line 125 appears to be wrong. Shouldn't there be a line break before the last equal sign, and shouldn't the last expression be equal to E_q[(\frac{p(z,x)}{q(z)})^2]?

5. Line 155: 'call' instead of 'c all'

6. Line 189: The proposed data sub-sampling scheme is also similar to the scheme first proposed in the Stochastic Variational Inference paper by Hoffman et. al., and should be cited? (http://jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf)

7. Line 203: Does KLVI really fail? Or does it just produce a worse approximations compared to Laplace/EP? Also , 'typically' instead of 'typical'.

8. Line 28/29: KLVI faces difficulties with light-tailed posteriors when the variational distribution has heavier tails. Does CHIVI not face the same difficulty given the same heavy-tailed variational distributions?
"
A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks,"Qinliang Su, xuejun Liao, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/35936504a37d53e03abdfbc7318d9ec7-Abstract.html,"Overview: this paper introduces the Truncated Gaussian (TruG) unit (as per eq 1, and its expectation in eq 2). By adjusting the cutoffs \xi_1 and \xi_2 this can mimic ReLU and sigmoid/tanh units.  It can be used as a stochastic unit in RBMs (sec 3), in temporal RBMs (sec 4), and in the TGGM (truncated Gaussian graphical model, sec 5).

An old but relevant reference is ""Continuous sigmoidal belief networks trained using slice sampling"" B. J. Frey, in M. C. Mozer, M. I. Jordan and T. Petsche (eds), Advances in Neural Information Processing Systems 9, 452-459, January 1997.  http://www.psi.toronto.edu/~psi/pubs2/1999%20and%20before/continuous_sigmoidal_belief_networks_tra_1613871.pdf

One might criticise this paper by saying that once one has come up with the TruG unit, one simply has to ""turn the handle"" on the usual derivations to get TruG-RBMs, temporal TruG-RBMs and TruG-TGGMs.

One things that I find odd is that is very well known in statistical physics that one can covert between 0/1 sigmoid coding and -1/1 tanh coding for binary spin systems by a rescaling of weights and some additive offsets. (This can often be a tutorial exercise for a Boltzmann machines class.)  So I find it surprising that in Table 1 there is not more agreement between the TruG[0,1] and TruG[-1,1] results.

In am also surprised that the experimental results for TruG-RBM shows such differences between the TruG[0,1] and TruG[-1,1] and the RBM results from [21] and [22], given the closeness in Figs 1(b) and 1(c).  The results in table 1 with learned truncation points show modest gains over the ReLU-like TruG[0,\inf] model.  For the TruG-temporal RBM we again see gains (probably larger here) over the TRBM and RTRBM baselines. Similar conclusions also apply to Table 4 for the TruG-TGGM.  Overall the results feel like modest gains, but almost always with the differences in favor of TruGs.

Quality: I believe the paper is technically correct (although I have not checked the math).

Clarity: clearly written.

Originality: one can invent Boltzmann machines with many different types of units. This feels like ""adding another one"", but it is quite nice how the TruG can mimic both ReLU and sigmoidal units.

Significance: it is not clear that this will have very much impact, although (the deterministic version of) such units could be used in standard deep networks, allowing a smooth transition from sigmoid to ReLU units.

","Summary: The paper uses doubly truncated Gaussian distributions to develop a family of stochastic non-linearities parameterized by the truncation points. In expectation, different choices of truncation points are shown to recover close approximations to popularly employed non-linearities. Unsurprisingly, learning the truncation points from data leads to improved performance.

Quality and Clarity —  The authors present a technically sound piece of work. The proposed family of non-linearities are sensible and the authors do a good job of demonstrating their utility across a variety of problems. The manuscript is sufficiently clear. 

Originality and Significance - The connection between the convolution of a Gaussian with an appropriately truncated Gaussian and popular neural network nonlinearities are well known and have been previously explored by several papers in supervised contexts (see [1, 2, 3]). In this context, learning the truncation points is a novel but obvious and incremental next step. To me, the interesting contribution of this paper lies in its empirical demonstration that learning the non-linearities leads to tangible improvements in both density estimation and predictive performance. It is also interesting that the learned non-linearities appear to be sigmoid-relu hybrids. 

Detailed Comments:
1) Figures 2 indicates that the learned upper truncations are all relatively small values, in the single digits. This could be a consequence of the initialization to 1 and the low learning rates, causing the recovered solutions to get stuck close to the initial values. Were experiments performed with larger initial values for the upper truncation points? 

2) a) Why are the s-learn experiments missing for TruG-TGGM? 
   b)  Table 4 seems to suggest that stochasticity alone doesn’t lead to significant improvements over the ReLU MLP. Only when learning of non-linearities is added does the performance improve. What is unclear from this is whether stochasticity helps in improving performance at all, if the non-linearities are being learned. I will really like to see comparisons for this case with parameterized non-linearities previously proposed in the literature (citations 6/7/8 in the paper) to tease apart the benefits of stochasticity vs having more flexible non-linearities.

Minor:
There exists related older work on learning non-linearities in sigmoidal belief networks [4], which should be cited.

[1] Hernández-Lobato, José Miguel, and Ryan Adams. ""Probabilistic backpropagation for scalable learning of bayesian neural networks."" International Conference on Machine Learning. 2015.
[2] Soudry, Daniel, Itay Hubara, and Ron Meir. ""Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights."" Advances in Neural Information Processing Systems. 2014.
[3] Ghosh, Soumya, Francesco Maria Delle Fave, and Jonathan S. Yedidia. ""Assumed Density Filtering Methods for Learning Bayesian Neural Networks."" AAAI. 2016.
[4] Frey, Brendan J., and Geoffrey E. Hinton. ""Variational learning in nonlinear Gaussian belief networks."" Neural Computation 11.1 (1999): 193-213.","This paper proposes an approach to setting an/or learning the nonlinearities in neural networks, which includes the classical sigmoid, hyperbolic tangent, and rectified linear unit (ReLU) as particular cases. The method widens the object of the learning algorithm from its linear combination weights, to include also the possibility of learning the nonlinearities. The contribution is solid and novel. Although it builds on closely related work in [31,5], I believe it extends those works enough to merit publication at NIPS."
Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation,"Yuhuai Wu, Elman Mansimov, Roger B. Grosse, Shun Liao, Jimmy Ba",https://proceedings.neurips.cc/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html,"I believe the primary contribution of paper is to apply K-FAC to RL environment successively with impressive results compared to competing alternatives.  This is more an application paper of an existing approach in a new context then a theoretical contribution paper, though they do demonstrate how to adapt K-FAC in this actor/critic context which may not be clear to a general user.  

Minor details:

Line 7: ""the Mujoco environment""

Line 14: ""complex behavior skills""

Line 20: ""executing"" -> permitting

Line 35: conjugate gradient -> the conjugate gradient method

Line 40: ""the curvature information""

Line 47: uses ""a"" Kronecker-factor approximation to ""the"" natural policy gradient ""method""

Line 50-53:  Our algorithm asynchronously computes second-order Kronecker matrix statistics and approximate inverses with a computational cost that is only ...

Line 58:  Should argument a depend on t?  a -> a_t

Line 83.  And in practice approximation are used

Line 88.  ""We"" denote the input

Line 108. Dependencies on tau for p is not present in definition.

Line 131.  ""the"" natural gradient ""method"" is 

Line 237.   to baselines A2C and TRPOP

 


","This paper investigates a method to approximating the policy function using a kronecker-factored trust region method. This is applied to deep networks that take as input the image of the game, and uses gradient methods to minimize the discounted cumulative reward. The kronecker factorization appears at each layer, and basically both simplifies the model and makes training time much faster. 

There is very little new theory in this paper; most follows a previous work [15], which is applied to pure deep learning; in contrast, this paper extends to deep reinforcement learning for games with images as state representations.

The main contributions of this paper is in 1) describing how this setup can be applied to deep reinforcement learning, and 2) providing several interesting numerical results on real games. In this aspect I believe the paper is solid; all experiments show clear benefit over existing methods, and from my own experience in reinforcement learning, this problem is well-motivated. The only comment I have is that there is only one work that the author compares against: A2C. Is this really the only other plausible method in this area that attempts to accelerate learning? 

Overall I believe the paper is well-written, the theory at least well-reviewed (if not extended) and the contribution in the experiments is good. 

Add'l comments:
 - lines 220-228: What is meant by Gauss-Newton metric? ","The manuscript discusses an important topic, which is optimization in deep reinforcement learning. The authors extend the use of Kronecker-Factored approximation to develop a second order optimization method for deep reinforcement learning. The optimization method use kronecker-factored approximation to the Fisher matrix to estimate the curvature of the cost, resulting in a scalable approximation to natural gradients. 

The authors demonstrate the power of the method (termed ACKTR) in terms of the performance of agents in Atari and Mujoco RL environments, and compare the proposed algorithm to two previous methods (A2C and TRPO).

Overall the manuscript is well-written and to my knowledge the methodology is a novel application to Kronecker-factored approximation. 
I have the following comments to improve the manuscript:

1- I believe the complexity of the ACKTR, A2C and TRPO algorithms are not the same. The authors compare the performance of these algorithms per iteration in most figures, which shows that ACKTR is superior to the other methods. I suggest that the authors make the same comparison with x-axis representing optimization time instead of number of iterations, thus taking into account the complexity of the different algorithms. This is done in Figure 2 for one of the games. I am proposing here that this time comparison also be performed in the other experiments.

2- I wonder how would a method like ADAM perform relative to ACKTR.

3- The details of the algorithm are distributed at different parts of section 3. I think it is important to include a summary of the algorithm at the end of section 3 or at least in the suppmats to make it clearer.

"
Optimistic posterior sampling for reinforcement learning: worst-case regret bounds,"Shipra Agrawal, Randy Jia",https://proceedings.neurips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html,"Posterior Sampling for Reinforcement Learning: Worst-Case Regret Bounds
=======================================================================

This paper presents a new algorithm for efficient exploration in Markov decision processes.
This algorithm is an optimistic variant of posterior sampling, similar in flavour to BOSS.
The authors prove new performance bounds for this approach in a minimax setting that are state of the art in this setting.


There are a lot of things to like about this paper:

- The paper is well written and clear overall.
- The analysis and rigour is of a high standard. I would say that most of the key insights do come from the earlier ""Gaussian-Dirichlet dominance"" of Osband et al, but there are some significant extensions and results that may be of wider interest to the community.
- The resulting bound, which is O(D \sqrt{SAT}) for large enough T is the first to match the lower bounds in this specific setting (communicating MDPs with worst-case guarantees) up to logarthmic factors.


Overall I think that this is a good paper and will be of significant interest to the community.
I would recommend acceptance, however there are several serious issues that need to be addressed before a finalized version:

- This paper doesn't do a great job addressing the previous Bayesian analyses of PSRL, and the significant similarities in the work. Yes, these new bounds are ""worst case"" rather than in expectation and for communicating MDPs instead of finite horizon... however these differences are not so drastic to just be dismissed as incomparable. For example, this paper never actually mentions that PSRL/RLSVI used this identical stochastic optimism condition to perform a similar reduction of S to \sqrt{S}.

- Further, this algorithm is definitively not ""PSRL"" (it's really much more similar to BOSS) and it needs a specific name. My belief is that the current analysis would not apply to PSRL, where only one sample is taken, but PSRL actually seems to perform pretty well in experiments (see Osband and Van Roy 2016)... do the authors think that these extra samples are really necessary?

- The new bounds aren't really an overall improvement in big O scaling apart from very large T. I think this blurs a lot of the superiority of the ""worst-case"" analysis over ""expected"" regret... none of these ""bounds"" are exactly practical, but their main value is in terms of the insight towards efficient exploration algorithms. It seems a significant downside to lose the natural simplicity/scalability of PSRL all in the name of these bounds, but at a huge computational cost and an algorithm that no longer really scales naturally to domains with generalization.

- It would be nice to add some experimental evaluation of this new algorithm, perhaps through the code published as part of (Osband and Van Roy, 2016)... especially if this can add some intuition/insight to the relative practical performance of these algorithms.



==============================================================================
= Rebuttal comments
==============================================================================

Thanks for the author feedback, my overall opinion is unchanged and I definitely recommend acceptance... but I *strongly* hope that this algorithm gets a clear name separate to PSRL because I think it will only lead to confusion in the field otherwise.

On the subject of bounds I totally agree these new bounds (and techniques) are significant. I do believe it would give some nice context to mention that at a high level the BayesRegret analysis for PSRL also reduced the S -> sqrt{S}.

On the subject of multiple samples the authors mention “An Optimistic Posterior Sampling Strategy for Bayesian Reinforcement Learning, Raphael Fonteneau, Nathan Korda, Remi Munos”, which I think is a good reference. One thing to note is that this workshop paper (incorrectly) compared a version of PSRL (and optimistic PSRL) that resamples every timestep... this removes the element of ""deep exploration"" due to sampling noise every timestep. If you actually do the correct style of PSRL (resampling once per episode) then multiple samples does not clearly lead to improved performance (see Appendix D.3 https://arxiv.org/pdf/1607.00215.pdf )

On the subject of bounds and large T, I do agree with the authors, but I think they should make sure to stress that for small T these bounds are not as good and whether these ""trailing"" terms are real or just an artefact of loose analysis.





","The authors present a reinforcement learning algorithm based on posterior sampling and show that it achieves a worst-case total regret bound that is an improvement over the best available bound by a factor of sqrt(S) for the case of infinite horizon average rewards, thus closing the gap between the known lower bound and achievable upper bound.

The paper is easy to read and technically sound. Although I did not go through the detailed proofs, the argument of how the regret bound is improved via Dirichlet concentration is compelling. The author should provide some details on the differences between the techniques used in proving the worst-case regret bounds compared to those used in proving the existing Bayesian-regret bounds.

Some empirical comparisons between the proposed algorithm and UCRL2 would be helpful in demonstrating the performance difference afforded via posterior sampling.
"
Efficient Second-Order Online Kernel Learning with Adaptive Embedding,"Daniele Calandriello, Alessandro Lazaric, Michal Valko",https://proceedings.neurips.cc/paper/2017/hash/366f0bc7bd1d4bf414073cabbadfdfcd-Abstract.html,"      The paper proposes an efficient second-order online kernel learning mainly by combining KONS and Nystrom method.

      NOVELTY
      The novelty is limited on both the methodological and theoretical contributions. The achieved results do not have profound implication for the advancement of theory and practice.

      WRITING QUALITY
      The English writing and organization of this paper are relatively good.  The reviewer strongly suggests the authors arrange Table 2 in the main paper rather than in Appendix because the experimental results in Table 2 are the core material. 

      COMMENTS
      This paper works on accelerating second-order online kernel learning and matches the desired state-of-the-art regret bound. The proposed method improves KNOS by using some efficient Nystrom methods instead of the true kernel matrix. However, it seems that this manuscript has not been completed, and many time and accuracy comparisons in Table 1 have not been reported. Although this paper claims that the effective dimension is too large to complete the results in line 341, this explanation for the incomplete experiments is not convincing. If the effective dimension is the concern, this paper should provide more theoretical and practical analysis to show how it affects the PROS-N-KONS. In addition, the reviewer does not find the complexity comparisons among the compared methods. Thus, it is not convincing at all t that the proposed method is efficient.

      Also, the reviewer has three concerns on this paper: 
      1) In the experimental setup, how to determine the values of beta, epsilon, and bandwidth in the kernel matrix? For beta and epsilon, the related parameter selection will take much time. If the learning performance is very sensitive to these two parameters, the efficiency of the proposed method decreases. Regarding the bandwidth, it has an effect on both the effective dimension and the learning accuracy. 

      2) The experimental comparison is not convincing. The efficiency is the key contribution of the proposed methods, but the run time comparison with baselines is missing. Moreover, the Dual-SGD is conducted in only one selected dataset, which makes the model comparison less reasonable.
      
      3) For the ridge leverage scores, it is surprising to find that the paper does not cite paper [Alaoui], which originally proposes the ridge leverage scores in Nystrom approximation with the application to regression analysis. Actually, to my understanding,  [Alaoui] only shows the size in Nystrom approximation could be reduced while does not claim their method would be faster than previous methods, and therein the leverage score approximation in  Theorem 4 could cost much time even more than the time spent in Theorem 3 (just check how to satisfy the condition of his Theorem 3 and 4). Hope this concern could be helpful to tackle your negative claim and results in line 341. 

      [Alaoui] El Alaoui, A., Mahoney, M. W. (2014). Fast randomized kernel methods with statistical guarantees. Advances in Neural Information Processing Systems.
","In the paper, the authors propose a second-order online kernel learning that achieves logarithm regret under directional curvature assumption. This algorithm can be viewed as an extension of previous two works: Efficient second order online learning by sketching [12] and Second-order kernel OCO with adaptive sketching [1] (especially the latter one), while the major innovation of this paper is replacing the high-dimensional feature map $\phi$ with a finite-dimensional approximation $\bar{\phi}$. Most of the notations are inherited from [1], like RLS and effective dimension, and the authors also give a fair comparison to the one in [1] in terms of the theoretical regret. However, this cannot be seen from the experimental parts since the algorithm in [1] is not compared as a competitor.  The whole comparison is rather arbitrary since this algorithm is majorly compared with FOGD and NOGD, while Dual-SGD is occasionally compared. 
Besides, the second-order methods are generally slower than first-order methods, but the authors do not report the calculation time for FOGD and NOGD. 

Besides, a minor error in Line 114.
","The authors study the problem of online kernel learning for the purposes of improved efficiency. They remark that first-order methods accumulate O(\sqrt{T}) regret with a per-step cost of O(t) while second-order methods accumulative O(\log(T)) regret with a per-step cost of O(t^2). To improve upon this, the authors propose to perform a Nystrom sketch of the feature map and then perform efficient second-order updates on this approximate RKHS. The authors show that the size of the embedding depends on the effective dimension of the data, which can be much smaller than t. The paper also includes experiments on real datasets showing that their method and some heuristic variants of it are competitive with both first and second-order methods on large data.

This paper is overall well-written and clear. The authors do a good job of motivating the problem they are trying to solve, explaining why their solution is intuitive, and then explaining the theoretical guarantees and how they compare to existing results. The theoretical result is interesting and compelling, and the experimental results seem to validate the theory. 

A few questions and comments:
1) Lines 173-175: The authors assert that using approximate updates and a fixed budget of SV would lose the log(T) regime because the reduction in step-size would slow down. Can they explain this in more detail or provide a reference to this?
2) Lines 285-286: The authors claim that better bounds can be obtained by optimizing Eqn 3 over \gamma. However, isn't d_eff^T(\gamma) unknown?  That would make this ideal impossible, especially in an online setting.
3) Table 1: On the slice dataset, doesn't FOGD outperform B-KONS? 
4) Lines 387-349: The authors argue that frequent restarts prevent PROS-N-KONS from performing well on the smaller datasets. However, doesn't CON-KONS avoid restarts, while performing similarly to PROS-N-KONS? 
5) Citation [7] and [8] appear to be the same.
     "
Solving Most Systems of Random Quadratic Equations,"Gang Wang, Georgios Giannakis, Yousef Saad, Jie Chen",https://proceedings.neurips.cc/paper/2017/hash/36a1694bce9815b7e38a9dad05ad42e0-Abstract.html,"The paper proposes a novel algorithm for phase retrieval that first finds a good initialization with maximum correlation and then refines the solution with gradient-like updates. The algorithm is guaranteed with high probability to converge to the true solution under noiseless setting and with bounded error under noise setting for most Gaussian sensing matrices. A highlight of the contribution is that the guarantee still holds even for small number of measurements near the information-theoretic limit. Experiments also demonstrate its practical success compared with existing methods.

The paper is very well written. It clearly outlines the problem, explains the difference and contributions compared with existing works, and presents good intuition why the algorithm works well.

The contributions from the two stages, both initialization and local refinement, seem to be novel enough to allow better statistical accuracies with few number of measurements near information-theoretic limit and lead better empirical results.","Update:  I've increased my rating from 6-7 because the authors did a good job addressing my concerns, and the distinction between theoretical and empirical claims seems a bit better upon a fresh read.

One more comment:  Given the empirical nature of the experiments, I think the authors should have compared to state-of-the-art phase retrieval methods that are known to out-perform WF methods (like Feinup's HIO method or Gerchberg–Saxton).  It's likely that such methods would perform at the information theoretic limit as well.  

=============================================

This paper presents a (somewhat) new method for solving phase retrieval problems that builds off of other phase retrieval schemes.  The method has 2 phases:
1) Construct an initial guess of the solution using a spectral initialization scheme
2) Refine the guess using a gradient descent method.

The new method is interesting, and empirically the method performs well.  My reservations about the paper stem from the fact that the claims it makes are dramatically over-sold (and possibly untrue, depending on interpretation), and gloss over existing contributions that others have made in this field.   This is a good paper with interesting results.  I rate it as ""marginal accept"" instead of ""accept"" because the presentation is at times misleading and ungenerous to other authors (who's work they build upon extensively). 
1)  The authors appear to be claiming to be the first method to solve ""almost"" all quadratic equations.  However, several other methods exists with similar (identical?) theoretical strength.    The authors don't seem to acknowledge this fact until line 266.
2)  In the abstract and intro, the authors make no distinction between empirical and theoretical guarantees.  This paper provides no theoretical guarantees of achieving the optimal number of measurements.  Any observations of this fact are purely empirical.  Empirical claims are fine with me, but the authors should be up-front about the fact that their results are only observed in experiments (the first time I read this paper the abstract and intro led me to believe you have a *proof* of achieving the lower bound, which you do not).  
3) Speaking of experiments, how did the authors choose the truncation parameters for the TWF comparison?  Did they tune this parameter for optimal performance?  The parameter should be reported to make experiments repeatable.
4) You claim that recovery takes place in time proportional to MxN.  I'm not sure this is true.  The convex optimization step can definitely be done in NxM time, but what about the (very important) initialization step?  I don't think you can make this claim unless you have a constant upper-bound on the amount of work needed to obtain the leading eigenvector (for example a constant bound on the number of power iterations that does not depend on N or M).  The authors should either justify this claim or remove it.  Or maybe the claim is only based on empirical experiments (in which case this should be made clear)?
   Finally, the authors should make clear that truncated WF flow methods have the same runtime bounds as this method (although I think the truncated WF paper avoids making any claims of linear overall runtime because of the issues mentioned above with the initializer).  

Also, you might want to cite the following (optional - I leave this up to the authors):
   - ""Phase Transitions of Spectral Initialization for High-Dimensional Nonconvex Estimation""   This paper does a rigorous analysis of spectral initiation methods, and proves tight bounds that don't require constants.  This is relevant because the empirical results you got in Fig1 are predicted by this theory.
   - On line 66:  You mention that convex methods require large storage and computation.  The recent papers ""Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation"" and ""PhaseMax: Convex Phase Retrieval via Basis Pursuit"" use convexity without the extra storage.  I doubt, though, that these methods can be performed with provably linear time complexity.
   - ""Phase Transitions of Spectral Initialization for High-Dimensional Nonconvex Estimation"" . This is an alternative formulation for truncated WF that has gotten some attention.  I think it's worth mentioning since this is in the same class of algorithms as the proposed method, and is very closely related.
  "
Online Reinforcement Learning in Stochastic Games,"Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu",https://proceedings.neurips.cc/paper/2017/hash/36e729ec173b94133d8fa552e4029f8b-Abstract.html,"The paper considers the problem of online learning in two-player zero-sum stochastic games. The main result is constructing a strategy for player 1 that guarantees that the cumulative rewards will never go below the maximin value of the game by more than a certain bound, no matter what strategy the other player follows. The bound is shown to grow sublinearly in the number of rounds T of the game, and polynomially on other problem parameters such as the diameter, the size of the state and action spaces. The results imply that the proposed algorithm can be used in self-play to compute near-maximin strategies for both players. The algorithm and the analysis are largely based on the UCRL algorithm of Auer and Ortner (2007) and the analysis thereof.

I have reviewed a previous version of the paper for ICML'17, and have to say that the quality of the writing has improved quite a bit; the authors obviously invested a lot of effort into improving the readability of the paper. The paper is of course still very technical with most of the analysis details deferred to the appendix. Being already familiar with the analysis, I cannot objectively judge how easy it is to follow for first-time readers (I believe that it still looks rather intimidating), but I am now entirely convinced that the proofs are correct. Since the previous round of reviews, the authors fixed all the technical issues I've pointed out, so at this point, I have no technical objections.

The contribution itself is novel and interesting: the paper proposes a new (but very natural) regret definition suitable for stochastic games and proposes an efficient algorithm that comes with strong guarantees on the proposed regret notion. The fact that such strong results are actually possible was quite surprising to me.

All in all, I think that this paper is ready to be published.

Detailed comments
=================
-definition of \rho^* after line 83: at this point, the existence of the limit is not ensured. Please at least say that this requires some mild assumptions that will be specified later.
-112: Assumptions 1 and 2: Maybe it would be useful to use different letters for the two diameter notions to avoid confusion. 
-154: ""Player 1 optimistically picks one model""---Well, not entirely optimistically, as the choice of the opponent's policy is done conservatively. Please rephrase for clarity.
-186: This Lemma 4 is very informal; please state explicitly that this is not the full statement of the lemma. Also, the statement itself is written a bit strangely.
-189: ""analyzing techniques"" -> ""analysis techniques""
-195: ""our algorithm makes [...]"" -> ""our algorithm uses policy [...]""
-261: Please highlight the fact that \pi_i^2 is a non-stationary policy, and in general all policies discussed in this section can be non-stationary.","This paper proposes a learning algorithm for two-player zero-sum games with a better offline sample complexity bound than previous algorithms, in the average reward setting. The analysis of bounds is based  on an innovative way to replace the opponent's non-stationary policy with a stationary policy.

The paper looks good and seems to present a significant progress over the state of the art. It is written extremely technically and concisely. However, there is no clear separation between the main paper and the supplementary material (the former depends heavily on the latter) and the whole paper is 33 pages long.

The content is too technical for me and I cannot evaluate its validity, but I'm quite convinced that NIPS is not an appropriate target for this work, which should be submitted to a journal instead.

My remarks below are mainly clarification requests from a naive reader who did his best to understand some of the content (and could not).

line 79, what does R stands for in \PI^{HR} and  \PI^{SR}?

line 82: it is standard to have M a a parameter of \rho? You may suggest that later you will use some approximation \hat{M} here...

line 85: I could not get what are the coordinates of vector h. ""Coordinatewisely"" is a too compact neologism ;)
Note that is other domains, vectors are written in bold, this helps decoding equations...

line 87: the Bellman equation => Bellman's equation (?)
Getting the given equation from the standard one is probably trivial, but you should either explain it or give a reference on how to get it.

Notations: this part is a nightmare. You didn't define the notion of game matrix before. I don't know what ""the probability simplex of dimension k"" is. I could not get any clear message from this small section. BTW, shouldn't it come earlier?

General comment: Lemmas, Definitions, Theorems, Remarks should each have their own counter.

line 114: are as follows => is

line 146: you should refer to Algorithm 1 somewhere in the text

line 152: I don't understand ""it can then create a confidence region Pk(s; a) for each transition probability"".

line 156: why do you need to double one count at each iteration? That is, why not triple or anything else?

line 160: LHS: notation not introduced

line 190: difficulties that (are) only present

line 192: you already introduced H_t line 76...

line 255: doesn't => does not

line 259: .Now => . Now

line 272: won't => will not

After section 4, I find the paper unreadable, because I always have to go to the supplementary part. This is not a good organization.

The main part of the paper stops abruptly without a discussion, a conclusion summarizing the main points and their significance, nor suggestions for future work.
","The authors propose a reinforcement learning algorithm for stochastic games in the infinite-horizon average-reward case. The authors derive regret bounds for the algorithm both in the online and offline settings. 

The paper is easy to read and technically sound, although I did not check all the proofs.

It seems that the regret bound of sqrt(T) is achieved only under the strong Assumption 1, while the more natural Assumption 2 only gives sqrt(T^{2/3}). It would be helpful if the authors could elaborate on whether sqrt(T) is achievable under Assumption 2, perhaps with a lower bound.

=============================================================
Update after author feedback period
=============================================================
The authors should compare the results with that of 
https://doi.org/10.1287/moor.2016.0779
In particular, the stochastic game setting can be related to the robust MDP setting with a finite number of uncertainty sets in the above paper, where a sqrt(T) regret bound is achieved under an assumption that I believe is (roughtly) between Assumption 1 and Assumption 2 in the present paper.

"
Independence clustering (without a matrix),Daniil Ryabko,https://proceedings.neurips.cc/paper/2017/hash/37d097caf1299d9aa79c2c2b843d2d78-Abstract.html,"The paper proposes a more general approach of independence clustering, where it considers mutual independence rather than pairwise independence. The authors provide algorithms and impossibility results given various assumptions (known/unknown distribution, known/unknown cluster size) on the problem. 

The first contribution (section 3, theorem 1) seems to be a naive solution to independence clustering, since it simply tests for conditional independence of a set and a variable and repeat the process. The second contribution (section 4, theorem 2) is a natural corollary of theorem 1, considering that the approximation from iid samples is a conventional approach. The third result (section 5.2, proposition 1) is more interesting in that it gives an impossibility result on stationary and ergodic process. Finally, the last result (section 5.3, theorem 3) defines sum-information which can be used as proxy to the mutual information in stationary ergodic processes. Although the convergence results are direct application of [20], the use of sum-information in independence clustering may encourage future work. ","The authors study the problem of clustering random variables into the finest clusters using time-series samples. They begin with the oracle case, and propose an algorithm called CLIN, which can find the correct cluster with at most O(kn^2) conditional independence tests where k is the number of clusters and n is the number of variables. The authors then show one can make use of the existing entropy/MI estimators or conditional independence testing algorithms to implement CLIN with iid samples. For the case of stationary samples, the authors propose a similar algorithm CLINk, which requires the knowledge on the number of clusters. 

The results are clearly presented, and I liked reading the paper. I have few minor comments. Line 148 (h(C|R) <= h(C|R\{x}) is simply due to the property of mutual information). The first two thirds of the paper on oracle/iid samples was much easier to follows, and especially I liked the way it is written: starting with CLIN algorithm’s description and replacing the oracle estimators with sufficient estimators. For the rest of the paper, by mentioning Remark 5 upfront, one can significantly improve the readability. Finally, it will be great if the authors can add some simulation results, which corroborate their results. ","ABOUT:
This paper is about clustering N random variable into k mutually independent clusters. It considers the cases when k is known and unknown, as well as the cases when the distribution of the N variables is known, when it needs to be estimated from n i.i.d. samples, and when it needs to be estimated from stationary samples. Section 3 provides an optimal algorithm for known distributions which uses at most a 2kN^2 oracle calls. Subsequent sections build on this algorithm in more complicated cases.

I do not know the clustering literature well enough to put this work in context. However, if the overview of the prior work is accurate, Sections 3 and 4 are a nice and important contribution. I have some reservations about Section 5. Overall, the exposition of results could be improved.

COMMENTS:

(1) I am not entirely convinced by the proof for Theorem 1. I believe the statement of the result is probably correct, but the proof needs to be more rigorous. It could be useful to state what exactly is meant by mutual independents for the benefit of the reader* and then to highlight in the proof how the algorithm does achieve a mutually independent clustering. For the Split function part, an inductive proof could be one way to make the proof rigorous and easier to follow.

(2) It does not seem that the stationary case in Section 5 is covered adequately. It kind of reads like the authors tried to do too much and actually accomplished little. In parts, it just looks like a literature review or a math tutorial (e.g. Section 5.1).

- I am not sure what the issue is with mutual information rate being zero is. One natural extension of the set up in Section 4 would be to cluster the N random variables according to their stationary distribution at time i with the n time series samples being then used to estimate this distribution as well as possible. In that case the mutual information is perfectly well defined and everything follows through. This does not seem to be what is actually being done, and instead the clustering is being performed over a whole infinite time series.

- The sum information in Definition 1 seems far less interesting and fundamental than the authors try to make it out to be. Also, a side note, people have spent time thinking about what a mutual information rate for a random process should be in full generality [A] and this question maybe deserves more attention than it is given here.


*I understand that some may find this too basic. But, having this definition side by side with the proof would help the reader confirm correctness. Somewhat more importantly, this issue of pairwise independence vs mutual independence is the crux of the paper. It should be stated explicitly (i.e. using math) what that means.

MINOR COMMENT
- Line 156: N not n

[A] Te Sun Han, Information-Spectrum Method in Information Theory .
Springer, 2003"
Effective Parallelisation for Machine Learning,"Michael Kamp, Mario Boley, Olana Missura, Thomas Gärtner",https://proceedings.neurips.cc/paper/2017/hash/38811c5285e34e2e3319ab7d9f2cfa5b-Abstract.html,"The main contribution of the paper is a new framework for parallel machine learning algorithms. The idea is to combine base learners more effectively than simply averaging. Specifically, subsets of hypotheses are replaced by their radon point. They prove complexity bounds for their method and also empirically compare their results with parallel algorithms in Spark and using base linear models in Weka.

The paper is quite interesting since it proposes essentially a black-box method to combine weak learners together. The approach seems to be an alternative to bagging weak learners together. The theoretical contribution is the analysis of the complexity of the radon machine, where, since the original samples can be broken down into multiple parts, the resulting parallel machine is much more efficient as compared to the base learner that operates on all the original samples.They also show a PAC bound on the radon machine. The theoretical contributions seems to be pretty good in the paper. One major concern was the practical aspect, particularly for high-dimensional data. It is not clear how well the proposed approach performs here. the experiments seem to be for very low dimensional datasets (18 features?) which is perhaps not realistic for modern machine learning problems in most domains. If the authors could emphasize on how high-dimensional could be handled by their method, the paper would be much stronger.","The authors proposed an approach for parallelizing learners (which hold certain assumptions) based on Radon points. The contribution of the paper w.r.t. the other similar works is the usage of Radon points instead of averaging (which has been used as baseline approach in the paper). The performance is compared with `no parallelization', `averaging schema', and `Spark-based' algorithms. They have shown experimentally the reduced learning time and similar or higher accuracy to the other algorithms.

It seems the theoretical part has already been investigated in another paper:``Parallelizing Randomized Convex Optimization"", which could be from the same authors. But this is not cited in the current paper and it needs to be elaborated what the paper's contribution for this part is.","The authors have presented a parallelization algorithm for aggregating weak learners based on Radon partitioning. They present theoretical analysis to motivate the algorithm along with empirical results to support the theory. The theoretical analysis is interesting, and the empirical results demonstrate training time and/or AUC improvements over multiple baseline algorithms, on multiple datasets. The authors also preemptively and convincingly address several questions/concerns in the Evaluation and Discussion sections. I recommend acceptance.

Specific Notes
--------------
- Line 51: ""the the"" -> ""the""
- Line 79: Is this the same size math font as elsewhere? It seems small and kind of hard to read.
- Figures 1-3: The text really needs to be bigger in the figure images.
- Figure 3: This one may be harder to distinguish if printed in black and white.
- Lines 318-320: ""the Radon machine outperforms the averaging baseline..."" To clarify, is this based on an average of paired comparisons?
- Line 324: A pet peeve of mine is the use of the word significant in papers when not referring to statistical significance results.
- Lines 340-341: Missing some spaces - ""machineachieved"", and ""machineis.""
- Lines 341-349: The difference in performance with the AverageAtTheEnd method doesn't seem terribly meaningful. Why wasn't this result included somewhere in Figure 1? Can you advocate more for your method here?
- Line 377: ""Long and servedio [22]"" The use of author names here seems inconsistent with citations elsewhere (even immediately following)."
Deep Mean-Shift Priors for Image Restoration,"Siavash Arjomand Bigdeli, Matthias Zwicker, Paolo Favaro, Meiguang Jin",https://proceedings.neurips.cc/paper/2017/hash/38913e1d6a7b94cb0f55994f679f5956-Abstract.html,"This is a well-written paper that introduces an approach to image restoration based on a ""hypothetical"" prior, the gradient of which can be approximated by a trained denoising autoencoder.

They key insight behind the paper is that if you construct a natural image prior in a particular way (specifically, if the natural image distribution is smoothed with a Gaussian kernel), then the gradient of this prior corresponds precisely to the error of an optimal denoising autoencoder (that is, the difference between the output of the autoencoder and its input).

In this way, the prior in itself never actually needs to be instantiated - instead, one trains an autoencoder at a fixed noise level (corresponding to the assumed Gaussian noise level of the corrupting process), and then, during inference, repeatedly evaluates the autoencoder at each step of the gradient descent procedure for image restoration to obtain the gradient at the current iterate.

The authors devise update steps for non-blind, noise-blind, as well as kernel-blind image deblurring.

One practical problem is that the autoencoder is trained for a fixed noise level.  The authors observe that the autoencoder does not produce meaningful results for input images that do not exhibit noise close to the training variance.  The proposed counter measure effectively amounts to adding Gaussian noise at a certain variance to the auto-encoder input (the current gradient descent iterate) at each step.  Despite being motivated by a lower bound involving noise sampling, this step is somewhat ad-hoc.

Question to the authors: Is this only need if the corrupted input image exposes a noise level other than the one the autoencoder was trained for?   Or is the gradient descent procedure itself producing less and less noisy iterates, so that even if the original corrupted image had the ""right"" noise level, the ""unmodified"" iterates become unsuitable for the autoencoder eventually, preventing further progress?

The authors present applications of their approach to 1) image deblurring (non-blind and noise-blind), 2) image deblurring (noise- and kernel-blind), 3) super-resolution, and 4) demosaicing.

The experiments seem very thorough in general and demonstrate appealing results.
I am a bit puzzled about Table 2, however - specifically the performance of RTF-6 [28] as a function of the noise variance.  In table 2, the PSNR of RTF-6 drops dramatically as the noise level is increased. To some extent this is normal (all methods suffer from higher noise level), but the dramatic drop of RTF-6 (32.36 at \sigma=2.55 to 21.43 at \sigma=7.65) is unexpected and not consistent with [28].  It is worth pointing out that while the RTF cascade is trained for a fixed noise level, there is a parameter \alpha of the inference routine that must be adjusted to match the noise level of the input.  It seems to me that this was not done when the results for Table 2 were produced.  I would recommend getting in touch with U. Schmidt to ensure his RTF code was properly used.

Overall, I find that this paper establishes interesting connections and hence has some theoretical appeal. Moreover, the authors clearly demonstrate that the approach is applicable to a wide range of image restoration tasks and yields high accuracy. Hence, I strongly recommend that the paper be accepted.","Summary
-------
The paper proposes a way to do Bayesian estimation for image restoration problems by using gradient descent to minimize a bound of the Bayes risk. In particular, the paper uses Gaussian smoothing in the utility function and the image prior, which allows to establish a connection between the gradient of the log-prior and denoising auto-encoders (DAEs), which are used to learn the prior from training data. Several image restoration experiments demonstrate results on par with the state-of-the-art.

Main comments
-------------
The positive aspects of the paper are its good results (on par with state-of-the-art methods) for several image restoration tasks and the ability of the same learned prior to be used for several tasks (mostly) without re-training. Another strong aspect is the ability to jointly do noise- and kernel-blind deconvolution, which other methods typically cannot do.

I found it somewhat difficult to judge the novelty of the paper, since it seems to be heavily influenced/based on the papers by Jin et al. [14] (noise-adaptive approach, general derivation approach) and Bigdeli and Zwicker [4] (using DAEs as priors, mean-shift connection). Please properly attribute where things have been taken from previous work and make the differences to the proposed approach more clear. In particular:
- The mean-shift connection is not really discussed, just written that it is ""well-known"". Please explain this in more detail, or just remove it from the paper (especially from the title).
- Using a Gaussian function as/in the utility function seems to be taken from [14] (?), but not acknowledged. The entire (?) noise-adaptive approach seems to be adopted from [14], please acknowledge/distinguish more thoroughly (e.g. in lines 168-171).

Although the experiments demonstrate that the proposed method can lead to good results, many approximations and bounds are used to make it work. Can you give an explanation why the method works so well despite this?
Related to this, if I understand correctly, the whole noise sampling approach in section 3.3 is only used to make up for the fact that the learned DAE is overfitted to noisy images? I don't understand how this fixes the problem.

Regarding generalization, why is it necessary to train a different DAE for the demosaicing experiments? Shouldn't the image prior be generic?
Related to this, the noise- and kernel-blind experiments (section 5.2) use different gradient descent parameters. How sensitive is the method towards these hyper-parameters?

The paper doesn't mention at all if the method has reasonable runtimes for typical image sizes. Please provide some ballpark test runtimes for a single gradient descent step (for the various restoration tasks).

Miscellaneous
-------------
- I found the abstract hard to understand without having read the rest of the paper.
- It does seems somewhat artificial to include p'(x)/p(\tilde{x}) in Eq. 5 instead of just removing this term from the utility function and simply replacing p(x) with the smoothed prior p'(x).
- Typo in line 182: ""form"" -> ""from""
- Please define the Gaussian function g_\sigma in Eq. 6 and later.
- Table 2:
	- EPLL/TV-L2 + NA not explained, how can they use the noise-adaptive approach?
	- Misleading results: e.g. RTF-6 [28] only trained for \sigma=2.55, doesn't make sense to evaluate for other noise levels. Please just leave the table entry blank.","This paper is outside my expertise, which is why I did not bid to review it!

It proposes to use a denoising autoencoder (DAE) indirectly - not as the main network, but to provide a differentiable regularisation cost which approximates a prior over natural data points. The connection between DAE, mean-shift and the distribution of natural data points apparently comes from prior work, though is used in a novel way here.

The paper is clearly written, complete and interesting.

Section 5.4 is an exception to ""clearly written"". Why do the authors take so much time to explain what type of camera was used when a third party created a dataset? What does ""Matlab's demosic function"" do, and how was it used here for initialisation? Is this task identical to denoising/deblurring but with a different type of noise, or is there something different about it? If the former, then why are the results reported differently (linear RGB PSNR, rather than dB as elsewhere)?

It is not clear to this reviewer how much contribution comes directly from the DAE. I presume that a DAE alone will do a poor job at restoring a blurred image, but it may be contributing something towards the evaluation criterion, so I wonder if it would be worth providing the evaluation statistic for the DAE used alone.
"
On Structured Prediction Theory with Calibrated Convex Surrogate Losses,"Anton Osokin, Francis Bach, Simon Lacoste-Julien",https://proceedings.neurips.cc/paper/2017/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html,"This paper presents a general theoretical framework on structure prediction via efficient convex
Surrogate loss minimization. Allowing for the exponential number of classes, an interesting and specific result in Theorem 6 shows that the learning complexity is strongly dependent on the calibration function, and tells us that 0-1 loss for classification problem is ill-suited for the structured prediction with very large cardinality of labels. In summary, this paper provides sound theoretical results in terms of both statistical consistency and algorithmic convergence, and thus provides a theoretical insight on the design of algorithms for structure prediction via an efficient convex relaxation. 
Here I have some specific questions as follows:
(1) Since the consistency of convex surrogates is well understood in the context of binary classification, what is the obvious difference or difficulty in term of theoretical proof between structure prediction with the exponential number of classes and the classical binary case?   
(2) In Section 4, this paper only considers the quadratic surrogate as special cases, it will be better that some robust loss functions such as the Huber loss and the pinball loss are also studied. 
","The paper studies the consistency of surrogate loss functions in structured prediction. Generally, minimizing the empirical risk directly in structured prediction is intractable, so instead people use surrogate losses like the structured hinge loss and the log loss. This paper is significant because it is the first to study under which circumstances minimizing these surrogate losses in the limit of infinite data implies minimizing the true task loss.

The paper approaches the problem via calibration functions, functions that measures the smallest excess of the surrogate loss when the excess of the actual loss is larger than a given nonnegative value. The authors show how to analyze the calibration function for various surrogate and task loss function pairs in order to determine whether they are consistent. Perhaps the most interesting aspect of this analysis is that it formalizes common intuitions about different task losses for structured prediction, such as that learning with the 0-1 loss is hard.

The paper and its supplementary material cover a lot of ground. A challenge for presentation is that parts of the paper read like a summary of the supplementary material with pointers to the appropriate sections.","The paper examines consistency of surrogate losses for multiclass prediction. The authors present their results using the formalism of structured prediction. Alas, there is no direct connection or exploitation of the separability of structured prediction losses. The paper is overly notated and variables are frequently overloaded. I got the feeling that the authors are trying to look mathematically fancy at the expense of readability. As for the technical content, I find the contribution in terms of new proof techniques quite marginal. The authors build on results by Toingo(Tong + Ingo) and the theorems seem lackluster to me.

From algorithmic prospective the paper has rather little to offer. The quadratic loss is either intractable to calculate or in the case of separable it amounts to sum of squares over the vector L_j. There's no attempt to experimentally validate the calibrated losses. Also absent is a sincere discussion on the importance of calibration given the empirical success of overly-specified models.


In ""Contributions"" paragraph the word ""exponential"" repeats over and over. It would not be clear to a reader who is not familiar with the setting what is the notion of exponential dependency.

The sentence ""...  sets of possible predictions and correct outputs always coincide and do not depend on x."" left me baffled. 

There is excessive overload of notation for L which is used as a function, specific losses, table, etc...

minimal -> smallest / lowest

We denote the y-th component of f with f y

""... for now, we assume that a family of score functions F consists of all vector-valued Borel measurable functions f : X → F where F ⊆ R k ..."" and then
what ?

Is it feasible to find inf φ ?

""... equals the smallest excess of the conditional surrogate risk when the excess of the conditional actual risk is larger than ..."" left me bewildered again.

I feel that paper would have rather little appeal at NIPS and a natural venue would be COLT or ALT.v"
Invariance and Stability of Deep Convolutional Representations,"Alberto Bietti, Julien Mairal",https://proceedings.neurips.cc/paper/2017/hash/38ed162a0dbef7b3fe0f628aa08b90e7-Abstract.html,"The paper studied some nice properties about deep convolutional kernel networks, i.e.CKNs are near invariant to groups of transformation and stable to the action of diffeomorohisms, if appropriate patch extraction operator，kernel mapping operator and pooling operator are carefully designed. Authors also showed that under certain assumption, the nice properties obtained from CKNs also applies to CNNs.

In general, the paper is well written and technically sound. The only weakness is that the paper does not provide any empirical experiments to validate the theoretical analysis. I wish to see some experimental results showing the conclusion that CKNs with appropriate settings are invariant to the transformation and diffeomorphism.

Another question is whether random projection or explicit kernel mapping with finite number of Fourier series can also be a candidate solution for the kernel approximation. If so, what is the effect on the stability.


","The primary focus of the paper is CKN (convolutional kernel network) [13, 14]. In this manuscript the authors analyse the stability [w.r.t. C^1 diffeomorphisms (such as translation), in the sense of Eq. (4)] of the representation formed by CKNs. They show that for norm-preserving and non-expansive kernels [(A1-A2) in line 193] stability holds for appropriately chosen patch sizes [(A3)]. Extension from (R^d,+) to locally compact groups is sketched in Section 4.

The paper is nicely organized, clearly written, technically sound, combining ideas from two exciting areas (deep networks and kernels). The stability result can be of interest to the ML community.  

-The submission would benefit from adding further motivation on the stability analysis. Currently there is only one short sentence (line 56-57: 'Finally, we note that the Lipschitz stability of deep predictive models was found to be important to get robustness to adversarial examples [7].') which motivates the main contribution of the paper.
-Overloading the \kappa notation in (A3) [line 193] might be confusing, it also denotes a function defining kernel K in Eq. (10).
-In the displayed equation between line 384 and 385, the second part ('and \forall v...') is superfluous; given the symmetry of kernel k, it is identical to the first constraint ('\forall u ...').
-Line 416: the definition of \phi is missing, it should be introduced in Eq. (10) [=<\phi(z),\phi(z')>_{H(K)}].
-Line 427-428: The inequality under '=' seems to also hold with equality, | ||z|| - ||z|| |^2 should be | ||z|| - ||z'|| |^2. 

References:
[3,6,8,13,14,18,25-27,30,31]: page information is missing.
[9]: appeared -> Amit Daniely, Roy Frostig, Yoram Singer. Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. Advances in Neural Information Processing Systems (NIPS), pages 2253-2261, 2016.
[17]: appeared -> Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Sch{\""o}lkopf. Kernel Mean Embedding of Distributions: A Review and Beyond. Foundations and Trends in Machine Learning, 10(1-2): 1-141. 
[19]: appeared -> Anant Raj, Abhishek Kumar, Youssef Mroueh, Tom Fletcher, Bernhard Sch{\""o}lkopf. International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR 54:1225-1235, 2017.
[32]: accepted (https://2017.icml.cc/Conferences/2017/Schedule?type=Poster) -> Yuchen Zhang, Percy Liang, Martin Wainwright. Convexified Convolutional Neural Networks. International Conference on Machine Learning (ICML), 2017, accepted. 
","The paper is well written and rigorous and it successfully shows how (some type of) functional spaces implemented by DCNNs can be described in terms of multilayer kernels. It also shows how the architecture has nice properties: (almost) invariance to group transformations and stability to diffeomorphic transformations. I recommend the acceptance given the comments below: 

*To the reviewer's understanding the main paper contribution is a rewriting of CNNS architecture in the context of RKHS.  
Please add in the text some comments why this reformulation is/will be useful (e.g. kernel formulation may lead to interesting results on generalization bounds?, is it computationally more efficient to implement using standard techniques/approximation on kernels?)
More in general is the novelty of the approach going beyond a reformulation of DCNNs in the context of RKHS?
Also clarify the novelty w.r.t. the paper ""Convolutional Kernel Networks"" of
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.


* The authors stress the importance of ""not loosing the signal information"". 
Why is so? DCNNs are often used in relation to some tasks (e.g. image classification). Unless the task is signal reconstruction only task-relevant information need to be preserved. Please comment on this.


* Regarding group invariance: is there any connection with the invariant kernel approach adopted by Hans Burkhardt (invariant kernels)?


* More of a curiosity: you define patch, pooling and kernel maps that commute with the group action. Will any other operator (in the set of operators that commutes with all group elements) ensure the equivariance property of DCNN w.r.t. the group transformations? 








"
Variational Memory Addressing in Generative Models,"Jörg Bornschein, Andriy Mnih, Daniel Zoran, Danilo Jimenez Rezende",https://proceedings.neurips.cc/paper/2017/hash/3937230de3c8041e4da6ac3246a888e8-Abstract.html,"The authors propose variational memory addressing. It augments
generative models with external memory and hard attention, and
interestingly, derives read and write mechanisms that mimick more
classical probabilistic graphical models than the more sophisticated
mechanisms as in, e.g., neural Turing machines.

In their formulation, external memory acts much like a global variable
in topic models and mixture models, whether they sample a ""membership""
given by the hard attention and proceed to generate the local variable
z and data x conditional on memory indexed by this membership. I found
this a particularly useful way of understanding memory in the context
of latent variable models, where writing corresponds to inference.

As the authors note in, e.g., L175-186, it seems the algorithm does
not scale well with respect to the external memory size. This can be
justified mathematically as the the variance of the black box
gradients with respect q(a) parameters increases with the size of a.
It is unlikely that VIMCO can help much in this regard. That said, I'm
impressed that the authors were able to get interesting results with
|M| up to 1024.","This paper is not the first one that addresses variational autoencoders with external memory modules (which the authors falsely claim). For instance,

Chongxuan Li, Jun Zhu, Bo Zhang, ""Learning to Generate with Memory,"" Proc. ICML 2016

solve the same problem, in a match more potent way than this work (I reckon this method as a quite trivial extension of existing VAEs).

Since the authors have failed to identify all the related work, and properly compare to them, this paper certainly cannot be accepted to NIPS.
","This paper proposes a memory-augmented generative model that performs stochastic, discrete memory addressing, by interpreting the memory as a non-parametric conditional mixture distribution. This variational memory addressing model can combine discrete memory addressing variables with continuous latent variables, to generate samples only with few samples in the memory, which is useful for few-shot learning. The authors implement a VAE version of their model and validate it for the few-shot recognition tasks on the Omniglot dataset, on which it significantly outperforms the Generative Matching Networks, which is an existing memory-augmented network model. Further analysis shows that the proposed model accesses relevant part of the memory even with hundreds of unseen instances in the memory. 


Pros:
- Performing discrete, stochastic memory addressing for memory-augmented generative model is a novel idea which makes sense. Also, the authors have done a good job in motivating why this model is superior to soft attention approach.
- The proposed variational addressing scheme is shown to work well in case of few-shot learning, even in case where existing soft-attention model fails to work.   
- The proposed scheme of interpreting the memory usage with KL divergence seems useful.

Cons
-Not much, except that the experimental study only considers character data, although they are standard datasets. It would be better if the paper provides experimental results on other types of data, such as images, and compared against (or coupled with) recent generative models (such as GANs)

Overall, this is a good paper that presents a novel, working idea. It has effectively solved the problem with existing soft-attention memory addressing model, and is shown to work well for few-shot learning. Thus I vote for accepting the paper. 

- Some typos:
Line 104: unconditioneal -> unconditional
Line 135: ""of"" is missing between context and supervised.
Line 185: eachieving -> achieving
Table 1 is missing a label ""number of shots"" for the numbers 1,2,3,4,5,10 and 19."
Shallow Updates for Deep Reinforcement Learning,"Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor",https://proceedings.neurips.cc/paper/2017/hash/393c55aea738548df743a186d15f3bef-Abstract.html,"The paper proposes an add on to DQN algorithm, by using an additional ""periodical re-training the last hidden layer of a DRL network with a batch least squares update"".

The authors explain the logic behind the idea really well, supported by the context of Deep RL and SRL algorithms. The paper first analyzes the behavior of SRL algorithms with high dimensional problems, then they compare the proposed algorithm in 3 different atari domains. 
The results show mostly improvement over DQN and DDQN algorithms on maximum results. The proposed algorithm has a more stable learning curve compared to DQN.

In my opinion, the motivation is somewhat comparable to DDQN, and address the same weakness of the DQN algorithm. On the other hand being able to combine the shallow updates with even DDQN without any additional samples, is a definite plus.

In the results section, the authors present the maximal average scores reached for different algorithms, but I was curious about the confidence intervals of these, and how do they take the average of maximum? Is it average over a sliding window, or multiple runs?

I liked the idea behind the Section 4.3, but I find it hard to follow / understand. Especially the figure 4, could be represented in a different way instead of incremental bar charts.

I would replace the sentence ""We discuss various works related to our approach."" with something more informative, or just remove it.

Overall I think the paper brings a good research direction with interesting results and discssion.","This paper describes a relatively straightforward RL algorithm that involves occasionally re-training the last layer of a DQN net with a batch-based linear RL algorithm.  The authors hope that the batch-based approach will converge to a more precise value-function for the given step of the overall algorithm, thus improving the policy's performance scores at that step.  By occasionally re-fitting the acting policy's value function more precisely through this batch step, the final overall learning algorithm will be less hindered by temporary artefacts due to the slowness of SGD, and therefore may hopefully converge both more stably and to a better final policy. 

Opinion:
The paper is well-written and easy to follow.  The experiments are well-structured and equally easy to understand.  I would have appreciated a little more detail as to the Bayesian prior's nuances, esp. relatively to L2 with which I am much more familiar.  What makes this particular prior so capable in maintaining stability during an LS pass?  The experiments seem to defend the claims in the paper, but I am having a bit of trouble with the final paragraph's suggestion that the best performing solutions were those that were closest to the original weights in L2.  Does this suggest that the LS step is simply 'cleaning' up a couple artefacts that the SGD has not quite brought to convergence?  More in-depth analysis of this point would be interesting, as I'm not entirely sure I understand /what/ exactly makes this work much better, other than a more robust value-iteration step that avoids bizarreness that can be present in an SGD-converged solution...

Also, please clean up your plots with reduced line thickness and alpha transparency on your lines, and also review your font sizes.  These plots are pretty ugly and a minimum amount of effort could make them just fine and easier t read ('import seaborn' should get you 90% of the way there).
","
The authors propose to augment value-based methods for deep reinforcement learning (DRL) with batch methods for linear approximation function (SRL). The idea is motivated by interpreting the output of the second-to-last layer of a neural network as linear features. In order to make this combination work, the authors argue that regularization is needed. Experimental results are provided for 5 Atari games on combinations of DQN/Double DQN and LSTD-Q/FQI.

Strengths:

I find the proposition of combining DRL and SRL with Bayesian regularization original and promising.
The explanation provided for the improved performance seems reasonable, but it could have been better validated (see below).

Weaknesses:

The presentation of Algorithm 1 and in particular line 7, is a bit strange to me, given that in Section 4, the authors mention that generating D with the current weights results in poor performance. Why not present line 7 as using ER only? Besides, would it be interesting to use p% of trajectories from ER and (1-p)% of trajectories generated from the current weights?

The experiments could be more complete. For instance, in Table 1, the results for LS-DDQN_{LSTD-Q} are missing and in Fig.2, the curves for Qbert and Bowling are not provided. 
For a fairer comparison between DQN and LS-DQN, I think the authors should compare Algorithm 1 against DQN given an equal time-budget to check that their algorithm indeed provides at the end a better performance.
Besides, the ablative analysis was just performed on Breakout. Would the same conclusions hold on different games?


Detailed comments:

l.41: the comment on data-efficiency of SRL is only valid in the batch setting, isn’t it? 
l.114, l.235: it’s 
l.148: in the equation above, \mu should be w_k. I find the notation w_k for the last hidden layer to be unsatisfying, as w_k also denotes the weights of the whole network at iteration k
l.205, l.245: a LS -> an LS
l.222: algortihm

"
Learning with Bandit Feedback in Potential Games,"Amélie Heliou, Johanne Cohen, Panayotis Mertikopoulos",https://proceedings.neurips.cc/paper/2017/hash/39ae2ed11b14a4ccb41d35e9d1ba5d11-Abstract.html,"SUMMARY 

The paper scrutinizes the concept of a Nash equilibrium. In non-cooperative games it is often difficult to compute Nash equilibria. Furthermore, it might be unreasonable to assume full rationality and many other properties that are commonly assumed. So is Nash equilibrium a good concept? Specifically, the authors ask: if all players of a repeated game follow a no-regret learning algorithm, does play converge to a Nash equilibrium? 

The authors focus on variants of the exponential weights algorithm in potential games. In this context they consider two different feedback models: semi-bandit and bandit. 

In the semi-bandit case the authors show, under mild conditions, that learning converges to a Nash equilibrium at a quasi-exponential rate. In the bandit case similar results hold when the algorithm is run with a positive mixing factor, to obtain convergence to an epsilon-Nash equilibrium. By letting the mixing factor epsilon go to zero at a suitable rate, it is possible to obtain convergence to an actual Nash equilibrium. 

The proofs are based on stochastic approximation techniques. 


REVIEW 

The paper is very well written, it was a pleasure to read. The problem is well motivated, relevant, and interesting. The results of the paper are of significant interest to the NIPS community. I am confident that the results are correct --- the high level proof sketches in the main text are certainly convincing. 

I do not have any feedback to the authors regarding the main text. However, I recommend that the authors take a few careful passes on the supplementary material, which could be written better. ","The authors show that exponential weight updates and its generalization FTRL, converges pointwise to a Nash equilibrium in potential games. The latter holds even when players receive bandit feedback of the game. 

Unfortunately, the main result, for the case of full feedback is already known in the economics literature, where Hedge is known as smooth fictitious play. It is presented in the paper: Hofbauer, Sandholm, ""On the global convergence of smooth fictitious play"", Econometrica 2002. This work is not even cited in the paper and it shows that smooth fictitious play and its extensions of FTRL (though they do not call it FTRL) does converge to Nash in potential games and in three other classes of games. 

Moreover, the result of Kleinberg, Piliouras, Tardos:""Multiplicative Updates Outperform Generic No-Regret Learning in Congestion Games"", STOC'09, is presented in the paper, for the actual sequence of play (unlike what the authors claim). So that paper shows that for the class of congestion games (which is equivalent to the class of all potential games (c.f. Monderer-Shapley)), the actual play of MWU converges to Nash equilibria and in fact almost surely to Pure Nash equilibria (except for measure zero game instances), which is an even stronger result. 

In light of the above two omissions, the paper needs to be re-positioned to argue that the extension to bandit feedback is not an easy extension of the above results. It is an interesting extension and I can potentially see that extension be presented at a NIPS quality conference, but it needs to be positioned correctly with the literature and the paper needs to argue as to why it does not easily follow from existing results. In particular, given that most of these results go through stochastic approximations, such stochastic approximations are most probably robust to unbiased estimates of the payoffs, which is all that is needed for the bandit extension. 
","The paper investigates the exponential weighting algorithm in no-regret situation, a model free approach, for arriving at Nash equilibrium, as opposed to previous investigations which guaranteed with vanishing regret based on definition of egret. No-regret dynamics exhibit good convergence if all players adopt this strategy. The paper considers both semi-bandit and bandit cases. It proved that the EW scheme converges to relaxed Nash equilibrium.

This is a well written paper. It motivates the problem well and explains clearly why the proofs provided in the paper are important.

No results are presented about the practical applications of the solution. It will be useful to explain the applications better and show some results. I appreciate the context provided in the outline section, though. Perhaps some examples of a few practical applications will benefit the paper.

It appears that computing the denominator in equation 2.6 is computationally expensive for large problem. A comment about it would be helpful."
A Greedy Approach for Budgeted Maximum Inner Product Search,"Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, Inderjit S. Dhillon",https://proceedings.neurips.cc/paper/2017/hash/39d352b0395ba768e18f042c6e2a8621-Abstract.html,"	  The aim of the paper is to propose a new greedy approach for Maximum Inner Product Search problem: given a candidate vector, retrieve a set of vectors with maximum inner product to the query vector. This is a crucial step in several machine learning and data mining algorithms, and the state of the art methods work in sub-linear time recently. The originality of the paper is to study the MIPS problem under a computational budget. The proposed approach achieves 
better balance between search efficiency and quality of the retrieved vectors, and  does not require a nearest neighbor search phase, as commonly done by state of the art approaches. The authors claim impressive runtime results (their algorithm is 200x faster than the naive approach), and a top-5 precision  greater than 75%. 
	  

	  
	  The paper is very dense (the space between two lines seems smaller than the one in the template). However, the paper is well-written and the procedure is well-explained. The proposed method seems also quite original, and comes with theoretical guarantees. The technical results seem sound.
	  
	  Some remarks:
	  
	  Figure 1 should be placed at the top of P.5, it is a bit difficult to follow without the later explanations.
	  
	  The bound used in P.4 needs to be more studied, in order to find, for instance, some properties (or better, an approximation). This bound is a key point of this procedure, and it is used at the beginning.
	  
	  P.5 ""visit (j,t) entries of Z"": (j,t) is a cell in the matrix Z, however you consider this notation as a number. Maybe ""j \times t"" entries?
	  
	  The reviewer would be interested to have access to the source code of the algorithm and the data, so as he can reproduce the expriments?","The papers proposes a greedy approach based to some sorting among the columns of the matrix of candidates  to solve the MIPS problem. Some analysis of the proposed solution is made, this leads to an efficient implementation and an asymptotic bound on the error is provided on a simplified case.  The analysis does seems very sharp as the provided bound is sublinear but yields better results for high dimensional vectors (this is likely to be an artifact of the uniform iid assumption over the entries). 

The proposed idea is nice and simple but the writing makes the paper harder to follow than it could be. 

The MIPS problem received a lot of attention from the community and the experimental part does not compare to the most recent approaches as ALHS (NIPS'14) nor ""Learning and Inference via Maximum Inner Product Search"" (ICML'16) which can be considered as an issue. The authors discards theses approaches because they consider that the computational budget cannot as easily controlled than with their approach, but in my opinion this reason is not strong enough to not report their performance on the figures. 
Moreover the time required to build the indexes is not reported in the time comparisons which can be acceptable when the number of user is very large in front of the number of items but again not very fair in terms of time comparisons.
  
From a practitioner point of view, MIPS is not widely used in production because it remains too slow for large scale systems (even 
 when using dense embeddings). One tend to prefer the use of hashtables to some set of products with some well chosen keys.  

","The paper considers the problem of Maximum Inner Product Search, which is an important retrieval problem for recommender systems task (among others), e.g. find an item which is most similar to a user's preference vector.

I'm not particularly familiar with related work on this topic (such as Budgeted MIPS), so had to learn as I went.

Essentially (as I understand it) Budgeted MIPS has a trade-off between latency and search quality. The idea here is to greedily develop algorithms that dynamically adjust this tradeoff to improve performance.

The actual procedure, as I understand it, seems fairly straightforward. And plenty of detail is given that what's proposed could easily be re-implemented. Even so, the method (as far as I can tell) is novel and appears to be backed up by some theoretical analysis that demonstrates its validity.

The analysis itself is a bit limited, in the sense that it makes strong assumptions about the data distribution (uniformity of the vectors). I can imagine several applications where this would not be realistic. However it seems realistic enough for applications like retrieval in a recommender system, which is an important enough application in and of itself. The experiments on real-world datasets seems to confirm that the proposed approach does indeed result in substantial speed increases.
"
Riemannian approach to batch normalization,"Minhyung Cho, Jaehyung Lee",https://proceedings.neurips.cc/paper/2017/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html,"Paper Summary
Starting from the observation that batch-normalization induces a particular
form of scale invariance on the weight matrix, the authors propose instead to
directly learn the weights on the unit-sphere. This is motivated from
information geometry as an example of optimization on a Riemannian manifold, in
particular the Stiefel manifold V(1,n) which contains unit-length vectors.  As
the descent direction on the unit sphere is well known (eq 7), the main
contribution of the paper is in extending popular optimization algorithms
(SGD+momentum and Adam) to constrained optimization on the unit-sphere.
Furthermore, the authors propose orthogonality as a (principled) replacement
for L2 regularization, which is no longer meaningful with norm constraints.
The method is shown to be effective across two families of models (VGG, wide
resnet) on CIFAR-10, CIFAR-100 and SVHN.

Review

I like this paper, as it proposes a principled alternative to batch
normalization, which is also computationally lightweight, compared to other
manifold optimization algorithms (e.g. natural gradient). That being said a few
flaws prevent me from giving a clear signal for acceptance.

(1) Clarity. I believe the paper would greatly benefit from increased
simplicity and clarity in the exposition of the idea. First and foremost,
concepts of element-wise scale invariance and optimization under unit-norm
constraints are much simpler to grasp (especially to people not familiar with
information geometry) than Riemannian manifolds, exponential maps and parallel
transport. I recommend the authors preface exposition of their methods with
such an intuitive explanation. Second, I also found there was a general lack of
clarity about what type of invariance is afforded by BN, and which direction
were the weights norms constrained in (column vs row). On line 75, the authors
write that ""w^T x is invariant to linear scaling"", when really this refers to
element-wise scaling of the columns of the matrix W \in mathbb{R}^{n x p},
where $p$ is the number of hidden units. In Algorithm 4, there is also a discrepancy
between the product ""W x"" (implies W is p x n) and the ""column vectors w_i""
being unit-norm.

(2) Experimental results. As this is inherently an optimization algorithm, I
would expect there to be training curves. Does the algorithm simply converge
faster, does it yield a better minimum, or both ? Also, I would like to see
a controlled experiment showing the impact of the orthogonality regularizer:
what is the test error with and without ?

(3) Baselines and related work. This work ought to be discussed in the more
general context of Riemannian optimization for neural networks, e.g. natural
gradient descent algorithms (TONGA, K-FAC, HF, natural nets, etc.). Path-SGD also
seems closely related and should be featured as a baseline.

Finally, I am somewhat alarmed and puzzled that the authors chose to revert to
standard SGD for over-complete layers, and only use their method in the
under-complete case. Could the authors comment on how many parameters are in this
""under-complete"" regime vs total number of parameters ? Are there no appropriate
relaxations of orthogonality ? I would have much rather seen the training curves
using their algorithm for *all* parameters (never mind test error for now) then
this strange hybrid method.","The paper proposes a Riemannian approach to batch normalization and suggests to modify SGD with momentum and Adam to benefit from invariances of the proposed approach. 

The paper is easy to follow but the validation of the proposed approach looks questionable to me. 
First, the results reported in Table 1 and Table 2 can be a result of better hyperparameter selection. For instance, the results obtained for WRN-28-10 on CIFAR-10 and CIFAR-100 do not differ sufficiently from the ones reported in the original WRNs paper. The authors do not reproduce the results of the original paper but obtain worse results in most cases. This might suggest that the experimental settings are not the same. Indeed, the authors report that they use batchsize 100 which alone might have some contribution to the observed differences. The authors do not report how many runs they performed so I assume that they perform just one run. 

In contrast to the default SGD with momentum, the proposed methods include a gradient clipping procedure which is known to have certain regularization effects. It is unclear whether the reported results can be explained by the gradient clipping alone.

The way how the results are reported looks very suboptimal to me. 
First, it is of interest to know whether the proposed approaches faster optimizes the training loss due to the proposed batch normalization. 
Alternatively, it might be that they are not faster in terms of optimization but the networks generalize better. Then, it would be of interest to clarify which process contributes most to better generalization if any. 
Unfortunately, both the main paper and supplementary material does not contain any details about it. Without having it, it might be assumed that the proposed methods include known useful tricks like weight clipping + involve attached hyperparameters (line 133) whose careful manual selection allowed to get slightly better results in a couple of runs. Please try to refute the latter hypothesis.  

""Note that we apply the proposed algorithm only when the input layer to BN is
under-complete, that is, the number of output units is smaller than the number of input units, because the regularization algorithm we will derive in Sec. 5.1 is only valid in this case.""

Could you please clarify how often it is the case in practice?

The paper (or supp. material) is missing details about the time cost of the proposed methods w.r.t. the default SGD with momentum. I guess that some non-negligible overhead might be expected. 

Finally, without further clarifications of the experimental part it is hard to asses the impact of the proposed method. Just showing (without a closer look) slightly better results on the CIFAR datasets is of limited importance because better results are already available, e.g., below 3\% for CIFAR-10 with the ""Shake-Shake regulation"" by Gasteldi, ICLR-2017.
","The paper presents a novel training algorithm for feedforward network to be used in presence of batch normalization. The purpose of the algorithm is to takle the presence of invariance in the network, by using a geometric approach based on manifold optimization, in particular using Grassmann manifold optimization methods.

The paper is well written and sound. The experimental part is convincing and experiments show good performances. I recommend acceptance for the conference. In the following some remarks and comments.

In Algorithm 1 you use g for the Riemannian gradient. In line 100, you use g for the vector of partial derivatives. As in Absil et al., I personally prefer ""grad f"" to represent the Riemannian gradient.

For eqs 6-8 I would add an explicit reference, as done for eq 3.

In the definition of the parallel transport (line 104), it is not explicit in my opinion the dependence of the operator on the point along the geodesic where you transport the vector: pt_y(\Delta, h), should also depend on t, which parametrize the geodetic gamma(t).

The paragraph lines 107-112 makes an important point about invariance, which is addressed by the comment ""our purpose is to locally compute the gradient direction using the operators defined above"". This is not clear to me, how does it solve the problem of the non invariance with negative numbers. Can you please better explain this part?

In line 210 you mention the use of dropout in combination with batch normalization. In the original batch normalization paper, the authors claim dropout is not necessarily in presence of batch normalization. Did you experiment this in your experiments? Can you comment on that?

When you pad the images in CIFAR-10, how do you fill the new pixel? Are they left black?

As to experimental part, can you please mention how many runs (executions) did you do? Can you mention variance?

Another important point is related to time complexity of the algorithm and the impact of the Riemannian approach compared to an Euclidean one, in terms of execution time for the experiments in Table 1.

Section 5.1 seems somehow a little bit disconnected from the rest of the paper. Let me better argument. The authors use FA to link the Grassmann manifold to the space of probabilistic function, and this give them a nice way to measure the KL divergence between the posterior and the prior. However in practice they say they use a different function, which appears in 15. My question is if the analysis based on FA can be useful in practice, or if gives just a function intersting from a theoretical perspective and but usable in practice, and if 15 could be instead introduced directly without referring to FA (and to the Grassmann manifold). Moreover, how do you use L^C in training? Does it play a role of regularizing term? I would add a reference to that in the Algorithm 4 you have in the paper.

Finally, in Figure 2, how do you change the learning rate in the experiments? can you plot it over the same plot?"
Adaptive Clustering through Semidefinite Programming,Martin Royer,https://proceedings.neurips.cc/paper/2017/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html,"The paper proposes a semidefinite relaxation for K-means clustering with isotropic and non-isotropic mixtures of sub-gaussian distributions in both high and low-dimensions.

The paper has a good technical analysis of the methods proposed in it. There are some supporting empirical experiments too. However, one thing would have been useful to see was how computation time scales with both p and n. The empirical experiments considered n=30, which is pretty small for many application situations. So, for a better picture, it would have been good to see some results on computation times.

The paper is nicely written and the flow of arguments in the paper is relatively clear. There are some minor typos like in line 111 in definition of b2, the subscript should be b in [n]\{a, b1}.

The paper extends an approach of semidefinite relaxation for K-means clustering to account for non-isotropic behavior of mixture components. The method proposed can work for high-dimensional data too. The method will be quite useful but possibly computationally heavy too. So, an idea about the computational resources necessary to implement this method would have been useful.","This paper is a nice contribution to the theory of SDP algorithms for clustering, inspired by some of the techniques arising in SDP algorithms for community detection in the stochastic block model. This paper adds a correction term to the usual semidefinite relaxation of k-means, in order to improve on the weakness of previous SDPs in the anisotropic case. The paper proves a detailed non-asymptotic recovery guarantee, and analyzes the asymptotic scaling in multiple regimes, comparing the resulting asymptotics to known phenomena in the stochastic block model. The numerical experiments are compelling, and the writing is clear and enjoyable.

Perhaps a slight weakness is that, while the introduction motivates a need for robust algorithms for clustering, questions of robustness could use more discussion. In particular, owing to the minima and maxima in the statement of \hat \Gamma^{corr}, I wonder whether this correction compromises robustness properties such as insensitivity to outliers.","This paper extends the PECOK method (Bunea et al, arXiv:1606.05100) to the case of the usual clustering setting, where PECOK was originally developed for relational data.  The approach is somewhat standard: first transform the data into a relational data, which in this case is just the gram matrix, and then apply the PECOK methodology.  Thus the conceptual innovation is rather marginal.

The main technical effort is to show exact clustering for sub-Gaussian error.  Conceptually, exact clustering is not realistic for any purpose.  Theoretically, the proof simply follows that of PECOK and union bound.  An interesting part is the de-biasing of the gram matrix (Proposition A.1). However, this seems to rely on the strong separation condition which is in turn needed for exact clustering.

The paper would be improved if the author(s) can work something out in the more practical regime of approximate clustering, e.g., with vanishing mis-clustering proportion.

On line 89, the author(s) used the word ""natural relaxation"" -- is it possible to show that $\mathcal{C}_K$ is the convex hull of $\mathcal{C}_K^{0,1}$ ?"
#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, Pieter Abbeel",https://proceedings.neurips.cc/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html,"This paper is already available on arxiv and cited 10 times. It is a very good paper introducing a new approach to count-based exploration in deep reinforcement learning based on using binary hashcodes. The approach is interesting, the presentation is didactical, the results are good and the related literature is well covered. I learned a lot from reading this paper, so my only criticisms are on secondary aspects.

I have few global points:

- I think it would be interesting to first train the convolutional autoencoder, and only then use your exploration method to compare its performance to state-of-the-art methods. This would allow for a more detailed analysis by disentangling the factors of potential inefficiency.

- I would put the ""related work"" section more forward in the paper, as the presentation of the methods would benefit from the points made in the first paragraph of related work and in the comparison with [4]. As a reader, I found that these points were important and arriving too late.

- Maybe section 5.4 of supplementary material should move into the main paper, and I don't share your main conclusion about it: If you look closely at Table 6, using n(s,a) is most often better than using n(s). To me, prefering n(s) to n(s,a) based just on the max in your table is inadequate, because maybe with an ""optimized"" (k,\beta) pair, you will find that using n(s,a) is better...

More local points:

line 41: ""However, we have not seen a very simple and fast method that can work across different
""domains."" How do you disqualify VIME here?

line 113: ""An alternative method must be used to ensure that distinct states are mapped to distinct binary codes."" Maybe the writing could be more direc there, presenting directly the appropriate approach.

Fig5: despite magnifying the picture several times, I cannot see the baseline. Didn't you forget it?

Footnote 4: the statement is weak. You should give reference to the state-of-the-art performance papers.

In the supplementary part:

Section 1 would benefit from a big table giving all params and their values. Beyond you, it would be nice if the community could move towards such better practices. As is, Section 1 is hardly readable and it is very difficult to check if everything is given.

Section 3: why do you show these images? Do you have a message about them? Either say something of interest about them, or remove them!

line 136: can (be) obtained (this was the only typo I could find, congratulations! ;))

","This paper presents a novel way of doing state-visit counts for exploration in deep RL. They discretize the state space with a hash function and count visits per discretized state in the hash table. Exploration reward bonuses are given based on these visit counts.

The authors present three different ways of doing the hashing. The first is to use a method from the literature called SimHash, based on Locality Sensitive Hashing. The second approach is to train an autoencoder on states with a binary code layer in the middle. This binary code layer is then used as the input to the SimHash function. Finally, they use hand-crafted features from Atari (BASS) and pass these through to SimHash.

The results are mixed. On continuous control tasks, they perform better than VIME on 2/4 tasks and worse than it on 2/4 tasks, using the SimHash approach. On Atari, they get the best results with the auto-encoder version on 2/6 games. 

In the comparison with ref 4, the authors say that approach requires designing and learning a good density model while there's only uses simple hash functions. This comparison does not seem quite fair, the author's approach that compares best to ref 4 requires designing and learning an auto-encoder to get the binary state code.

The idea of discretizing continuous state into a hash table to track visit counts for exploration is a good one. However, it would be nice if there was some single approach that worked across the set of domains instead of 3 different ways of calculating visit counts. The results are also not clearly better than other approaches for doing exploration reward bonuses. 

Thanks for your feedback and clarifications that even your pixel based version of SimHash is performing better than state-of-the-art approaches. ","Summary:
This paper combines count based exploration with hashing in order to deal with large and continuous state spaces. Two different types of hashing are examined. Static hashing, and learned hashing using autoencoders. As is shown in the experimental results section, this approach leads to state-off the art performance in 2 out of the 6 tested Atari 2600 video games, and competitive performance on 1 more.

Quality:
There are a number of areas where this paper could improve on:
- The fact that the performance of the proposed approach is more than an order of magnitude worse than the state of the art in Montezuma, and significantly worse than the state of the art in 2 more domains should not be glossed over. A discussion on the reasons that this is the case should be included in the final version of the paper.
- The experiments in figures 3 and 4 were only repeated over 3-5 random seeds. This is far from enough to obtain statistically significant results.
- Over how many random seeds were the experiments in table 1 that are not part of figure 4 repeated?

Clarity:
Overall this paper is well written and easy to understand. One thing that struck me as strange is that according to the supplementary material making the exploration bonus depend on state-actions rather than just states does not lead to better performance. It would be nice to see a discussion about why this may be the case in the main body of the paper.

Originality:
As far as I can tell the approach presented in this paper is original.

Significance:
This paper presents a significant step forward in bridging the gap between theory and practice in reinforcement learning. I expect the results presented to serve as a building block for followup work."
Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition,"Naoya Takeishi, Yoshinobu Kawahara, Takehisa Yairi",https://proceedings.neurips.cc/paper/2017/hash/3a835d3215755c435ef4fe9965a3f2a0-Abstract.html,"The authors proposed a fully data-driven method for Koopman analysis learning invariant subspaces. The authors proposed a standard linear regression with additional constrains that are implemented using a Neural Network to estimate the Koopman modes.

The authors nicely review the main concepts of Koopman operator and dynamic mode decomposition. They proposed two costs functions; the first one finding the parameter $g$ that minimizes the error between data and estimated regression model and another one minimizing the observations $y$.  

There is however some issues that need further clarifications.  Eq (6) is used to find the embedding between $\bold y$ and $\tilde x$ and this is included in eq (5).  What is the actual form of the minimization? This is just verbally stated but is not clear how the minimization has been modified. 

One major point that needs to be clarified is the coupling of network itself because g and h are coupled. It is however not clear if this are decomposed as a single network or 2 networks. In the later case, is this trained end-to-end?. What are the backpropagation rules for Eq (7)?

Another minor comments is with regards to Taken's theorem, the authors must state what do they mean by ""faithful representation."" since that term do not have any scientific meaning. ","This paper presents a method that takes advantage of MLPs to learning non-linear functions that support Koopman analysis of time-series data. The framework is based on Koopman operator theory, an observation that complex, nonlinear dynamics may be embedded with nonlinear functions g where a spectral properties of a linear operator K can be informative about both the dynamics of the system and predict future data. The paper proposes a neural network architecture and a set of loss functions suitable for learning this embedding in a Koopman invariant subspace, directly from data. The method is demonstrate both on numerical examples and on a few applications (Lorenz, Rossler, and unstable phenomena data).

The paper is exceptionally clearly written, and the use of a neural network for finding Koopman invariant subspaces, a challenging and widely applicable task, is well motivated. The figures nicely illustrate the results, and I find the examples convincing. There's a few questions I was curious about. Firstly, how well does LKIS tolerate larger magnitudes of noise? Does it converge on the right Koopman eigenfunctions, or is there a point past which it fails? Second, how do the g's found by LKIS compare with those found by EDMD and kernel DMD?

I read through the author’s rebuttal and my colleagues’ review, and decided to not change my assessment of the paper.","This paper proposed a data-driven method for learning nonlinear systems (both observation functions and their eigen/spectral representations) using the tool of Koopman spectral analysis. The authors proposed to minimize the RSS square loss within a LS-regression framework. In addition, the author also proposed to use MLP/ANN as the parametric function family that transform the data into a linear space. The proposed algorithm was evaluated on several synthetic non-linear models and real world unstable time series. 

* The selection of compared algorithms in section 5/6 seems rather basic or random. It is not quite clear why such baselines are competitive or state-of-the-art.  
* For the Santa Fe dataset, since OC-SVM is used, would it be possible to provide some metric other than the qualitative estimation? For example, AUC, Precision, etc. 
* (correct me if I am wrong) I could not locate how the hyper/free parameters (k, p, n) should be tuned in practice. It is not clear what are the optimal choice of these free parameters in the provided examples.  Meanwhile, more details on the SGD parameters are not clear. 

"
Online Prediction with Selfish Experts,"Tim Roughgarden, Okke Schrijvers",https://proceedings.neurips.cc/paper/2017/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html,"
This paper studies the problem of learning with expert advice when the experts are strategic. In particular, the authors focus on the study of (randomized) multiplicative weights algorithms and assume the experts aim to maximize their weights in the next round. The authors first show that the algorithm is incentive compatible if and only of the weight update function is a proper scoring rule. They then derive the regrets bounds with spherical scoring rules. The gap between the regret bounds between the setting with and without strategic experts are also derived.

Overall, I like the paper.  The technical results seem to be sound ( I didn’t carefully check all the proofs). While there are some assumptions I hope can be relaxed or better justified, I think the current results are interesting, and I would lean towards accepting the paper.

Minor comments:

- The connection between the proper scoring rule and weight update function is intuitive and interesting. The assumption that experts aim to maximize their weights is fine but seems a bit restrictive. I am wondering how easy it is to relax the assumption and extend it to, for example, the setting that experts aim to maximize some monotone function of the weights.  

- In Proposition 8, the authors show that they can get IC for groups with transferable utility for free. However, it might not be the case if the weights need to be normalized (e.g., the weights of all agents need to sum to 1). It seems to me this could be a more reasonable setting since experts might care about how well they are doing compared with other experts rather than just maximizing their own weights?

- This is out of the scope of the paper: In the current paper, the experts already have the predictions, and they just need to decide what to report. I would be interested to see discussion on the setting when experts can exert different amounts of effort to obtain predictions with different qualities.
","The paper studies the online prediction problem with ""selfish"" expert advice. In this setting, experts are allowed to make predictions other than their true beliefs in order to maximize their credibility. The contributions of the paper consist of three parts:

1) Definition of ""incentive-compatible"" (IC) algorithms which helps reasoning formally about the design and analysis of online prediciton algorithms. Establishing the connection between IC algorithms and Proper Scoring Rules.

2) Designing deterministic and random IC algorithms for absolute loss with non-trivial error bounds. 

3) Establishing a hardness results for selfish setting comparing to honest setting via providing lower bounds for both IC and non-IC algorithms.

The paper is written and organized well, and consequently, easy to read. The ""selfish"" setting of the paper is novel and well-motivated by real-world scenarios. The paper provides adequate and sound theoretical analysis in worst-case along with some experimental results illustrating the behavior of its introduced algorithms in typical data. The contributions of the paper seems theoretically significant. 


","In this paper, the standard (binary outcome) ""prediction with expert advice"" setting is altered by treating the experts themselves as players in the game.  Specifically, restricting to the class of weight-update algorithms, the experts seek to maximize the weight assigned to them by the algorithm, as a notion of ""rating"" for their predictions.  It is shown that learning algorithms are IC, meaning that experts reveal their true beliefs to the algorithm, if and only if the weight-update function is a strictly proper scoring rule.  Because the weighted-majority algorithm (WM) has a weight update which is basically the negative of the loss function, WN is thus IC if and only if the (negative) loss is strictly proper.  As the (negative) absolute loss function is not strictly proper, WM is not IC for absolute loss.  The remainder of the paper investigates the absolute loss case in detail, showing that WM with the spherical scoring rule performs well, and moreover performs better than WM with absolute loss; no other deterministic IC (or even non-IC) algorithm for absolute loss can beat the performance in the non-strategic case; analogous results for the randomized case.

I like the results, but find the motivation to be on loose footing.  First, it is somewhat odd that experts would want to maximize the algorithm's weight assigned to them, as after all the algorithm's weights may not correspond to ""performance"" (a modified WM which publishes bogus weights intermittently can still achieve no regret), and even worse, the algorithm need not operate by calculating weights at all.  I did find the 538 example compelling however.

My biggest concern is the motivation behind scoring the algorithm or the experts with non-proper scoring rules.  A major thrust of elicitation (see Making and Evaluating Point Forecasts, Gneiting 2011) is that one should always measure performance in an incentive-compatible way, which would rule out absolute loss.  The authors do provide an example in 538's pollster ratings, but herein lies a subtlety: 538 is using absolute loss to score *poll* outcomes, based on the true election tallies.  Poll results are not probabilistic forecasts, and in fact, that is a major criticism of polls from e.g. the prediction market community.  Indeed, the ""outcome"" for 538's scoring is itself a real number between 0 and 1, and not just the binary 0,1 of e.g. whether the candidate won or lost.

I would be much more positive about the paper if the authors could find a natural setting where non-proper losses would be used.

Minor comment:  Using ""r"" as the outcome may be confusing as it often denotes a report.  It seems that ""y"" is more common, in both the scoring rules and prediction with expert advice literature."
Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach,"Slobodan Mitrovic, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub M. Tarnawski, Volkan Cevher",https://proceedings.neurips.cc/paper/2017/hash/3baa271bc35fe054c86928f7016e8ae6-Abstract.html,"Problem studied : The paper studies the following question. The goal is to find a set of elements T from some ground set E in a streaming fashion such that, if we delete any set of m elements from T then we can still find a set of k elements S such that f(S) is maximized where f is a submodular function. 
Applications: Maximizing submodular functions has multiple applications and we know streaming algorithms for this problem. The reason why a robust version is useful is if we find a solution and then we might need to remove some of the elements (say if you compute a set of movies for the user and then find that the user is not interested in some of them). 
Results: The paper gives a constant factor approximation to this problem while returning a set T of size (m*logk+k)*log^2k.

Comments: The paper's results are correct to the best of my understanding. The solution is non-trivial and the algorithm and it's analysis are technically challenging. While the problem is interesting my main concern is the relevance of this paper to the conference. The applications and their motivation are quite weak and the problem is not general enough. ","This paper proposes a two-stage procedure for robust submodular maximization with cardinality constraint k. In the first stage, a robust streaming algorithm STAR-T is designed, which is based on a partitioning structure and an decreasing thresholding rule. In the second stage, the simple greedy method is used. The STAR-T is proven to have constant approximation guarantee when k is up to infinity (Thm4.1). lem4.2~4.3 prove that STAR-T with simple greedy method can deal with the loss of elements. 
Experimental results show that the proposed algorithm can do well on the problem of influence maximization and personalized movie recommendation.

This work is interesting as it is the first streaming algorithm for robust submodular maximization. Maybe the streaming setting will have wide range of applications and this work is very enlightening. The STAR-T algorithm has two hyper parameters, \tao and w. In Thm 4.5, the authors showed how to approximate \tao. The authors showed “w=1” can do well in Experiments. It’s convenient to use STAR-T without trying different hyper parameters.

The major problem I concerned is that the experiment is not very expressive. The STAR-T-Greedy algorithm can not always outperform the Sieve-St algorithm. 
In addition, it’s strange that the objective function of Influence Maximization is f(Z)=|N(Z) \cup Z| (line 219 in this paper), which is not the same with the function in the referred article (D. Kempe, J. Kleinberg, and É. Tardos, “Maximizing the spread of influence through a social network,” in Int. Conf. on Knowledge Discovery and Data Mining (SIGKDD), 2003). It is not clear to me that how STAR-T performs on such a spread model.","This paper studies the problem of submodular maximization in a streaming and robust setting. In this setting, the goal is to find k elements of high value among a small number of elements retained during the stream, where m items might be removed after the stream. Streaming submodular maximization has been extensively studied due to the large scale applications where only a small number of passes over the data is reasonable. Robustness has also attracted some attention recently and is motivated by scenarios where some elements might get removed and the goal is to obtain solutions that are robust to such removals. This paper is the first to consider jointly the problem of streaming and robust submodular maximization.

The authors develop the STAR-T algorithm, which maintains O((m log k + k )log^2 k) elements after the stream and achieves a constant (0.149) approximation guarantee while being robust to the removal of m elements. This algorithm combines partitioning and thresholding techniques for submodular optimization which uses buckets. Similar ideas using buckets were used in previous work for robust submodular optimization, but here the buckets are filled in a different fashion. It is then shown experimentally that this algorithm performs well in influence maximization and movie recommendation applications, even against streaming algorithms which know beforehand which elements are removed. 

With both the streaming and robustness aspects, the problem setting is challenging and obtaining a constant factor approximation is a strong result. However, there are several aspects of the paper that could be improved. It would have been nice if there were some non-trivial lower bounds to complement the upper bound. The writing of the analysis could also be improved, the authors mention an “innovative analysis” but do not expand on what is innovative about it and it is difficult to extract the main ideas and steps of the analysis from the proof sketch. Finally, the influence maximization application could also be more convincing. For the experiments, the authors use a very simplified objective for influence maximization which only consists of the size of the neighborhood of the seed nodes. It is not clear how traditional influence maximization objectives where the value of the function depends on the entire network for any set of seed nodes could fit in a streaming setting."
Neural Program Meta-Induction,"Jacob Devlin, Rudy R. Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli",https://proceedings.neurips.cc/paper/2017/hash/3bf55bbad370a8fcad1d09b005e278c2-Abstract.html,"The paper is making an evaluation of several approaches for neural network-based induction of computer programs. The main proposal is on the use of meta-learning, in order to exploit knowledge learned from various tasks, for learning on a specific task with a small number of instances. For that purpose, three approaches are proposed: 1) transfer learning for adapting an existing model trained on a related task; 2) meta program induction, where the model has been trained to work on a variety of tasks; and 3) meta program adapted for a given task. The paper also proposes to make use of a synthetic domain Karel, which comes from an educational language for teach computer programming, which consists in moving a robot in a 2D grid through computer instructions. Results are reported with varying the number of instances used for program induction for the three meta approaches proposed + plain method, with results showing some advantages with little number of instances. However, results still are far from good in all cases with little number of instances, with results in the 30%-40% ballpark.

The paper is tackling an interesting problem. However, the results and usability of the approaches are not supporting well the proposal. Yes, the meta approaches help the induction process with a very small number of training instances. But still, the results are poor and not clearly of any practical usefulness.

Moreover the Karel domain may be convenient for testing program induction, but it appears to be far from practical program induction context. The search space with such setting is very specific, close to the classical maze toy problems of reinforcement learning. Unless for some specific applications (not given in the paper), I am not confident that other program induction setting with behave similarly. The search space of programs for the Karel domain appears to be quite smooth, the program with likely achieve valid results with some random solutions. With most real-life problems, this is not likely to be true, finding valid programs would be by itself challenging.

Overall, the paper is well-written, the authors were apparently careful at preparing it. However, the paper stays quite conceptual in its presentation, hindering much low-level details. I found the technical presentation of the proposal (Sec. 8) hard to follow, it tried to pack everything in little space, with explanations not being as precise and detailed as it should. I am still unsure of what the authors did in practice. I am not knowledgeable of the related literature, there are maybe some elements that are obvious for researcher working on that specific problem -- but if this is the case, proper references providing all required background is not properly given. Capacity to reproduce the experiments appears to be very low to me, given that a lot of details are missing. 

In brief, the proposal in interesting, but the experimental results are not there, so the usefulness of making meta learning for neural program induction has not been demonstrated empirically. Also, the lack of details in the presentation of the technique appears incomplete, while the problem domain tackled is not necessary representative of practical domains for program induction. Therefore, I cannot recommend acceptance of the paper, although I think the core idea of meta-learning for program induction may still be worthful. Stronger experimental evidences supporting the approach are needed to make it acceptable.
","The paper investigates neural nets learning to imitate programs from varying numbers of input-output examples, both in the plain supervised learning setting and by leveraging transfer across problems. This is done in the domain of Karel, a simple programming language that describes how input grid worlds are transformed into output grid worlds through actions of a so-called hero who moves around in those worlds according to instructions given in a Karel program. The paper randomly generates many such Karel programs and, for each program, a number of corresponding input-output worlds. The learning problem for the neural architectures is to learn from input-output examples how to apply the same transformation to worlds as the (latent) Karel program. The paper compares plain supervised learning, supervised learning that's initialized using the closest previous task, k-shot meta-learning, and k-shot meta-learning that is refined based on the current task. The paper finds that in the Karel domain, each of these techniques has its use depending on the number of tasks and input-output examples per task available.

The paper seems technically sound.

There is another natural comparison that the authors didn't make, which is to follow the k-shot approach, but to learn a function from the example tasks to the *weights* of a neural net that turns input into output. That is, the task encoder generates the weights of both input encoder and output decoder. In this setting, adaptation is more natural than in the current META+ADAPT case, since you can literally just start with the weights generated by the meta network and optimize from there in exactly the same way that plain supervised learning does.

The paper is generally clear, but I don't sufficiently understand the Karel task and the difficulty of the associated learning problem. The paper shows a Karel sample task, but I would have liked to see more examples of the randomly sampled tasks (perhaps in an appendix) so that I can better understand how representative the sample task is. This would have answered the following questions: (1) How complex are the programs sampled from the Karel DSL CFG? A thing that can happen if you sample expressions from recursive PCFGs is that most are really simple, and a few are very complex; the expressions are not of similar size. Is that the case here? (2) How large are the I/O grids? How much variation is there? (3) I'm surprised that the LSTM can output the required diff to transform inputs into outputs as well as it does. If the diffs are long, I would have expected issues with the LSTM ""forgetting"" the initial context after a while or having trouble to exactly produce the right values or not generalizing correctly. The fact that this works so well makes me wonder whether the learning problem is easier than I think. I was also surprised to see that the NLL is as good as it is (in Figure 5). Because the computational structure of the conv net is quite different from the way the input-output examples are produced by the Karel program (the hero moves around locally to manipulate the grid, the conv net produces the output in one go), I think I would have expected the neural net to only approximately solve the problems. (This impression may well be due to the fact that I don't have a good intuition for what the problems are like.)

The table comparing to related work is helpful. That said, I'm still a bit unclear on the extent to which the paper's main contribution is the systematic evaluation of different program induction methods, and to what extent the paper claims to actually have invented some of the methods.

I think this paper will be useful to practitioners, conditioned on the results generalizing across domains. As the paper says, the results do seem fairly intuitive, so it seems plausible that they do generalize, but it would certainly have been nice to replicate this in another domain.","Neural Program Meta-Induction
This paper considers the problem of program induction using neural networks as the model class. Straightforward applications of neural nets require large labelled datasets for the task. The authors propose meta learning to leverage knowledge across different tasks, resembling other meta-learning works such as Santoro et al. The authors further propose that finetuning the model (pretrained on background tasks) on the target task further improves the performance. The paper presents evaluation on a benchmark constructed out of the Karel domain-specific language.

The proposed technique closely resembles Santoro’s memory-augmented neural networks[1], and Duan’s one-shot imitation learning[2]. While the latter finds a mention, the former does not. While Duan’s work is specific to imitation learning, Santoro considered the general problem of meta-learning, and evaluate on the omniglot task. It is not clear from the paper how the proposed method is different from MANNs, and whether the distinction is only limited to the choice of benchmark to evaluate on. Santoro and Duan’s models are comfortably applicable to the Karel task, and the proposed model (of this paper) is applicable to omniglot. Thus, the methods could have been compared, and the lack of comparison is conspicuous. It should also be noted that there seems to be no architectural or algorithmic novelty in the proposal which specializes this technique for program induction as opposed to general meta learning/k-shot learning. Program induction often refers to the problem of explicitly learning programs, or at least the model space (even if implicit) being a program (i.e. a sequence of instructions with if-then-else, loops, etc.). The proposed method does not learn a “program”.

While the proposed method is a straightforward application of MANNs to a program induction task (which I define as any task where the model is a program), such an application and its empirical evaluation has not been done previously in the literature according to my knowledge (I request the other reviewers to correct me here if I’m wrong).

The empirical evaluation on the Karel benchmark compares 4 types of models. PLAIN refers to a model simply trained on input-output pairs for the target task. PLAIN+ADAPT is a model trained on non-target background task (mimicking imagenet-style transfer learning) and then finetuned on the target task. META refers to an LSTM model which takes in training input-output examples for the task, and then predicts output for an unseen task. Thus, the IO examples encode the task, which allows this model to be trained on multiple tasks and thus hopefully take advantage of cross-domain knowledge. META+ADAPT finetunes a META model on the target task.

The results show that
1. META+ADAPT is always better than META.
2. PLAIN+ADAPT is always better than PLAIN.
3. META(+ADAPT) is better than PLAIN(+ADAPT) in small dataset regime.
4. PLAIN(+ADAPT) is better than META(+ADAPT) in large dataset regime.
The conclusions seem intuitive.

To improve the paper further, the authors should make it more clear the contributions of this paper, in particular how it relates to Duan and Santoro's models. A quantitative comparison with those models in common domains of applicability (such as omniglot) would also be helpful. Finally, program induction-specific architectures and algorithms would help in the novelty factor of the paper.

[1]: https://arxiv.org/abs/1605.06065
[2]: https://arxiv.org/abs/1703.07326

-- Post Rebuttal --

Thanks for your response. It would be very valuable of Karel task was open-sourced so that the community could make further progress on this interesting task. Please also add citations to Santoro et al. and discuss the relationship of your model with Santoro et al. I increase the score by 1 point.

"
The Scaling Limit of High-Dimensional Online Independent Component Analysis,"Chuang Wang, Yue Lu",https://proceedings.neurips.cc/paper/2017/hash/3cfbdf468f0a03187f6cee51a25e5e9a-Abstract.html,"The paper studies the high-dimensional scaling limit of a stochastic update algorithm for online Independent Component Analysis.
The main result of the paper is an exact characterization of the evolution of the joint empirical distribution of the estimates
output by the algorithm and the signal to be recovered, when the number of observations scales linearly with the dimension of the problem.
The authors argue that in the limit, this joint distribution is the unique solution to a certain partial differential equation,
from which the performance of the algorithm can be predicted, in accordance with the provided simulations.

1- Overall, the result is fairly novel, and provides interesting insight into the behavior of stochastic algorithms in this non-convex problem.
The paper is written in a fairly clear manner and is (mostly) easy to read.
My main concern is that the mathematics are written in a very informal way, so that it is not clear what
the authors have actually proved. There are no theorem or proposition statements.
E.g., under what conditions does the empirical measure have a weak limit?
Under what (additional) conditions does this limit have a density?
And when is this density the unique solution to the PDE?
The heuristic justification in the appendix is not very convincing either (see bullet 4).

2- A very interesting insight of the theory is that the p variables of the problem decouple in the limit,
where each variable obeys a diffusion equation independently of the others.
The only global aspect retained by these local dynamics is via the order parameters R and Q. 
This phenomenon is standard in mean-field models of interacting particle systems, where the dynamics of the particles decouple when the order parameters converge to a limit.
The authors should probably draw this connection explicitly; the relevant examples are the Sherrington-Kirkpatrick model, rank-one estimation in spiked models, compressed sensing, high-dim. robust estimation…

3- The connection to the SDE line 175 should be immediate given that the PDE (5) is its associated Fokker-Planck equation. 
The iteration (14) is easier seen as a discretization of the SDE rather than as some approximate dynamics solving the PDE. Therefore, the SDE should come immediately after the statement of the PDE, then the iteration.

4- The derivations in the appendix proceed essentially by means of a cavity (or a leave-one-out) method,
but end abruptly without clear conclusion. The conclusion seems to be the decoupled iteration (14), not the actual PDE.
This should be made clear, or otherwise, the derivation should be conducted further to the desired conclusion.
The derivation contain some errors and typos; I couldn't follow the computations
(e.g., not clear how line 207 was obtained, a factor c_k should appear next to Q_k^p on many lines...)
  

5-There are several imprecisions/typos in the notation:
line 36: support recover*y*
eq 2: sign inconsistency in front of \tau_k (- gradient, not +)
line 100: x_k should be x
line 133: u should be \xi (in many later occurrences)
eq 8: \tau not defined (is it the limit of the step sizes \tau_k?)
line 116: t = [k/p] should be k = [tp]","The article gives a scaling limit analysis of the ICA algorithm. The scaling limit is obtained in the regime where the dimension ""p"" of the vector to be recovered goes to infinity -- the scaling limit is a PDE that describes the evolution of the probability density of the joint distribution between ""x"" and ""xi"".

The sketch of proof is convincing and standard (computation of mean/variance <--> drift/volatility), although I have not checked all the   details.

The most interesting part is indeed the insights one can gain through this scaling analysis.

1) in some particular cases, the analysis shows that it is necessary to have small enough learning rate and a ""good"" initial guess in order to converge to a non-trivial solution. That's extremely interesting and gives theoretical justification for empirical observations previously obtained in the literature.

2) in other particular cases, the analysis explains why (by introducing a convex ""effective"" objective function)  online ICA is empirically successful even though minimizing a non-convex objective function.

","\section*{Summary}

The paper looks at an online learning method for a generative model as give in equation (1) in the paper, 
with the unknown being the $p$-dimensional parameter $\xi$. 	
The online learning algorithm is described in equation (2).
The main contribution of the paper is that it provides a PDE limit for an empirical measure (in particular for its 
transition density) involving the co-ordinates of $\xi$ and the iterative estimate $x_{k}$ - after appropriate time rescaling.
The authors argue that getting such an explicit limit allows for a better understanding of the performance of the algorithm in high dimensions.

\section*{Comments}

I am afraid the content of the paper is beyond my areas of expertise. 
I do appreciate that similar results have proven to be useful in the context of high-dimensional MCMC 
algorithms, so I would guess that the direction followed in the paper is of interest.
"
Practical Locally Private Heavy Hitters,"Raef Bassily, Kobbi Nissim, Uri Stemmer, Abhradeep Guha Thakurta",https://proceedings.neurips.cc/paper/2017/hash/3d779cae2d46cf6a8a99a35ba4167977-Abstract.html,"
Overall, the improvement compared to the state-of-the-art [4] is not clearly stated.
In Table 1, it would be better to show the comparison with existing methods.
Does this work enable to reduce the server time and user time with achieving the same utility and privacy achieved by  [4]?
In Sec 3.2, what is the difference between FreqOracle protocol and the basic randomizer protocol in [4]? 

Line 187:
In the TreeHist protocol, the high probability bound on the number of nodes survived is shown whereas the pruning strategy is not described at all. In algorithm 3 and algorithm 6, I found the pruning logic, but this point should be mentioned in the main body.  I could not find justification on the high probability bound on the number of nodes in the appendix.

In Section 5.1, the authors explain the results as follows:
...for higher frequencies both RAPPOR and our algorithm perform similarly. However, in lower frequency regimes, the RAPPOR estimates are zero most of the times, while our estimates are closer to the true estimates.
This should be numerically evaluated. What is important in the heavy hitter problem is that the frequencies of higher rank elements are estimated accurately and it is not clear the proposed method performs better than RAPPOR in higher ranks. 

The major contribution of this study is an improvement of computation time at user and server side. It would be helpful for readers to show how much the computation time is improved compared to [4], particularly at the user side. 




","Private Heavy Hitters is the following problem: a set of n users each has an
element from a universe of size d. The goal is to find the elements that occur
(approximately)
most frequently, but do so under a local differential privacy constraint.
This is immensely useful subroutine, which can be used, e.g. to estimate the
common home pages of users of a browswer, or words commonly used by users of a
phone. In fact, this is the functionality implemented by the RAPPOR system in
chrome, and more recently in use in iphones.

To make things a bit more formal, there is an a desired privacy level that
determines how well one can solve this problem. A \gamma-approximate heavy
hitters algorithm outputs a set of elements that contains most elements with
true relative frequency at least 2\gamma and a negligible number of elements with
relative frequency less than \gamma. The best achievable accuracy is \sqrt{n log
d}/eps, and Bassily and Smith showed matching lower and upper bounds. In that
work, this was achieved at a server processing cost of n^{5/2}. This work gives
two more practical algorithms that achieve the same gamma with a server
processing cost of approximately n^{3/2}.

On the positive side, this is an important problem and algorithms for this
problem are already in largescale use. Thus advances in this direction are
valuable.
On the negative side, in most applications, the target gamma is a separate
parameter (see more detailed comment below) and it would be much more accurate
to treat it as such and present and compare bounds in terms of n, d and gamma.
Clarifying these dependencies better would make the work more relevant and
usable.

Detailed comment:
In practice, the goal is usually to collect several statistics at a small
privacy cost. So a target eps, of say 1, is split between several different
statstics, so that the target gamma is not \sqrt{n log d}/eps, but \sqrt{n log
d}/ k\eps, when we want to collect k statistics. Often this k itself is
determined based on what may be achievable, for a fixed constant gamma, of say
0.01. In short then, the gamma used in practice is almost never \sqrt{n log
d}/eps, but a separate parameter. I believe therefore that the correct
formulation of the problem should treat gamma as a separate parameter, instead
of fixing it at ~\sqrt{n}."
Mixture-Rank Matrix Approximation for Collaborative Filtering,"Dongsheng Li, Chao Chen, Wei Liu, Tun Lu, Ning Gu, Stephen Chu",https://proceedings.neurips.cc/paper/2017/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html,"This paper proposed a mixture rank matrix factorization which decompose a low rank matrix as a mixture of sub-matrices with low-rank. The proposed approach is an extension of probabilistic matrix factorization and it's been shown that it has a superior performance compare to existing methods. I have the following concern:
1- Paper presentation is not very well and it includes all the derivation for optimization algorithm which could move to the appendix and instead some analysis regarding the convergence and computational complexity could be added to the main text.
2- This is not quite clear from the text that how the model perform differently if the submatrices have overlap structure. I guess in this case there would be a lot of scenarios and computationally make it intractable.
3- By imposing more structure in the matrix, there are more hyperparamters to be optimized and looking at the experimental results, in most cases the improvement is very marginal and not convincing.
","This is an excellent paper, proposing a sound idea of approximating a partially defined rating matrix with a combination of multiple low rank matrices of different ranks in order to learn well the head user/item pairs (users and items with lots of ratings) as well as the tail user/item pairs (users and items we few ratings). The idea is introduced clearly. The paper makes a good review of the state-of-the-art, and the experiment section is solid with very convincing results.

In reading the introduction, the reader could find controversial the statement in lines 25-27 about the correlation between the number of user-item ratings and the desired rank. One could imagine that a subgroup of users and items have a large number of ratings but in a consistent way, which can be explained with a low rank matrix. The idea is getting clear further in the paper, when explained in the light of overfitting and underfitting. The ambiguity could be avoided in this early section by adding a comment along the line of “seeking a low rank is a form of regularization”.
"
Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods,"Veeranjaneyulu Sadhanala, Yu-Xiang Wang, James L. Sharpnack, Ryan J. Tibshirani",https://proceedings.neurips.cc/paper/2017/hash/3e60e09c222f206c725385f53d7e567c-Abstract.html,"This paper considers graph-structured signal denoising  problem. The particular structure enforced involves a total variation type penalty involving higher order discrete derivatives. Optimal rates for penalized least-squares estimator is established and minimax lower bound is established. For the grid filtering case the upper bounds are established under an assumed conjecture. 

The paper is well written. The main proof techniques of the paper are borrowed from previous work but there are some additional technicalities required to establish the current results. I went over the proofs  (not 100% though) and it seems there is no significant issues with it. Overall I find the results of the paper as interesting contributions to trend filtering literature and I recommend to accept the paper. 

Minor: it is not clear why lines 86-87 are there in the supplementary material. 

","This paper studies the problem of estimating function values over a d-dimensional grid. The authors propose a new grid trend filtering penalty, they show estimation rates of their newly proposed algorithm as well as they improve previous rates for graph-trend filtering for the special case d = 2. They complement these rates by showing lower bounds which nearly match the estimation rates of the grid and graph trend filtering. The authors also comment on how TV-graph and TV-grid classes relate to classical Holder classes. 

The authors should double check the paper for typos as I saw several typos:

159: possibleto
225: linear smoothers such that

How did the authors choose the tuning parameters in practice to produce Figure 1?

What is the intuition why Conjecture 1 holds? As it is written it seems the authors assume it just to make their proof go through.

","This paper established the lower and upper bounds of two total variation (TV) classes: TV-graph and TV-grid under the generalized lasso estimator with certain chosen penalty operators, and proved that the two corresponding trend filtering methods are minimax optimal.  

The motivation of broaden the existing TV denoising results to quantify the minimax rate in higher order graph/grids is clear, the presentation of the paper is nice, and the math derivation is clear. 

The content is mathematically/statistically solid, thought not clear about the influence on imaging due to reviewer's background. (BTW, has the incoherence of A.5 in supplemental material figured out already?)


"
Robust Conditional Probabilities,"Yoav Wald, Amir Globerson",https://proceedings.neurips.cc/paper/2017/hash/3eb414bf1c2a66a09c185d60553417b8-Abstract.html,"This paper considers the problem of bounding conditional probabilities given second-order marginals of the features. The bounds are used in a semi-supervised learning setting, and shown to be comparable with VAE, but not as good as Ladder.

The problem of estimating lower bounds of conditional probabilities is interesting. While the approach of estimating conditional probabilities using parametric models can lead to estimation errors due to the parametric presentation, the lower bounds can introduce errors due to being overly conservative and seems to require approximations in practice (section 6). It would be helpful to comment on this.

The writing could have been improved. For example, in Section 6, it is unclear how continuous features are turned to discrete ones. Does MLP stand for multi-layer perceptron? Does Fig 1 report errors (instead of accuracies)? How are the benchmark algorithms set up? In the caption for Fig 2, red should be green?","This paper studies the problem of computing probability bounds, more specifically bounds over probability of atoms of the joint space and conditional probabilities of the class, under the assumption that only some pairwise marginal as well as some univariate marginal values are known. The idea is that such marginals may be easier to obtain than fully specified probabilities, and that cautious inferences can then be used to produce predictions. 

It is shown that when the marginals follow a tree structure (results are extended to a few other structures), then the problem can actually be solved in closed, analytical form, relating it to cover set and maximum flow problems. Some experiments performed on neural networks  show that this simple method is actually competitive with other more complex approaches (Ladder, VAE), while outperforming methods of comparable complexity. 

The paper is elegantly written, with quite understandable and significant results. I see no reason to not accept it. Authors may be interested in looking at the following papers coming from the imprecise probability literature (since they deal with partially specified probabilities, this may be related):

* Benavoli, A., Facchini, A., Piga, D., & Zaffalon, M. (2017). SOS for bounded rationality. arXiv preprint arXiv:1705.02663.

* Miranda, E., De Cooman, G., & Quaeghebeur, E. (2007). The Hausdorff moment problem under finite additivity. Journal of Theoretical Probability, 20(3), 663-693.

Typos:

* L112:  solved Our —> solved. Our
* L156: assume are —> assume we are
* References: 28/29 are redundant ","The paper introduces results on bounding the joint and conditional probability of multivariate probability distributions, given knowledge of univariate and bivariate marginals. 

Theorem 4.1 is to my opinion a novel and interesting result, which defines the core of the paper. In addition, solutions are provided to interpret the optimization problem in this theorem. Two applications, one in semi-supervised learning and another in multi-label classification, are studied. 

Despite being dense and expecting a lot of background from the reader, I think that this paper is well written. At least I was able to understand the theoretical results after doing some effort. Moreover, I also believe that the results might be useful for different types of applications in machine learning. Apart from the applications mentioned in the paper, I believe that the results could also be used to provide tighter worst-case regret bounds in multi-label classification when using the Hamming loss as a surrogate for the subset zero-one loss. In that scenario, optimizing the Hamming loss corresponds to marginals, whereas subset zero-one loss corresponds to the joint distribution, see. e.g. Dembczysnki et al. On label dependence and loss minimization in multi-label classification. Machine Learning 2012.  

To my opinion, this is an interesting paper. 
"
Attention is All you Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html,"This work introduces a quite strikingly different approach to the problem of sequence-to-sequence modeling, by utilizing several different layers of self-attention combined with a standard attention. The work uses a variant of dot-product attention with multiple heads that can both be computed very quickly (particularly on GPU). When combined with temporal embeddings, layer norm, and several other tricks, this technique can replace the use of RNNs entirely on seq2seq models. Since this removes a serial training bottleneck, the whole system can be trained much more efficiently. Even better the system achieves state-of-the-art results on translation, and significantly improves the performance of seq2seq only parsing models. 

I feel this work is a clear accept. Seq2seq is so influential that major improvements of this form will have significant impact on the field of NLP almost instantly. This work is already the talk of the community, and many people are trying to replicate these results already. While none of the underlying techniques here are strikingly novel in themselves, the combination of them and the details necessary for getting it to work as well as LSTMs is a major achievement. 

As part of this review, I spent a lot of time reimplementing the work and looking through the code. Here are a couple suggestions of areas that I got tripped up on:

- There are a lot of hyperparameters in the code itself that I had to extract, might be nice to include these in the paper.
- The learning rate schedule seems to really matter. Using simple SGD works fine for LSTM, but seems to fail here 
- Inference for this problem is quite different than other NMT systems, might be worth discussing a bit more. 
","The paper presents a new architecture for encoder/decoder models for sequence-to-sequence modeling that is solely based on (multi-layered) attention networks combined with standard Feed-Forward networks as opposed to the common scheme of using recurrent or convolutional neural networks. The paper presents two main advantages of this new architecture: (1) Reduced training time due to reduced complexity of the architecture, and (2) new State-of-the-Art result on standard WMT data sets, outperforming previous work by about 1 BLEU point.

Strengths:
- The paper argues well that (1) can be achieved by avoiding recurrent or convolutional layers and the complexity analysis in Table 1 strengthens the argument.
- (2) is shown by comparing the model performance against strong baselines on two language pairs, English-German and English-French.
The main strengths of the paper are that it proposes an entirely novel architecture without recurrence or convolutions, and advances state of the art.

Weaknesses:
- While the general architecture of the model is described well and is illustrated by figures, architectural details lack mathematical definition, for example multi-head attention. Why is there a split arrow in Figure 2 right, bottom right? I assume these are the inputs for the attention layer, namely query, keys, and values. Are the same vectors used for keys and values here or different sections of them? A formal definition of this would greatly help readers understand this.
- The proposed model contains lots of hyperparameters, and the most important ones are evaluated in ablation studies in the experimental section. It would have been nice to see significance tests for the various configurations in Table 3.
- The complexity argument claims that self-attention models have a maximum path length of 1 which should help maintaining information flow between distant symbols (i.e. long-range dependencies). It would be good to see this empirically validated by evaluating performance on long sentences specifically.

Minor comments:
- Are you using dropout on the source/target embeddings?
- Line 146: There seems to be dangling ""2""","Summary: This paper presents an approach for machine translation using attention based layers. The model does not include convolution or rnns and still achieves state of the art on WMT14 English-German and English-French data sets. The model uses parallel attention layers whose outputs are concatenated and then fed to a feed forward position-wise layer.

Qualitative Assessment:
The paper reads well and is easy to follow. The experimental setup is clear and provides enough details for replication.
The paper provides many useful hints such as scaled dot product attention which improves gradient flow.
A lot of content is presented and I hope to see a more in depth version."
A General Framework for Robust Interactive Learning,"Ehsan Emamjomeh-Zadeh, David Kempe",https://proceedings.neurips.cc/paper/2017/hash/3f647cadf56541fb9513cb63ec370187-Abstract.html,"
The paper proposes a general framework for interactive learning. In the framework, the machine learning models are represented as the nodes in a graph G and the user feedback are represented as weighted edges in G. Under the assumption of “if s, s* are the proposed and target models, then any (correct) user feedback s’ must lie on the shortest s-s* path in G”, the author showed that the Multiplicative Weights Update algorithm can efficiently learn the target model. The framework can be applied to three important machine learning tasks: ranking, clustering, and classification. The problem investigated in the paper is interesting and important. The theoretical results of the paper is convincing. However, these results are not empirically verified on real tasks. My comments on the paper are listed as follows:

Pros.
1.	A interactive learning framework which models the models and user feedback with graph. The framework can learn the target model in the presence of random noise in the user’s feedback. 
2.	The framework is really general and can be applied to major machine learning tasks such as classification, clustering and ranking. 

Cons.
1.	My major concern on the paper comes from the absence of empirical evaluation of the proposed framework. The major assumption of the framework is “any (correct) user feedback s’ must lie on the shortest s-s* path in G”. Also, the authors claimed that the proposed framework is robust, which can learn the correct s* even in the presence of random noise in user’s feedback. It is necessary to verify the correctness of these assumption and conclusions based on real data and real tasks. The framework is applied to the tasks of ranking, clustering, and classification, achieving the interactive ranking, interactive clustering, and interactive classification models. It is necessary to compare these derived models with state-of-the-arts models to show the effectiveness of these models. Currently, the authors only showed the theoretical results.  
2.	There is no conclusion section in the paper. It is better to conclude the contributions of the paper in the last section. 
","The paper introduces a new, elegant framework for interactive learning that generalizes several previous interactive learning frameworks, including the classical equivalence query model of Angluin and the interactive clustering model of Balcan and Blum. Building upon the results of Emamjomeh-Zadeh et al. for their ""Binary Search in Graphs"" model, this paper gives general meta-algorithms for learning models interactively as well as algorithms for specific problems.

The introduced model is elegant and original, and surprisingly flexible to generalize many well-known previous results from disparate fields of learning theory, such as classical results about learning hyperplanes with equivalence queries (by Maass and Turán) or the much more recent results of Awasthi and Zadeh about clustering arbitrary concept classes in the interactive clustering model of Balcan and Blum. The model also yields efficient new algorithms for interactively learning rankings.

The paper is excellently written and the introduced model should be of interest to anyone who has worked on various interactive or query frameworks of learning. This new model will likely inspire further research in the area of interactive learning. Although the paper builds heavily on the previous work of Emamjomeh-Zadeh et al. which contains a significant part of the technical machinery used here, the idea of applying Emamjodeh-Zadeh et al.'s results to interactive learning is a novel and clever idea and several of the applications are technically nontrivial.

Overall, the paper is a pleasure to read and deserves to be shared with the NIPS community."
Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions,"Maria-Florina F. Balcan, Hongyang Zhang",https://proceedings.neurips.cc/paper/2017/hash/3f998e713a6e02287c374fd26835d87e-Abstract.html,"This paper talks about noise tolerant learning (both active and passive)  for s-concave distributions. Learning results have been established for the more restrictive log-concave distributions and this paper goes beyond log-concave distributions and provides variants of well known AL algorithms such as Margin Based Active learning (for the active learning setup) and a variant of the  Baum's algorithm (for passive learning setup). While the algorithms themselves are not new (in stylistic sense), a lot of technical results regarding s-concave distributions needs to be derived that can be used in the analysis and design of the above mentioned algorithms. The central contribution is essentially deriving these convex geometric results. I do not have any major objective comments, except about the importance of this work. My main reservation is that while the paper seems to advance the state-of-art in learning theory, I am not sure if these algorithms and results will have much impact outside the learning theory community.  ","This paper extends several previous results on active learning and efficient passive learning from isotropic log-concave distributions to isotropic s-concave distributions with a certain range of s values. The latter is a more general class, that includes log-concave distributions. The results are derived from several new geometric bounds on such distributions, which are similar to previous bounds for log-concave distributions, but more general, capturing also the fact that s-concave distributions can have fat tails. All of the proofs are provided in the appendix, which I did not verify. The algorithms used are the same ones previously used for log-concave distributions.

The results presented in the paper are quite technical. While the paper is relatively clearly written, the results themselves are presented in a highly technical manner which is not easy to interpret. For instance, Theorem 15 and 16 give very complicated label complexity bounds for a single algorithmic iteration. They do not give a single, interpretable, label complexity bound which holds for the entire algorithm. I recommend that the authors improve this aspect of their presentation.

Overall, the paper presents results that advance the theory of active and passive learning. However, due to the technicality of the results and the presentation, it is hard to gain scientific insight from them.","This review is adapted from my review from COLT 2017 - My feedback to this paper has not changed much since then.

This paper studies a new family of distributions, s-concave distributions, which appears in works of (Brascamp and Lieb, 1976; Bobkov, 2007, Chandrasekaran, Deshpande and Vempala, 2009). The main result is a series of upper and lower bounds regarding its probability distribution function and its measure over certain regions. These inequalities can be readily applied to (active) learning linear separators and learning the intersection of two halfspaces.

Overall this is an interesting paper, extending the family of distributions in which the problem of learning linear separators can be efficiently solved. This may spur future research on establishing new distribution-dependent conditions for (efficient) learnability.

Technical quality: on one hand, the results are impressive, since the family of admissible distributions (in the sense of Awasthi, Balcan and Long, 2014 and Klivans, Long and Tang, 2009) is broadened; on the other hand, the analysis only works when -1/(2n+3)<=s<=0. In high dimensional settings, the range of s is fairly small. 

Novelty: the result of this paper follows the reasoning in (Lovasz and Vempala, 2007); but this is definitely a non-trivial extension.

Potential Impact: this work may spur future research on distributional conditions for efficient learnability."
Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee,"Alireza Aghasi, Afshin Abdi, Nam Nguyen, Justin Romberg",https://proceedings.neurips.cc/paper/2017/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html,"The paper presents a technique to sparsify a deep ReLU neural network by solving a sequence of convex problems at each layer. The convex problem finds the sparsest set of weights that approximates the mapping from one layer to another. The ReLU nonlinearity is dealt with by treating the activated and deactivated cases as two separate sets of constraints in the optimization problem, thus, bringing convexity.

Two variants are provided, one that considers each layer separately, and another that carries the approximation in the next layer optimization problem to give the chance to the next layer to counterbalance this error. In both cases, the authors provide bounds on the approximation error after sparsification.

The paper is very clear and well-written. A number of theoretical results come in support to the proposed method.

The experiments section shows results for various networks (fully connected, and convolutional) on the MNIST data. The results are compared with a baseline which consists of removing weights with smallest magnitude from the network. The authors observe that their method works robustly while the baseline methods lands in some plateau with very low accuracy.

The baseline proposed here looks particularly weak. Setting a large number of weights to zero slashes the variance of the neuron preactivations, and the negative biases will tend to drive activations zero. Actually, in the HPTD paper, the authors obtain the best results by iteratively pruning the weights.

A pruning ratio of 80% is quite low. Other approaches such as SqueezeNet have achieved 98% parameter reduction on image data without significant drop of performance.","In this paper the authors introduce an algorithm that sparsifies the weights of an already-trained neural network. The algorithm is a convex program based on l1-minimization and it applies layer-wise for layers with linear transform followed by ReLU. Theoretically the authors show that the outputs of the reduced network stay close to those of the original network; they also show that for Gaussian samples, with high probability the sparse minimizer is unique. They further apply their algorithm on MNIST dataset to show it is effective. The results in this paper are interesting and applicable to a wide range of trained networks. I checked the proofs and in my opinion they are correct and concise. The paper is well-written and the results of the experiments are illustrated clearly. 
	  
On the negative side, the algorithm is only applicable for layers with ReLU nonlinearities (this is an important assumption in their proof), which rather limits the adaptability of the result. 
	  
In the current version, reduction is only performed for fully-connected layers in CNNs. It will be very useful to have comparisons for reduced convolutional layers. The authors are urged to add results and comparisons for convolutional layers in the final version. ","## Summary
The authors address an important issue of compressing large neural networks and propose to solve a convex problem for each layer that sparsities the corresponding weight matrices.

## Major
The paper is nicely written and the method seems sound–though I can’t comment on the correctness of the theorems, as I did not follow the proofs 100%.
The biggest flaw is the restriction of the experiments to MNIST; we can’t say if the results scale to different data domains such as vision or typical regression data sets.

## Minor
A reference to [1] is missing, in which an MDL inspired pruning method is proposed.

"
"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games","Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, C. Lawrence Zitnick",https://proceedings.neurips.cc/paper/2017/hash/3fb451ca2e89b3a13095b059d8705b15-Abstract.html,"The main proposal of the paper is a real-time strategy simulator specifically designed for reinforcement learning purposes. The paper presents with several details the architecture of the simulator, along with how gaming is done on it and some experimentations with the software with some RL techniques implemented in the software.

Although I think there are good values in making with software for research, I don’t think that NIPS is the right forum for presenting technical papers on them. Machine Learning Open Source Software (MLOSS) track from JMLR or relevant workshop are much relevant for that. And in the current case, a publication in the IEEE Computational Intelligence and Games (IEEE-CIG) conference might be a much better fit.

That being said, I am also quite unsure of the high relevance of real-time strategies (RTS) games for demonstrating reinforcement learning (RL). The use of RTS for benchmarking AI is not new, it has been done several time in the past. I am not convinced it will the next Atari or Go. It lacks the simplicity and elegance of Go, while not being particularly useful for practical applications. The proposed implementation is sure designed to allow fast during simulation, making learning can be done much faster (assuming that the bottleneck was the simulator, not the learning algorithms). But such a feat does not justify a NIPS paper per se, there contribution is implementation-wise, not on learning by itself.   

*** Update following rebuttal phase and comments from area chair ***

It seems that the paper still fit in the scope of the conference. I updated my evaluation accordingly, being less harsh in my scoring. However, I maintain my opinion on this paper, I still think RTS for AI is not a new proposal, and I really don't feel it will be the next Atari or Go. The work proposed appears correct in term of implementation, but in term of general usefulness, I still remain to be convinced.

","ELF is described to be a platform for reinforcement learning research. The authors demonstrate the utility of ELF by implementing a RTS engine with 3 game environments. Further, they show that trained AIs can be competitive under certain scenarios. 

Curriculum training is shown to be beneficial

ELF can be a useful resource to the community.","A reinforcement learning platform ELF is introduced in this paper with focus on real time strategy games. The current ELF has game engine supporting three environments, Mini-RTS, Tower Defence and Capture the Flag. It supports efficient parallelism of multiple game instances on multi-core CPU due to multi-thread C++ simulator. ELF can host games written in C/C++. It can switch environment-actor topologies such as one-t-one, one-to-many and many-to-one. Game bot can be trained base on both raw pixels and internal data. Several baseline RL algorithms are implemented in pytorch. Its command hierarchy allows manipulation of commands on different levels. Rule-based AI can also be implemented in ELF. Due to these propoties, they can train game bot for RTS game in end-to-end manner by using ELF, and achieve better performance than built-in rule-based AI. 

Using ELF, this paper shows that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum can beat the rule-based built-in AI. Two algorithms are tested: A3C and MCTS. In A3C, trained AI uses a smaller frame skip than the opponent. These results are helpful to future study.

The paper is well-written. The proposed ELF platform largely helps to future research of developing RL algorithm for RTS game. The experiments give some insights about network architecture and training strategy.

Although Mini-RTS can be seem as a simplified version of StarCraft, it is interesting to see how ELF and the presented curriculum learning performs on more complex RTS games."
Task-based End-to-end Model Learning in Stochastic Optimization,"Priya Donti, Brandon Amos, J. Zico Kolter",https://proceedings.neurips.cc/paper/2017/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html,"The paper proposes an approach to train predictive models performance of which is not based on classic likelihood objectives, but is instead based on performance on some task external to the model. In order to achieve this, the model parameters are optimized so as to minimize the loss on external task, which in turn may involve a sub-optimization problem that depends on model parameters. A synthetic and real-data experiments are presented that clearly illustrate the usefulness of proposed approach. 

The introduction is very well-motivated and the exposition is generally clear. The paper is technically sound and builds on sound foundations - I see no obvious flaws. 

The paper contains good motivation about why the proposed approach is necessary. I see this work as a worthwhile and well-motivated application of existing technical contributions. The important technical piece that make this approach possible, which is differentiation though an argmax are already presented in [Amos 2016]. While building on existing results, this work applies them in a very relevant and well-motivated formulation of task-based learning and I believe would be of interest to the machine learning community.

The proposed benefit, but also a potential issue of the proposed approach is that the model is not independent of the task. Is is possible to characterize how much the model may be overfitting to the specific task and whether it generalizes well to a different (potentially only slightly different) task? 

It would be good to reference related work on meta-learning “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks” by Finn et al, as that’s another example of differentiation through an optimization procedure.","--Brief summary of the paper:
The paper proposes a learning method for solving two-stage stochastic programming problems which involve minimizing f(x,y,z) w.r.t. z. The main idea of the paper is to learn a predictive model p(y|x;theta) such that the task's objective function f is directly optimized. In contrast, traditional approaches learn p(y|x;theta) to minimize a prediction error without considering f. The main technical challenge in the paper is to solve a sub-optimization problem involving argmin w.r.t. z, and the proposed method can do so in an efficient manner by assuming that the optimization problem is convex in z. The method is experimentally evaluated on two problems and it is shown to outperform traditional methods.

--Major comments:
The idea of adopting end-to-end learning to solve two-stage stochastic programming is interesting. However, I have a major concern for the proposed method which is the lack of convergence guarantees. Since the optimization problem is assumed to be convex in z, the obtained solution z*(x;theta) is supposed to be the ""true"" optimal if data is drawn from the true distribution p(x,y). However, a solution obtained using the predictive model p(y|x;theta) is unlikely to be true optimal unless p(y|x;theta) is the true conditional distribution p(y|x). (This issue is commonly known as model bias in the context of model-based reinforcement learning which usually involves non-convex objectives.) Since the proposed method does not theoretically guarantee that p(y|x;theta) converges to p(y|x) even when the model hypothesis is correct, it seems likely that even for a convex optimization problem the method may only find a sub-optimal solution. For this reason, I think having convergence guarantees or error bounds either for the predictive model or for the obtained solution itself are very important to theoretically justify the method and would be a significant contribution to the paper.

--Questions:	
1) It is not clear why Algorithm 1 requires mini-batches training since Line 7 of the algorithm only checks the constraint for a single sample.
2) In the first experiment, why does the performance of the end-to-end policy optimization method depend on the model hypothesis when it does not rely on a predictive model?

--Minor suggestions:
1) In line 154 the paper argue that the model-free approach requires a rich policy class and is data inefficient. However, the model-based approach also requires a rich model class as well. Moreover, the model-based approach can suffer from model bias while the model-free approach cannot.
2) The applicability of the proposed method is quite limited. As mentioned in the paper, solving a sub-optimization problem with argmin is not trivial and convexity assumption can help in this regard. However, practical decision making problems may involve non-convex or unknown objective functions. A variant of the proposed method that is applicable to these tasks would make the method more appealing.
3) The last term of Eq.(4) should have an expectation over the density of x.

--Comments after author's response:
I feel more positive about the paper after reading the author’s response. Now I think that the proposed method is an important contribution to the field and I will increase my score. However, I am still not convince that the proposed method will be useful outside domains with convex objectives without empirical evidences.

","Frankly I didn't understand the problem setup. What are \mathcal{X, Y, Z}? compact sets in R^n? Unbounded sets? Or maybe these are finite-sets?

I understand that they describe an end-to-end optimization scheme like [29], but the problem set of stochastic programming (e.g. inventory stacking ) is unfamiliar to me. The results of the load forecasting task looks like it might be impressive, but again, I am unfamiliar with these problems. 

I'd love to see result on problems more familiar to me (and I suspect the NIPS crowd in general) e.g. continuous control problems.
"
Fader Networks:Manipulating Images by Sliding Attributes,"Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic DENOYER, Marc'Aurelio Ranzato",https://proceedings.neurips.cc/paper/2017/hash/3fd60983292458bf7dee75f12d5e9e05-Abstract.html,"The authors propose a way to learn a disentangled representation of images that scales to reasonably large inputs (256x256), and that can be used to generate natural looking variations on these images by traversing the learnt attributes. The authors do so by using labelled attributes information for supervision and augmenting an autoencoder setup with an additional GAN objective in the latent space (the GAN discriminator is trying to guess the attributes from the latent space of the autoencoder).

The proposed Fader networks perform well in terms of learning a disentangled representation and generating natural looking sampled with swapped attributes. However, its performance is not perfect (as indicated by the human evaluation scores) and the overall novelty of the approach seems to be somewhat incremental. Saying that, the paper is very well written.

One thing I found surprising was that the authors did not discuss beta-VAE (Higgins et al, 2017) in their literature review. When discussing unsupervised approaches to disentangled factor learning the authors mention that the methods they cite find difficult to ""automatically discover high-level concepts such as gender or age"". Beta-VAE was able to discover age and gender as factors of variation on celebA in a completely unsupervised manner. Of course, being a VAE based method it did suffer from blurry samples, but I feel that the authors should cite the paper given that it achieves their stated objective of learning a ""fader""-like architecture.","The paper claims to introduce a new encoder/decoder that can disentangle meaningful attributes (eg. young vs old, male vs female) in the latent representation. These disentangled attributes can be ""faded"" up or down to generate images that vary in how strongly the attribute is expressed.

The problem they tackle is unsupervised learning of images, where the images come with semantic attributes, but the data does not specify transformations between them.

The core idea is to learn to encode an image in a latent space in a way that is agnostic to the attributes of interest. In effect, it learns to infer a posterior over an image encoding with the attributes of interest marginalized out. This way, the attribute can be input at generation time to effectively apply the constraint that only images with that attribute should be generated. They acknowledge this is related to domain-adversarial training.

Their idea is implemented via adversarial training on the latent representations. A ""discriminator"" network is trained to predict the input attribute from the latent encoding, and the encoder's training is penalized in proportion to the discriminator's performance. There is also a reconstruction objective for the decoder. The architecture uses a standard mixture of CNNs, relus, batchnorm, and dropout.

To generate images with different attributes, the target attribute is simply input at test time. This allows generated images to vary between attributes such as age, gender, glasses, eye opening, and mouth opening.

The results are quite convincing. The generated images are realistic, and for some attributes (eg. gender) the transformations are very good. Some are a little weaker, but those can probably be remedied with some tweaking.

Overall the work is solid and merits publication. The contribution isn't transformative, but the idea is simple, effective and generally important. Everything was executed well in the paper and the writing is clear and easy to follow. ","*** Summary ***

This paper aims to augment an autoencoder with the ability to tweak certain (known) attributes. These attributes are known at training time and for a dataset of faces include aspects like [old vs young], [smiling vs not smiling], etc.

They hope to be able to tweak these attributes along a continuous spectrum, even when the labels only occur as binary values. 

To achieve this they propose an (encoder, decoder) setup where the encoder maps the image  x to a latent vector z and then the decoder produces an image taking z, together with the attributes y as inputs. 

When such a network is trained in the ordinary fashion, the decoder learns to ignore y because z already encodes everything that the network needs to know. To compel the decoder network to use y, the authors propose introducing a adversarial learning framework in which a discriminator D is trained to infer the attributes from z. Thus the encoder must produce representations that are invariant to the attributes y.

The writing is clear and any strong researcher should be able to reproduce their results from the presentation here. 


*** Conclusions ***

This paper presents a nice idea. Not an inspiring, or world changing idea, but a worthy enough idea. Moreover, this technique offers a solution that large number of people are actually interested in (for good or for malice). 

What makes this paper a clear accept is that the authors take this simple useful idea and execute it wonderfully. The presentation is crystal clear. The experiments are compelling (both quantitatively and qualitatively). The live study with Mechanical Turk workers answers some key questions any reasonable reviewer might ask.

I expect this work to actually be used in the real world and happily champion it for acceptance at NIPS 2017. 

*** Small Writing Quibble ***

""We found it extremely beneficial to add dropout""

Adverbs of degree (like ""very"", ""extremely"", etc) add nothing of value to the sentence. Moreover, it expresses an opinion in a sly fashion. Instead of saying dropout is ""extremely"" important, say ***why*** it is important. What happens when you don't use dropout? How do the results compare qualitatively and quantitatively?

"
VAE Learning via Stein Variational Gradient Descent,"Yuchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/443dec3062d0286986e21dc0631734c9-Abstract.html,"%%% UPDATE: Thank you for your response, which has been read %%%
%%% The score I gave has increased by one level, after I was assured that this work is of some interest to research on VAE %%%
%%% Please ensure that ALL details needed to reproduce the experiments are included in any revision of the paper %%%

This paper contains a lot of different ideas around variational auto-encoders and Stein's method, which are brought together in a way that appears novel. However I feel that the presentation could be improved and more details about the (complicated) implementation should have been included.

- The background on variational auto-encoders was not clear, to this reviewer who has not worked with VAEs. For instance, first p(x | z_n ; \theta) is called the ""encoder"", then q(z | x ; \phi) is called the ""encoder"" - which is it, and why is it given this name? 

- \top should be used as transpose in latex.

- What is the domain of \theta? Related, how is \grad_\theta \psi(\theta ; D) defined if both \theta and \psi are vectors?

- Several approximations are composed en route to the final method that is proposed. It is unclear how all these approximations interact - this isn't assessed theoretically - so that all of the assessment must be based on the empirical results. 

- Unfortunately several details necessary to reproduce the results seem to have been omitted. e.g. the value of \epsilon (step size for SVGD), the number of SVGD iterations (or the stopping criterion), the initial locations of the particles, etc. In particular, the initial location of the particles would presumably be quite influential to the performance of SVGD.

- I did not understand Theorem 2. The claim seems to be that KL_{q,p}^k is involved in a tighter variational lower bound, but what is the significance of the fact that Lim_{k \rightarrow \infty} KL_{q,p}^k(\Theta ; D) = 0? This is not really explained.

- On line 178, ""q_0"" has not been defined, I think.

- In general there are a lot of moving parts in the proposed method, and most of these ""levers"" are unexplored in the experiments. For instance, how does the choice of k impact on performance?

- An MCMC benchmark is used for a couple of the experiments - there is presumably an implicit argument that the proposed method is faster than MCMC. However, this isn't actually tested and computational times against MCMC are not included.


","Recent work has shown that the kernel Stein discrepancy can be viewed in the context of gradient flows on distributions, i.e., the optimal test function of the kernel Stein discrepancy is also a type of functional gradient of a perturbed distribution w.r.t. the KL divergence [https://arxiv.org/abs/1608.04471]. The main idea in this work is to use this gradient flow property to improve the training of a variational autoencoder. Instead of using backprop on a single sample at a time, the authors show one can use this gradient flow property of the kernel Stein discrepancy to backprop w.r.t. to an entire sample, which substantially reduces the variance of the gradient that regularizes the latent space toward the marginal distribution of p(theta, z). The authors also implement these ideas with those of importance weighted VAEs, and accordingly achieve better results than the vanilla VAEs. Since these ideas are quite often orthogonal to other ideas used to improve VAEs, this seems like a key ingredient to obtain state-of-the-art VAEs and a worthy contribution to the literature.

Detailed Comments:
L38: Its a bit awkward to call p(x|z) and q(x|z) the encoder (for q(x|z)
it seems natural but p(x|z) is just a conditional distribution).
L84: Are these z_{jn} supposed to be z_{jn}^{t-1}? Or are they really
all drawn from q(Z) still as suggested in Theorem 1?
L86: Same as before. Also, shouldn't this be theta_{j'} and not theta_j'?
L178: What's q0(theta)? Is this supposed to be q(theta)? If so whats q(Z)?
My understanding is these are how you draw the initial theta and z samples,
correct? If so, I've noticed that many of the following examples leave
the prior p(theta, z) unspecificed...
L186-190: Are theta the parameters to be learned here? If so, what's the
prior p(theta) on theta? If the statement before is incorrect,
what's the form of the decoder? This is confusing.
L187: How many layers are in the MLP? Just one layer of hidden units?
L216: How many layers in the MLP?","This well-written paper presents a method for training variational autoencoders by performing alternating updates on the codes Z and parameters theta using Stein variational gradient descent (SVGD). SVGD was previously proposed as a general-purpose algorithm for variational inference but had not yet been applied to autoencoders prior to this paper. A rich set of experiments is presented, showing that the Stein VAE outperforms the standard VAE in recognizing multimodal posteriors and accurately capturing posterior variance rather than underestimating it.

I wish some timing comparisons had been provided for training the Stein VAE versus the standard VAE, but this is a minor comment. The experiments are certainly far more compelling than those in the original SVGD paper."
Approximation and Convergence Properties of Generative Adversarial Learning,"Shuang Liu, Olivier Bousquet, Kamalika Chaudhuri",https://proceedings.neurips.cc/paper/2017/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html,"The authors present a formal analysis to characterize general adversarial learning. The analysis shows that under certain conditions on the objective function the adversarial process has a moment-matching effect. They also show results on convergence properties.

The writing is quite dense and may not be accessible to most of the NIPS audience. I did not follow the full details myself. I defer to reviewers closer to the area. One general reservation I have about a formal analysis along the line presented is whether the results actually provide any insights useful in the practice of generative adversarial networks. In particular, do the results suggest properties of domains / distributions that make such domains suitable for a GAN approach, and, conversely, what practical domain properties would suggest a bad fit for GAN? Do the results provide guidelines for the choice of deep net architecture or learning parameters? Ideally, a formal analysis provides such type of feedback to the practitioners. If no such feedback can be provided, the formal results may still be of interest but unfortunately, they will remain largely a theoretical exercise. But at least, I would like to see a brief discussion on the impact on the practice of these results because it's the practical success of the GAN approach that has made it so exciting. (A similar issue arises with any formal framework that ""explains"" the success of deep learning. Such explanations become much more interesting if they also identify domain properties that predict where deep learning would work well and where it would not, in relation to real-world domains. I.e. what are the lessons for the practice of deep learning?)

I'm happy with the authors' response. It would be great to see the feedback incorporated in the revised paper. I've raised my rating.
","The paper studies of the convergence properties of a large family of Generative Adversarial Networks. The authors introduce the concept of adversarial divergence \tau(\mu||\nu), which is defined as the supremum of the expectation of some convex function f under the product measure of \mu and \nu. It is shown in the paper that the objective functions of a large family of GAN variations in the literature can be formulated as adversarial divergences. The adversarial divergence is characterised as strict if there is a unique minimiser \nu=\mu for \tau(\mu||\nu), given the target distribution \mu. If case of a non-strict divergence (i.e. more than one minimising distribution exist) it is shown that minimising \tau(\mu||\nu) has a moment matching effect for \mu and \nu, under some additional assumptions. Finally, the authors show that converge in \tau(\mu||\nu) implies also convergence towards the set of optimising distributions for \tau(\mu||\nu), given that the sample space is a compact metric space.
	      
I think that the paper makes some interesting contributions to the theory of GANs. The formulation of a large family of objective functions in the literature as adversarial divergences is well-demonstrated, and the convergence results provide useful insights on the properties of the different GAN approaches.
","This paper mainly addresses two important questions in generative adversarial learning by the introduced adversarial divergence: the first is to relate the optimality of divergence to certain matching conditions under generalized moment matching framework; the second is to discuss the weak convergence and relative strengthen of different divergences. In general, the presentation in this paper provides a relatively deep understanding of GAN-like models. It will benefit the research in this community, and point a theoretical way for future working, rather than most of the heuristic tricks, so I vote for accept.

Some other comments:
1. Line 111: (f) can you provide an example or short description of transportation cost function in the appendix for readers better understanding? 
2. Line 149: Does the assumption of theorem 4 hold for (f)?
3. Line 210: typo, converges
4. Can the property of convergence provide some practical guide in GAN training? Like which distance is a better choice for loss function?
5. Although this is a theoretical paper, I hope the authors can support their claims with some experimental results"
Local Aggregative Games,"Vikas Garg, Tommi Jaakkola",https://proceedings.neurips.cc/paper/2017/hash/44ac09ac6a149136a4102ee4b4103ae6-Abstract.html,"
This work extends learning for contextual potential games beyond undirected tree structures to local aggregative games in which payoffs are based on the aggregate strategy of a player’s neighbors and the player’s own strategy.

The paper proves that approximate pure strategy Nash equilibria exist for all games with limits on their Lipschitz constants. A hardness result for the number of queries needed to learn the game structure from observed Nash equilibria is developed in Theorem 3. 

Has progress been made on the elusive proof eluded to in Line 41? As described in Page 4, this seems like a critical missing component of the submitted work.

Despite the seemingly unresolved negative results of Theorem 3, a margin-based approach is employed to recover the structure and payoffs of the game.

The real experiments on political “games” are interesting and show reasonable results. However, the “ground truth” for these settings is open to debate (e.g., Breyer is often considered more conservative than Sotomayor and a more natural connector from Liberals to Conservatives). The experimental results could be improved by generating synthetic games and showing that the true structure is recovered.

My assessment for this paper is mixed. Though the method seems to work well in practice, the underlying theory has not been fully developed in a way to resolve the hardness result of Theorem 3.

----

Thank you for your response. I look forward to synthetic experiments and also suggest compressing Figure 4 and 5 so that more of the description for this datsaset can be placed into the main text.","Summary: 
The authors consider games among n players over a digraph G=(V,E), where each player corresponds to a node in V, the payoff function of each player depends on the choices of its own and those of neighbors. A further assumption is that the payoff function is Lipschitz and convex (or submodular). For games with sufficiently smooth payoff functions, the authors show that an approximate PSNE exists. Also, the authors prove that there exists a digraph for which any randomized algorithms approximating PSNE needs exponentially many value-queries for a submodular payoff function. 
Then the authors consider a variant of the game called gamma-local aggregative game, for which the payoff function depends on not only neighbors but all other players under the condition that other players choices affect the payoff in the magnitude of gamma^(distance from the player). For such games, the authors show a sufficient condition that an approximate PSNE exists. They further consider a learning problem for the games where, given approximate PSNEs as examples, the learner’s goal is to predict the underlying digraph. An optimization problem for the learning problem is proposed. Finally, the authors show experimental results over real data sets.

Comments: 
The proposed game is a generalization of the aggregative game to a graph setting. The technical contribution of the paper is to derive sufficient/necessary conditions for the existence of approximate PSNEs for the games. On the other hand, the theoretical results for learning graphs from approximate PSNEs is yet to be investigated in the sense that no sample complexity is shown. However, the proposed formulation provides interesting results over real data sets. 

I read the authors' rebuttal comments and I would appreciate if the authors reflect your comments to me in the final version.
 
","This paper presents theoretical analyses of local aggregative games that covers a wide range of multi-agent interaction scenarios. It is an important problem. The analyses are provided from multiple aspects, such as the existence of epsilon-Nash equilibrium, information theoretical lower bound, and the stability of its generalized gamma-aggregative games. An algorithm is also provided on learning the digraph structure and payoffs from data. The analyses are novel and seem to be correct (although having not checking in details). My issues mostly lie on the empirical analysis section, where although some experiments are carried out, it is still unclear how the proposed learning algorithm perform both quantitatively and and qualitatively, and how these experiments are connected to the aforementioned analyses.

"
An Error Detection and Correction Framework for Connectomics,"Jonathan Zung, Ignacio Tartavull, Kisuk Lee, H. Sebastian Seung",https://proceedings.neurips.cc/paper/2017/hash/4500e4037738e13c0c18db508e18d483-Abstract.html,"The paper propsoes two algorithms, one for finding and one for correcting errors in the shape of segments of electron microscopy images for neural circuit reconstruction. It combines these algorithms in a heuristic fashion so as to greedily update an initial segmentation.

* Contribution

I understand that the first deep network (called the detector) estimates, for any ROI, whether the segment overlapping the central pixel in this ROI needs correction. I also understand that the second deep network (called the corrector) estimates the shape of the correct segment that overlaps with the central pixel. 

It seems to me that, taking only the output of the corrector, at every pixel, and a metric of segmentations such as the VI, one obtains a consensus problem for the entire volume. However, this problem is not studied or even mentioned in this paper. Instead, some greedy algorithm (described informally in Section 5) based in some way on some superpixel segmentation (mentioned in Section 5) is used to incrementally update an initial segmentation.

While the empirical results may be of interest to a connectomics audience (which I cannot judge), I doubt that this greedy algorithm has applications beyong connectomics. 

I conclude that this paper is not of sufficient interest to the wider NIPS audience.

* Presentation

The paper is well-structured and quite well-written, except for the technical sections 3, 4 and especially 5 that are entirely informal, which makes it hard to understand how the algorithms work in exact detail. 

The relation of a problem that is hard to solve to a problem whose solution is easy to verify should be toned down in the introduction as well as in the discussion of related work, as this idea is familiar to most computer scientists from the definition of the complexity class NP. In particular, I do not agree ""that the error detection task is better posed than the supervoxel agglomeration task.""

* Related work

The relation of the proposed method to GANs [8,9] and visual attention [10] is far-fetched, speculative and not supported in this work by technical arguments. If the authors wish to hint on such connections, they should do so only in an outlook, at the end of the paper.
","A nice targeted error detection and correction framework for connectomics, leading to well quantified improvements in performance.","The paper tackles the important, but relatively unexplored, area of error detection and targeted error correction in connectomics. There is a clear need for this type of methods in the field, primarily in order to optimize the allocation of human proofreading time. The authors make the observation that to a human segmentation errors are much easier to detect than to correct, and propose to automate this process.

Specifically, they propose to build an error detector module in the form a multi-scale 3d CNN, taking as input a binary object mask and predicting whether it is equal to an object in the ground truth segmentation. At inference time, the network is applied on overlapping windows distributed over a grid to identify and localize errors in the segmentation.

The paper also proposes an error correction module -- a 3d CNN reconstructing the object containing the central pixel, similarly to flood-filling networks (FFNs), which the authors cite as related work. Instead of predicting a binary mask like in FFNs, The authors propose to predict a k-dimensional vector for each point of the output, so voxels of the same object have a similar vector, and different objects have not. This vector field is then transformed into a binary mask with an exponential transform. The stated goal of this approach is to soften the dependency on the precise location of the central object. The authors should consider including some additional information about why they chose this particular form of the transform to generate the binary mask, and whether other forms were considered; what value of k was used in the experiments, as well as any experimental data showing that this approach indeed improves the results compared to a direct binary encoding of the output.

The paper is missing information on which losses were used to train the network, which seems particularly important for the error correction module where the k-dimensional vectors are arbitrary and presumably cannot be fully specified based on the training data.

In section 5, the confidence threshold should be explained in more detail -- how was this confidence computed, and what threshold was used? The text also states that the termination condition was an error-free state as predicted by the error detector, or two corrections per iteration. Why was this particular condition used? Would applying the network more than two times result in a better segmentation? Would such a process converge to a stationary state of the segmentation?

It should also be explained explicitly, whether the error corrector deals with both split and merge errors, or only split errors. If merge errors are handled, it should be mentioned what happens in case they are at the supervoxel graph level, and within the supervoxels itself.

In section 6.3, the parameters of the Adam optimizer should be explicitly mentioned.

The paper presents a detailed analysis of performance of the presented systems. The authors should be commended for computing per-object VI scores to make the metrics easier to understand.

Technical comments about the text:
- Fig. 1 is hard to understand, and it is unclear what the nodes represent. The caption mentions horizontal layers, but it does not look like the data flows through the network vertically.
- Line 48 states that the ""overhead of agglomeration prohibits the use of deep learning"". This does not seem to be the case, as deep nets have been used for agglomeration, see e.g. https://papers.nips.cc/paper/6595-combinatorial-energy-learning-for-image-segmentation.pdf for a recent example.
- Appendix A, Table 4: Why are layer IDs not consecutive, and if this is intentional, what do the IDs mean?
"
Hindsight Experience Replay,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, Wojciech Zaremba",https://proceedings.neurips.cc/paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html,"The main idea of the work is that it can be possible to replay an unsuccessful trajectory with a modification of the goal that it actually achieves.

Overall, I'd say that it's not a huge/deep idea, but a very nice addition to the learning toolbox. When it is applicable, it seems like an idea that should be used. I worry that it is rarely applicable, though, without a tremendous amount of task insight. Consider the ""kick the ball into the goal"" example. If we want to speed up the learning of the task, we would likely need to use the idea to help guide the robot to the ball, to guide the robot's foot to make contact with the ball, and to guide the robot's aim to send the ball into the goal. At this level, it doesn't feel particularly different from (potential-based) reward shaping, which has been used in precisely this way.

One claim could be that HER is a version of reward shaping that is more effective with current RL implementations (deep learning). I think that would be a much more useful comparison than the ones given in the paper. It puts the technique into context more completely.

Note that there are small errors in the text should be fixed, for example: ""The last dimension specify"", ""have lead"", ""can not"", ""the the most"".

""less than 20cm for sliding"": Are the units right, there? I watched the video and it looked like it was placings things much much more
closely than that. How big is 20cm relative to the size of the robot's workspace?

The paper compares HER to an implicit curriculum. As I said above, I think it is more akin to reward shaping. But, in the curriculum space, I see it more as a method for generating ""easy missions"" from the existing ones.	

Again, overall, I like the idea and I can see some cases where it could be used. But, I think the paper doesn't put the technique into its proper context and that weakens the overall impact of the work.

NOTE: The rebuttal says, ""We agree that there is a connection between HER and reward shaping. Please note that experimental comparison of HER and reward shaping is included in Sec. 4.4 and shows that HER performs much better than reward shaping in our environments."". I do not think that this comment directly addresses my question, which was ""At this level, it doesn't feel particularly different from (potential-based) reward shaping, which has been used in precisely this way."". That is, the shaping in 4.4 is not potential-based shaping. My sense (from trying to generalize your idea to more complex problems) is that you need to identify notions of subgoals to use your technique and you need to identify notions of subgoals to use potential based shaping. With those same subgoals identified, which technique is more successful? My concern is that the power of the algorithm in the demonstrations you showed is more about identifying subgoals than it is about how you are using them.
","Summary:
This paper introduces a method called hindsight experience replay (HER), which is designed to improve performance in sparse reward, RL tasks. The basic idea is to recognize that although a trajectory through the state-space might fail to find a particular goal, we can imagine that the trajectory ended at some other goal state-state. This simple idea when realized via the experience replay buffer of a deep q learning agent improves performance in several interesting simulated arm manipulation domains.


Decision:
I think this paper is an accept. The main idea is conceptually simple (in a good way). The paper is clear, and includes results on a toy domain to explain the idea, and then progresses to show good performance on simulated arm tasks. I have several questions that I invite the authors to respond to, in order to refine my final judgement of the paper.

I will start with high-level questions. The auxiliary task work [1] (and hinted at with the Horde paper), shows that learning to predict and control many reward functions in parallel, results in improvement performance on the main task (for example the score in an atari game). One can view the auxiliary tasks as providing dense reward signals for training a deep network and thus regularizing the agent’s representation of the world to be useful for many task. In the auxiliary task work, pixel control tasks seems to improve performance the most, but the more general idea of feature control was also presented. I think the paper under review should discuss the connections with this work, and investigate what it might mean to use the auxiliary task approach in the simulated arm domain. Are both methods fundamentally getting at the same idea? What are the key differences?

The work under-review is based on episodic tasks with goal states. How should we think about continuing tasks common in continuous control domains? Is the generalization straightforward under the average reward setting?

The related work section mentions prioritized DQN is orthogonal. I don’t agree. Consider the possibility that prioritized DQN performs so well in all the domains you tested, that there remains no room for improvement when adding HER. I think this should be validated empirically.

One could consider options as another approach to dealing with sparse reward domains: options allow the agent to jump through the state space, perhaps making it easier to find the goal state. Clearly there are challenges with discovering options, but the connection is worth mention.

The result on the real robot, is too quickly discussed and doesn’t seem to well support the main ideas of the paper. It feels like: “lets add a robot result to make the paper appear stronger”. I suggest presenting a clearer result or dropping it and pulling in something else from the appendix.

Footnote 3 says count-based exploration method was implemented by first discretizing the state. My understanding of Bellemare’s work was that it was introduced as a generalization of tabular exploration bonuses to the case of general function approximation. Was discretization required here, and if not what effect does it have on performance?

The paper investigates the combination of HER with shaped rewards, showing the shaping actually hurts performance. This is indeed interesting, but a nice companion result would be investigating the combination of HER and shaping in a domain/task where we know shaping alone works well.     

[1] Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., & Kavukcuoglu, K. (2016). Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397.","In this paper, a new algorithm for RL is proposed, called ""Hindsight Experience Replay"" (HER). It is based on adding the description of the goal to be attained to the input of an off-policy algorithm like DQN or DDPG. It is then possible to augment the experience replay buffer with ""fake"" goals that were actually attained in the episode, in addition to the ""true"" goal we were trying to achieve. This helps converge when rewards are sparse, as it implicitly defines a form of curriculum where the agent first learns to reach goals that are easily attainable with random exploration, allowing it to progressively move towards more difficult goals. Experiments on robot manipulation tasks, where the goal is described by the target position of some object being manipulated (ex: pushing or grasping), show improvement over DDPG, count-based exploration and reward shaping, along with the ability to transfer a policy from a simulation environment to the real world.

This is a great paper, extremely clear, with extensive experiments demonstrating the benefits of the method which, although very simple to understand and implement, is novel as far as I'm aware of, and may be an inspiration for further meaningful advances in RL. It also has the advantage of being ""compatible"" with many existing algorithms and their variants.

I don't have much to say as I find the current submission to be very convincing already. My only high-level suggestion would be to make it clearer that:
(1) HER does not technically *require* ""sparse and binary"" rewards, even if it may work better in some cases in such settings
(2) (this is my opinion based on my understanding, if I am wrong it would be interesting to discuss it in more depth) HER is mostly meant to be used with a continuous ""goal space"", obtained from a mapping of the state space, such that knowledge of how to solve a goal can be re-used to solve a nearby goal (in this goal space). If that's not the case (ex: discrete goals represented by a one-one vector), there might still be some benefits to HER as extra goals may help find meaningful features, but it seems to me it won't transfer knowledge between goals as effectively, and in particular will still suffer if rewards for the goal of interest are very sparse. Note also that with discrete goals it gets pretty close to the  algorithm from ""Reinforcement Learning wih Unsupervised Auxiliary Tasks"" (which could be cited here).

Minor:
- l.216: ""specify"" => specifies
- ""in practice it is advisable to train on multiple goals even if we care only about one of them"": it does not seem so obvious to me, because the success rates in Fig. 2 and Fig. 3 are not evaluated on the same goal. Maybe the goal from Fig. 3 is particularly hard to reach?
- Why no red curve in Fig. 3? (especially since HER does not get the best result on the Sliding task)
- Appendix l. 456: ""kicking"" => sliding
- It would have been interesting to see experiments combining reward shaping with HER where reward shaping is only used to ""fine-tune"" the reward near the goal state
- Another potentially interesting experiment would have been to investigate if there could be additional benefits to also using ""fake"" goals that were not attained -- possibly sampled near an attained goal"
Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data,"Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher",https://proceedings.neurips.cc/paper/2017/hash/4558dbb6f6f8bb2e16d03b85bde76e2c-Abstract.html,"The paper considers the problem of finding a rank k approximation for a PSD matrix A in the streaming model. The PSD matrix is given as a sequence of updates A = A + H where H is a symmetric matrix. The paper considers a variant of a popular Nystrom method for this task. The algorithm stores the product Y = A*Omega where Omega can be a random Gaussian matrix. The paper proposes a different way to recover an approximation to A from Y from previous works.

Technically, the paper is very similar to previous works by Gittens and Halko et al. The proofs are fairly simple (a bonus in my opinion) as the sketching matrix can be assumed to be Gaussian and the distribution of Y = A*Omega (Omega is the Gaussian matrix) is well understood. The downside is that the proofs are probably hard to generalize to other types of sketching matrices, such as SSFT mentioned in the paper.

The experiments are promising as the proposed method seems to outperform previous algorithms. The new algorithm is a small modification of previous algorithms and uses the same sketch: the difference is only in the routine to obtain the approximation from the sketch. Thus, it can be plugged in and replace previous algorithms without much modification to the rest of the system.","The authors address the problem of fixed-rank approximation of a positive
semidefinite (psd) matrix from streaming data. The authors propose a simple
method based on the Nystrom approximation, in which a sketch is updated as
streaming data comes in, and the rank-r approximation is obtained as the best
rank-r approximation to the full Nystrom approximation. For Gaussian or
uniformly distributed (Haar) orthogonal sketch matrices, the authors prove a
standard  upper bound on the Schatten-1 (aka Nuclear) norm of the approximation error.
Detailed empirical evidence are discussed, which indicate that the proposed
method improves on previously proposed methods for fixed-rank approximation of
psd matrices. 

The paper is very well written and easy to follow. The result appears to be
novel. The proposed method admits a theoretical analysis and seems to perform
well in practice. Implementation details are discussed. This paper whould be a
welcome addition to the steaming PCA literature and is a clear accept in my
opinion. 

Minor comment: 
- The ""experimental results"" section does not mention which test
matrix ensemble was used, and whether the true underlying rank r was made
available to the algorithms.
- Some readers will be more familiar with the term ""nuclear norm"" than with the
  term ""Schatten-1 norm""
- It would be nice to discuss the increase  in approximation error 
due to streaming, compared with a classical sketch algorithm that is given all
the data in advance. 
","This paper describes a different way of estimating a low rank PSD matrix from a sketched PSD matrix. From what I can tell, the main difference is that, in the usual case, the sketched test matrices (Y) may be far from orthogonal, and low-ranking the middle matrix itself may produce bad results. Therefore the paper proposes an efficient way of low-ranking the entire Nystrom-estimated matrix, and shows much better results.

I believe this paper is very well written, with context well described and competing methods evenly mentioned.

Suggestion: lines 138-143, which essentially just restates alg. 3, is not very enlightening. specifically I don't understand the need for the Cholesky factorization, since it does nothing to make things thinner. We can just as easily take the SVD of Y_nu*B, which is equally thin, and if the shift is large enough then everything should be stable, without the need of the extra factorization step. I suggest replacing this segment with an explanation rather than restatement of the alg."
The Numerics of GANs,"Lars Mescheder, Sebastian Nowozin, Andreas Geiger",https://proceedings.neurips.cc/paper/2017/hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html,"This paper presents a novel analysis of the typical optimization algorithm used in GANs (simultaneous gradient ascent) and identifies problematic failures when the Jacobian has large imaginary components or zero real components. Motivated by these failures, they present a novel consensus optimization algorithm for training GANs. The consensus optimization is validated on a toy MoG dataset as well as CIFAR-10 and CelebA in terms of sample quality and inception score.

I found this paper enjoyable to read and the results compelling. My primary concern is the lack of hyperparameter search when comparing optimization algorithms and lack of evidence that the problems identified with simultaneous gradient ascent are truly problems in practice.

Major concerns:
1. The constant discriminator/generator loss is very weird. Any ideas on what is going on? Does this hold when you swap in different divergences, e.g. WGAN?
2. Which losses were you using for the generator/discriminator? From Fig 4, it looks like you’re using the modified generator objective that deviates from the minimax game. Do your convergence results hold for non-zero sum games when f and g are different functions, i.e. f != -g?
3. Experiments: You note that simultaneous and alternating gradient descent may require tiny step sizes for convergence. In your experiments did you try tuning learning rates? Additional optimization hyperparameters? Without sweeping over learning rates it seems unfair to say that one optimization technique is better than another. If consensus optimization is more robust across learning rates then that should be demonsrated empirically.
4. Empirical validation of the failures of simultaneous gradient ascent: are zero real components and large imaginary parts really the problem in practice? You never evaluate this. I expected to see empirical validation in the form of plots showing the distribution of eigenvalues of the Jacobian over the course of optimization for simultaneous gradient ascent.


Minor concerns:
1. From the perspective of consensus optimization and \gamma being a Lagrange multiplier, I’d expect to see \gamma *adapt* over time. In particular, ADMM-style algorithms for consensus optimization have dynamics on \gamma as well. Have you explored these variants?
2. Many recent approaches to optimization in GANs have proposed penalties on gradients (typically the gradient of the discriminator w.r.t. Its inputs as in WGAN-GP). Are these related to the approach you propose, and if so is another explanation for the success of consensus optimization just that you’re encouraging a smooth discriminator? One way of testing this would be to remove the additional \nabla L(x) term for the generator update, but keep it for the discriminator update.
3. Figure 5 (table comparing architectures and optimizers) is a nice demonstration of the stability of Consensus relative to other approaches, but did you sweep over learning rate/optimization hyperparameters? If so, then I think this would make a strong figure to include in the main text.
4. I had a hard time following the convergence theory section and understanding how it relates to the results we see in practice.","This is a very nice, elegant paper on the optimization challenges of GANs and a simple, well motivated fix for those problems. The continuous-time perspective on optimization makes it very clear why minimax problems suffer from oscillations that regular optimization problems do not, and the proposed solution is both theoretically motivated and simple enough to be practical. I anticipate this will be useful for a wider variety of nested optimization problems than just GANs and should get a wide viewing. My only major suggestion for improving the paper would be to provide a wider evaluation, especially with difficult-to-train architectures known for mode collapse. As is, the results in the main paper show comparable performance to vanilla GAN training with much more stable discriminator and generator objectives, but not a major improvement over cases where alternating gradient ascent fails. Also in Figure 5 in the appendix, it's not clear why consensus optimization fails for DCGAN training - a better discussion of that would help. But it generally seems like a promising approach and I look forward to seeing it applied in problems like inverse reinforcement learning and deterministic policy gradients for continuous control.","The paper presents an analysis of the optimization of GANS by analyzing the jacobian of the vector field of the zero sum game. 
A saddle point   exists if this jacobian  is  negative definite . The paper proposes to penalize the norms of  gradients with respect to the parameters of the generator and the discriminator in order to ensure such conditions. 

clarity:

The paper is clear and well written  and proofs seem correct. Experiments are supportive of the findings.

Comments: 

- penalizing the l_2 norm of the gradient , requires automatic differentiation , which is feasible in tensorflow, but it involves computing the hessian of generator and discriminator losses. Giving the wall clock timing needed to enforce this constraint is important to see the additional overhead needed to enforce stability. 

- I wonder why not just enforce the l_{infinity} norm of those gradient , by clipping the gradients with respect to the parameter, this is a common practice in recurrent networks. Notice this is different from weight clipping using in WGAN. 
It would be interesting to compare the two approaches since clipping the gradient is light weight computationally as it does not require automatic differentiation. 

- Penalizing the gradient requires computing the hessian, hence gradient descent of this penalized loss can be seen as an approximate second order method: the  effective gradient is (I - gamma Hessian (theta,phi) ) * (nabla_phi f , nabla_theta g  )  under assumption that the eigenvalues  of I + gamma H   are less then one (this is the assumption also in the paper ), (I+ gamma H)^ (-1)  is approximately equal to( I -gamma Hessian ) ( assuming that high order powers of H vanish). which means  that penalizing the gradient yields to an approximate newton update (Hessian being  here regularized ).

It would be interesting to discuss this angle of second order optimization in the paper. My question here is how much do you think second order optimization is important for the convergence of those saddle point problems such as GANS? penalizing the gradient seems to be equivalent to a first order krylov descent https://arxiv.org/pdf/1301.3584.pdf ? since the hessian is computed by automatic differentiation by penalizing the norms of the gradients is it worth seeing how hessian free optimization perform in the GAN context? or adding other powers of the hessian as in krylov subspace descent?"
Cortical microcircuits as gated-recurrent neural networks,"Rui Costa, Ioannis Alexandros Assael, Brendan Shillingford, Nando de Freitas, TIm Vogels",https://proceedings.neurips.cc/paper/2017/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html,"
			A new recurrent neural network model is presented. It has similar functional features as LSTMs
			but additive gating instead of multiplicative gating for the input and output gates.
			With this additive gating mechanism there are some striking similarities to cortical circuits.

			In my opinion, this papers could be really interesting for both computational neuroscientists and machine learners.
			For small network sizes the proposed model performs as good or better than simple LSTMs on some non-trivial tasks.
			It is, however, somewhat disappointing that For the language modelling task it seems, however, that the multiplicative forget gates are still needed for good performance in larger networks.

			Further comments and questions:

			1. I guess the sigma-subRNN have also controlled forget gates (option i on line 104). 
			   If yes, it would be fair to highlight this fact more clearly in the results/discussion section.
			   It is somewhat disappointing that this not so biologically plausible feature 
			   seems to be needed to achieve good performances on the language modelling tasks.
			2. Would it be possible to implement the controlled forget gate also with a subtractive gate.
			   Maybe something like c_t = relu(c_{t-1} - f_t) + (z_t - i_t), 
			   which could be interpreted as a strong inhibition (f_t) reseting the reverberating loop before new input (z_t - i_t) arrives.
			   But maybe the sign of c_t is important....
			3. line 111: Why is it important to have rectified linear transfer functions?
			   Doesn't sigma-subRNN perform better in some cases? 
			   From the perspective of biological plausibility the transfer function should have the range [0, f_max]
			   where f_max is the maximal firing rate of a neuron (see e.g. the transfer function of a LIF neuron with absolute refractoriness).
			4. line 60 ""inhibitory neurons act subtractively"": I think one should acknowledge that
			   the picture is not so simple in biological systems (e.g. Chance et al. 2002 https://doi.org/10.1016/S0896-6273(02)00820-6 or 
			   Müllner et al. 2015 https://doi.org/10.1016/j.neuron.2015.07.003).
			5. In my opinion it is not needed to explicitely state the well known Gershgorin circle theorem in the main text.
			6. Fig 3f: BGRNN? Should this be subRNN?

I read through the author’s rebuttal and my colleagues’ review, and decided to not change my assessment of the paper.
		","The authors introduce a new variant of a gated unit for recurrent neural networks. The performance of the RNN with this unit is found to be similar or superior to a standard LSTM. An analysis indicates possible reasons for this performance. In addition, the authors claim that the new unit is similar to the cortical circuit in biological brains.

There are two separate strands to this work – machine learning and neuroscience. The neuroscience part is rather weak (detailed below), and I think the work should only be judged according to machine learning merit.

It is true that subtraction is in general considered more biologically plausible than multiplication. In this respect the current work moves us a step towards biological realism. The overall biological motivation, however, is not very strong. For instance, the authors claim that “excitatory and inhibitory currents directly cancel each other linearly at the level of the postsynaptic membrane potential”, thereby ignoring conductances. In particular, the concept of shunting inhibition is an example of a case where inhibition does not simply linearly combine with excitation. 
The interpretation of balanced excitation and inhibition, as motivating the term (z-i) in the model is also not clear. The fact the both ‘z’ and ‘i’ have the same coefficient is not enough for balance. Their magnitude should be the same. The authors do not claim or show that the difference (z-i) is close to zero in all network neurons.
A further point that is hard to understand is the way the forget gate is used. It seems that this part remains multiplicative (and a footnote mentions this is not very plausible). At another section, however, the authors note that they use “a more biologically plausible simple decay [0, 1]”, but it’s hard to understand what is exactly meant here.

Regarding the machine learning value, the results are not dramatically different from LSTMs, and the comparison less rigorous than e.g. [1].

[1] Greff, Klaus, et al. ""LSTM: A search space odyssey."" IEEE transactions on neural networks and learning systems (2017).
","The paper describes how networks of gating  balanced inhibition and excitation can be constructed that are capable of learning similar to LSTM. 

I found the idea proposed in the paper very interesting. 

The authors however argue bit strong that the cortical organization maps to RNNs: others have argued other mappings (ie predictive coding [1], or [2]). While the mapping may fit, I would argue that that is not sufficient as ""proof"". 

I found the paragraph in 4.1 a bit confusing, it seems to suggest the gradient vanishes for LSTMs, whereas this really only applies to the current input and current output. 
on l131, it is stated that the error-flow to the input layer in an LSTM unit vanishes when one of the gates is 'closed' 
This is technically true indeed, however, since the gates are fed by sigmoid activations, gates are rarely truly 0.

Figure 2 is very hard to understand because the colors for LSTM vs sub gating come out too similar, same for Fig 3 f and Fig 5a

The temporal MNIST task could benefit from a bit more detail to explain what is going on. 

How does the presented LSTM performance compare to the original performance reported by Hochreiter & Schmidhuber? 


[1] Bastos, Andre M., et al. ""Canonical microcircuits for predictive coding."" Neuron 76.4 (2012): 695-711.
[2] George, Dileep, and Jeff Hawkins. ""Towards a mathematical theory of cortical micro-circuits."" PLoS computational biology 5.10 (2009): e1000532."
Deep Lattice Networks and Partial Monotonic Functions,"Seungil You, David Ding, Kevin Canini, Jan Pfeifer, Maya Gupta",https://proceedings.neurips.cc/paper/2017/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html,"The authors propose an network architecture with a combination of linear encoding, lattices (non-linear function) and piece-wise linear functions with constraints of monotonicity to preserve partial monotonicity w.r.t user specified subset of input variables and the target. 

Questions:

Excuse me for being unfamiliar with corresponding literature.

Why is  it required to specify the feature subset on which we want to impose monotonicity w.r.t the prediction? cant it be figured by the neural network from the data? (especially when training on large number of samples)

The authors chose a positive DNN for comparison. Assuming the monotonicity constraints are resulting in regularization in DLNs (resulting in better test scores), the authors can consider comparing against a regular neural network with out any constraints.

How the authors chose the architecture of the DNN for each dataset? 
","This paper proposes a deep neural network structure which acts as a function approximator for partial monotonic functions. 
The previous works that try to approximate a partial monotonic function are only one/two layers deep with hand designed architectures. 
The key contribution of this paper is to develop a deep version of partial monotonic function. A deep network composed with the components proposed in this paper is proved to have end to end monotonicity property.

While I'm not very familiar with this line of work, my feeling is that deep neural net with functional class constraint is a useful topic. From the previous work listed in this paper, I feel that this work is the first to design an end to end trainable deep network that is proved to be monotonic.","This paper proposes a novel deep model that is monotonic to user specified inputs, called deep lattice networks (DLN's), which is composed of layers of linear embeddings,  ensembles of
 lattices and calibrators. The results about the function classes of DLN's are also provided. 

Authors should put more effort to make the paper readable for the people that are not familiar with the technicality.  However the contributions are interesting and significant. As [3] proposed a 2 layer model,  which has a layer of calibrators followed by an ensemble of lattices, the advantages of proposed deeper model could be presented in a more convincing way by showing better results as the results seem closer to Crystals and it is not clear whether the produced results are stable. 

The computational complexity should be also discussed clearly.  Each projection step in optimization should be explained in more detail. What is the computational complexity of the projection for the calibrator? What proportion of training time the projections take?  Are the projections accurate or suboptimal in the first iterations? And how the overall training time compares to baselines?

Does the training of the proposed model produces bad local minima for deeper models? At most how many layers could be trained with current optimization procedure?

It seems most of the results are based on the data that is not publicly available. To make the results reproducible, authors should add more publicly available data to experiments. Also artificial data  would help to demonstrate some sanity checks, e.g. a case that two layer [3] did not work well but deeper model work better, the intermediate layer visualizations etc . 

Authors should also explain better why the model has interpretability and debuggability advantages."
Contrastive Learning for Image Captioning,"Bo Dai, Dahua Lin",https://proceedings.neurips.cc/paper/2017/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html,"The proposed method is an algorithm called Contrastive Learning used for image captioning.
It's base idea is akin to use of intra/inter cluster similarity concept in clustering.
The intended goal is to add ""distinctiveness"" to captions generated by current methods.
The paper is well organized, clear in the presentation of the proposed method and in presenting the rationale for each of its  aspects (section 4.2).
It also draws a comparison with similar methods (Introspective Learning, Generative adversarial Networks) highlighting their main differences.
Emphasis of the proposed method on enhancing caption distinctiveness through learning ""negative"" pairs while maintaining overall performance is also made clear.
The results presented on MSCOCO and InstaPIC datasets show promising improvements.

NB: Do not seem to locate the supplemental materials mentioned in paragraph 5.2.
","The paper proposed a contrastive learning approach for image captioning models. Typical image captioning models utilize log-likelihood criteria for learning, which tends to result in preferring a safer generation that lacks specific and distinct concept in an image. The paper proposes to introduce contrastive learning objective, where the objective function is based on density ratio to the reference, without altering the captioning models. The paper evaluates multiple models in MSCOCO and InstaPIC datasets, and demonstrates the effectiveness as well as conducts ablation studies.

The paper is well-written and has strength in the following points.

* Proposing a generalizable learning method
* Convincing empirical study

The paper could be improved on the following respect.

* Results might look insignificant depending on how to interpret
* Insufficient discussion on distinctiveness vs. human-like description

For the purpose of introducing distinctiveness in the image captioning problem, the paper considers altering the learning approach using existing models. The paper takes contrastive learning ideas from NCE [5], and derives an objective function eq (10). By focusing on the contrastive component in the objective, the paper solves the problem of learning under MLE scheme that results in a generic description. Although the proposed objective is similar to NCE, the approach is general and can benefit in other problem domains. This is certainly a technical contribution.

In addition, the paper conducts a thorough empirical study to show the effectiveness as well as to generalization across base models and datasets. Although the result is not necessarily the best all the time depending on the evaluation scenario, I would point out the proposed approach is independent of the base model yet consistently improving the performance over the MLE baseline.

One thing I would point out is that the paper could discuss more on the nature of distinctiveness in image captions. As discussed in the introduction, distinctiveness is certainly one component overlooked in caption generation. However, the paper’s view on distinctiveness is something that can be resolvable by algorithms. I would like to argue that the nature of caption data can induce generic description due to the vagueness of image content [Jas 2015]. Even if an image can be described by distinctive phrases, people can describe the content by comfortable words that are not always distinctive [Ordonez 2013]. In this respect, I would say the choice of the dataset may or may not be appropriate for evaluating distinctive phrases. The paper can add more on data statistics and human behavior on image description.

* Jas, Mainak, and Devi Parikh. ""Image specificity."" CVPR 2015.
* Ordonez, Vicente, et al. ""From large scale image categorization to entry-level categories."" ICCV 2013.

Another concern is that image captioning is extensively studied in the past and unfortunately the paper might not be able to impact a lot in the community.

In overall, the paper is well written and presenting convincing results for the concerned problem. Some people might not like the small improvements in the performance, but I believe the result also indicates a good generalization ability. I think the paper is above the borderline.","In Table 1, there are missing two important comparisons, AdaptiveAttention + IL and AdaptiveAtention + GAN.

In lines 138-139 it is claimed positive correlations between self retrieval and captioning metrics, however 3 experiments are clearly not enough to claim that, it would need dozens and a proper statistical analysis.

According to Table 2 the results of the proposed method are equal or just marginally better than the baseline [14], please clarify why.

In Table 2, there is missing row PG-BCMR [13] which has better results on C40 that proposed method (see https://competitions.codalab.org/competitions/3221#results), and several other entries clearly better.

As mentioned in [1],[3] improving on captioning metrics (BLUE, Meteor, Rouge, Cider, Spider, ...) doesn't translate into better captions when evaluated by humans. Therefore the paper would be much more convincing if they carried human evaluations.

Are the numbers reported in Table 4 computed on the validation set? If so please clarify it.

The paper mentions supplemental materials, but those are missing."
Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net,"Anirudh Goyal ALIAS PARTH GOYAL, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio",https://proceedings.neurips.cc/paper/2017/hash/46a558d97954d0692411c861cf78ef79-Abstract.html,"This paper proposes an extension of the information destruction/reconstruction processes introduced by Sohl-Dickstein et al. (i.e. NET). Specifically, the authors propose learning an explicit model for the information destroying process, as opposed to defining it a priori as repeated noising and rescaling. The authors also propose tying the parameters of the forwards/backwards processes, with the motivation of more efficiently seeking to eliminate spurious modes (kind of like contrastive divergence in undirected, energy-based models).

The general ideas explored in this paper are interesting -- i.e. training models which generate data through stochastic iterative refinement, and which provide mechanisms for ""wandering around"" the data manifold. However, the proposed model is a fairly straightforward extension of NET and does not produce compelling quantitative or qualitative results. The general perspective taken in this paper, i.e. reinterpreting the NET approach in the context of variational inference, was presented earlier in Section 2.3 of ""Data Generation as Sequential Decision Making"" by Bachman et al. (NIPS 2015). If more ambitious extensions of the ideas latent in NET were explored in this paper, I would really like it. But, I don't see that yet. And, e.g., the CIFAR10 results are roughly what one gets when simply modelling the data using a full-rank Gaussian distribution. This would make an interesting workshop paper, but I don't think there's enough strong content yet to fill a NIPS paper.","This is an interesting paper that presents a new generative modeling idea using stochastic recurrent nets. It is inspired by the physical annealing process to obtain a transition operator that can ""walk-back"" to the original data distribution. The tightness of the variational bounds is important. While it presents some quantitative results on a few data sets and compared with a few other generative models, a little more discussion of the results would be useful in the main paper. The overall paper is also a little difficult to understand at times, so, the writing can be improved for better readability.","The paper proposes an exciting practical and theoretical framework for unsupervised estimation of distribution. The framework is reminiscent of Generalized Denoising Auto-Encoders and likewise proposes to train the transition operator in a Markov Chain (e.g. a deep network).

The performance of the method is evaluated on several datasets and is impressive although maybe not ground breaking.

On the other hand, the theoretical framework connects many existing models in very interesting ways and offers interesting insights as to their relationship.

The model also gives a sound solution to sampling with a transition operator which does not respect the detailed-balance condition. This is a very important insight that will probably be very helpful to theoreticians and practitionners alike."
Linear Time Computation of Moments in Sum-Product Networks,"Han Zhao, Geoffrey J. Gordon",https://proceedings.neurips.cc/paper/2017/hash/473447ac58e1cd7e96172575f48dca3b-Abstract.html,"This paper proposes a method to compute moments in SPNs in
linear time and space, which is a useful procedure to perform Bayesian
updating using moment matching approaches. This is indeed an important
topic which is worth investigating. It seems that the derivation is
quite straightforward and the lack of a practical use puts this
submission in a different position from the ""usual"" NIPS paper. My
main concern is that I was not able to follow the result of Corollary 2
and Theorem 2, which are the most important of the paper. It is said to 
follow from Lemma 1 and 2, but the definitions of the trees T_F and T_T 
need different parameters to be encoded/propagated depending on the
edge (k,j). I was not able to see how to avoid the quadratic complexity.
On the same note, it is mentioned in the introduction that the task is
known to be computable in quadratic time, but later (beginning of page
5) it feels that not even quadratic is known. I think the details of
the algorithm and its complexity could be made clearer.

Minor issues:
- Equations are not punctuated.
- Equations are referred without parenthesis enclosing the number.
- Some acronyms are not defined (even if they are obvious, they should be defined).
- Notation between V(.;.) and V(.|.) is incompatible, see Eq.(1).
- |S| is defined as the size of the graph of the SPN (which is not necessarily the size of the SPN, even if proportional).
- v_i and v_j (line 80) could be mentioned ""...every pair (v_i,v_j) of children..."".
- After line 175, ""(4) ="" is not so visually appealing. Better to use M_p(f(w_{k,j})).
- ""So...thus"" of lines 185-186 sounds repetitive.

===after authors' response===
The query about the theorem has been clarified to me.","This paper solves an important problem of computing moments in SPNs.

The reader wonders whether this paper only helps with Dirichlet priors on sum node weights.  How does this interact with priors on leaf nodes or other sum node weight priors?

It is disappointing to not see any experiments, especially comparing against heuristic methods to smooth SPN weights with false counts.  [Post-rebuttal:  I disagree that heuristic methods of parameter estimation are not comparable to methods proposed by this paper.  Readers would be very curious how Hard/Soft Gradient Descent or E.M. with false count smoothing (for which each step uses less space and time compared to the proposed method) compares in terms of space/time per step and overall measures of likelihood and rates of convergence (accuracy and speed).]

Section 3

The remarks are somewhat verbose.

Algorithm 1 should be self-contained and describe exact expressions (""based on Eq. 11"" is a tad informal)

Section 4

It would be best if Algorithm 2 was self-contained and did not require resorting to the appendix.

Appendix

Eqn. 16, 17 and l. 327: Should correct the ambiguous denominators to show what is inside the sum.","I thank the authors for their response. As mentioned in my review, I also see the contributions of this paper on an algorithmic level. Nevertheless I would encourage the authors to at least comment on the existing empirical results to give readers a more complete picture.

--

The submitted manuscript develops a linear time algorithm for computation of moments in SPNs with general DAG structure, improving upon the quadratic time complexity of existing algorithms.

I like the contributions of this paper and I think the proposed algorithms are novel (given that the authors are the same as those of an arXiv paper from a couple of months ago) and could turn out to be useful. On the other hand, the contributions of the paper are somewhat limited. It would be beneficial to demonstrate the usefulness of the algorithms on some task like parameter estimation. In particular, it would be interesting to see whether the uncertainties that can be computed from the approximate posterior are useful in some way, e.g. to quantify uncertainty in queries etc. 

Regarding ""remark 3"" in lines 228-236. Given that the authors draw this close connections, the neural networks with products unit studied in the past should be mentioned. These product units are more general than the products computed in SPNs (they have tuneable parameters). 

A couple of more comments:
* Lines 251 to 254 give the impression that the stated result about moment matching in the case of exponential family distributions as approximating distributions is novel/from the authors. I found the same result in 

    Shiliang Sun, ""A review of deterministic approximate inference techniques for Bayesian machine learning"", 2013

and expect there are even older similar results.
* There seems to be an arXiv version of the same paper which I have seen some time ago that goes beyond the submitted paper, e.g. it includes experiments. In these experiments, the method proposed in this paper do ""only"" compare favourably to other methods in terms of runtime, but not in terms of model fit. That is ok, as I see the contribution of this paper on a different level. However, as these results are known, I feel that they should have been mentioned somewhere (could have been anonymous). "
SGD Learns the Conjugate Kernel Class of the Network,Amit Daniely,https://proceedings.neurips.cc/paper/2017/hash/489d0396e6826eb0c1e611d82ca8b215-Abstract.html,"Proving the learnability of the stochastic gradient descent algorithm is an important task in deep learning. The authors consider this problem in a graphical framework in terms of computational skeletons and provide PAC learning type analysis. Since the problem itself is very difficult, the presented results are acceptable. The work is not at the top level to this reviewers due to the following two reasons:

1. The skeleton with the same number r of incoming neurons and the homogeneous linear kernel defined for the input node are pretty special. This leads to the proof of learnability of polynomials only, though Corollary 5 gives a special case of approximating a continuous function when it is realizable by the distribution. 

2. The lower bound for r of order O(\epsilon^{-2} \log (1/\epsilon)) required for the accuracy \epsilon in Theorems 1 and 2 is very demanding. 
","This paper studies the problem of learning a class of functions related to neural networks using SGD. The class of functions can be defined using a kernel that is related to the neural network structure (which was defined in [Daniely et al. 2016]). The result shows that if SGD is applied to a neural network with similar structure, but with many duplicated nodes, then the result can be competitive to the best function in the class.

Pros:
- This is one of the first analysis for SGD on multi-layer, non-linear neural networks.
- Result applies to various architectures and activation functions.

Cons:
- The definition of the kernel is not very intuitive, and it would be good to discuss what is the relationship between functions representable using a neural network and functions representable using this kernel?
- The same result seems to be easier to prove if all previous layers have random weights and SGD is applied only to last layer. It's certainly good that SGD in this paper runs on all layers (which is similar to what's done in practice). However, for most practical architectures just training the last layer is not going to get a good performance (when the task is difficult enough). The paper does not help in explaining why training all layers is more powerful than training the last layer.

Overall this is still an interesting result.","This paper addressed important theoretical problem how a standard stochastic gradient descend algorithm can guarentee on learning in polynomial time to approximate the best function in the conjugate kernel space of the network.It is claimed by authors that this is the first polynomial time guarentee for neural network learning with depth more than two.

In general, the paper is clearly written but authors uses too many pages for background introduction such kernel classes and neural network learning,leaving little room for proofs of main theorems in the paper and thus make it quite challenging to fully understand all technical details.

Another minor issue is the typo of descend. Authors keep using decend in the paper.

Overall, I think this paper addresses an important theoretical problem on neural network learning."
Learning to Pivot with Adversarial Networks,"Gilles Louppe, Michael Kagan, Kyle Cranmer",https://proceedings.neurips.cc/paper/2017/hash/48ab2f9b45957ab574cf005eb8a76760-Abstract.html,"The paper proposes an adversarial framework for training a classifier that is robust to systematic uncertainties. The paper studies a scenario where the data generation process of interest is governed by an unknown random variable Z where the random variable defines a family of data generation distribution (i.e. a different distribution is incurred by a realization of the random variable). It then applies the proposed method to train a classifier whose performance is independent of the random variable. The algorithm was tested on a synthetic dataset and a high energy physics example. No comparison with prior arts were drawn.

Paper strength

The paper extends the generative adversarial framework to a robust classification problem against systematic noise in the data generation mechanism. 

The paper shows that the optimal classifier of the proposed framework is a classifier whose output is independent to the input variable. 

Paper weakness

The paper fails to motivate the problem. Due to the difference of the members in the family of data distribution, samples from some member is naturally easier to classify than members from the other distribution (For example, consider the case that z is very large in Section 5.1). In other words, it is common to have classification outputs correlates with the unknown variable. A classifier would do better for samples from some members and do worse for samples from the other members. By adding the proposed adversary, which encourages the output to be independent with Z. It basically forces the classifier to do worse for the easy cases for matching the performance for the hard cases. The experiment results reports in the paper also suggest this is what happens. The overall accuracy is traded-off for consistency. It is unclear why it is advantageous to do so. A more reasonable approach would be to improve the performance across the members. 

No comparison with prior works. The paper does not report comparison with baseline approaches that also consider the existence of Z when training the classifier. The only baseline model is a plane classifier model that ignores the existence of Z.

Experiment details are missing. Many details such as the data format in the section 5.2 is missing. It is difficult to understand the experiment setting.


The discussions on whether the optimal and pivotal classifier is achievable is very vague. The results that the training curves shown in the paper are away from the optimal values do not mean that the optimal and pivotal classifier is not achievable. The achievability might be just limited by the training of the network capacity. Hence, the statement is not solid.
","This paper addresses accounting for unknown nuisance parameters that may have introduced selection bias and/or confound classification results. The paper is very interesting, and I feel the approach is important and significant in a number of settings. 

Unfortunately, I feel like the experiments are rather limited. Particularly, the authors make the claim that there are no datasets with continuous pivot variables to train on, but this can be easily constructed. For instance, one could select MNIST (or CIFAR, Imagenet, etc) images with a selection based on the intensity or variance over the pixels (which could be modeled a number of ways). In addition, this should be applicable to the base where there is systematic variation added to certain classes, which may have been a result of instrument or other selection biases. This could have also been simulated. While MNIST / CIFAR / etc are also just toy datasets that maybe are far removed from the experimental setting that the authors are envisioning, they contain high-dimensional structure on which successful pivoting would greatly strengthen the paper.

Another point of concern is I don't get a clear understanding of how to choose the nuisance prior and how those choices affect results. 

Finally I think a big application  could be an agent / environment setting. Have the authors thought about this and can they comment?

L24: it would be good to have some citations here
","The paper considers the problem of learning the parameters $\theta_f$ of a model $f(X;\theta_f)$ (for example a regression or a classification model) such that $f(X;\theta_f)$ is a pivotal quantity with respect to some nuisance variables $Z$. A pivotal model $f(X;\theta_f)$ is insensitive to the variations of $Z$. In other words, the objective is to train classifiers (or regressors) that are both optimal with respect to the parameters $\theta_f$ and robust with respect to the variables $Z$. The authors propose a framework that relies on the use of adversarial networks for this purpose. In particular, the output of the model $f(X;\theta_f)$ will be the input of an adversarial model that has an antagonistic objective with respect to model $f(X;\theta_f)$. It is shown that the minimax solution of this two-player game will be a model that is both optimal and pivotal (assuming that such a model exists).
      
I think that this application of adversarial networks is novel and interesting for the community of NIPS. As far as I can understand, the theoretical results of Section 4 are correct, and the value of the approach is demonstrated by an interesting case study."
Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration,"Jason Altschuler, Jonathan Niles-Weed, Philippe Rigollet",https://proceedings.neurips.cc/paper/2017/hash/491442df5f88c6aa018e86dac21d3606-Abstract.html,"This paper proposes an algorithm for solving unregularized OT problems.
The two-step algorithm (Sinkhorn + rounding) obtains an epsilon-accurate
approximation in O(n^2 log n / epsilon^3) time.
The authors further propose a greedy variant of Sinkhorn.

I am not an expert in the field but this paper seems like an important
contribution. The paper also includes a new analysis of Sinkhorn where the
distance to the transportation polytope is measured in total variation,
which as the authors argue is more relevant.

I found it a bit unfair that the authors talk about cubic dependence
in line 48 for interior points methods but when it comes to their method
talk about linear dependence in n^2. In fact, I found the near-linear ""time""
claims in the title and in the paper misleading. IMO, near linearity is w.r.t.
the cost matrix size.

The ""rounding to a feasible point"" section was a bit too short.  It was not
entirely clear to me what calculating ""the optimal transport with respect to
the total variation distance"" means. What precise objective (if any) does
Algorithm 2 minimize?

The cubic dependence on epsilon and on L suggests that there are regimes where
network flow solvers are faster, in addition to being exact. This should be
properly discussed. 

The empirical evidence that Greenkhorn outperforms Sinkhorn seems a bit
anecdotal. In their experiments, the authors plot their results against the
number of row / col updates. However, on GPU, the cost of updating one or
several rows / cols should likely be more or less the same.
I would have been more convinced by wall-clock time comparison.
More evidence on more datasets, possibly in the supplementary, would be welcome.

Minor comments
--------------

Line 6: ""provides guidance towards parameter tuning for this algorithm""

I am not sure whether this is really relevant for the abstract.
This is not really discussed in the paper and the user has to choose epsilon anyway.

Line 9: mention that Greenkhorn is greedy

Line 72: perhaps mention that the solution is only feasible asymptotically?

Line 89: progam -> program

Line 101 (Theorem 3): epsilon -> epsilon'? 

Theorem 5: if space permits, repeat the definitions of s and \ell

Line 231: give a brief justification / intuition for the choice of rho(a, b)
","In this paper the authors discuss entropic regularized optimal transport with a focus on the computational complexity of its resolution. Their main contribution is a result giving relation between accuracy (in term of error wrt the constraint) and number of iterations for Sinkhorn (and their) algorithm. It show that a given accuracy can be obtained with a complexity near linear to the size of the problem which is impressive.
Using the in-depth computational study the authors propose a new algorithm with the same theoretical property of Sinkhorn but more efficient in practice. very small numerical experiment suggest that the propose GreenKhorn method works in practice better than Sinkhorn.

This paper is very interesting and propose a theoretical computational study that can be of huge help to the community. It also explain why the recent entropic regularization works so well in practice and when it shouldn't be used.  The new Greedy Sinkhorn algorithm is very interesting and seem to work very well in practice. I still have a few comment that I would like to be addressed in the rebuttal before giving a definite rating.

Comments:

- The whole paper seem a bit of a mess with theorems given between definitions and proofs scattered everywhere except below the theorem. I understand the the authors wanted to tell a story but in practice it makes the paper more difficult to read. Beginning with theorem 1 is ok and finishing with its proof is also OK. But the remaining should be done in classical math stuff theorem 2 followed by proof then theorem 3 that uses theorem 2 and so on.

- One of the take home message from theorem 1 is actually not that good: if you want to decrease the l1 error on the constraint by 10, you will need to perform 10^3=1000 times more iterations. this also explain why sinkhorn is known to converge rather quickly to a good enough solution but might take forever to actually converge to numerical precision. It should have been discussed a bit more. Actually I would really like to see numerical experiments that illustrate the O(1/k^{1/3}) convergence of the error.

- The propose Greenkhron is very interesting but it has strong similarity with the stochastic sinkhorn of [GCPB16]. My opinion is that the proposed method is better because it basically update only the variable that violate the most the constraints whereas stochastic will spend time updating already valid constraints since all variables are updated. Still good scientific experiments require the authors to provide these comparison and the fact that the code for [GCPB16] is available online gives the authors no excuse.





","***
Post rebuttal : Sounds good to me!
***

This paper shows that, with appropriate choice of parameters, the Cuturi-Sinkhorn algorithm, and a greedy variant called Greenkhorn, both calculate an approximate solution to the optimal transport problem in near linear-time in the input size. I like the clean analysis which is quite easy to follow, and with the power of hindsight, it is clear that the 1-norm (or TV) is the right error metric for entropic penalties (especially because of Pinsker, ie -entropy is strongly convex wrt TV norm).

Minor question: In the statement of theorem 1, when the runtime is actually O(n^2 + S), which is O(max(n^2,S)), why do the authors prefer to report the runtime as O(n^2S) = O(n^2 L^3 log n/e^3)? They differ significantly when epsilon is a function of n, which might be desirable for statistical reasons (when the quantity actually being calculated is the empirical OT distance, but the quantity of interest is the population OT distance, we may not desire accuracy e to be a constant but instead decaying with n).

Can the authors conjecture on whether the dependence on epsilon can be improved, or do the authors have some reason to believe that the dependence is essentially optimal?

Please clarify in the notation section what r_i(F) and c_j(F) mean, they are not introduced before being used in Algorithm 2,3. I obviously can guess what they mean, but it's easier for everyone if this is added. Nevertheless, this algorithm and associated result are quite nice, and the proof of theorem 1 is so clean and intuitive. (however, the forward reference in the proof of theorem 3 to theorem 5,6 suggests that some reorganization may be beneficial)

I think the authors may want to rename some of their ""theorems"" as ""propositions"" or ""corollaries"". The one or two central results in the paper (which even a non-expert should be tempted to read) should be labeled theorems, it is not good mathematical writing style to have 6 theorems in a short paper, all of which are essentially parts of the same goal -- it's like the use of bold or italics, if it is overused, it loses its effect of catching the reader's attention. 

Overall, I like the paper a lot, it seems correct, it is clean and not unnecessarily complicated, it is important, the empirical results are promising, and if the paper is reorganized and rewritten a bit, it can be a great nips paper."
Ensemble Sampling,"Xiuyuan Lu, Benjamin Van Roy",https://proceedings.neurips.cc/paper/2017/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html,"This paper introduces ensemble sampling as an approach to approximate Thompson sampling. Ensemble sampling is useful for the cases where sampling from the posterior is intractable. The idea of ensemble sampling is to maintain and increamentally update an ensemble of models and sample uniformly from these models whenever an approximation to stochastic sampling is needed. Even though the whole idea is general, it is not formulated in general and is only built on two use cases, linear bandit and neural networks. Moreover, the analysis (convergence bounds) is only developed for the case of linear bandit and not for neural network. Overall, the paper is written and organized well; it is easy to follow and addresses an important problem. Below are some of my concerns. 

The procedures for updating models is not clearly motivated and explained. The authors suggest maintaining a covariance matrix and proposing the incremental updates in line 104 as one possible procedure but it is not clear how this selection is made and what are other possibilities. No motivation or justification is provided. Is there a science behind this that can guide us to design a new one? 

Two examples are introduced in the paper; one for linear bandit and the other for Neural Network. Analysis are not provided for the case of neural network even though that is the use case for which using ensemble sampling is helpful in practice. Note that stochastic sampling for linear bandit in tractable. 

Figure 1a shows that ensemble sampling is better in early iterations and worse in later iterations compared to Thompson sampling. Why is that? Moreover, Figure 1b is not consistent with figure 1a in this regard. Almost to t=100, ensemble sampling with any number of models (plotted in Figure 1a) is better than stochastic sampling. 

","This paper presents a particle filter like finite ensemble approximation version of the posterior (Thompson) sampling approach and theoretical analysis for the bandit problems with potentially complex models. The analysis establishes that the regret can be broken as the sum of two terms, the regret of the ideal Thompson sampling and the regret due to the error introduced by the finite filter. The proof is sound and makes use of tricks, such as the reduction of the union bound on the action counts independent of the action order. Experiments are conducted on Gaussian linear bandits and neural networks, and the results presented confirm the practicality of the proposed approach for mildly large ensembles.  Generally, the work is of some practical use and significance, but the paper would be strengthened with more details for and more convincing experimental results, given the relatively light theoretical contribution. ","This paper proposes ensemble sampling which essentially approximates Thompson Sampling for complex models in a tractable way. This will be useful in a variety of applications. There does not seem to be an obvious mistake in the proofs. However, there are a number of limitations of the proposed method. See below for detailed comments: 

1. What is the underlying noise model for the rewards? Please clarify this. 
2. The weights of the neural network are assumed to be Gaussian distributed. Why is this a valid assumption? Please explain. 
3. Please explain under what assumptions can the proposed method be applied? Can it applied for other model classes such as generalized linear models, and SVMs?
4. A neural network offers more flexibility. But is this truly necessary for applications where the time horizon is not too large, and the number of samples are limited? 
5. Missing references in lines 85 - 92. Please add citations wherever necessary. 
6. In line 3 of Algorithm 1, the model is picked uniformly at random. Can the results be improved by using a more intelligent sampling scheme?
7. Using a sample from a randomly perturbed prior and measurements seems similar to the Perturb and MAP framework in [1]. Please cite this work and describe how your framework is different. 
8. To update the models, it is necessary to store the entire history of observations. This is inefficient from both a memory and computational point of view. 
9. In line 120, ""some number of stochastic gradient descent iterations"". Please clarify this. How many are necessary? Do you model the error which arises from not solving the minimization problem exactly?
10. In Theorem 3, the approximation error increases with T. The approximate regret is then in some sense linear in T. Please justify this. Also, what is the dimension dependence of the approximation error? Also, please give some intuition on the linear dependence of on the gap \delta. 
11. In section 5, the experiments should show the benefit of using a more complex model class. This is missing. Please justify this. 
12. A sampling based technique should be used as a baseline. This is also missing. Please justify. 
13. In line 249, ""we used a learning rate of.. "". How did you tune these hyper-parameters? How robust are your results to these choices?
[1]. Gaussian Sampling by Local Perturbations,"
Practical Data-Dependent Metric Compression with Provable Guarantees,"Piotr Indyk, Ilya Razenshteyn, Tal Wagner",https://proceedings.neurips.cc/paper/2017/hash/49b8b4f95f02e055801da3b4f58e28b7-Abstract.html,"The paper consider the problem of compressing the representation of a set of points in the Euclidean space so that the pairwise distances are preserved and one needs to store as few bits as possible.

The problem has been studied in theory but it would be helpful if the authors give more explanation of how this problem is useful in practice. The space needed to store the points is not a barrier in most cases. Additionally, having a data structure that can only give distances between points without anything else is not helpful for tasks that require any kind of indexing e.g. searching for nearest neighbor.

The paper gives an algorithm that run faster than previous work but also produce a slightly suboptimal compressed size. The algorithm is basically a version of a compressed quadtree with a small modification: when compressing a long path in the compressed quadtree into a single edge, one does not store the entire description of the path but only the most significant bits (roughly log(1/epsilon) bits for approximation 1+epsilon).

Overall, the technical contribution seems limited. However, if the authors can establish this problem as a useful solution for important tasks then the paper would be a nice contribution.","This paper presents an impressive algorithm that embeds a set of data points into a small dimensional space, represented by a near-optimal number of bits. The most appealing aspect of this paper is to provide a simple but provable dimensionality reduction algorithm, and supported by empirical experiments.

=== Strengths ===
This paper is supported by strong theoretical analysis. And, it is much simpler algorithm than [5] with a slightly worse bit complexity.

=== Weaknesses ===
1. This is an example of data-dependent compression algorithm. Though this paper presents a strong theoretical guarantee, it needs to be compared to state-of-the art data-dependent algorithms. For example,
 - Optimized Product Quantization for Approximated Nearest Neighbor Search, T. Ge et al, CVPR’13
 - Composite Product Quantization for Approximated Nearest Neighbor Search, T. Zhang et al, ICML’14
 - And, other recent quantization algorithms.

2. The proposed compression performs worse than PQ when a small code length is allowed, which is the main weakness of this method, in view of a practical side.

3. When the proposed compression is allowed to have the same bit budget to represent each coordinate, the pairwise Euclidean distance is not distorted? Most of compression algorithms (not PCA-type algorithms) suffer from the same problem.

=== Updates after the feedback ===
I've read other reviewers' comments and authors' feedback. I think that my original concerns are resolved by the feedback. Moreover, they suggest a possible direction to combine QuadSketch with OPQ, which is appealing.

","  This paper presents a new, practical approach to a field with a huge number of previous papers.  They show the potential benefits of their data-adaptive compressive encoding by Accuracy and Distortion experiments on several datasets.  When parameters are set correctly for each dataset, the method *can* clearly outperform good methods such as PQ.  (So the last sentence of the abstract might be a bit of a strong claim, since sometimes the representations can be worse).
  The method description is compact, yet readable.  It is precise enough that I feel I could sit down, code their algorithm and reproduce their results.
  It seems, however, that further work is still needed to develop heuristics for adapting the L and Gamma to values appropriate to the dataset. Incorrect settings *might* perform worse than PQ, which at least has decent (smooth) Accuracy/Distortion curves as its parameter (# of landmarks) is varied.
  I would have liked to see runtime comparison with PQ.
  It would be nice to have a public ""reference implementation"" in the future, to spur further development in the community.

- line 217: ""Section 4.1"" should be ""Figure 7"", which in turn should probably be ""Table 3"".
"
Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery,"Jie Shen, Ping Li",https://proceedings.neurips.cc/paper/2017/hash/4a2ddf148c5a9c42151a529e8cbdcc06-Abstract.html,"Summary. This paper studies support recovery of the partial hard thresholding algorithm, and have derived, under certain conditions, the iteration complexity of the partial hard thresholding algorithm.

Quality. There seems no explicit description about the condition under which the PHT(r) algorithm terminates, which makes the statement of Proposition 1 difficult to understand.
It would have been better if concrete arguments on specific problem examples were given. They are presented only in a very brief and abstract manner in the last paragraph of Section 2. In particular, if nonzeros in \bar{x} are distributed according to a distribution without discontinuity at x=0, then x_min should scale as O(1/n), so that it should become difficult to satisfy the x_min condition when n is large.

COMMENT AFTER REBUTTAL: In the above, I should have written that x_min should scale as O(1/d). I am pretty sorry for my careless mistake.

Clarity. I think that the description of the simulations is so brief that one cannot relate the simulation setups with the theoretical results. More concretely, the function F(x) adopted in the simulations should be explicitly stated, as well as the condition numbers and other parameters appearing in the theoretical results. Of interest also would be whether the 10,000 iterations are sufficient with regard to the theoretical guarantees. Also, nonzeros of the signals are generated as Gaussians, so that x_min values vary from trial to trial, as well as the s values specified.
Lemma 4 and Theorem 5 in the main text appear in the supplementary material as Lemma 19 and Theorem 20 without explicit statement that they are the same.

Originality. I think that this work would be moderately original, in that it seems that it has extended the existing arguments on support recovery via hard thresholding to partial hard thresholding, which would certainly be non-trivial.

Significance. Since the partial hard thresholding includes the conventional hard thresholding as well as what is called the orthogonal matching pursuit with replacement as special cases, the theoretical support-recovery guarantees for the partial hard thresholding algorithm presented here should be of significance. 

Minor points:

Line 16: ha(ve -> s) found
Line 56: in turn indicate(s)
Line 76: (Lowercase -> A lowercase) letter(s)
Line 132: restricted (strongly) smooth
Line 196: The(n) the support
Line 287: i.i.d. (standard) normal variables.
Line 304: significantly reduce(s)","Review prior to rebuttal:

The paper provides analytical results on partial hard thresholding for sparse signal recovery that are based on RIP and on an alternative condition (RSC and RSS) and that is applicable to arbitrary signals.

The description of previous results is confusing at times. First, [4] has more generic results than indicated in this paper in line 51. Remark 7 provides a result for arbitrary signals, while Theorems 5 and 6 consider all sufficiently sparse signals. Second, [13] provides a guarantee for sparse signal recovery (Theorem 4), and so it is not clear what the authors mean by “parameter estimation only” on line 59. If exact recovery is achieved, it is obvious that the support is estimated correctly too. Third, it is also not clear why Theorem 2 is described as an RIP-based guarantee - how does RIP connect to the condition number, in particular since RIP is a property that can go beyond specific matrix constructions?

The simulation restricts itself to s-sparse signals, and so it is not clear that the authors are testing the aspects of the results that go beyond those available in existing work. It is also not clear how the results test the dependence on the condition number of the matrix.

Minor comments follow.
Title: “A Towards” is not grammatically correct.
Line 128: the names of the properties are grammatically incorrect: convex -> convexity, smooth -> smoothness? or add “property” at the end of each?
Line 136: calling M/m “the condition number of the problem” can be confusing, since this terminology is already used in linear algebra. Is there a relationship between these two quantities?
Line 140: there is no termination condition given in the algorithm. Does this mean when the gradient in the first step is equal to zero? When the support St does not change in consecutive iterations?
Line 185: the paper refers to a comparison of the analysis of [4] being “confined to a special signal” versus the manuscript’s “generalization… [to] a family of algorithms”. It is not clear how these two things compare to one another.
Line 301: degrade -> degradation
Line 304: reduces -> reduce

Response to rebuttal: The authors have addressed some of the points above (in particular relevant to the description of prior work), but the impact of the contribution appears to be ""borderline"" for acceptance. Given that the paper is acceptable if the authors implement the changes described in the rebuttal, I have set my score to ""marginally above threshold"".","This paper analyzes the support recovery performance of the PHT algrorithm of Jain et al. While in Jain et al.'s paper the main focus (at least with respect to the PHT family) was estimation error, the current paper aims to extend this to support recovery performance. 

The technical contribution of the paper appears sound to me. However, its not entirely clear how different the techniques are to those used in [4]. I am willing to revise my rating if the authors can make this more clear. 

The presentation of the paper needs improvement. 
- There are several typos throughout. A thorough pass needs to be made. 
- The title of the paper is grammatically incorrect -- perhaps the authors meant ""Towards a Unified Analysis of Support Recovery"". This phrase ""a towards unified analysis"" is used in other places in the manuscript, and its incorrect. 
- In the abstract the authors use \kappa and say that it is THE condition number, while it is unclear for a while which condition number it is. This should be replaced by something like ""where \kappa is an approporiate condition number"", since the authors probably dont want to introduce the formal definition in the abstract. 


"
Selective Classification for Deep Neural Networks,"Yonatan Geifman, Ran El-Yaniv",https://proceedings.neurips.cc/paper/2017/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html,"The paper proposes a practical scheme of adding selective classification capabilities to an existing neural network. The method consists of:
1. Choosing a score function that captures how confident the network is in its prediction, analysed are MC-dropout scores for networks trained with dropout and the maximum softmax score for networks with a softmax output with the second performing empirically better.
2. Defining the desired confidence level and error rate.
3. Running a binomial search to establish a score threshold such that with the desired confidence level the classifier will have an error rate smaller than the specified one on the samples it chooses to classify.

The procedure uses an existing bound on the true error rate of a classifier based on a small sample estimate (Lemma 3.1) and uses binomial search with a Bonferroni correction on the confidence level (Algorithm 1) to find the score threshold.

Experimental results validate the approach and show good agreement between the algorithm inputs (desired error rate) and observed empirical error rates on a test set.

The strong points of the paper are the practical nature of it (with the softmax response score function the procedure can be readily applied to any pretrained neural network) and the ease of specifying the algorithm’s desired confidence level and error rate (which is modeled after ref [5]). While the paper builds on well known concepts, the careful verification of the concepts adds a lot of value.

The paper lacks simple baselines, that could showcase the importance of using the binomial search and the bound on the classifier’s error rate. In particular, I would like to know what happens if one chooses the score threshold as the lowest value for which the error rate on a given tuning set is lower than e specified value- would the results be much more different than using the bound from Lemma 3.1? Knowing this baseline would greatly motivate the advanced techniques used in the paper (and would raise my score of this paper).

Nitpicks: the Algorithm 1 uses an uninitialized variable r*
","Selective classification is the problem of simultaneously choosing which data examples to classify, and subsequently classifying them. Put another way, it’s about giving a classifier the ability to ignore certain data if it’s not confident in its prediction. Previous approaches have focused on assigning a small cost for abstaining. This paper proposes a post-hoc strategy where, if a classifier’s confidence can be accurately gauged, then this confidence is thresholded such that the classifier obtains a guaranteed error rate with high probability.

The main novelty with this paper is the proposed SGR algorithm and associated theory. This relies on an ideal confidence function, which is not available in practice, so two methods, SR and MC-dropout are tested. The results are promising, obtaining a low test error with a reasonably high coverage.

Getting into specifics: it’s not obvious how you solve Equation (4). I’m assuming it’s a simple line search in 1D, but it would be helpful to be explicit about this. Also, what is the complexity of this whole procedure? It looks like it’s mlog(m)?

It’s interesting that mc-dropout performed worse on Imagenet, do you have any intuition as to why this might be the case? It may be helpful to visualize how the confidence functions differ for a given model. I suppose one can easily test both and take the one that works better in practice.

As far as I know, there is no explicit validation set for CIFAR-10 and CIFAR-100. They each have 50,000 training points with a separate 10,000-point test-set. Did you split up the test-set into 5,000 points? Or did you use the last batch of the training set for Sm? I think a more proper way to evaluate this would be to use some portion of the last batch of the training sets as validation, and evaluate on the full test set. It would be helpful for you to mention what you did for Imagenet as well; it looks like you split the validation set up into two halves and tested on one half? Why not use the full test set, which I think has 100,000 images?

There’s a typo in section 5.3 (mageNet).

One point of weakness in the empirical results is that you do not compare with any other approaches, such as those based on assigning a small cost for abstaining. This cost could be tuned to get a desired coverage, or error rate. It’s not clear that a post-hoc approach is obviously better than this approach, although perhaps it is less expensive overall.

Overall I like this paper, I think it’s a nice idea that is quite practical, and opens a number of interesting directions for future research.
","The paper addresses the problem of constructing a classifier with the reject
option that has a desired classification risk and, at the same time, minimizes the
probability the ""reject option"". The authors consider the case when the
classifiers and an associate confidence function are both known and the task is
to determine a threshold on the confidence that determines whether the
classifier prediction is used or rejected. The authors propose an algorithm
finding the threshold and they provide a statistical guarantees for the method.

Comments:


- The authors should provide an exact definition of the task that they attempt
to solve by their algorithm. The definition on line 86-88 describes rather the
ultimate goal while the algorithm proposed in the paper solves a simpler
problem: given $(f,\kappa)$ find a threshold $\theta$ defining $g$ in equation
(3) such that (2) holds and the coverage is maximal.

- It seems that for a certain values of the input arguments (\delta,r^*,S_m,...)
the Algorithm 1 will always return a trivial solution. By trivial solution I
mean that the condition on line 10 of the Algorithm 1 is never satisfied and
thus all examples will be at the end in the ""reject region"". It seems to me that
for $\hat{r}=0$ (zero trn error) the bound B^* solving equation (4) can be
determined analytically as 
   $B^* = 1-(\delta/log_2(m))^{1/m}$. 

Hence, if we set the desired risk $r^*$ less than the number $B^* =
1-(\delta/log_2(m))^{1/m}$ then the Algorithm 1 will always return a trivial
solution. For example, if we set the confidence $\delta=0.001$ (as in the
experiments) and the number of training examples is $m=500$ then the minimal
bound is $B^*=0.0180$ (1.8%). In turn, setting the desired risk $r^* < 0.018$
will always produce a trivial solution whatever data are used. I think this
issue needs to be clarified by the authors.

- The experiments should contain a comparison to a simple baseline that anyone
would try as the first place. Namely, one can find the threshold directly using
the empirical risk $\hat{r}_i$ instead of the sophisticated bound B^*. One would
assume that the danger of over-fitting is low (especially for 5000 examples used
in experiments) taking into account the simple hypothesis space (i.e. ""threshold
rules""). Without the comparing to baseline it is hard to judge the practical
benefits of the proposed method.
  
- I'm missing a discussion of the difficulties connected to solving the
numerical problem (4). E.g. which numerical method is suitable and whether there
are numerical issues when evaluating the combinatorial coefficient for large m
and j. 


Typos:
- line 80: (f,g)
- line 116: B^*(\hat{r},\delta,S_m) 
- line 221: ""mageNet""
"
Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space,"Liwei Wang, Alexander Schwing, Svetlana Lazebnik",https://proceedings.neurips.cc/paper/2017/hash/4b21cf96d4cf612f239a6c322b10c8fe-Abstract.html,"
This paper investigated the task of image-conditioned caption generation using deep generative models. Compared to existing methods with pure LSTM pipeline, the proposed approach augments the representation with an additional data dependent latent variable. This paper formulated the problem under variational auto-encoder (VAE) framework by maximizing the variational lowerbound as objective during training. A data-dependent additive Gaussian prior was introduced to address the issue of limited representation power when applying VAEs to caption generation. Empirical results demonstrate the proposed method is able to generate diverse and accurate sentences compared to pure LSTM baseline. 

== Qualitative Assessment ==
I like the motivation that adding stochastic latent variable to the caption generation framework. Augmenting the prior of VAE is not a novel contribution but I see the novelty applied to caption generation task. Performance-wise, the proposed AG-CVAE achieved more accurate performance compared to both LSTM baseline and other CVAE baselines (see Table 2). The paper also analyzed the diversity of the generated captions in comparison with the pure LSTM based approach (see Figure 5). Overall, the paper is generally well-written with sufficient experimental details.

Considering the additive Gaussian prior as major contribution, the current version does not seem to be very convincing to me. I am happy to raise score if my concerns can be addressed in the rebuttal.

* Any strong evidence showing the advantages of AG-CVAE over CVAE/GMM-CVAE?
The improvements in Table 2 are not very significant. For qualitative results (Figure 5 and other figures in supplementary materials), side-by-side comparisons between AG-CVAE and CVAE/GMM-CVAE are missing. It is not crystal clear to me whether AG-CVAE actually adds more representation power compared to CVAE/GMM-CVAE. Please comment on this in the rebuttal and include such comparisons in the final version of the paper (or supplementary materials).

* Diversity evaluation: it is not clear why does AG-CVAE perform worse than CVAE. Also, there is no explanation about performance gap from different variations of AG-CVAE. Since CVAE is a stochastic generative model, I wonder whether top 10 sentences are sufficient for diversity evaluations? The results will be much more convincing if the authors provide a curve (y-axis is the diversity measure and x-axis is the number of sentences).

Additional comments:
* Pre-defined means of clusters for GMM-CVAE and AG-CVAE (Line 183)
It is not surprising that the authors failed to obtain better results when means of clusters u_k are free variables. In principle, it is possible to learn duplicate representations without any constraints (e.g., sparsity or orthogonality) on u_k or c_k. I would encourage the authors to explore this direction a bit in the future. Hopefully, learnable data-dependent prior can boost the performance to some extent.
","In general, I find the paper to be fairly well-written and the idea to be both quite intuitive and well motivated. However I do have a couple of questions regarding several technical decisions made in the paper.

* For the encoder (Fig. 4), is it really necessary to explicitly generate K posterior mean/variances separately before taking the weighted average to combine them into one? Or, compared with having the encoder network directly generate the posterior mean/variances, how much gain would the current architecture have?

* Given the fact that ultimately we only need the decoder to do conditional caption generation (i.e. there is no **actual** need for doing posterior inference at all), it seems to me that VAE might be an overkill here.
Why not directly learn the decoder via maximum-likelihood? You can actually still sample z from p(z|c) and then marginalize it to approximately maximize p(x|c) = \sum_{z_i} p(z_i|c)p(x|z_i, c). This should be an even stronger LSTM baseline with z still present.

* For GMM-CVAE, how did you compute the KL-divergence?

* During training, the standard deviations \sigma_k in the prior essentially controls the balance between KL-divergence (as the regularization) and reconstruction error (as the loss). I'm therefore interested to know whether the authors have tried different values as they did during test time. I raise this issue also because to me it is more reasonable to stick with the same \sigma_k both for training and testing.

Typo
* L.36: ""maximize (an upper bound on) the likelihood"" should have been ""lower bound"" I think.","
        This paper presents and tests two variants of a prior on the VAE latent space for image captioning.  Specifically they both employ a Gaussian mixture model prior on the latent apace, and then a better performing variant whereby the clusters are encourage to represent objects in a given image --- i.e. any given image will be generated via a linear combination of samples from clusters corresponding to the objects it contains.

        Detailed comments:
        * The paper is well written and appears to be well tested against previous baselines.  While the idea is straightforward, and perhaps not a standalone contribution by itself the testing appears to be reasonably conclusive, and would appear to yield state of the art results (when checked against the online MSCOCO baseline dataset). I am curious as to why the CGAN was not also benchmarked, however (there are now techniques for sentence generating GAN models via e.g. the Gumbel-Softmax trick)?
        * It would be interesting to combine the GMM latent space model with a DPP prior over the cluster centers, to further encourage diversity in representation, a hierarchical latent space might also be an interesting direction to explore, although I am strongly of the opinion that the performance of these models (particularly on small datasets) is entirely dependent on how close the implicit prior encoded by the choice of mapping and latent structure is to the true data generating distribution, in which case it may be that the AG-CVAE is close to optimal.
      "
Deconvolutional Paragraph Representation Learning,"Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/4b4edc2630fe75800ddc29a7b4070add-Abstract.html,"The paper presents an innovative model to endode sentences in vectors that is based on CNNs rather than on RNNs. 
The proposed model outperforms LSTM in the encoding-decoding task, that is, recostructing sentences from embedding vectors.

Yet, there is a major problem: what is the network encoding in the vector? Why has it a better performance in reconstructing the original sentence? How does it encode the notion of sequence?
There are many NN models and many NN papers. Yet, the reasons why these models are better are obscure. 
Encoding sentences in vectors has been also a realm of compositional distributional semantics. Moreover, there are works that attempt to encode syntactic structures in vectors and recover them back [1]. This is done without learning. 

[1]Ferrone et al. (2015) Decoding Distributed Tree Structures. In: Dediu AH., Martín-Vide C., Vicsi K. (eds) Statistical Language and Speech Processing. Lecture Notes in Computer Science, vol 9449. Springer, Cham
","This paper proposes a purely Convolutional-Deconvolutional framework for encoding/decoding sentences. The main contributions of this paper are to propose the deconvolution module as a decoder of sentences and to show the effectiveness of this deconvolution module.

In particular, when the output sentence length is long, the deconvolution module performs significantly better than RNN-based decoders do. RNN-based decoder works poorly when the output sentence length is long, which is known as the exposure bias. Deconvolution decoders do not rely on neighborhood predicted for producing output sentences; therefore, preidction mistakes do not propagate the subsequent outputs. This advantage enables to avoid the exposure bias. As a result, this characteristic leads to the effective semi-supervised learning framework for leveraging plenty of unlabeled text resources to solve various NLP tasks, even when these resources contain long sentences.

This paper is well-written. The discussion is well-organized. The contributions of this paper are summarized clearly. The effectiveness of the proposed method is carefully validated by well-designed experiments. I think the quality of this paper is clearly above the threshold of the acceptance.

I have one minor concern: does the deconvolutional module outperform the RNN with attention, too? Although the exposure bias occurs even when we use the attention mechanism, the attention might boost the performance of RNN-based seq2seq methods. If the authors could add the comparison between the deconvolutional module and the RNN with attention, it would be better.","This paper proposes encoder-decoder models that uses convolutional neural networks, not RNNs. Experimental results on NLP data sets show that the proposed models outperform several existing methods.

In NLP, RNNs are more commonly used than CNNs. This is because the length of an input varies. CNNs can only handle a fixed length of an input, by truncating input text or padding shortage of the input. In experiments, it is okay to assume the length of an input, such as 50-250 words; however, in reality, we need to handle an arbitrary length of text information, sometimes 10 and sometimes 1,000 words. In this respect, RNNs, especially, LSTMs, have high potential in NLP applications.

Major Comments:

1. For summarization experiments, Document Understanding Conference (DUC) data sets and CNN news corpus are commonly used.

2. For the headline summarization, if you use shared data, we can judge how your models are better than other methods, including non-neural approaches.

3. Table 2 shows that the CNN-DCNN model can directly output an input. Extremely high BLEU scores imply low generalization. It has a sufficient rephrasing ability, it cannot achieve the BLUE scores over 90. 

4. In image processing, similar models have already been proposed, such as:

V. Badrinarayanan, A. Handa, and R. Cipolla. SegNet: a deep convolutional encoder-decoder architecture
for robust semantic pixel-wise labelling. arXiv preprint
arXiv:1505.07293, 2015.

Please make your contributions clear in your paper.

5. I am not sure how your semi-supervised experiments have been conducted. In what sense, your methods are semi-supervised?

6. Regarding the compared LSTM-LSTM encoder-decoder model, is it an attention-based encoder-decoder model?

7. I suggest to try machine translation data sets, in your future work.
"
Learning to See Physics via Visual De-animation,"Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, Josh Tenenbaum",https://proceedings.neurips.cc/paper/2017/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html,"This paper presents an approach to physical scene understanding which utilizes the combination of physics engines and renderers.  The approach trains a network to predict a scene representation based on a set of input frames and subject to the constraints of the physics engine.  This is trained using either SGD or reinforcement learning depending on whether a differentiable physics engine is used.  The approach is applied to several simple scenarios with both real and synthetic data.

Overall the idea in the paper is interesting and promising, however, the paper as it's written has several issues.  Some of these may be able to be addressed in a rebuttal.

First, there are some significant missing references, specifically related to using physical models in human tracking.  Kyriazis and Argyros (CVPR2013, CVPR 2014); Pham et al (PAMI 2016); Brubaker et al (ICCV 2009, IJCV 2010); Vondrak et al (SIGGRAPH 2012, PAMI 2013).  In terms of simple billiards-like scenarios, see also Salzmann and Urtasun (ICCV 2011).  This is a significant blind spot in the related works and should be addressed.

Second, the paper is missing some important technical details.  In particular, it is never specified how the system actually works at test time.  Is it simply run on successive sets of three input frames independently using a forward pass of the perception model?  Does this mean that the physics and graphics rendering engines are not used at all at test time?  If this is the case, why not use the physics engine at test time?  It could, for instance, be used as a smoothing prior on the per-frame predictions of the perception model?  This issue is particularly relevant for prediction of stability in the block world scenario.  Is the network only run on a single set of three frames?  This makes it particularly difficult to assess the technical aspects of this work.

Other comments:
 - Absolute mass is never predictable without knowing something about what an object is made of.  Only relative mass plays a role in the physics and visual perception can only tell you volume (assuming calibration), not density.  Any network which appears to ""learn"" to predict absolute mass is really just learning biases about mass/density from the training set.  This is fine, but important to recognize so that overboard claims aren't made.
 - The table entries and captions are inconsistent in Fig 7(b-c).  The table used ""full"" and ""full+"", whereas the caption references ""joint"" and ""full"".
","
      	SUMMARY
      	In this paper, the authors presented a data-driven approach for physical scene understanding: given a scene image, the system detects objects with state estimation, then reconstructs the scene, simulates it in a physics engine to predict its future states and render the prediction back to scene images. The approach has been evaluated on two tasks, namely the billiard game and the block stability prediction, and has shown better performance over existing methods.
      	STRENGTH
      	The most prominent contribution is the paper is probably among the first to construct a pipeline to fully reconstruct both a physical simulation and graphics reconstruction for physics scene understanding. Also, the method has been proved to work on the provided tasks in paritcular, the end-to-end training did prove to outperform separately training of individual components. Further, the description is general clear albeit it's on a very coarse level which is detailed as follows.
      	WEAKNESS AND CONCONERN
      	1) As already mentioned in previous section, the description lacks certain levels of details for complete reproduction, for example, how is the physics engine implemented, it's understandable that the authors left out some details with proper reference, however it is not very clear, in paricular as physics engine is an important component in the system, how the engine is set up and the set up affect the results.
      	2) In addition to 2), there are concerns about  the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.
      	3) The authors should also cite recent work on frame prediction. this is very related.
      "
Adversarial Symmetric Variational Autoencoder,"Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/4cb811134b9d39fc3104bd06ce75abad-Abstract.html,"The paper proposes a variant of the Variational Auto-Encoder training objective. It uses adversarial training, to minimize a symmetric KL divergence between the joint distributions of latent and observed variables p(z,x)=p(z)p_\theta(x|z) and q(z,x)=q(x)q_\phi(z|x) . 
The approach is similar to the recent [ Mescheder, Nowozin, Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks, 2016 ] in its joining VAE and GAN-like objective, but it is original in that it minimizes a symmetric KL divergence (with a GAN-like objective), which appears crucial to achieve good quality samples. It is also reminiscent of ALI [ Dumoulin et al.  Adversarially learned inference, ICLR, 2017. ] as it attempts to match joint distributions.

Experiments convincingly show that the approach manages to bridge to a large degree the gap in sampling quality with that of GANs, while at the same time achieving a very good tractable log-likelihood bound. Illustrations of ALI's comparatively poor reconstructions is also an interesting point. All this makes this research, in my eyes, a worthy and significant contribution. 

The main weakness of the paper to me is that I found the flow of the mathematical argument at times hard to follow. For ex.:
- the role and usefulness of Lemma 1 and Corollary 1.1 appears much later than where they are introduced.
- It is not shown that Eq 8 corresponds to a lower bound on the log-likelihood (as Eq 1 is known to be); is it or is it not? The argumentation of l 144 to 148 feels a little convoluted and indirect. I agree that Eq 8 is maximized when p(x,z) and q(x,z) match, but it is unclear to me, and from the paper, what the tradeoff between its two terms achieves when q does not have the capacity to match p. This should be discussed.

l 183: Proposition 1. ""equilibrium … is achieved""
About GAN objectives you mentioned earlier in l 119 that ""This objective mismatch may lead to the well-known instability issues associated with GAN training"". Now your objective in Eq 12 uses a similar min-max objective. Couldn't there be similar instability or collapse issues? 

l 238: experimental evaluation of NLL, ""estimated via the variational lower bound""; is this with the traditional VAE variational lower bound of Eq 1 ? Or with some later equation?

In several places, you specify + const. It would be useful to briefly afterwards state what that const is and that it is constant w.r.t. what, as this is not always clear. 

There are a number of errors in some of the equations:

l 145: ""E_p(z) log p_\theta(z) = "" should be  ""E_p(z) log q_\phi(z) = ""

Eq 10: I think $log q_\phi(x|z)$ should be $log q_\phi(z|x)$  

Eq 11: has the term ordering (hence the sign) reversed in comparison to Eq 10. It should be changed to the same terms ordering as Eq 10 (to be consistent with Eq 10 and Eq. 12).

l 150,152: ""is not possible, as it requires an explicit form for"", not only that, log p_\theta(x) is intractable.
","This paper presents a method which aims to ""symmetrize"" the standard Maximum Likelihood (ML) objective. The approach is based on approximately minimizing both KL(q(x,z)||p(x,z)) and KL(p(x,z)||q(x,z)), where q(x,z) and p(x,z) are the ""encoder"" and ""generator"" distributions. The authors perform this feat using a mix of VAE and GAN techniques. The basic idea is to extract some of the log ratios in the KL divergences from ""discriminators"" trained on a couple of logistic regression problems. This leverages the fact that the optimal solution for logistic regression between distributions p and q is a function f such that f(x)=log(p(x)/q(x)) for all x.

The motivation of mitigating the ""over-dispersion"" which comes from ML's focus on KL(q||p) is reasonable. ML does not penalize the model heavily for generating data where none exists in the ground truth, and many parts of the prior p(z) may not be trained by the regular VAE objective when KL(q(z|x)||p(z)) is large in training (which it typically is for images). The untrained parts of p(z) may produce bad data when sampling from the model.

I'm not confident that the log likelihoods reported for MNIST, CIFAR10, and ImageNet were measured according to the protocols followed in previous work. The description of the MNIST experiments does not seem to match previous papers, which rely on explicitly binarized data. Also, the best published result on this dataset is 78.5 nats, from ""An Architecture for Deep, Hierarchical Generative Models"" (Bachman, NIPS 2016).

The result reported for AS-VAE-r on CIFAR10 is surprisingly good, considering the architecture described in the supplementary material. The best ""VAE only"" result on this task is 3.11 from the most recent version of the IAF paper by Kingma et al. The IAF number required significant effort to achieve, and used a more sophisticated architecture. The 3.32 bpp reported for AS-VAE in the submitted paper is also a bit surprising, given the quality of reconstructions shown in Figure 3.

The qualitative results in Figures 1-4 do not seem to improve on previous work.

--Minor Corrections--
1. $\log q_{\phi}(x|z)$ in Eqn 10 should be $\log q_{\phi}(z|x)$.
2. $\log p_{\phi}(z)$ on line 217 should be $\log q_{\phi}(z)$.

","# Summary
Motivated by the failure of maximum-likelihood training objectives to produce models that create good samples, this paper introduces a new formulation of a generative model. This model is based on a symmetric form of the VAE training objective. The goal of this symmetric formulation is to learn a distribution that both assigns high probability to points in the data distribution and low probability to points not in the data distribution (unlike a maximum-likelihood objective which focuses on learning distributions that assign high probability for points in the data distribution). As the resulting model is not tractable, the authors propose to estimate the arising KL-divergences using adversarial training.

# Clarity
This paper is mostly well-written and easy to understand.


# Significance
This paper describes an interesting approach to generative models, that attempts to fix some problems of methods based on maximum-likelihood learning. The proposed method is interesting in that it combines advantages from ALI/BiGans (good samples) with properties of VAEs (faithful reconstructions). However, by using auxiliary discriminators, the approach is only a rough approximation to the true objective, thus making it difficult to interpret the log-likelihoods estimates (see next section). The authors therefore should have evaluated the reliability of their log-likelihood estimates, e.g. by using annealed importance sampling.

# Correctness
=== UPDATE: this concern was succesfully addressed by the authors' feedback ===
While the derivation of the method seems correct, the evaluation seems to be flawed. First of all, the authors evaluate their method with respect to the log-likelihood estimates produced by their method which can be very inaccurate. In fact, the discriminators have to compare the distributions q(x,z) to q(x)q(z) and p(x,z) to p(x)p(z) respectively which are usually very different. This is precisely the reason why adaptive contrast was introduced in the AVB-paper by Mescheder al. In this context, the authors' statement that ""AVB does not optimize the original variational lower bound connected to its theory"" (ll. 220-221) when using adaptive contrast is misleading, as adaptive contrast is a technique for making the estimate of the variational lower bound more accurate, not an entirely different training objective. It is not clear why the proposed method should produce good estimates of the KL-divergences without such a technique. 
For a more careful evaluation, the author's could have run annealed importance sampling (Neal, 1998) to evaluate the correctness of their log-likelihood estimates.

At the very least, the authors should have made clear that the obtained log-likelihood values are only approximations and have to be taken with a grain of salt.

It is also unclear why the author did not use the ELBO from equation (1) (which has an analytic expression as long as p(z), q(z | x) and p(x | z) are tractable) instead of L_VAEx which has to be approximated using an auxiliary discriminator network.

If the authors indeed used L_VAEx only for training and used the ELBO from (1) (without a discriminator) for evaluation, this should be clarified in the paper.

# Overall rating
While the technique introduced in this paper is interesting, the evaluation seems to be flawed. At the very least, the authors should have stated that their method produces only approximate log-likelihood estimates which can therefore not be compared directly to other methods.

# Response to author's feedback
Thank you for your feedback. The authors addressed my major concern about the evaluation. I therefore raised the final rating to ""6 - Marginally above acceptance threshold"". Note, however, that AR #2 still has major concerns about the correctness of the experimental evaluation that should be addressed by the authors in the final manuscript. In particular, the authors should show the split of the ELBO for cifar-10 into reconstruction and KL-divergence to better understand why the method can achieve good results even though the reconstructions are apparently not that good. The authors should also make sure that they performed the MNIST-experiment according to the standard protocol (binarization etc.)."
Model evidence from nonequilibrium simulations,Michael Habeck,https://proceedings.neurips.cc/paper/2017/hash/4da04049a062f5adfe81b67dd755cecc-Abstract.html,"# Summary of the paper
The paper studies new estimators of the evidence of a model based on ideas from statistical physics.

# Summary of the review
The paper introduces simple methods to estimate the evidence of a model, that originate in the statistical physics literature. The methods seem interesting and novel (at least in the stats/ML literature). My main concerns are that the paper 1) lacks in clarity for an ML audience, and 2) lacks a fair comparison to the state-of-the art. In particular, is the contribution a mathematical curiosity, or is there an applicative setting in which it is the method of choice?

# Major comments
- Section 2: Sequential Monte Carlo samplers [Del Moral, Doucet, Jasra, JRSSB 2006] should also appear here, as they give extremely competitive evidence estimators. In particular, they allow more design choices than AIS.
- Section 3: this section could gain in clarity and structure. Maybe you could isolate the important results as propositions. It also reads strangely to an ML reader, as the authors give lots of names and abbreviations (CFT, JE) for results that seem to be derivable in a few lines. Are these names really standard?
- L91: this paragraph about merits and pitfalls of JE is unclear to me.
- Section 5: as for Section 3, this section could gain in structure. For example, I would have liked the pseudocode of the main algorithm to be isolated from the main text, and I would have liked the BAR and the histogram methods to be clearly separated. After reading this section, it wasn't clear to me what the whole estimation procedure was, and what buttons you were going to tweak in the experimental Section 6.
- Section 6.1: why do you start the reverse paths from fixed extremal states? I thought you needed them to be samples from the target distribution, or did I misunderstand something? In this small Ising case, perfect sampling would even be available via monotone coupling from the past.
- Section 6.4: this is a typical example where SMC samplers would shine.
- Section 6: overall, you do not mention in which setting your estimators would be state-of-the-art. For instance, the ground truth is actually found via parallel tempering. Why and when should I use your estimators and not parallel tempering?

# Minor comments
- Eqn 4: what is D[x]? I guess it's the d(path) in the integral, but since the notation is nonstandard, it should be explained.
- L97 ""a key feature""
- L103 ""then""
","The authors provide a very thorough review of the use of non equilibrium fluctuation theorems from the statistical physics literature for the purpose of estimating the marginal likelihood of statistical models.  In particular, two approaches are introduced from the physics literature that to my knowledge haven’t been used in the field of statistics/machine learning.

I thought this was an good paper.  It is clearly written and ties together many ideas that form the basis of a number of different but related marginal likelihood estimators.  The authors explain the BAR and histogram estimator which I haven’t seen used in statistics and machine learning before.  There is also a nice selection of simulation studies on some non-trivial statistical models.

My only comment is that the authors should be careful about how they phrase their contribution, particularly their use of the word “new” - they should make clear that these are estimators that have already been published in the physics literature, but aren’t commonly used in statistics and machine learning.  Aside from this minor issue, I think this paper makes a useful contribution by sharing knowledge between different fields.","This paper discusses tackling marginal likelihood
estimation, a central problem in Bayesian model selection, using
insights provided by so-called nonequilibrium fluctuation
theorems. While Jarzynksi's Equality (JE) allows a natural sequential
Monte Carlo estimator of log Z, the estimator can be high-variance if
the resultant sequence of nonequilibrium distributions have
insufficient overlap, which is a common problem for complex,
multimodal problems. The fluctuation theorem allows (under generally
non-realistic assumptions; more shortly) a bounding of the estimate no
matter how fast the annealing schedule is. In addition to providing
bounds, the reverse work distribution can be incorporated into
estimators for log Z itself that are (hopefully) an improvement over
the one provided by JE. Experiments are run on various statistical
models.

I found this to be an interesting paper and enjoyed reading it. My
main reservations are concerned with the feasibility of simulating
the reverse process in most situations that are
practically-challenging. It appears that the authors do recognize this
limitation, but provide a brief and not overly satisfying discussion
at the end of p.4. Most of the experimental results focus on problems
with a simple low-energy structure (the ferromagnetic Ising and Potts
models, and mixture models with 3 components) The RBM example is a bit
more intriguing, but when trained on MNIST, one can reasonably infer
the existence of a relatively small number of modes. In general, the
method assumes access to an oracle providing a perfectly-trained
model, where the data points do indeed represent the modes of the
distribution (1000 sweeps of Gibbs sampling at low temperature will
move only locally and cannot be expected to move the state too far
from the training points.) While the agreement of the current method
and PT log Z results in this case is encouraging, the fit of the model
is generally difficult to verify in practice- indeed that's the
reason for conducting the simulation in the first place.

To phrase the objections in another way, optimization is generally a
very different problem from sampling, as represented by the
distinction between problems in NP and those in #P. There exist
several interesting types of problems for which computing the ground
state energy is tractable, but counting the number of such minima (obtaining the
ground state entropy) is not. Generating samples uniformly from the
ground state distribution, as would be required for the reverse
methodologies, will then be problematic.

Another issue was that on first reading, it was not immediately clear
what the paper's contributions are; presumably, it is the combined
forward/reverse weighted estimator presented on page 5? The remaining
ones have been previously introduced in the literature. While valid,
I'm ambivalent as to whether this contribution warrants an entire NIPS
paper. Certainly, the connection between marginal likelihood and free
energy estimation is widely-known among Bayesian computational
statisticians.

Despite these misgivings, I still found the paper to be quite cool and
am encouraged to see progress in this research area.


Technical correctness:
-------------------

The paper appears to be technically correct to my knowledge.

Clarity:
-------

The paper is quite well-written and easy to follow for someone
familiar with Sequential Monte Carlo and Bayesian statistics. The
presentation could be better-adapted to the NIPS context, however, as
physics-flavored notions such as ""work"" may seem distant to a machine
learning audience.

Specific issues:
---------------

252-253: In what sense is simulation of marginal RBM 3x faster than
that of the joint? The joint simulation can be parallelized and
simulated on a GPU due to the conditional independence structure of
the RBM. The statement should be clarified."
Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma,"Zhuoran Yang, Krishnakumar Balasubramanian, Zhaoran Wang, Han Liu",https://proceedings.neurips.cc/paper/2017/hash/4db0f8b0fc895da263fd77fc8aecabe4-Abstract.html,"Summary: This article studies the estimation problem of single index and multiple index models in high dimensional setting. Using the formulation of second order Stein's lemma with sparsity assumptions, they propose an estimator formulated as a solution to a sparsity constrained semi-definite programming. The statistical rate of convergence of the estimator is derived and a numerical example is given.

Overall, I find the article interesting and useful, but their emphasis on ""heavy tail"" case and also partly non-Gaussianity is rather intriguing. Looking at the details, the main contribution the authors claim seems to be related to an extension of an optimality result for the estimation of sparse PCA from sub-Gaussian case to essentially sub-Exponential case through a Bernstein inequality. It would be helpful to clarify this point better.

Detailed comments are summarized below.

(a) Stein's lemma: I think one of the first references to an estimator based on Stein's lemma is H\""ardle and Stoker (1989), using the so-called ""method of average derivatives"", which does not require Gaussianity. 
(b) Derivative condition: Normally the condition on the second derivative would be stronger than that on the first derivative, yet, the article suggests the other way around. Is there any other place to pay the price, or does it really provide more general framework?
(c) Heavy tail case: The notation of ""heavy-tail"" (as a pareto-like tail to most readers, I suspect, e.g. Barthe et al., 2005) is confusing and misleading in this context. Especially, I would think the heavy tail problem in economics has more to do with extremes and I doubt that the new approach is applicable in that context. Hence, I would suggest to remove the reference to the heavy-tail part, and instead make a better link to the contributions in terms of a relaxation of the tail conditions. 
(d) Moment condition: In fact, I wonder how the moment condition on the score function could be translated in terms of the standard moment conditions (and tail conditions) on the original random variables. It would be helpful to demonstrate this point, and perhaps give a condition to make those assumptions comparable.  
(e) Relation to Sparse PCA: Given the proximity to the formulation of the estimator to sparse PCA, I was imagining that a similar technique of the proofs from sparse PCA would have been used here, (except utilizing another (Berstein-type) concentration inequality), however, the relation to sparse PCA was presented as a consequence of their results. Then, in addition to the similarity to sparse PCA, could you also demonstrate a fundamental difference in techniques/considerations, if any, (other than the moment condition, or to overcome the moment condition?) required to deal with estimation problems of single index and multiple index models?  
(f) Numerical algorithm: The algorithm to compute the estimator is mentioned only in Section 5. I would suggest to include some computational details in Section 3 as well, perhaps after (3.6).
(g) Minimum eigenvalue condition: I suppose without sparsity assumption, the minimum eigenvalue (line 174, p.5) should be zero for high dimensional case when d > k. Then, instead of ""Finally"" to introduce this condition, which seems to suggest that they are unrelated, would the connection need to be emphasized here as well?

Minor comments and typos:
(i) line 54, p2: (... pioneered by Ker-Chau Li, ...): From the flow of the arguments, it would be natural to add the references [20,21,22] here. 
(ii) line 75, p2: I understood ""SDP"" as ""semi-definite programming"" in the end, but it would be better to be clear from the beginning here.
(iii) equation (4.1) p.4: typo.
(iv) heck references: the style of references is not coherent.


References:
Barthe, F., Cattiaux, P. and Roberto, C. (2005) Concentration for independent random variables with heavy tails, Applied Mathematics Research Express (2). 39-60.
H\""ardle, W. and Stoker, T.M. (1989) Investigating smooth multiple regression by the method of average derivatives, JASA 84 (408), 986-995.
","This paper considers the estimation of semi-parametric sparse models, with possibly non-Gaussian (but random) covariates. This can be applied to non-linear sparse inverse problems with non-Gaussian sensing operators, which seems of particular interest in several compressed sensing problems. The new estimator is defined as the solution of a convex problem, and statistical convergence rates are proven, based on the second-order Stein identity.

I must say that the paper is far from my research area, so I could not get too much into details. But it tackles an important issue, and it seems to me that the paper proposes a novel and valuable contribution, while being well written and well presented.
","This paper studies estimation of sparse single and multiple index models.  The goal is to construct good estimators under weaker assumptions on the covariate distribution and link function.

This paper is poorly written.  In the introduction the author(s) emphasized on the generality of their methods, such as how it would work for a wide range of covariate distributions and link functions.  But in the main methodology section (Section 3), the development focused only on covariate distributions with independent, identically distributed components, which is far more restrictive than almost any existing works.  Equation 3.2 only holds when the link function is phase retrieval.  So the entire section 3 is restricted to a highly restrictive covariate distribution and a single link function.  It is entirely unclear how the method works in other scenarios.

In the introduction it is mentioned repeatedly that the proposed method needs to assume that the covariate distribution is known.  It is unclear how this assumption is used in the methodological and theoretical developments.  But this is a very strong assumption. For example, in high dimensional linear regression, if the covariate distribution is known, then the problem becomes almost trivial.

The author(s) also claimed to make a contribution to heavy-tailed sparse PCA, which is over-stating.  The only contribution in Section 4 is the use of a robust estimator of the input matrix $\bar\Sigma$.  The robust estimator itself is not new.  The SPCA formulation in eqs. (3.7) and (4.1) is standard and well-known (d'Aspremont et al SIAM Review 2007, Vu et al NIPS 2013), which the authors were aware of but did not cite in Sections 3 and 4.  Given the existing results on sparse PCA analysis and truncated average estimate, the consistency of heavy-tailed sparse PCA is trivial: the input matrix is entry-wise consistent and hence $\ell_1$ penalized sparse PCA is consistent.

Typos:
- line 44: ""statistical optimal"" --> ""statistically optimal""
- line 52: ""is rather restrictive"" --> ""are rather restrictive""
- Equation 2.1: Remove the second ""="".
- First line of Eq. 3.3 (also eqs 3.7 and 4.1): ""+"" --> ""-"", unless you use a negative value of $\lambda$.


####### Response after author feedback ########
Thanks for the feedback.  Yes. I agree that there was some misunderstanding in my first review.  But still section 4.1 is simply a special case of the analysis of sparse PCA which only assumes that the input matrix is entry-wise consistent.  This point has been illustrated, for example, in the perspective of robust optimization in d'Aspremont et al and later developments (for example, the proof used in Wang et al arXiv:1307.0164)."
Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data,"Stéphanie ALLASSONNIERE, Juliette Chevallier, Stephane Oudard",https://proceedings.neurips.cc/paper/2017/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html,"This paper describes a mixed modelling approach on longitudinal data embedded in Riemannian manifolds (as opposed to general Euclidean space). The work seems to be an extension of Schiratti et al. (2015), which first introduced this modeling framework. While the time-trajectories in the original work were limited to be geodesics (increasing or decreasing curves in the one-dimensional case), the authors of this manuscript propose to add a breakpoint model that allows for the dynamics to change. They apply this model to the problem of analysing tumour growth data from renal metastatic cancer patients.

I will be upfront and say that while I have some knowledge of linear mixed models for longitudinal data in Euclidean spaces, I know next to nothing about working on a manifold. Perhaps for that reason, it was not entirely clear to me what the advantage is. The authors state that ""anatomical data are naturally modelled as points on Riemannian manifolds"", but why is this? One weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.

If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.

(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).

Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.

Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","General comments:

      This paper describes a mixed effects model of trajectories built with piecewise logistic trajectories and the more general framework within which this model falls.  The paper is very interesting and the strengths include a detailed description of a general framework for piecewise-geodesic curve models, a specific model that is fit well for the application selected and a model that includes the covariance of model parameters along with an associated analysis.  The weaknesses of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail.  This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices.  In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times.  Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful.  Lastly, the details of the experiment are lacking.  In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.

      Specific comments:

      - l132: Consider introducing the aspects of the specific model that are specific to this example model.  For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
      - l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation?  If it is correct, please define t_R^m.  It is used subsequently and it's meaning is unclear.
      - l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
      - Throughout, the authors use the term constrains and should change to constraints.
      - l124: What is meant by the (*)?
      - l134: Do the authors mean m=2?
      - l148: known, instead of know
      - l156: please define \gamma_0^{***}
      - Figure 1: Please specify the meaning of the colors in the caption as well as the text.
      - l280: ""Then we made it explicit"" instead of ""Then we have explicit it""","The paper proposes a mixed effects dynamical model which describes individual trajectories as being generated from transformations of an average behaviour. The work is motivated by modelling of cancer treatment, but the proposed method is more generally applicable.

The paper is well-written and -structured, and it considers an interesting application. I have limited knowledge of Riemannian geometry, but the methodology was not hard to follow and appears sound (to the extent that I can judge); that said, I cannot comment on the novelty of the approach. I generally found the presentation to be clear (despite some inconsistencies in the notation). The evaluation is fairly comprehensive and explained well in the text, and the model seems to perform well, especially with regards to capturing a variety of behaviours. It is interesting that the error does not monotonically increase as the sample size increases, but I suppose its variation is not that great anyway.

Overall, I believe this is a strong paper that is worthy of acceptance. Most of my comments have to do with minor language or notation issues and are given below. I only have two ""major"" observations. The first concerns Section 2.2, where the notation changes from the previous section. Specifically, the time warping transformation (previously \psi) is now denoted with \phi, while the space warping (previously \phi) is now strangely \phi(1) (line 154). This is pretty confusing, although presumably an oversight. Secondly, in Figure 1 and its description in Section 4.1, there is no explanation of what the vertical axis represents - presumably the number of cancerous cells, or something similar?

Other than that, I think it would be useful to slightly expand the explanation in a couple of places. The first of these is the introduction of the transformations (Section 2.1), where I personally would have welcomed a more intuitive explanation of the time and space warping (although Figure 1b helps with that some). The second place is Section 2.2: here, various decisions are said to be appropriate for the target application (eg lines 140, 155, 166), but without explaining why that is; some details of how these quantities and decisions correspond to the application might be useful. Some of the terminology could also be better explained, especially relating to the treatment, e.g. ""escaping"", ""rupture"". Finally, to better place the work in context, the paper could perhaps include a more concrete description of what the proposed model allows (for example, in this particular application) that could not be done with previous approaches (Section 4.3 mentions this very briefly). 

Detailed comments:
-----------------
l21: Actually -> Currently?
l22: I don't think ""topical"" is the appropriate word here; perhaps ""contested""?
l23: [Mathematics] have -> has
l24: [their] efficiency -> its
l29: the profile are fitting -> the profiles are fitted?
l36: [amounts] to estimate -> to estimating
l45: in the aim to grant -> with the aim of granting?
l50: temporary [efficient] -> temporarily
l54: little [assumptions] -> few
l54: to deal with -> (to be) dealt with
l55: his -> its
l56: i.e -> i.e.
Equation between lines 81-82: t_R^m in the subscript in the final term should be t_R^{m-1} (similarly in Equation (*))
l91: I count 2m-2 constraints, not 2m+1, but may be mistaken
l93: constrain -> constraint (also repeated a couple of times further down)
l106: ""we decree the less constrains possible"": I am not sure of the meaning here; perhaps ""we impose/require the fewest constraints possible""?
l118: in the fraction, I think t_{R,i}^{l-1} should be t_R^{l-1} (based on the constraints above)
l131: used to chemotherapy monitoring -> either ""used for/in chemotherapy monitoring"" or ""used to [do something] in chemotherapy monitoring""
l134: n = 2 -> m = 2 (if I understand right)
l139: especially -> effectively / essentially (also further down)
l154: \phi_i^l (1), as mentioned earlier in the comments
l156: *** -> fin?
l160: I think (but may be wrong) that the composition should be the other way around, i.e. \gamma_i^1 = \gamma_0^1 \circ \phi_i^1, if \phi is the time warping here
l167: [interested] to -> in
l177: independents -> independent
l183-185: is it not the case that, given z and \theta, the measurements are (conditionally) independent?
l241: the table 1 -> Table 1
Figure 2: there is a thick dark green line with no observations - is that intentional?
Table 1: I found the caption a little hard to understand, but I am not sure what to suggest as an improvement
l245: the biggest [...], the best -> the bigger [...], the better
l257: Symetric -> Symmetric
l260: is 14, 50 intended to be 14.50?
l260: Figure 3a, -> no comma
l264: early on [the progression] -> early in / early on in
l264: to consider with quite reserve -> to be considered with some reserve
Figure 3: (last line of caption) large scale -> large range?
l280: ""we have explicit it"": I'm not sure what you mean here
l284: Last -> Lastly"
SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks,"Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo, Wonyong Sung",https://proceedings.neurips.cc/paper/2017/hash/4e2a6330465c8ffcaa696a5a16639176-Abstract.html,"This paper proposes an efficient way to approximate the softmax computation for large vocabulary applications. The idea is to decompose the output matrix with singular value decomposition. Then, by selecting the most important singular values you select the most probable words and also compute the partition function for a limited amount of words. These are supposed to contribute for the most part of the sum. For the remaining words, their contributions to the partition function is only approximated. 

The idea is nice and the experimental results show a significant inference speed up, along with similar performance. 

My main concern is that sometimes the paper is poorly written. Just one example: the authors use the term ""logit"" to denote the exponential of a dot product. This is not correct. The logit function is the inverse of the sigmoidal ""logistic"" function. This choice of term is therefore incorrect and misleading. 
More examples are given below. However the idea is both simple and nice. 


- Page 2, line 73: Moreover, self normalized models requires a huge training time.


- Page 3, line 93: It is not ""approximately"". It's the same. The softmax function outputs the probability distribution over the whole vocabulary. 

- Line 102: reword this sentence 

- Line 114: for obtaining accurate results -> to obtain accurate results

- Exactly instead of correctly in line 117


Page 5: Why you didn't use the full softmax on the wiki data ? The vocabulary is small and the computational cost is really cheap!


Page 6: The script to compute the BLEU score is included in the Moses released, but it's not the Moses toolkit. However, which one did you use ?

To have a stronger baseline you could use bpe to decompose words. Moreover, it could be more meaningful to have a larger vocabulary. For instance if you translate from English to German, the output vocabulary is really larger, even if you use bpe. I understand the goal is to show that your svd softmax performs as well as the full softmax with a huge gain in time. However, this gain of time could benefit to the NMT system by allowing a longer training in a state of the art setup with the svd softmax. 




","This paper presents a simple approach to speed-up the softmax computation for large vocabulary neural networks. The perform SVD on the output embedding matrix.

At inference time, they first compute N-top candidates (preview logits) based only on W dimensions where W < embedding dimension, and then compute a softmax over the entire vocabulary but computing the  actual values of the logits corresponding to the top N candidates. This reduces the complexity from O(VD) to O(VW + ND). 

They evaluate their approach using three metrics
1. KL divergence between the approximate and actual softmax
2. Perplexity
3. top k coverage since that is important for beam search

Their results show that on language modeling tasks and the 1 billion word benchmark, their approach seems to perform comparable with using full softmax with a reduction of 70% complexity on the GPU. 

This is a simple approach and it seems to work. It would have been good to see if it stands up when tried on SOTA systems for MT and language modeling.
","This paper addresses the question how to efficiently EVALUATE the softmax at the output layer of an RNN/LSTM.  The underlying idea is to use an SVD decomposition of the output layer.

The authors first present results for LM on the One billion word benchmark.  Their goal is not to measure perplexity, but to be able to know the top K LM probabilities (with correct normalization).  I'm not impressed by the results in Table 2. In the best case, the proposed approach is about 6x faster than  the full softmax over the 800k vocabulary. This still corresponds to an 130k softmax which is quite expensive. There are alternative approaches like byte pair encoding (which is very successfully used in NMT), and which use vocabularies in the range of 20-30k. 

Also, the authors clearly state that their focus is on fast top K evaluation, but they still need to train the model. I consider that training a full 800k softmax is prohibitive ! The authors admit themselves that they had to use half-precision parameters to fit the model into GPU memory.  Of course, one could use NCE or similar techniques during training, but the authors should then show that their method is compatible with NCE training.  In my opinion, an efficient softmax speed-up technique should consider training and evaluation !

The authors then provide results for NMT: the proposed method has no impact on the BLEU score.

Finally, it's nice that the authors have an optimized implementation on GPU. Do they plan to make it freely available ? Overall, they achieve a 3x speed improvement on GPU in comparison to a full 260k vocabulary.  As stated above, training such huge vocabularies is prohibitive.

In my opinion, the real question is whether the proposed method can be used with (BPE) vocabularies of 20-64k and still give a measured 3-5x improvement in evaluation mode on GPU.  That would be a much better message.

I also would like to see comparative results (performance and speed) with respect to other softmax speed-up techniques. You need to show that your method is better than other speed-up techniques, not in comparison to a huge softmax (which is never used in practice ...)
"
Concentration of Multilinear Functions of the Ising Model with Applications to Network Data,"Constantinos Daskalakis, Nishanth Dikkala, Gautam Kamath",https://proceedings.neurips.cc/paper/2017/hash/4e732ced3463d06de0ca9a15b6153677-Abstract.html,"This paper discusses concentration of bilinear functions of Ising systems and shows concentration of optimal up to logarithms order. I have read thoroughly the appendix, and the main results appear to be correct -- and I find them interesting. My main concerns with the paper are:

(i) The writing: the paper refers to the supplementary material too much to my liking. For example sections 3.2.x for x in [2,...,5], do not read well, since the details provided are insufficient or vague. Part of the applications section also refers to the supplement multiple times.

(ii) Although the datasets used in the application are interesting I think the analysis does not relate well to the theoretical study. First, using MPLE may be unstable and inconsistent in such high-dimensional data (n = 1600) which would render the second step -- running an MCMC based on those parameters useless. Note that while [BM16] prove the consistency of the MPLE, that typically requires the dimension to be fixed, and much smaller than the sample size. Second, why the fact that the bilinear functions concentrate almost sub-exponentially justify their use as test statistics -- as one can use any type of test statistic with the same idea?","Summary:
 
  The paper considers concentration of bilinear functions of samples from an ferromagnetic Ising model under high temperature (satisyfing Dobrushin's condition which implies fast mixing). The authors show that for the case when there is no external field, that any bilinear function with bounded coefficients concentrate around a radius of O(n) around the mean where n is the number of nodes in the Ising Model. The paper also shows that for Ising models with external fields, bilinear functions of mean shifted variables again obey such concentration properties.

   The key technical contribution is adapting the theory of exhangeable pairs from [Cha05] which has been applied for linear functions of the Ising model and apply it to bilinear functions. Naive applications give concentration radius of n^{1.5} which the authors improve upon. The results are also optimal due to the lower bound for the worst case.

Strengths:

 a) The key strength is the proof technique - the authors show that the usual strategy of creating an exchangeable pair from a sample X from the Ising model and X' which is one step of glauber dynamics is not sufficient. So they restrict glauber dynamics to a desirable part of the configuration space and show that the same exchangeable trick can be done on this censored glauber dynamics. They also show that this glauber dynamics mixes fast using path coupling results and also by a coupling argument by chaining two glauber Markov Chains tightly. This tightly chained censored glauber dynamics really satisfies all conditions of the exchangeable pairs argument needed. The authors show this carefully by several Lemmas in the appendix. I quite enjoyed reading the paper.

b) I have read the proofs in the appendix. Barring some typos that can be fixed, the proof is correct to the best of my knowledge and the technique is the major contribution of the paper.

c) The authors apply this to test if samples from a specific distribution on social networks is from an Ising model in the high temperature regime. Both synthetic and real world experiments are provided.

Weaknesses:

  Quite minor weaknesses:
    a) Page 1 , appendix : Section 2, point 2 Should it not be f(X)- E[f(X)] ??
    b) Why is Lemma 1 stated ? It does not help much in understanding Thm 1 quoted. A few lines connecting them could help.
    c) Definition 1, I think It should be intersection and not union ?? Because proof of lemma 5 has a union of bad events which should be the negation of intersection of good events we want in Defn 1. 
    d) Typo in the last equation in Claim 3. 
    e) Lemma 9 statement. Should it not be (1-C/n)^t ??
    f) In proof of Lemma 10, authors want to say that due to fast mixing, total variation distance is bounded by some expression. I think its a bit unclear what the parameter eta is ?? It appears for the first time. It will be great if the authors can explain that step better .
    g) Regarding the authors experiments with Last.fm - Whats the practical use case of testing if samples came from an Ising model or not ? Actually the theoretical parts of this paper are quite strong. But I just wanted to hear author's thoughts.
    h) Even for linear functions of uniform distribution on the boolean hypercube, at a different radius there are some anti concentration theorems like Littlewood Offord. Recently they have been generalized to bilinear forms (by Van Vu , terry tao with other co authors). I am wondering if something could be said about anti concentration theorems for Bilinear forms on Ising Models ?? "
OnACID: Online Analysis of Calcium Imaging Data in Real Time,"Andrea Giovannucci, Johannes Friedrich, Matt Kaufman, Anne Churchland, Dmitri Chklovskii, Liam Paninski, Eftychios A. Pnevmatikakis",https://proceedings.neurips.cc/paper/2017/hash/4edaa105d5f53590338791951e38c3ad-Abstract.html,"This paper proposes an online framework for analyzing calcium imaging data. This framework is built upon the popular and now widely used constrained non-negative matrix factorization (CNMF) method for cell segmentation and calcium time-series analysis (Pnevmatikakis, et al., 2016). While the existing CNMF approach is now being used by many labs across the country, I’ve talked to many neuroscientists that complain that this method cannot be applied to large datasets and thus its application has been limited. This work extends this method to a real-time decoding setting, making it an extremely useful contribution for the neuroscience community.

The paper is well written and the results are compelling. My only concern is that the paper appears to combine multiple existing methods to achieve their result. Nonetheless, I think the performance evaluation is solid and an online extension of CNMF for calcium image data analysis and will likely be useful to the neuroscience community.

Major comments:
- There are many steps in the described method that rely on specific thresholds or parameters. It would be useful to understand the sensitivity of the method to these different hyperparameters. In particular, how does the threshold used to include new components affect performance?
- Lines 40-50: This paragraph seems to wander from the point that you nicely set up before this paragraph (and continue afterwards). Not sure why you’re bringing up closed loop, except for saying that by doing this in real time you can do closed-loop experiments. I’m not sure the idea that you want to communicate here. 
- The performance evaluations do not really address the use of isotonic regression for deconvolution. In Figure 3C, the traces appear to be demixed calcium after segmentation and not the deconvolved spike trains. Many of the other comparisons focus on cell segmentation. Please comment on the use of deconvolution and how it possibly might help in cell segmentation.
- The results show that the online method outperforms the original CNMF method. Can the authors comment on where the two differ? 

Minor comments:
Figure 1: The contours cannot be seen in B. Perhaps a white background in B (and later in the results) can help to see the components and contours. C is hard to see and interpret initially as the traces overlap significantly.
","The authors present an online analysis pipeline for analyzing calcium imaging data, including motion artifact removal, source extraction, and activity denoising and deconvolution. The authors apply their technique to two 2-photon calcium imaging datasets in mouse.

The presented work is of high quality. The writing is clear and does a great job of explaining the method as well as how it relates to previous work. The results are compelling, and the authors compare to a number of benchmarks, including human annotation.

I encourage the authors to release source code for their analysis pipeline (I did not see a reference to the source code in the paper).

Minor comments
-----------------
- I was confused by the Methods section on motion correction. A few different methods are proposed: which is the one actually used in the paper? Are the others proposed as possible extensions/alternatives?
- Unless I am mistaken, the claim that ""OnACID is faster than real time on average"" depends on the details of the dataset, namely the spatial and temporal resolution of the raw data? Perhaps this can be clarified in the text.
- fig 3E: change the y-labels from ""Time [ms]"" to ""Time per frame [ms]"" (again, this depends on the spatial resolution of the frame?)
- line 137: are the units for T_b in frames or seconds?","This paper describes a framework for online motion correction and signal extraction for calcium imaging data. This work combines ideas from several previous studies, and introduces a few innovations as well.

The most pressing concern here is that there isn't a core new idea beyond the CMNF and OASIS algorithms. What is new here is the method for adding a new component, the initialization method, and the manually labeled datasets. This is likely quite a useful tool for biologists but the existence of a mathematical, computational or scientific advance is not so clear.

Additionally, it's not so clear why we should trust the manual annotations. Ideally neurons would be identified by an additional unambiguous label (mcherry, DAPI, etc.). This, together with the relative similarity of CMNF and the current method call into question how much of an advance has been made here.

The in vivo datasets need to be more clearly described. For example, Which calcium indicator was used?

What happens if and when the focal plane drifts slowly along the z axis?

It's also frequently mentioned that this is an online algorithm, but it's not clear what sort of latency can actually be achieved with the current implementation, or how it could be linked up with existing microscope software. If this is only possible in principle, the paper should stand better on its conceptual or computational merits.

The spatial profiles extracted for individual neurons should be also shown."
Action Centered Contextual Bandits,"Kristjan Greenewald, Ambuj Tewari, Susan Murphy, Predag Klasnja",https://proceedings.neurips.cc/paper/2017/hash/4fa177df22864518b2d7818d4db5db2d-Abstract.html,"
This paper considers the problem of contextual bandits where the
dependency for one action maybe arbitrarily complex, but the relative
expected rewards of other actions might be simple. This is an
attractive framework for problems with ""default"" actions, such those
considered by the authors. Hence, I recommend acceptance even though
the model is very simple and is a nearly direct application of
existing results. However, the authors should clarify the links with the restless bandit literature (e.g. [1]) and the non-linear bandit literature (e.g. [2]). The hypothesis to be tested is whether or not the algorithm is better when the modelling assumptions are satisfied (the conjecture is that it is). It would also be nice to see some results where the assumptions are not satisfied: how does the algorithm perform then?

That said, there are quite a few typos and confusing notation in the paper. It should be cleaned up. 

*** Section 2

l.89-90: 
(a) At one point you have $r_t(a_t)$ to be the reward, but shouldn't this depend on $s$?
(b) Immediately afterwards, you write $r_t(\bar{s}_t, a_t)$ for the expected reward, but there is no expectation symbol. 
(c) You refer to $s$ as both a feature vector and a state vector. 

l. 90: $\theta$ is not explained.

l.98-99: you introduce $\tilde{s}$, but I suppose it should be $\bar{s}$. 

l. 106-17, 112-113: I assume $\theta$ is some kind of parameter vector, please explain.

*** Section 3

l. 162-176: Before $\bar{a}^*_t$ referred to the optimal action given that we are not playing action $0$. Please clarify what you mean by $\bar{a}$ and $\bar{a}_t$

*** References

[1] https://arxiv.org/pdf/1107.4042.pdf
[2] Bubeck and Cesa-Bianchi, ""Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems"", Section 6.2.

*** Post-author review

I have read the authors' response and I agree, yet a comparison with those baselines would be good. In addition, seeing what happens when the assumptions are not satisfied would be nice.","In the context of multi-armed bandit problems for mobile health, modelling correctly the linearity or non-linearity of the rewards is crucial.
The paper tries to take advantage of the fact that a general ""Do nothing"" action is generally available, that corresponds to the baseline. The idea is that, while the reward for each action may not be easily captured by a linear model, it is reasonable to assume that the relative error between playing an action and playing the ""Do thing"" action follows a linear model. 
The article revisits bandit strategies in this context.

Comments:
1) Line 29: See also the algorithm OFUL, and more generally Kernel-UCB, GP-UCB, to cite a few other algorithms with provable regret minimization guarantees.

2) Line 35, 51, and so on: I humbly suggest you replace ""mhealth"" with something else, e.g. ""M-health"", since ""mhealth"" looks like a typo.

3) Question about the model: is \bar {f}_t assumed to be known? If not and if it is completely arbitrary, it seems counter-intuitive that one can prove any learning guarantee, given that a bandit setting is considered (only one action per step), and \bar {f}_t may change at any time to create maximal confusion.

4) Line 132: ""might different for different""

5) Line 162: What is \bar{a}_t ? I can only see the definition for a_t and \bar{a}_t^\star.
Likewise, what is \bar{a} on line 165? It seems these are defined only later in the paper, which is confusing. Please, fix this point.

6) Question: What is the intuition for choosing \pi_t to be (up to projection) P(s_{t,\bar a}^\top \bar \theta \geq 0) ? Especially, please comment on the value 0 here.

7) Lemma 5: Should 0 be replaced with a in the probability bound?

8) Section 9: I understand that you heavily used the proof of Agrawal and Goyal, but I discourage the use of sentences such as ""For any filtration such that ... is true"". I suggest you rather control probability of events.

9) Proof, line 62: there is a missing reference.

.................

In the end the setting of the paper and definition of regret look a bit odd, but the authors provide an interesting idea to overcome the possibly nasty non-stationary term, thanks to randomization. Also deriving the weighted least-squares estimate and TS analysis in this context is interesting, even though relying heavily on previous existing analysis for TS. Finally, the idea to consider that only the relative errors follow a linear model might be generalized to other problems. 

Weak accept."
Cost efficient gradient boosting,"Sven Peter, Ferran Diego, Fred A. Hamprecht, Boaz Nadler",https://proceedings.neurips.cc/paper/2017/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html,"1. Summary of paper

This paper introduced a technique to learn a gradient boosted regression tree ensemble that is sensitive to feature costs and the cost of evaluating splits in the tree. Thus, the paper is similar to the work of Xu et al., 2012. The main differences are the fact that the feature and evaluation costs are input-specific, the evaluation cost depends on the number of tree splits, their optimization approach is different (based on the Taylor expansion around T_{k-1}, as described in the XGBoost paper), and they use best-first growth to grow the trees to a maximum number of splits (instead of a max depth). The authors point out that their setup works either in the case where feature cost dominates or evaluation cost dominates and they show experimental results for these settings.


2. High level subjective

The paper is clearly written. Apart from introducing a significant amount of notation, I find it clear to follow.

The contribution of the paper seems a bit small given the work of XGBoost and GreedyMiser. 

The paper seems to have sufficient experiments comparing the method with prior work and showing how model parameters affect performance. I have one confusion which may warrant an additional experiment, which I describe below.

I think this model could be interesting for practitioners interested in classification in time-constrained settings.


3. High level technical

One thing that is unclear is in Figures 2a and 2b, how was the cost measured for each method to obtain the Precision versus Cost curves? It seems to me that for CEGB cost is measured per-input and per-split, but for the other methods cost is measured per-tree. If this is the case this seems a bit unfair. It seems to me that the cost of each method should be evaluated in exactly the same way in order to compare each method fairly, even if the original papers evaluated cost in a different way. For this reason it's unclear to me how much better CEGB is than the other methods. If the costs are measured differently I think it would be really helpful to modify Figure 2 so that all of the costs are measured the same way. 

Personally I think the paper would improve if Figure 1 was removed. I think it's pretty clear what you mean by breadth-first vs. depth-first vs. best-first. I would replace this figure with one that shows the trees generated by CEGB, GreedyMiser and BudgetPrune for the Yahoo! Learning to Rank dataset. And it shows in detail the features and the costs at each split. Something that gives the reader a bit more insight as to why CEGB is outperforming the other methods.

I think it would also be nice to more clearly emphasize what improvements over GreedyMiser you make to arrive at CEGB. For instance, it would be nice to write out GreedyMiser in the same notation and show the innovations (best-first, not limited-depth, different optimization procedure) that lead to CEGB. Then you could have a figure showing how each innovation improves the Precision vs. Cost curve.

In Section 5.2 it's not completely clear to me why GreedyMiser or BudgetPrune cannot be applied here. Why is the cost of feature responses considered an evaluation cost and not a feature cost?


4. Low level technical

- In equation 7, why is \lambda only in front of the first cost term and not the second?
- In equation 12, should the denominators of the first 3 terms have a term with \lambda added, similar to the XGBoost paper?
- What do the 'levels' refer to in Figure 4b?
- Line 205: I would recommend redefining \beta_m here as readers may forget its definition. Similarly with \alpha on Line 213.


5. Summary of review

While this paper aims to introduce a new method for cost efficient learning that (a) outperforms the state-of-the-art and (b) works in settings where previous methods are unsuitable, it is unclear to me if these results are because (a) the cost is measured differently and (b) because a cost is defined as an evaluation cost instead of a feature cost. Until these confusions are resolved I think the paper is slightly below the acceptance threshold of NIPS.","**summary: The paper proposes a gradient boosting approach (CEGB) for prediction under budget constraints. During training CEGB minimizes empirical loss plus a cost penalty term. Three types of costs 
	  are incorporated: feature cost per example and across all examples as well as evaluation cost. It then looks for the best split greedily that minimizes the second order approximation of the loss objective. Experiments on
	  show improved performance over state of the art methods on two datasets.	  
	  
	  **Significance: The proposed algorithm is closely related to GreedyMiser (Xu et al 2012). The main difference lies in the construction of the weak regressors. The key contribution of CEGB is to allow new splits at 
	  any current leaves rather than in a pre-defined fashion. To alleviate the computation for searching for the best leaf to split in each iteration, second-order approximation of the loss is used. Although the ideas of 
	  best-first tree learning, second-order loss approximation are well explored in the gradient boosting literature, applying them to the cost efficient setting is novel. CEGB is thus a refinement of GreedyMiser. 
	  
	  **Results: CEGB is only compared to the state-of-the-art methods BudgetPrune and GreedyMiser on two datasets: Yahoo and MiniBooNE. In particular, the authors did not include the Forest dataset as used by BudgetPrune (Nan et al 2016).
	  My experience is that gradient boosting based methods perform poorly compared to random forests based methods (BudgetPrune) on the Forest dataset. This paper would be made stronger if more datasets are included to compare different methods.
	  On another note, the results in Figure 2(b) is exceedingly good for CEGB without tree cost. It's quite surprising (and hard to believe) that the precision can rise above 0.14 with average feature cost of ~20. 
	  How many trees are used and what's the average number of nodes in each tree to get such a good performance?
	  
	  **Others: The authors repeatedly claimed that BudgetPrune assumes that feature computation is much more expensive than evaluation cost and therefore irrelevant in several experiments. This is not true. The formulation of of BudgetPrune by Nan et al 2016 
	  does take into account evaluation costs and it can be traded-off with the feature cost.
	  
	  **Overall: I think this paper provides a refinement of GreedyMiser and the experimental results look promising. Adding more datasets and comparisons would make it even stronger. 	  
	  
          **After discussion with the reviewers, I feel that the novelty of this paper is not that much given the work of XGBoost and GreedyMiser. But the experimental results show improvement over the state-of-the-art. The authors promised additional datasets to be included in the final version. So I'm still leaning to accept this paper. ","The authors present a novel algorithm to learn ensembles of deep decision (or regression) trees that optimize both prediction accuracy and feature cost. The resulting prediction system is adaptive in the sense that for different inputs different features sets of features will be acquired/evaluated thus resulting in a reduced cost while mainitaing prediction accuracy.   

The authors approach is an iterative algorithm that extends gradient tree boosting with a cost-sentive objective function. The algorithm iteretavely adds decision trees to the ensemble. Each new candidate tree is trained to approximately optimize the cost of the entire ensemble. To accomplish individual tree learning given the cost-constrains w.r.t the existing ensemble, the authors introduce a cost-aware impurity function.

The authors demonstrate that their algorithm outperforms existing methods in cost-aware ensemble learning on several real datasets.
Overall, I think the paper is well written, the algorithm derivations are interesting and novel, and the problem being addressed is highly relevant to many domains."
Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks,"Surbhi Goel, Adam Klivans",https://proceedings.neurips.cc/paper/2017/hash/50905d7b2216bfeccb5b41016357176b-Abstract.html,"Overview: The paper deals with a class of generalization bounds for kernel-based (potentially improper learning), when the kernel matrix is ""approximately low rank"". More precisely, if the the kernel matrix satisfies a decay of the eigenvalues, one can ""compress"" it by subsampling a small number of the columns, and solving an optimization problem over the smaller subset of columns. This allows the a more efficient concentration bound for proving generalization. Finally, the methods are applied to depth-2 neural networks. 

Comments on decision: while the result is interesting, and I applaud the direction of making assumptions on the data-distribution, which seems crucial for proving interesting result for scenarios in which neural networks work -- I find the current results in the paper too preliminary and essentially, direct in light of prior work. 
In particular, the relationship between compression and generalization is known (e.g. David et al. '16), as are the subsampling results when eigenvalue decay holds. This makes the general kernel learning result a ""stitching"" of prior work. 
The neural network results again follow from approximation results in Goel et al. -- so are not terribly new. (Additionally, it looks like they were written last-minute -- the bounds are only worked out for depth-2 networks. ) ","This paper studies the problem of learning concepts that can be approximated
in a Reproducing Kernel Hilbert Space (RKHS) with bounded norm (say norm at
most B, which will appear in the complexity bounds). 

Previous results have shown sample complexity bounds that are polynomial in B
for learning neural networks, and Shamir had shown a lower bound of Omega(B) 
for various problems. A main contribution of this paper is to is to show that   
when the eigen values of the Gram matrix decay sufficiently fast, the sample
complexity can be sub-linear, and even logarithmic in B. As an application 
they show that for data distributions that satisfy the polynomial decay, better
sample complexity results can be obtained for relu's and sigmoid networks
of depth one. 

The argument is that when the Gram matrix has fast eigen-decay, it has a 
small effective dimension, allowing for better compression, which in their case
simply means that it can be approximated using a few columns. They use a series
of reductions, each explained clearly, to obtain the compression scheme, and 
the complexity guarantees.                                  

- L169: I think the subscripts are mixed-up. 

- Can one say something by directly compressing the Gram-matrix, instead of sampling 
(say using PCA).
","This paper considers the problem of square-loss regression for several function classes computed by neural networks. In contrast to what previous hardness results might suggest, they show that those function classes can be improperly learnt under marginal distribution assumptions alone. Specifically, they use previous structural results of approximate RKHS embedding and assume an eigenvalue decay assumption on the empirical gram matrix (namely, it is approximately low-rank). They then apply a recursive Nystrom sampling method to obtain an approximate and compressed version of the gram matrix, which is then used instead in the learning. The error of the resulting regression is then bounded using known compression generalization bounds, hence obtaining
generalization bounds for kernelized regression using Compression Schemes. These in turn are claimed to be tighter than previously used bounds in terms of the required norm regularization (denoted B) in the RKHS. Under the above assumption, this leads to more efficient learning.

Despite the numerous typos and unclear sentences (see below for specific comments), the paper can be well understood for the most part and I found it quite interesting. It non-trivially combines several known results to obtain very nice conclusions about a still not well understood topic. 

The main technical contribution is obtaining generalization bounds for kernelized regression using Compression Schemes. As the authors comment, it is indeed interesting how compression schemes can be used to improve generalization bounds for other function classes. One such work is [KSW], showing that compression-based generalization bounds similar to those considered here can be used to obtain a computationally efficient algorithm for computing a Bayes consistent 1-NN classifier in general metric space.

To be more certain about my review please reply to the following:

- It is claimed that the proposed algorithm has fully-polynomial time complexity in all the relevant parameters. Unfortunately, this dependence is not transparent and is implicit in the results, in particular on its dependence on B. It is only claimed in Thm. 1 that the runtime is O(log B). Please also discuss why a linear dependence in B is considered not fully polynomial.

- The dependence on B is Section 7 is hidden, making it very hard to follow as how the new compression bound comes into play. The proof in the supplementary doesn't help much either. As this is the main advantage, it should be much more transparent.

Specific comments:
- Line 38: ""fully polynomial-time algorithms are intractable"" is self contradicting
- Line 42: ""... are necessary for efficient learnability"" - I believe you should add ""by gradient based algorithms"". Otherwise, the next sentence in the paper is a contradiction.
- A better connection to the results in [Zhang] is needed.
- Thm 1.: the role of \eps should be introduced, even if informally.
- Line 62: Paragraph is unclear at first reading. Better to rephrase.
- Line 78: missing ""with high probability"" at the end of sentence?
- Paragraph starting at line 79 is highly unclear. E.g. ""compress the sparsity"" makes no sense. ""bit complexity"" is unclear.
- Definition 1: Unclear to what ""with probability 1-\delta"" refers to. Is it to property P or the empirical error bound. Missing comma? What is the property P you actually use in your proofs?
- Thm. 2 has many typos. In addition, missing iid assumption. Is there no assumption on the loss function l? More importantly, in the refereed papers, only one direction of the bound is proved, as opposed to the bidirectional (i.e. having absolute value on the lhs of the bound) you give here.
- Definition 2 has many typos.
- Line 74: ""Let us say"" -> Suppose?; Also eps -> (eps,m)?
- Line 187: ""improve the dependence on sqrt(m)"". Please add reference.
- The sentence beginning in line 292 is highly unclear.
- Many typos all over.
- The bibliography misses many entries (year, journal, etc.).

References:
[KSW] - Kontorovich, Sabato and Weiss. Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions. 2017.
[Zhang] - Tong Zhang. Effective dimension and generalization of kernel learning. 2013.
"
"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events","Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Mr. Prabhat, Chris Pal",https://proceedings.neurips.cc/paper/2017/hash/519c84155964659375821f7ca576f095-Abstract.html,"Climate change is a critically important area of research and modeling of spatially localized climate patterns can be imperative. 
In this work, the authors describe the creation of the ExtremeWeather dataset generated from a 27 year run of the CAM5 model. As configured, the output consists of  a 768x1152 image with 16 channels. 4 extreme weather events have ground truth labeling generated using the TECA framework. ~79K images, about half of which are labeled are generated.  A 3D convolutional encoder decoder architecture was employed.
Only the surface quantities were modeled (3D, with 3rd dimension being time). Experiments were run in 2D (without the temporal aspecct) and 3D 

The connections between climate change and local weather patterns could be made more clear to audiences less familiar with the topic.  The reason for using simulated data versus observations could also be articulated better. Not sure why only the surface level data was used in generating the dataset -one would think that the atmospheric data (30 levels) would have implications for the 4 extreme weather events that were modeled.
In many cases, the choice of architecture/optimal parameters seems to be a little ad-hoc. ","
This paper presents a new dataset, a model and experimental results on this dataset to address the task of extreme weather events detection and localization. The dataset is 27 year weather simulation sampled 8 times per day for 16 channels only the surface atmospheric level. The proposed model is based on 3D convolutional layers with an autoencoder architecture. The technique is semi-supervised, thus training with a loss that combines reconstruction error of the autoencoder and detection and localization from the middle code layer.

In general the paper is very well written and quite clear on most details. The experimental results only use a small part of the data and are a bit preliminary, but still they do show the potential that this data has for future research.

Some comments/concerns/suggestions are the following.

- Since the paper presents a new dataset it is important to include a link to where the data will be available.

- In line 141 it says that only surface quantities are considered without any explanation why. Is it because the simulation would have been significantly more expensive so only surface computed? Is it a decision of the authors to make the dataset size more manageable? Are there any future plans to make available the full 30 atmospheric levels?

- The caption of table 1 says ""Note test statistics are omitted to preserve anonymity of the test data."" I assume this means that the ground truth labels are not being made available with the rest of the data. If this is the case, in the paper it should be explained what is the plan for other researchers to evaluate the techniques they develop. Will there be some web server where results are submitted and the results are returned?

- From what I understand the ground truth labels for the four weather events considered were generated fully automatically. If so, why was only half of the data labeled? I agree that exploring semi-supervised methods is important, but this can be studied by ignoring the labels of part of the data and analyze the behavior of the techniques when they have available more or less labeled samples.

- Is there any plan to extend the data with weather events that cannot be labeled by TECA?

- VGG is mentioned in line 51 without explaining what it is. Maybe add a reference for readers that are not familiar with it.

- In figure 3 the the graphs have very tiny numbers. These seem useless so maybe remove them saving a bit of space to make the images slightly larger.

- Line 172 final full stop missing.

- Be consistent on the use of t-SNE, some places you have it with a dash and other places without a dash.
      ","This paper introduces a climate dataset. For baseline performance, the authors present a multichannel spatiotemporal encoder-decoder CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. The paper presents a  27-year run, from 1979 - 2005, of the CAM5 (Community  Atmospheric Model v5), configured at 25-km spatial resolution, with ground- truth labelling for four extreme weather events: Tropical Depressions (TD) Tropical Cyclones (TC),  Extra-Tropical Cyclones (ETC) and Atmospheric Rivers (AR). At this resolution, each snapshot of the global atmospheric state in the CAM5 model output corresponds to a 768x1152 image, having 16  ’channels’ (including surface temperature, surface pressure, precipitation, zonal wind, meridional,  wind, humidity, cloud fraction, water vapor, etc.).  The goal is a 4 class classification (ETC, TD, TC, AR).

The paper alludes to the climate dataset being similar to a multi-channel video dataset. A greater need for video datasets that can better understand multimodal inputs, understand video relationships and also push the boundaries of deep learning research. Towards this goal, many new datasets such as Kinetics, AVA, ""something something"" have been recently proposed. I feel the related work can be expanded significantly.

How did you obtain this dataset? Is this a public dataset from CAM5 community? You create a much smaller dataset. What prevents you from using a larger dataset? More details about the dataset collection process are necessary. Furthermore, if you are just focussing on the weather/climate community, discussing previous work to handle weather data will be useful (there is some hint in the conclusions).

More importantly, additional simpler baselines such as using CNN, and other feature information such flow that is currently used for object and activity recognition in videos will be helpful.

Overall, I am not convinced from the paper how this dataset pushes for a new class of video dataset. While I agree that climate research is important, it is unclear if this is a niche area/dataset and how improving the results of this dataset will broadly applicable to improving the state of . Finally, additional baselines will also benefit the paper.
"
A Meta-Learning Perspective on Cold-Start Recommendations for Items,"Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, Hugo Larochelle",https://proceedings.neurips.cc/paper/2017/hash/51e6d6e679953c6311757004d8cbbba9-Abstract.html,"This is an interesting and well-written paper but there are some parts that are not well explained, hence my recommendation. These aspects are not clear:
1. I am not sure about the ""meta-learning"" part. The recommendation task is simply formulated as a binary classification task (without using matrix factorization). The relation to meta-learning is not convincing to me.
2. ""it becomes natural to take advantage of deep neural networks (the common approach in meta-learning)"" - this is not a valid claim  - deep learning is not the common approach for meta-learning; please see the papers by Brazdil and also the survey by Vilaltra & Drissi.
3. What is the input to the proposed 2 neural network architectures and what is its dimensionality? This should be clearly described.
4. I don't understand why in the first (liner) model the bias is constant and the weights are adapted and the opposite applies for the second (non-linear) model - the weights are fixed and the biases are adapted. This is an optimisation task and all parameters are important. Have you tried adaption all parameters?
5. The results are not very convincing - a small improvement, that may not be statistically significant.
6. Evaluation
-Have you sampled the same number of positive and negative examples for each user when creating the training and testing data?
-How was the evaluation done for items that were neither liked or disliked by a user? 
-Why are the 3 baselines called ""industrial""? Are they the typically used baselines?
-Is your method able to generate recommendations to all users or only for some? Is it able to recommend all new items? In other words, what is the coverage of your method?
-It will be useful to compare you method with a a pure content-based recommender. Is any of the beselines purely content-based?
7. Discuss the ""black-box"" aspect of using neural networks for making recommendations (lack of interpretability)

These issues need to be addressed and explained during the rebuttal.
","This paper aims to address the item cold-start problem in recommender systems. The cold-start problem is a long-standing research topic in recommender systems. There is no innovation in the problem itself but the idea of using meta-learning to address this problem is novel to some degree. Two models, one shallow and one deep, are proposed for the task.

Regarding the design of the method, I have a concern about the user part. Although the authors argue that one advantage of using meta-learning here is that we can explore a variety of ways to combine user and item information, they seem to lose some user information by an aggregating function.

There are some missing related works for the item cold-start problem and tweet recommendation. Other than content filtering, there is another line of research for the cold-start problem which try to use active learning ideas to address this problem. The authors should be aware of them and provide a discussion in the related work section. Regarding tweet recommendation, there are also some previous works which treat it as a binary prediction problem or ranking problem and exploit different features (e.g. content). I think the factorization machine variants proposed in [Hong et al. Co-factorization machines: modeling user interests and predicting individual decisions in twitter. In WSDM, 2013] should be employed as a baseline for this work (for fairness, you can implement them based on the same features, i.e. user, item and item contents). It would be interesting to see if the method proposed here can beat them, considering they do not lose user information and some of them use ranking losses rather than point-wise losses.

There are some typos, e.g. line 97 and line 101.
","This paper proposes a cold-start recommendation system to recommend tweets to users. The approach is based on multi-task learning, where each user is a different task, and layers of a neural network are shared across users. Two alternatives are considered: one in which the bias terms are fixed across users, but the other weights can change; and one in which the bias term can change, and the other weights are fixed.

The paper is fairly well-written and the approach appears sound. While there are some similar approaches (as described in the related work), the proposed variant appears novel, and large-scale experiments comparing against production algorithms suggest that the method improves accuracy.

It is understandably difficult to give sufficient details of the existing production system, but without these details, it is hard to understand fully why the proposed approach works better. Please consider a more thorough error analysis to understand when and why the approach outperforms the baselines. Is it indeed performing better under ""colder"" conditions? Can any qualitative insight be provided by analyzing the learned user embeddings? What are the various hyperparameters tuned in Sec 4.1 and how sensitive is the method to this assignment?

In summary, this appears to be a reasonable, if somewhat incremental, contribution to the recommendation community. To be a stronger contribution, it should provide insight as to when and why the approach works better than baselines.

"
Learning Unknown Markov Decision Processes: A Thompson Sampling Approach,"Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, Rahul Jain",https://proceedings.neurips.cc/paper/2017/hash/51ef186e18dc00c2d31982567235c559-Abstract.html,"The paper addresses the exploration/exploitation problem of MDPs, and offers theoretical bounds on regret in infinite-horizon weakly communicating MDPs when using Thompson Sampling to generate policies with carefully controlled trajectory lengths.  The bounds are asymptotically the same as the best bound for regret with any approach on weakly communicating MDPs.  There are some experiments showing competitive or improved performance with other approaches.

The paper is well-written and clear, though necessarily theoretically dense.  I also did not find any errors.

In terms of impact, it seems to be highly relevant to a small audience, though as an RL person who does not regularly think about regret, this is my least confident part of the review.

The experiments are small and unambitious, but appropriate for the theoretical nature of the paper.","
The authors present a reinforcement learning algorithm based on posterior sampling and show that it achieves an expected Bayesian regret bound that matches the best existing bound for weakly communicating MDPs.

The paper is easy to read and seems technically correct, although I did not check all the proofs.

My main concern is with respect to the main result, which is the regret bound. The scaling O(HS sqrt{AT}) for the (arguably weaker) expected regret is similar to the best existing bounds for the worst-case total regret. This does not explain why Thompson sampling is superior to other optimistic algorithms -- as illustrated in the empirical examples. In particular, the advantage of the proposed policy-update schedule is unclear.
","The paper proposes TSDE, a posterior sampling algorithm for RL in the average reward infinite horizon setting. This algorithm uses dynamic episodes but unlike Lazy-PSRL avoids technical issues by not only terminating an episode when an observation count doubled but also terminating episodes when they become too long. This ensures that the episode length cannot grow faster than linear and ultimately a Bayesian regret bound of O(HS(AT)^.5) is shown.

Posterior sampling methods typically outperform UCB-type algorithms and therefore a posterior sampling algorithm for non-episodic RL with rigorous regret bounds is desirable. This paper proposes such an algorithm, which is of high interest. The paper is well written and overall easy to follow. I have verified all proofs but Theorem 2 and found no mistakes. The presentation is rigorous and the main insights are discussed sufficiently. This paper is therefore a good contribution to the NIPS community.

A comparison against UCRL2 with empirically tightened confidence intervals would have been interesting. An more extended comparison of TSDE for MDPs with increasing size would have also been interesting to empirically determine the dependency of the regret on the problem quantities. This would have given insights on whether we can expect to prove a tighter regret bound (e.g. with \sqrt(S) dependency) without changing the algorithm. Nonetheless, all of this is somewhat out of the scope of this paper. 

Minor detail: The condition on the bottom of page 4 should hold for all s."
Deep Hyperspherical Learning,"Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, Le Song",https://proceedings.neurips.cc/paper/2017/hash/5227b6aaf294f5f027273aebf16015f2-Abstract.html,"This paper presents a very neat idea of using a unique operator to replace the original conv filter operation, which is matrix inner product. Instead of computing similarity between filter weights and pixel values (or activation values if intermediate layers), they compute the angle between them in the hypersphere space. They also defined three types of operators all as functions of the angle: linear, cosine, and sigmoid. To better suit such an operation they also redefined the regularization term and loss function. Ablation study shows the effectiveness of each addition introduced, and better results are generated for image classification task in CIFAR 10 and CIFAR 100 and image feature embedding task. Improvements are seen in terms of both higher accuracy and easier and faster convergence.

The paper is theoretically sound, clearly structured, experiments well organized, and the writing and presentation is elegant.  

The drawback, though, is that the author didn't extend experiments to larger and more realistic image classification tasks such as ImageNet, and gave no explanation or hint of future work on it. It can also be worthwhile to test with more diverse tasks such as certain small scale reinforcement learning problems.
 ","This paper presents a novel architecture, SphereNet, which replaces the traditional dot product with geodesic distance as the convolution operators and fully-connected layers. SphereNet also regularizes the weights for softmax to be norm 1 for angular softmax. The results show that SphereNet can achieve superior performance in terms of accuracy and convergence rate as well as mitigating the vanishing/exploding gradients in deep networks.

Novelty: Replacing dot product similarity with angular similarity has widely existed in the deep learning literature. With that being said, most works focus on using angular similarity for Softmax or loss functions. This paper introduces spherical operation for convolution, which is novel in the literature as far as I know.

Significance: The spherical operations the paper introduces achieve faster convergence rate and better accuracy performance. It also mitigates the vanishing/exploding gradients brought by dot products, which is a long standing problem. It will be interesting to lots of people in ML community.

Improvement: I have several suggestions and questions as listed below:
- For angular Softmax, the bias is removed from the equation. However, the existence of bias is very important for calibrating the output, especially for imbalanced datasets. It would be great if the authors could add that back. Otherwise, it's better to try current model on an imbalanced dataset to measure the effect.
- In the first two subfigures of Figure 4, the baseline methods such as standard CNN actually converges faster at the very beginning. But there is a big accuracy drop in the middle. What is the root cause for that? The paper seems no giving any explanation.
- Though the experimental results show faster convergence rate in terms of accuracy vs iteration, it's unknown whether this observation still hold true for accuracy vs time. Traditional dot product operators leverage fast matrix multiplication libraries for speedup, while there is much less support for angular operators.

Minor on grammar:
- Line 216, ""this problem will automatically solved"" -> ""be solved""
- Line 319, ""The task can also viewed as a ..."" -> ""be viewed""","In this paper, a new hyperspherical convolution framework is proposed by using the angular representations. It is different from CNNs and uses SphereConv as its basic convolution operator. Experiments are conducted to show the effectiveness of the proposed method. However, there are some concerns to be addressed.
First, about the Signoid SphereConv, how to choose the value of k? Is there any suggestion? Second, how to choose the SphereConv operator. To achieve better results, do we have to try each SphereConv operator? Compared to the original conv, how about the complexity of the proposed conv? From the results, the improvement of the proposed conv is somewhat limited."
Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts,"Raymond Yeh, Jinjun Xiong, Wen-Mei Hwu, Minh Do, Alexander Schwing",https://proceedings.neurips.cc/paper/2017/hash/52292e0c763fd027c6eba6b8f494d2eb-Abstract.html,"The paper proposes a unified framework to solve the
      textual grounding problem as an energy minimization problem, and uses
      an efficient branch and bound scheme to find the global optimum. 
      This differs from approaches that evaluate on generated region
      proposals.
      
      The energy function used in this framework is a linear combination of
      feature functions as score maps, each of either the form of word
      priors, geometric information cues, or image based segmentations and
      detections.  It's easy to add more feature functions of the same form
      to the framework.  The top-level weights combining the features
      functions are formulated and learned in the form of structured SVMs.
      
      There are many parts of the proposed system, including the many
      feature functions as score maps being combined in the energy function,
      weight learning as a structured SVM problem, global inference using
      the efficient sub-window search, etc., but the paper seems to have
      given enough details and references to describe all parts of the
      system.
      
      One thing I would be interested to know, not clearly given in the
      paper, is how efficient the inference is, exactly, in terms of
      computation resource and time.","

The authors propose a method for textual grounding, that is identifying the
correct object bounding box described by words that are in general ambiguous
but specific enough withing the given image. The main advantage of the method
is that unlike other methods it does not rely on having access to a good set of
proposal bounding boxes.

The authors show how to setup an energy function which can be global minized
using efficient subwindow search (a branch-and-bound-based method).  The energy
function is a linear combination of ""image concepts"" weighted by the words
contained in the query. These weights specify how much a given word relates to
a given concept.  The image concepts are features encoding priors of the most
frequent words, probability maps for selected nouns obtained from a
convolutional neural network, and two geometric cues on the size of the
bounding box.  Finally, the authors formulate the parameter learning as a
structured support vector machine.

The paper builds upon the efficient subwindow search method applying it to
textual grounding in an interesting non-trivial way. The paper is in general
well written, reasonably easy to understand, the experiments well done and the
results seem convincing.

There are two things that might be limiting:
1. The image concepts are combined linearly in the energy function which brings
   the problems with some queries, such as ""dirt bike"" discussed by the
   authors.  However, it also brings efficient evaluation of the bound.
2. The runtime of the method. The authors should add a comparison of runtimes
   (or at least the runtime of their method) to the tables comparing the
   accuracy.


Comments:

L6: we able -> we are able
Fig.2 vs. Eq.1: please use either argmax or argmin consistently
L100+1: ""to depends"" -> depends
L102: sever -> severe
L113 and Eq.2: the indicator \iota_s seems superfluous, removing it and using iteration over s \in Q would make reading simpler
L135: accumulate -> accumulates
L235: us -> use

","Summary
The approach proposes a simple method to find the globally optimal (under the formulation) box in an image which represents the grounding of the textual concept. Unlike previous approaches which adopt a two stage pipeline where region proposals are first extracted and then combined into grounded image regions (see [A] for example), this approach proposes a formulation which finds the globally optimal box for a concept. The approach assumes the presence of spatial heat maps depicting various concepts, and uses priors and geometry related information to construct an energy function, with learnable parameters for combining different cues. A restriction of the approach is that known concepts can only be combined linearly with each other (for instance the score map of “dirt bike” is “dirt” + “bike” with the learn weighting ofcourse), but this also allows for optimal inference for the given model class. More concretely the paper proposes an efficient technique based on [22] to use branch and bound for efficient sub-window search. Training is straightforward and clean through cutting-plane training of structured SVM. The paper also shows how to do efficient loss augmented inference during SVM training which makes the same branch and bound approach applicable to cutting-plane training as well. Finally, results are shown against competitive (near- state of the art approaches) on two datasets where the proposed approach is shown to outperform the state of the art.


Strengths
- Approach alleviates the need for a blackbox stage which generates region proposals.
- The interpretation of the weights of the model and the concepts as word embeddings is a neat little tidbit.
- The paper does a good job of commenting on cases where the approach fails, specifically pointing out some interesting examples such as “dirt bike” where the additive nature of the feature maps is a limitation.
- The paper has some very interesting ideas such as the use of the classic integral images technique to do efficient inference using branch and bound, principled training of the model via. a clean application of Structural SVM training with the cutting plane algorithm etc.

Weakness
1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22]
3. It is unclear what SPAT means in Table. 2.
4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts?
5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown  in Fig. 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*)
6. L130: It was unclear to me how the geometry constraints are exactly implemented in the algorithm, i.e. the exposition of how the term k2 is computed was uncler. It would be great to provide details. Clear explanation of this seems especially important since the performance of the system seems highly dependent on this term (as it is trivial to maximize the sum of scores of say detection heat maps by considering the entire image as the set).


Preliminary Evaluation
The paper has a neat idea which is implemented in a very clean manner, and is easy to read. Concerns important for the rebuttal are marked with (*) above.

[A] Hu, Ronghang, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2016. “Modeling Relationships in Referential Expressions with Compositional Modular Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1611.09978.
[B] Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. 2016. “Modeling Context Between Objects for Referring Expression Understanding.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1608.00525.
[C] Sun, Qing, and Dhruv Batra. 2015. “SubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals.” In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1378–86. Curran Associates, Inc."
Unbiased estimates for linear regression via volume sampling,"Michal Derezinski, Manfred K. K. Warmuth",https://proceedings.neurips.cc/paper/2017/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html,"The authors make connection between volume sampling and linear regression (matrix pseudo-inverses). The main contribution is that volume sampling is an unbiased estimator to the computation of  matrix pseudo-inverses. Moreover, an application of their main contribution is applied to the case of linear regression with few samples.

The paper is well-written and the main contributions are very well presented. The only concern that I have is the applicability of the results. That said, I find the main contribution of the paper very interesting.

Major comments:
1) Is it possible to give concentration bounds on the number of samples required for some approximation to X^{+}?

Minor comments:
*) Theorem 5, L161: Please define ""general position"", the term is (probably) only clear to a computational geometry audience.","I could go either way on this paper, though am slightly positive. The short summary is that the submission gives elegant expectation bounds with non-trivial arguments, but if one wants constant factor approximations (or 1+eps)-approximations), then existing algorithms are faster and read fewer labels. So it's unclear to me if there is a solid application of the results in the paper. In more detail:

On the positive side it's great to see an unbiased estimator of the pseudoinverse by volume sampling, which by linearity gives an unbiased estimator to the least squares solution vector. I haven't seen such a statement before. It's also nice to see an unbiased estimator of the least squares loss function when exactly d samples are taken. The authors use an inductive argument, with determinants canceling in the induction, and Sherman-Morrison. They give a simple proof of Cauchy-Binet as well. The n^2 speedup the authors give for exact volume sampling over previous work is also important.

On the negative side, since their exact volume sampling is nd^2 time for a single volume sample (as opposed to approximate volume sampling which is faster), it seems there are more efficient solutions for these problems, in particular for standard least squares. The authors don't claim improvement for least squares, but rather they consider the case when reading the labels to the examples is expensive and they only want to read a small number s of labels. The authors are right in that leverage score sampling would require reading d log d labels to get any relative error approximation, and although one could use Batson-Spielman-Srivastava on top of this, I think this would still read d log d labels instead of the d labels that volume sampling uses (even though Batson-Spielman-Srivastava would end up with d sampled columns, to choose these d columns as far as I know would need to read at least d log d labels. This would be worth discussing / fleshing out). But, the authors only get a d-approximation by doing this and need to take d independent volume samples, or d^2 total samples, to get a constant factor approximation to regression. On the other hand, leverage score sampling with d log d samples would get a constant factor approximation. 

So regarding applications, if one asks the question - with any amount of time complexity, how can I read as few labels as possible to get an O(d)-approximation, then I think the authors give the first algorithm for this problem. However if one wants a constant approximation or 1+eps-approximation, previous work seems to give faster algorithms and fewer samples.

It would be good if the authors compare their guarantees / running times to others for estimating the pseudoinverse and optimal regression vector, such as the recent ones in:
Price, Song, Woodruff, Fast Regression with an $\ell_\infty$ Guarantee, ICALP, 2017
and the earlier ones for estimating the regression solution vector in:
Sarlos, Improved Approximation Algorithms for Large Matrices via Random Projections, FOCS, 2006
(the running time should be combined with more modern input sparsity time subspace embeddings)

I have read the rebuttal and am still positive on the paper. 
","Summary:

This paper studies the following problem: Given an n X d matrix A with n >> d, approximate the pseudoinverse of A by sampling columns and 
computing the pseudoinverse of the subsampled matrix. The main result is that by doing volume sampling: sample a set S with probability 
proportional to A_S A^T_S, we get an unbiased estimator for the pseudoinverse. 

In particular, the authors apply this to linear regression to show that if L(w*) is the optimal loss and L(w_S) is the loss obtained by using
a sample of size s=d, then E[L(w_S)] = (d+1)L(w*). This is a difference from i.i.d. sampling where one needs to sample d log d columns
before a multiplicative error bound can be achieved. 

The authors show that by computing the variance of the estimator, one can take many random samples and get tight bounds on the loss of the
average solution. Using their ideas, the authors also give an improved algorithm to do volume sampling for s > d. Their new algorithm is a 
quadratic improvement over previous results. 

This is a very well written paper with a significant advance over prior results in the area. The one unsatisfying aspect of the result is that 
currently the authors can't prove much better bounds on the expected loss when sampling s > d columns. The authors should do a better job of mentioning
what exactly breaks down in their analysis and how significant a challenge is it to achieve such bounds.

Overall, the paper is a good fit for publication to NIPS and will be of interest to many participants.  "
Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces,"Songbai Yan, Chicheng Zhang",https://proceedings.neurips.cc/paper/2017/hash/556f391937dfd4398cbac35e050a2177-Abstract.html,"Summary:

This paper studies the problem of learning linear separators under the uniform 
distribution with noisy labels. The two noise models considered are: bounded noise where the
label of each example if flipped independently with some probability upper bounded
by eta < 1/2, and the adversarial noise model where some arbitrary eta fraction of the
labels are flipped. For the bounded noise model previous result shows how to learn
the separator to additive accuracy epsilon with time and sample complexity that 
is polynomial in the dimensionality d but exponential in the noise parameter (1-\eta).
The authors propose a perceptron based algorithm that achieves sample complexity
that is near optimal. Furthermore, the algorithm is an active learning algorithm 
and has label complexity logarithmic in 1/epsilon. In addition, since it is a perceptron
based algorithm it can run in the streaming setting. For the adversarial noise case
the authors show that the algorithm can handle noise that is epsilon/log(d) where
epsilon is the desired accuracy. This is worse than the best known result but 
the algorithm has better run time. 

Techniques: The algorithm involves running the perceptron algorithm but update only
if the mistake lies within a certain margin of the current guess of the separator. 
This margin needs to be updated after every epoch. The technical difficulty is to 
show that, with high probability, each epoch is getting closer to the target. This
is easy to show in expectation but requires a careful martingale analysis to get
a high probability bound.



This is a solid theoretical paper that tackles a challenging problem of handling 
bounded noise and improves over the best known result. The paper is well written and 
the analysis techniques used should have applications to other settings as well.","This paper gives improved bounds for active learning of halfspaces (linear thresholds) under the uniform distribution on inputs and different bounded classification noise models. The improvement is either in the sample complexity or in the running time. The algorithms proposed are simple, clever variants of the perceptron method. On the positive side, it is a good step towards improving our understand of noise-tolerance and efficient active learning. 

The downside is that the results are specialized to the uniform distribution (this should be stated explicitly in the abstract, intro and theorem statements), since halfspaces can be learned from any distribution, and even under random and other milder noise models. ","This paper analyzes a variant of the Perceptron algorithm for active learing of 
origin-centered halfspaces under the uniform distribution on the unit sphere. It 
shows that their algorithm learns using a nearly tight number of samples in the 
random independent noise of bounded rate. Previous work had exponentially worse 
dependence on the noise rate. In addition, it shows that this algorithm can deal 
with adversarial noise of sufficiently low rate. The latter result improves 
polynomially on the sample complexity but requires a stronger condtion on the noise 
rate. 

The assumptions in this setting are very strong and as a result are highly unlikely to 
hold in any realistic problem. At the same time there has been a significant amount 
of prior theoretical work on this and related settings since it is essentially the only 
setting where active learning gives an exponential improvement over passive 
learning in high dimensions. This work improves on some aspects of several of 
these works (although it is somewhat more restrictive in some other aspects). 
Another nice feature of this work is that it relies on a simple variant of the classical 
Perceptron algorithm and hence can be seen as analyzing a practically relevant 
algorithm in a simplified scenario.   

So overall this is solid theoretical contribution to a line of work that aims to better 
understand the power of active learning (albeit in a very restricted setting). A 
weakness of this work is that beyond the technical analysis it does not have much 
of an algorithmic or conceptual message. So while interesting to several experts it's 
unlikely to be of broader interest at NIPS. 

---response to rebuttal ---

>> ""This label query strategy is different from uncertainty sampling methods in practice, which always query for the point that is closest to the boundary. We believe that our novel sampling strategy sheds lights on the design of other practical active learning algorithms as well.""

This is an interesting claim, but I do not see any support for this in the paper. The strategy might be just an artifact of the analysis even in your restricted setting (let alone a more practical one). So my evaluation remains positive about the technical achievement but skeptical about this work making a step toward more realistic scenarios.
"
Renyi Differential Privacy Mechanisms for Posterior Sampling,"Joseph Geumlek, Shuang Song, Kamalika Chaudhuri",https://proceedings.neurips.cc/paper/2017/hash/56584778d5a8ab88d6393cc4cd11e090-Abstract.html,"This paper proposes two tunable posterior sampling mechanisms for exponential families for Renyi Differential Privacy. (Although not clear from the exposition, RDP is a known variant of DP) Both of them,
just like recent approaches, rely on manipulating the statistics. The
first, Diffused Posterior, reduces the weight of the observed data (in
a manner similar to the epsilon parameter in [13]). The second one
increases the weight of the prior instead (analogously to the
concentration constant in [3]). In fact, it would probably be nice if
the authors could expand a bit on the similarities between these
ideas.  The paper provides also a nice pair of examples, where they
demonstrate the practicality of these mechanisms.

No obvious typos apart from a repetition of refs 9, 10.
","
This paper analyzes the privacy cost of posterior sampling for exponential family posteriors and generalized linear models by using the recently proposed privacy definition of Renyi-DP. Specifically, they applied the privacy analyses to two domains and presented experimental results for Beta-Bernoulli sampling and Bayesian Logistic Regression.

In my opinion, the privacy analyses that the paper considers can be interesting and valuable for the community. But, the paper does not motivate enough the necessity of the usage of Renyi-DP. It is a relaxed notion of pure DP like CDP and z-CDP but the readers cannot learn from the paper that when they can use it or why should they use it.

In terms of technical correctness of the the paper, the algorithm presented relies on RDP and standard DP definitions, I believe the analyses are correct. But, the problem is the authors did not cite the definitions which are already published in the literature (e.g. Definition 1-2-3-4-…). So, it is hard for me (and I suppose for the other readers) to distinguish which of the definitions/theorems are proposed by the authors and which of them are already published.

Furthermore, the search process to find an “appropriate” value of r and m is not clear. In the experiments section they searched over six values but we do not know whether all of them are valid or not. What do you mean by “appropriate”? Besides, I prefer to see the connection/correlation between the values r/m and accuracy (their intuitive interpretation would be immensely helpful). Maybe I miss it, but the paper does not provide any formal utility guarantee, which is something interesting (at least we can see the privacy/accuracy connection empirically). 

Finally, in Figure 2c and 2d and in the real data experiments, there is no baseline such as the non-private case. OPS is not a proper baseline because we cannot compare pure-DP and RDP (maybe you can formalize the connection somewhere in methodology and then we can compare). So, it will be nice to show what is a good result for these experiments when privacy is not a concern. "
Variable Importance Using Decision Trees,"Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S. Talwalkar",https://proceedings.neurips.cc/paper/2017/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html,"The article tackles the problem of variable importance in regression trees. The strategy is to select the variables based on the impurity reduction they induce on label Y.  The main feature of this strategy is that the impurity reduction measure is based on the ordering of Y according to the ranking of the X variable under consideration, therefore it measures the  relationship between Y and any variable in a more robust way than simple correlation would. The authors prove that this strategy is consistent (i.e. the true explanatory variables are selected) in a range of settings. This is then illustrated on a simulated example where the results displayed are somewhat the ones one could have expected: the proposed procedure is able to account for monotone but non linear relationships between X and Y so it yields better results than simple correlations. In the same time, it cannot efficiently deal with interaction since explanatory variables are considered one by one - compared with procedures that consider a complete tree rather than only  a stump.

Overall the  idea that is presented is very simple but as mentioned by the authors the theoretical analysis of the strategy is not completely trivial. Although there are some weaknesses in the manuscript the ideas are well presented and the sketchs of proof presented in section 3.1 and 3.2 provide a good lanscape of the complete proof. However there are some weaknesses in the Simulation section, and it is a bit surprising (and disappointing) that there is no discussion section at all.

Regarding the Simulation section, first it seems that the quality criterion that is represented is actually not the criterion under study in section 2 and 3. The theoretical results state the consistency of the procedure, i.e. the probability that the complete set of s predictors is retrieved, so the authors could have displayed the proportion of runs where the set is correctly found. Instead, they display the fraction of the support that is recovered, and it is not clear whether this is done on a single simulated dataset or averaged. If only on a single simulation then it would be more relevant to repeat the experiment on several runs to have a clearer picture of what happens. Another remark is that the authors mention that they consider the model Y=X\beta +f(X_S) + w, but do they consider this model both to generate the data and perform the analysis, or just for the generation and then a purely linear model is used for analysis ? It seems that the second hypothesis is the good one but this should be clearly mentioned. Also it would be nice to have on an example the histogram of the criterion that is optimized by each procedure (when possible) to empirically evaluate the difficulty of the problem.

Regarding the proof itself, some parts of it should be more commented. For instance, the authors derive the exact distribution of \tilde{U}, but then they use a truncated version of it. It may seem weird to approximate the (known) true distribution to derive the final result on this distribution. The authors should motivate this step and also provide some insight about what is lost due to this approximation. Also, is there any chance that the results presented here can be easily extended to the classification case ? And lastly, since the number of active feature is unknown in practice, is there any way the strategy presented here could help estimating this quantity (also I understand it is not the core of the article) ?




","This paper studies the variable selection consistency of decision trees. A new algorithm called ""DStump"" is proposed and analyzed which simply checks the variable importance via comparing the corresponding impurity reduction at the root node. DStump needs to know the correct number of true features to work. For both the linear models with uncorrelated design and the general additive nonlinear models, the model selection consistency is proved to hold true with overwhelming probability. The analysis is further generalized to linear correlated design where the gap of signal strength is of course required. The analysis with a truncated beta distribution is inspiring and promising. The results on decision tree are new (to the best of my knowledge). The paper is written with a good style and the literature review is also sufficient. Because of the time restriction, I have not gone through the math details in the attached technical report.

Some minor points:
1. The authors are encouraged to add a short paragraph before Algorithm1 to define the problem.
2. In lines 240-241, while all the y's are just real numbers, how can they be regarded as drawn from Uniform[0,1/2]?","This paper deals with the problem of variable selection using decision tree.
More precisely,the authors provide a new algorithm called DStump which relies on the impurity-based information to perform variable selection. 
They provide theoretical guarantees for the proposed method.  
I find the paper interesting and very clear. 
The problem of variable selection using decision tree is not new but there is a lack of theoretical analysis.
Hence, from a theoretical point of view, I think that this paper provide a relevant contribution. 

Minor comment:
Why do you not use the permutation-based importance method (used in random forests) in the simulations ?   "
A simple model of recognition and recall memory,"Nisheeth Srivastava, Edward Vul",https://proceedings.neurips.cc/paper/2017/hash/57aeee35c98205091e18d1140e9f38cf-Abstract.html,"The authors suggest a unifying mechanism to explain a variety of results on recall vs recognition memory based on the idea that in recall items are retrieved with a list as a cue and in recognition a list is retrieved with an item as a cue. It is an interesting and to my knowledge novel idea, which may contribute towards a better understanding of neuropsychological and computational processes underlying memory. However, the paper also has several weaknesses:
- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.
- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.
- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.
- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?
","This is a beautifully written paper on a central aspect of human memory research. The authors present a simple model along with a clever experimental test and simulations of several other benchmark phenomena.

My main concern is that this paper is not appropriate for NIPS. Most computer scientists at NIPS will not be familiar with, or care much about, the intricate details of recognition and recall paradigms. Moreover, the paper is necessarily compact and thus it's hard to understand even the basic descriptions of phenomena without appropriate background. I also think this paper falls outside the scope of the psych/neuro audience at NIPS, which traditionally has focused on issues at the interface with machine learning and AI. That being said, I don't think this entirely precludes publication at NIPS; I just think it belongs in a psychology journal. I think the authors make some compelling points addressing this in the response. If these points can be incorporated into the MS, then I would be satisfied.

Other comments:

The model doesn't include any encoding noise, which plays an important role in other Bayesian models like REM. The consequences of this departure are not clear to me, since the models are so different in many ways, but I thought it worth mentioning.

In Figure 2, I think the model predictions should be plotted the same way as the experimental results (d').

I didn't understand the point about associative recognition in section 5.3. I don't think a model of associative recognition was presented. And I don't see how reconsolidation is relevant here.

Minor:

- Eq. 1: M is a set, but in the summation it denotes the cardinality of the set. Correct notation would be m \in \mathcal{M} underneath the summation symbol. Same issue applies to Eq. 2.

- I'm confused about the difference between M' and M_k.

- What was k in the simulations?","This Bayesian model is rather simple and intuitive. It gives  an explanation for the recognition and recall of memory. The model and experiments can only explain short-term working memory not the long-term declarative memories.  This paper introduce a simple mode for recognition and recall. It provides an Bayesian explanation for short-term working memory. However, recognition and recall normally involve long-term declarative memory, such as semantic or episodic memory, which this model can not give a reasonable explanation. The experiments can only explain the working memory from a Bayesian perspective, not the declarative memory as claimed by the authors.

This paper might be more suitable for a publication at a cognitive conference /journal. "
Implicit Regularization in Matrix Factorization,"Suriya Gunasekar, Blake E. Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, Nati Srebro",https://proceedings.neurips.cc/paper/2017/hash/58191d2a914c6dae66371c9dcdc91b41-Abstract.html,"This paper considers a foundational problem in machine learning: why do simple algorithms like gradient descent generalize well? Can we view these simple algorithms as implementing some kind of implicit regularization? The authors address this problem in the case of the simplest kind of non-convex learning model: gradient descent for matrix factorization. They (mostly) show that this method converges to the least nuclear norm solution.
Importantly, they are honest about the limitations of their current theory, which currently requires commutativity of the measurement matrices.

The questions asked by this paper are stronger than the answers. The authors seem well aware of the weaknesses of their results so far:
* theory holds only for commutative case, which is not practically relevant
* theory holds only in the continuous (gradient flow) limit
* connection to deep learning is tenuous
However, the questions (and partial answers) seem interesting enough to merit publication.
It seems likely to this reviewer that these weaknesses are limitations of the technical tools currently available, rather than of the broader ideas presented in this paper.
The numerical experiments, in particular, are thorough and support the conjecture.

line 45: typo ""deferent""
figure 2 caption: missing period
Conjecture: your experiments observe that the frobenius norm of the initial point seems to matter. State why this norm does not appear in your conjecture.
line 160: where did the first equation come from? Also, I believe you have not defined s_\infty explicitly.","This paper studies an interesting observation:
with small enough step sizes and initialization close enough to the origin,
even for an underdetermined / unregularized problem of the form (1),
gradient descent seems to find the solution with minimum nuclear norm.
In other words, starting from an initialization with low nuclear norm,
gradient descent seems to increase the norm of the iterates ""just as needed"".

The authors state that conjecture and then proceed to prove it for the
restricted case when the operator A is based on commutative matrices.

The paper is pretty interesting and is a first step towards understanding
the implicit regularization imposed by the dynamics of an optimization algorithm
such as gradient descent.

There seems to be many small errors in Figure 1 and 2 (see below).

Detailed comments
-----------------

Line 46: I think there are way earlier references than [16] for multi-task
learning. I think it is a good idea to give credit to earlier references.

Another example of model that would fall within this framework is
factorization machines and their convex formulations.

Line 63: I guess X = U U^T? X is not defined...

Figure 1: 

- the figure does not indicate (a) and (b) (I assume they are the left and right plots)

- what does the ""training error"" blue line refer to?

- X_gd is mentioned in the caption but does not appear anywhere in the plot

Figure 2:

- I assume this is the nuclear norm of U U^T

- X_gd (dotted black line) does not appear anywhere

- It is a bit hard to judge how close the nuclear norms are to the magenta line
without a comparison point

- How big is the nuclear norm when d < 40? (not dislpayed)

Line 218: I assume the ODE solver is used to solve Eq. (3)?
Please clarify.
","In this manuscript the authors present an interesting problem for the optimization community and in particular for machine learning applications, due to its implication.

According to the authors, the proposed method  obtain the minimization of the nuclear norm subject to linear constraints and considering only symmetric positive semidefinite matrices.

First, a general conjecture is presented and then a particular case is proved. This is a theoretical work and the main contribution is to prove the previous particular case, and also provide an open problem to the community (the conjecture) which is very interesting.  
From my point of view, this is a working process manuscript, and some details should be completed. 
In the proof of theorem 1, which is the main contribution, there is a subtlety in the proof that should be considered. Which is the limit of s_t? or Which is the limit of s_t/\beta?  Or,   \eta(\beta) has limit? This should be answered because this was not considered in the proof, although it was mentioned in the previous section of the proof that |s_t | should tend to infinity when alpha goes to 0. But this should be proved..

The authors state that the proposal is more general. Please, give some insights or proof in these cases for example, what happens if the matrix is non symmetric or is not positive semidefinite…? 

The empirical evidence should be improved. The example with 3 by 3 matrices
 is too basic. The author should consider more practical examples or experiments.
"
Continuous DR-submodular  Maximization: Structure and Algorithms,"An Bian, Kfir Levy, Andreas Krause, Joachim M. Buhmann",https://proceedings.neurips.cc/paper/2017/hash/58238e9ae2dd305d79c2ebc8c1883422-Abstract.html,"For set functions, submodularity is equivalent to diminishing returns (DR), however, for integer-lattice functions or continuous functions, DR-submodular functions form an important subclass of submodular functions, and their maximization captures non-convex problems of interest such as maximizing the softmax extension of DPPs.

In this paper, the authors study non-monotone continuous DR-submodular maximization under general down-closed convex constraints. They prove a relation between approximate stationary points and the global optimum which enables them to use recent results on non-convex Frank-Wolfe to devise a two-phase 1/4-approximation guarantee. They also give a non-monotone Frank-Wolfe variant with 1/e-approximation guarantee. Experiments, however, suggest that the former (two-phase) is always comparable or better than the latter in practice. The analysis in Subsection 3.2 also simplifies previous work [7, 20] by a different proof.

Overall, I find the technical contributions worthy of acceptance.","This paper presents a new approach for non-monotone DR sub modular optimisation. The strength of this paper over existing approaches is proposing a solution to a more general problem setting than having an approach subject to constraints such as the box constraints. The proofs are well constructed and presented. However the experimental results are somehow very limited. The parameters used for the simulated data are very limited and not sure if the authors have performed a much more simulations with different parameters to assess more in depth the ""performance"" of their approach over the existing ones. Also, for the real data set example, it is not very clear if that the proposed two-phase Frank-Wolfe algorithm performs much better than the others. From the figures, it seems performing just slightly better but not very convincing. For real-world problems, it is typically difficult to verify whether the assumptions hold or not, so a comparison with the DoubleGreedy algorithm would have been appropriate to see how much gain we have by considering a more general constraint. ","The paper proposes two algorithms with approximation guarantee and convergence rate on non-monotone continuous DR-submodular maximization under general down-closed convex constraint, which extends previous results on ""box"" constraints.
Experiment results show the proposed algorithms have better performance compared to projected gradient on both synthetic and real-world data.
The paper also generalizes the problem to a conic lattice setting, and shows reduction to continuous submodular optimization.

The paper is theoretically concrete. The properties of constrained DR-submodular maximization discussed in the paper,
especially the local-global relation could be intriguing for future research.

Thanks for the feedback. I think if the authors can come up with more applications of the proposed constrained DR-submodular optimization, the contribution of the work could be improved significantly.


Minor:
line 460 (supplement) ""show"" instead of ""shwo""

"
"Decoupling ""when to update"" from ""how to update""","Eran Malach, Shai Shalev-Shwartz",https://proceedings.neurips.cc/paper/2017/hash/58d4d1e7b1e97b258c9ed0b37e02d087-Abstract.html,"This paper proposed to train a classifier such as DNN using noising training data.  The key idea is to maintain two predictors, and update based on the two predictors' disagreement.

+ Pros
Theoretical analysis shows under mild conditions, the proposed method will converge 

- Cons
- The overall idea is similar to the semi-supervised learning method co-training, which the authors cited but no detailed discussion.
Although co-training is designed for semi-supervised learning, it's simple to adapt it for dealing with noisy data

- The experiment part is very weak. Only an example on very small gender classification data will not lead to a claim  ""this simple approach leads to state-of-the- art results.""  Though theoretical analysis shows it converges, lack of experiment results makes it difficult to see how useful this method is 



","The authors propose a relatively simple approach to dealing with noisy labels in deep learning settings by only updating on samples for which two classifiers disagree. A theoretical foundation is shown in the case of linearly separable data. The authors then empirically validate their method on a complex face dataset. Results show improvement over baseline methods and show that they can be combined with baseline methods for further improvement.  

Comments
- The literature review does a good job of placing the idea in context
- Method appears general, scalable, and is demonstrated to be effective
- Paper demonstrates that the algorithm can converge 
- Some of the theoretical results, even in the linear case, don't justify the method without assumptions on the data distribution. The foundations laid out however can form the bases of future theoretical analysis of this algorithm.


","Summary

The paper proposes a meta algorithm for training any binary classifier in a manner that is robust to label noise. A model trained with noisy labels will overfit them trained for too long. Instead, one can train two models at the same time, initialized at random, and update by disagreement: the updates are performed only when the two models' prediction differ, a sign that they are still learning from the genuine signal in the data (not the noise); and instead, defensively, if the models agree on their predictions and the respective ground truth label is different, they should not perform an update, because this is a sign of potential label noise. A key element is the random initialization of the models, since the assumption is that the two should not give the same prediction unless they are close to converge; this fits well with deep neural networks, the target of this work.

The paper provides a proof of convergence in the case of linear models (updated with perceptron algorithm and in the realizable case) and a proof that the optimal model cannot be reach in general, unless we resort to restrictive distributional assumptions (this is nice since it also shows a theoretical limitation of the meta-algorithm). The method works well in practice in avoiding to overfit labels noise, as shown by experiments with deep neural networks applied to gender classification on the LFW dataset, and additional tests with linear models and on MNIST.


Comments

The paper is very well written: the method is introduced intuitively, posed in the context of the relevant literature, proven to be sound in a simple theoretical setting and shown to be effective on a simple experimental set up, in realistic scenario with noise. Additionally, this work stands out from the large set of papers on the topic because of its simplicity and the potential of use in conjunction with others methods.

Proofs are easy to follow and seem flawless. Experimental results are promising on simple scenarios but will need future work to investigate the robustness and effectiveness on at scale and in multi-class  --- although I don't consider this a major issue because the paper is well balanced between theory and practice.

Also, do you find any interesting relation with boosting algorithms? In particular I am referring to ""The strenght of weak learnability"" by R. Schapire, that introduced a first form (although impractical) of boosting of weak classifier. The algorithm presented back then uses a form of ""update by disagreement"" for boosting, essentially training a third model only on data points classified the same way by the former two models.

Minors
25 -> better
107 -> probabilistic
183 -> algorithm"
Regret Analysis for Continuous Dueling Bandit,Wataru Kumagai,https://proceedings.neurips.cc/paper/2017/hash/58e4d44e550d0f7ee0a23d6b02d9b0db-Abstract.html,"The paper analyzes the dueling bandit problem when the action space is a convex and compact subset of the d-dimensional Euclidean space. Moreover, the noisy comparison feedback for the choice (a,a') is such that P(a > a') can be written as sigma(f(a') - f(a)) where f is a strongly convex and smooth function and sigma is a sigmoidal link function. The player's goal is to minimize a notion of regret which, under the above assumptions, is equivalent to previously proposed notions. The proposed algorithm uses ideas from convex bandit optimization with self-concordant regularization. In essence, the dueling bandit regret is shown to be upper bounded by an expression that is similar to a convex bandit regret with a loss function P_t that is locally strongly convex and smooth. At each time step, the player uses (a_t,a_t') where a_t is determined through stochastic gradient descent and a_t' is used to define a stochastic gradient estimate. Additional (simpler) results relate the dueling bandits regret bounds to regret bounds in stochastic convex optimization with pairwise comparison feedback. Via the adaptation of known lower bounds for stochastic optimization, the regrets are shown to be optimal ignoring log factors.

The paper builds on previous results by [9] on convex bandit optimization which used similar strongly convex / smooth assumptions. However, the adaptation to dueling bandits is not trivial. Overall, the paper looks technically strong with interesting results. I am a bit disappointed by the general lack of intuition in explaining the approach and the algorithm. Also, the related work section does not compare with the rates obtained in previous works, especially [2] which also used convex action sets, even though the assumptions are not identical.","The submitted paper presents a stochastic mirror descent algorithm for the dueling bandits problem defined in Yue & Joachims 2009. The paper gives a regret bound based on a regret definition established in the dueling bandits framework (where two nearly optimal arms have to be selected in order to minimize regret), an connects this result to convergence rates with respect to the optimality gap in convex optimization. The latter result is the most interesting contribution of the paper.

While the paper is self-contained and the reviewer could not find an error in the proofs (not being an expert in bandit learning), the paper could be improved considerably by putting it into the greater context of stochastic zero-order convex optimization. The gradient estimation approach given in Section 3.2 is reminiscent of work by Duchi et al. (2015). Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations. The convergence analysis given in the cited paper can also be interpreted as an expected regret bound for online bandit learning. How do the convergence rates compare? Earlier work on zero-order optimization by Ghadimi & Lan or Nesterov & Spokoiny is cited in this paper.

Furthermore, it would be nice to get a clearer picture how the presented work is different from related work in online bandit learning. Wouldn't the analysis of Yue & Joachims 2009 be easily extended to the setting of the paper by adding an appropriate proximal operator to their algorithm? Where does the assumption of linearity restrict Zhang et al.'s analysis to apply to the presented convex optimization framework?

Overall recommendation: The paper is certainly theoretically solid - a bit more explanation would not hurt and possible lead to wider reception of the presented work.

"
One-Sided Unsupervised Domain Mapping,"Sagie Benaim, Lior Wolf",https://proceedings.neurips.cc/paper/2017/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html,"This paper tackles the problem of unsupervised domain adaptation. The paper introduces a new constraint, which compares samples and enforces high cross-domain correlation between the matching distances computed in each domain. An alternative to pairwise distance is provided, for cases in which we only have access to one data sample at a time. In this case, the same rationale can be applied by splitting the images and comparing the distances between their left/right or up/down halves in both domains. The final unsupervised domain adaptation model is trained by combining previously introduced losses (adversarial loss and circularity loss) with the new distance loss, showing that the new constraint is effective and allows for one directional mapping.

The paper is well structured and easy to follow. The main contribution is the introduction of a distance constraint to recent state-of-the-art unsupervised domain adaptation pipeline. The introduction of the distance constraint and the assumption held by this constraint, i.e. there is a high degree of correlation between the pairwise distances in the 2 domains, are well motivated with experimental evidence.

Although literature review is quite extensive, [a-c] might be relevant to discuss. 

[a] https://arxiv.org/pdf/1608.06019.pdf 
[b] https://arxiv.org/pdf/1612.05424.pdf
[c] https://arxiv.org/pdf/1612.02649.pdf

The influence of having a distance loss is extensively evaluated on a variety of datasets for models based on DiscoGAN, by changing the loss function (either maintaining all its components, or switching off some of them).  Qualitative and quantitative results show the potential of the distance loss, both in combination with the cycle loss and on its own, highlighting the possibility of learning only one-sided systems. However, given that the quantitative evaluation pipeline is not robust (i.e. it depends on training a regressor), it is difficult to make any strong claims on the performance of the method.

The authors qualitatively and quantitatively assess their contributions for models based on CycleGAN as well. Quantitative results are showed for SVHN to MNIST mapping. However, among the large variety of mappings shown in the CycleGAN paper, authors only pick the horse to zebra mapping to make the qualitative comparison. Given the subjectivity of this kind of comparison, it would be more compelling if the authors could show some other mappings such as season transfer, style transfer, other object transfigurations or labels2photo/photo2label task. Quantitative evaluation of CycleGAN-based models could be further improved by following the FCN-scores of the CycleGAN paper on the task labels/photo or photo/labels.

Finally, in the conclusions, the claim “which empirically outperforms the circularity loss” (lines 293-294) seems a bit too strong given the experimental evidence.","This paper introduces one-sided unsupervised domain mapping that exploits smoothing assumption (images that are similar in domain A should be similar in domain B). The paper is well written and easy to follow. 

The contribution of the paper is based on simple observation that allows the authors to perform domain mapping without performing cycles (A->B->A). The empirical results seem to be pointing that the smoothness assumption itself leads to good results.

Table 2: In DiscoGan paper the reported RMSE for car2car task is 0.11 (see Fig. 5(c) in https://arxiv.org/pdf/1703.05192.pdf), the authors for the same task report the result of 0.306. What might be the reason of such big discrepancies in the reported scores? Is it only due to regressor (as mentioned in line242)?

Comparison with CycleGAN. Would it be possible to show more qualitative results on different tasks used in CycleGAN paper (e. g. label to photo)?

Line 228: “Datasets that were tested by DiscoGAN are evaluated here using this architecture” -> please rephrase.

Line 298: “We have proposed an unsupervised distance-based loss for learning a single mapping (without its inverse) and which empirically outperforms the circularity loss.” I’m not sure the experimental evidence presented in the paper is sufficient to support this claim.","This paper introduces a novel loss to train an image-to-image mapping from unpaired data. The idea is that there is a high correlation between the pairwise distances in the source and target domains. These distances can be measured between two different samples or two regions in the same sample.

Positive:
- Using pairwise distances in this context is new and interesting.

Negative:
- It seems to me that, while the notion of pairwise distances can indeed be well-suited in some cases, it is much less so in other ones. For example, two images depicting the same object but in different colors will be very different, but the corresponding edge images would look very similar. Similarly, two Street View House images depicting the same number but in different colors will look different, but should correspond to very similar MNIST images. In essence, while the statement on Line 86 is correct, the reverse is not necessarily true.

- In Section 2 (Fig. 1), the authors perform different motivating experiments using the results on the CycleGAN. In my opinion, these experiments would better motivate the method if they were done on data with known pairs. These results might just come from artefacts of the CycleGAN.

- I am surprised that the one-sided version of this method works. In essence, the distance-based loss on Page 5 does not really make use of the target data, except in the mean \mu_B and std \sigma_B. While this may already contain important information for a domain such as object edges, it seems that it would throw away much information for other domains. Can the authors comment on this?

- Although [18] is discussed as a weakly-supervised method, I believe that it can apply quite generally. For examples, the features of deep networks, such as the VGG network, trained on ImageNet have proven effective in many tasks. By using such features, the weak supervision would come at virtually no cost. I would suggest the authors to compare their results with [18] based on such features. The corresponding loss could also be added to the different ones used in Section 4.

- It is not always clear that the metrics used for quantitative evaluation (based on additional regressor, classifier, VGG features, or directly on loss values as in the supplementary material) really reflect human perception. For example, in the supplementary material, I find the results of DiscoGAN in Fig. 1 more convincing than those of the proposed method. In my opinion, a user study would be more valuable.

- The introduction makes it sound like this is the first method to work in the unsupervised scenario (Lines 28-29). It becomes clear later that the authors do not mean to claim this, but I think it should be clarified from the beginning.
"
Poincaré Embeddings for Learning Hierarchical Representations,"Maximillian Nickel, Douwe Kiela",https://proceedings.neurips.cc/paper/2017/hash/59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html,"The authors propose an embedding method which learns distributed representations in hyperbolic space rather than Euclidean space. As a result the embeddings capture not only semantic, but also hierarchical structure. The authors introduce the Poincare embeddings and explain how to fit them to data using a stochastic gradient method appropriate for hyperbolic space.
In an experimental section, the Poincare embeddings are evaluated on embedding taxonomies, embedding a network and embedding lexical entailment.

In all of these applications the hierarchical nature of the data is encoded in the training data. The abstract, introduction and model section made it seem like it would be possible to learn the hierarchical structure of words from standard text corpora which do not have any relational annotations. Maybe I misunderstood. Is there a way to learn a taxonomy of the vocabulary from unstructured text corpora (say from the Gigaword collection) using Poincare embeddings? How would you choose the loss L in that case?

Also, what is the loss L in Section 4.3?

The paper is well written, technically interesting, and has interesting results.","This paper proposes Poincare embeddings for learning representations of symbolic data. The new method represents symbolic data in an n-dimensional Poincare ball, and can simultaneously learn the similarity and the hierarchy of objects. An algorithm based on Riemannian optimization is introduced to learn these Poincare embeddings. Experimental results show that the Poincare distance outperforms the Euclidean distance and translational distance in tasks like taxonomy embedding, network embedding, and lexical entailment.

This paper is generally well-written. The method is clearly presented. The idea seems interesting and might be of potential impact for future work. My only concern is that the empirical evaluation appears to be a bit weak. In each of the tasks evaluated, the authors design a specific learning paradigm for this task (e.g., the loss function of eq.(6) in taxonomy embedding and the edge probability of eq.(7) in network embedding). All the methods use the same paradigm but with different distance metrics (i.e., Poincare distance, Euclidean distance, and translational distance). But this specially designed paradigm (e.g., the edge probability of eq.(7) in network embedding) could be particularly advantageous to the Poincare distance while disadvantageous to the baselines. Comparing with state-of-the-art baselines (in their original forms) will significantly strengthen the evaluation, e.g., DeepWalk[1] and LINE[2] in network embedding.

For your reference:
[1] Perozzi et al. DeepWalk: Online learning of social representations. In KDD'14.
[2] Tang et al. Line: Large-scale information network embedding. In WWW'15.","Summary
=======

  The paper proposes a link prediction model that embeds symbols in a
  hyperbolic space using Poincaré embeddings. In this space, tree
  structures can more easily be represented as the distance to points
  increases exponentially w.r.t. Euclidean distance. The paper is
  motivated and written well. Furthermore, the presented method is
  intriguing and I believe it will have a notable impact on link
  prediction research. My concerns are regarding the comparison to
  state-of-the-art link prediction and how the method performs if the
  assumption about a hierarchy in the data is dropped.


Strengths
=========

  - Impressive results for embedding hierarchies in a non-Euclidean
    space using a much smaller dimension.


Weaknesses
==========

  - Maybe I am missing a point, but why is there no comparison with
    state-of-the-art neural link prediction models (e.g. HolE/ComplEx)?
    This would strengthen the experimental evaluation a lot as TransE
    has consistently be outperformed by these methods. Furthermore, how
    does Poincaré compare to TransE in terms of runtime?
  - I would have liked to see more experimental results of the
    limitations of the proposed method. Given that many real-world
    datasets have less pronounced hierarchies than WordNet, it would
    have been nice to show what happens if a hierarchy of symbols cannot
    be assumed for a large portion of the data. For instance, how would
    the method perform on FB15k? Generally, I would like to see a more
    thorough discussion about when Poincaré embeddings can be used in
    practice and when not. For instance, the authors mention word2vec,
    GloVe and FastText in the introduction and I guess that Poincaré
    embeddings cannot serve as drop-in replacement for word vectors in
    these models as we don't know how to perform certain translations in
    hyperbolic space or how to apply backprop? Such limitations should
    be clarified in the paper.


Minor Comments
==============

  - Embedding hierarchies in Euclidean has been attempted in prior work
    that should be mentioned:
    - For textual entailment and images: Ivan Vendrov, Ryan Kiros, Sanja
      Fidler, Raquel Urtasun. Order-Embeddings of Images and
      Language. ICLR 2016
    - For link prediction: Thomas Demeester, Tim Rocktäschel, Sebastian
      Riedel. Lifted Rule Injection for Relation Embeddings. EMNLP 2016
  - Footnote 2: ""would be more more difficult"" -> ""would be more
    difficult""
  - L277: ""representations whats allow us to"" -> ""representations that
    allow us to""
"
Variance-based Regularization with Convex Objectives,"Hongseok Namkoong, John C. Duchi",https://proceedings.neurips.cc/paper/2017/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html,"The article ""Variance-based Regularization with Convex Objectives"" considers the problem of the risk minimization in a wide class of parametric models. The authors propose the convex surrogate for variance which allows for the near-optimal and computationally efficient estimation.

The idea of the paper is to substitute the variance term in the upper bound for the risk of estimation with the convex surrogate based on the certain robust penalty. The authors prove that this surrogate provides good approximation to the variance term and prove certain upper bounds on the overall performance of the method. Moreover, the particular example is provided where the proposed method beats empirical risk minimization. 

The experimental part of the paper considers the comparison of empirical risk minimization with proposed robust method on 2 classification problems. The results show the expected bahaviour, i.e. the proposed method is better than ERM on rare classes with not significant loss on more common classes.

To sum up, I think that this is very strong work on the edge between the theoretical and practical machine learning. My only concern is that the paper includes involved theoretical analysis and is much more suitable for full journal publication than for short conference paper (the Arxiv paper is 47 pages long!!!).","This paper introduces a variance-based scheme for risk minimization, using ideas from distributionally optimization and Owen's empirical likelihood. The key concept is a robustly regularized risk able to capture both the bias and variance uniformly for all models in hypothesis spaces. It is shown that the minimization of this robust regularized risk automatically attains a nearly optimal balance between approximation and estimation errors. As compared to existing model selection approaches, a remarkable property of the approach here is that the resulting optimization problem is convex. Theoretical and empirical analysis are performed to show its advantage over ERM scheme. The paper is well written and well motivated.

I only have some minor comments:
  (1) Definition 2.1 The first $\psi$ should be $\psi_n$
  (2) Appendix, line 421, 426: it seems that you want to refer to (7) instead of (8)?
  (3) Appendix, equation below line 421, it seems that $E_p[\hat{f}(X)]$ should be $E_p[f(X)]$? The last term $\frac{2M\rho}{n}$ is missing in eq (22)?
  (4) Last two displays in the proof of Lemma A.3: it seems that $\mathcal{F}_l$ should be $\mathcal{F}_L$ and $\mathcal{F}$, respectively? ","Summary
This work offers an alternative to the traditional empirical risk minimization. The approximation of the bias and variance terms is often a non convex function in the parameter(s) to be estimated, which affects the computational efficiency of the procedure. 
Thus the authors propose an expansion of the variance (in the bias/variance decomposition of the risk) that enables them to write  the robust empirical risk (on a certain class of probabilities) as the sum of the empirical risk and the variance term. It shows that the robust empirical risk plus a variance term is a good surrogate for the empirical risk. Thus whenever the loss function is convex, this approach provides a tractable convex formulation. 


Advantages & drawbacks
The authors provide theoretical guarantees in high probability to ensure that robust empirical risk trades automatically between approximation and estimation errors. They provide asymptotic and non asymptotic result.


Remarks
It is not clear what is Owen's empirical likelihood. Can it be made more explicit ?

The main important assumption is that the loss function has a compact support of the for $[-M,M]$ with $M\geq 1$. To derive the results of the paper, $M$ needs to be known. Is there any chance to extend your result to the regression setting nonetheless ?"
"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening","Kevin Lin, James L. Sharpnack, Alessandro Rinaldo, Ryan J. Tibshirani",https://proceedings.neurips.cc/paper/2017/hash/5abdf8b8520b71f3a528c7547ee92428-Abstract.html,"The paper considers the 1d fused lasso estimator under squared loss. A new proof technique is introduced that yields tighter rates than prior work, and makes it possible to study extensions to misspecified and exponential family models.

The paper is a pleasure to read, with a rich discussion of prior work and comparison against previous results. 
 In view of Remark 3.7. however the rates obtained in the present work are not as sharp as those in Guntuboyina et al (2017) for the mean model. It would therefore be more transparent, if the authors mention this upfront, e.g. in section 2.   In the present version of the manuscript, Remark 3.7 comes about as a surprise and one feels disappointed... 

The strength of the contributions here is the idea of the lower interpolent. This should be better emphasized, e.g. by providing a gist of this concept e.g. line 81 rather than leaving the definition buried in the proof sketch. 

If possible, it would be important to provide some explanation as to why the authors think that the proof technique of Guntuboyina et al (2017) cannot be extended to misspecified and exponential family models. 

As future work it would be interesting to see whether the lower interpolent technique can be used to analyze the fused lasso estimator in the context of linear regression 

","The authors study the problem of multiple change point detection and provide provide bounds for the fused-lasso estimator. The problem of change point detection is well studied in the statistics literature and the authors provide good references. 

The paper is well written and flows well. As claimed by the authors, one of the major contribution of this paper is in the technique of the proof and, unfortunately, the authors had to put it in the appendix to fit the NIPS page limits. I can not think of a way to fit that in the main section without a major rewrite. In its current form, the appendix is longer than the main paper.

A recent independent development, Guntuboyina et al. (2017), provide the bounds for the lasso estimator. The authors claim that their method of proof is of independent interest as that generalizes to other related problems. It is not clear why the methods of this generalizes but that of Guntuboyina et al. (2017) does not. I would have loved to see discussions on how theorem 3.1 compares to the main results in Guntuboyina et al. (2017) which seem to be stronger.

A comment on inequality (7): This seems to follow directly from the definition of \hat\theta unless I am missing something. Isn’t \hat\theta defined to be the parameter that minimizes 
1/2 ||y-\theta||_2^2 + \lambda ||D\theta||_1

","This paper gives a sharp error bound for the fused lasso estimator. To give a sharper bound, a new concept ""lower interpolant"" is introduced. The derived bound improves the existing bound and is applicable also to a misspecified model and an exponential family.

I feel that the paper gives an interesting theoretical tool. The given bound gives significantly improved rate for especially small W_n (the shortest interval between the break points).

One drawback is that the generality of the lower interpolant is not clear. If it could be applied for other types of structured regularizations, then the result would be more significant.

"
Cross-Spectral Factor Analysis,"Neil Gallagher, Kyle R. Ulrich, Austin Talbot, Kafui Dzirasa, Lawrence Carin, David E. Carlson",https://proceedings.neurips.cc/paper/2017/hash/5b970a1d9be0fd100063fd6cd688b73e-Abstract.html,"The authors propose a method called Cross-Spectral Factor Analysis that decomposes obserfved local field potentials (LFPs) into interacting functional connectomes, called Electomes defined by differing spatiotemporal properties.  They then use Gaussian process theory to model interactions between these coherence patterns in a way that may reveal dynamic interaction of the different regions of the brain at mesoscale.

This approach is interesting in that it provides a way to rigorously model differing regions LFPs at a scale not readily accessible by certain other modalities such as fMRI or EEG.  

The presentation is generally clear and the overall approach is well motivated with good premise.  The approach is fairly standard from a modeling and learning perspective,  although the authors have good insight into applicability of these techniques to simulataneous LFP analysis.  

The authors do no offer any real interpretation of the results of the method, and perhaps this is natural given that no effective ground truth is known.  At a minimum it would seem that more detail could be given on the stability or robustness of the derived Electomes and this should be expanded particularly for publication.","The paper considers the problem of performing dimensionality reduction for Local Field Potential (LFP) signals that are often used in the field of neuroscience. The authors propose a factor analysis approach (CSFA), where the latent variables are stationary Gaussian Processes that are able to capture correlations between different channels.
The parameters of the factor analysis (i.e. factor scores s_w, for w=1...W) can then be optimised along with the kernel parameters \Theta, by maximising the marginal likelihood. For that purpose the authors employ a resilient backpropagation approach. A discriminative approach is also presented (dCSFA), where the goal is to optimise a heuristic that relies on both the GP marginal likelihood and the loss function for the classification.

Although I feel that the findings of the paper could be valuable for the field of neuroscience, I think that some claims regarding the underlying theory are not sufficiently supported. Most importantly, it is not clear why the maximisation of the likelihood in Equation (6) would not be prone to overfitting. It is claimed that ""the low-rank structures on the coregionalization matrices dramatically ameliorate overfitting"", but it is not clear how this low rank is imposed.

Minor Comments:
The naming of the variables in the equations is not very consistent, and that could create confusion.
For example:
Equation (2): \tau is not introduced
line 124: C is not introduced (the number of channels, I presume)
y_w is presented as a function of either x or t (since we have time-series, having t everywhere would make sense)","The authors propose a factor analysis method called CSFA for modelling LFP data, which is a generative model with the specific assumption that factors, called Elecotomes, are sampled from Gaussian processes with cross-spectral mixture kernel. The generative model is straightforward use of CSM, and the estimation is apparently also a known form (resilient back prop; I never heard of it before). I do like the dCSFA formulation. The proposed method focuses on spectral power and phase relationship across regions, and is claimed to bring both better interpretability and higher predictive power. They also extend CSFA to be discriminative to side information such as genetic and behavioral data by incorporating logistic loss (or the likes of it). The authors applied the proposed method to synthetic and real mouse data, and compared it with PCA.

== issues ==
=== Page limit ===

This paper is 10 pages. (references start at page 8 and goes to page 10)

=== Section 2.1 ===
The authors claim that reduced dimensionality increases the power of hypothesis testing. In general, this is not true. Any dimensionality-reduction decision is made implicitly upon a hypothesis of dimensionality. The hypothesis testing performed after reduction is conditional on it, making the problem more subtle than presented.

=== Section 2.5 ===
lambda in Eq(7) is chosen based on cross-validation of the predictive accuracy. Why was it not chosen by cross-validation of the objective function itself? The authors should report its value along the accuracy in the result section. Would it be extremely large to make the first likelihood term ineffective? 

=== FA or PCA ===

Eq 3 shows that the noise is constant across the diagonal. The main difference between FA and PCA is allowing each dimension of signal to have different amount of noise. This noise formulation is closer to PCA, isn't it? But then I was confused reading sec 3.2. What's the covariance matrix of interest in sec 3.2? Over time? space? factors? frequencies?

=== ""Causal"" ===

The word 'causal' frequently appears in the first 2 pages of the text, setting high expectations. However, the method itself does not seem to have any explanatory power to discover causal structure or inherent causal hypothesis. Or is the reader supposed to get it from the phase plots?

=== comparison with baseline PCA in frequency domain ===

How was the window of data converted to the ""Fourier domain""? Was it simply a DFT or was a more sophisticated spectrum estimator used, e.g., multi-taper methods. I ask this because raw FFT/DFT is a bad estimator for the spectrum (very high variance regardless of the length of time window). Neuroscientists often use packages that support such estimation (e.g., MATLAB package chronux). It would not be fair if those are not taken into account.

=== averaging over 6 mice ===

Was the CSFA/dCSFA for Fig 3 computed to produce common factor across 6 mice? What justifies such approach?

== minor ==
 - L 230, 231: absolute KL div numbers are not useful. At least provide standard error of the estimator. (KL div is often very hard to estimate!!)
 - Subsection 2.1 & 2.3.1 reads like filler texts. (not much content)
 - L 124: $s\tilde B_q$? what's $s$ here?
 - L 228: to to
 - overall writing can be improved
"
Self-Normalizing Neural Networks,"Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter",https://proceedings.neurips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html,"I am putting ""accept"" because this paper already seems to be attracting a lot of attention in the DL community and I see no reason to squash it.

However I do have some reservations.  There is all this theory and a huge derivation, showing that under certain conditions this particular type of unit will lead to activation norms that don't blow up or get small.  Basically it seems to come from the property of SELUs, that on at least one side, as the input to the nonlinearity gets farther from zero, the output gets compressed, so making the input larger doesn't make the output larger.  Intuitively it seems to me that many nonlinearities should have this property, including sigmoid-type nonlinearities-- so what relation does this theory have to the experiments?  Also, there are ways to deal with this problem (batch-norm, layer-norm etc.),  so all this theory seems kind of orthogonal to the practical question of whether SELUs are actually better for deep learning.  And although you go to all this trouble to set a particular value of ""alpha"" in the SELU, I don't see that it's fundamentally any different from a regular ELU, only you'd need to select different variances of the parameters to get your theory to work.

As for the experimental results, they all seem to be on extremely small, unstructured datasets (""unstructured"" == there is no sequence structure, for instance).  It could very well be that ELUs (OK, SELUs) work well for this type of data.

Anyway, I'm recommending accept; my only peeve is that the huge derivation, and the amount of attention this paper seems to be attracting, are IMO a little out of proportion to the actual contribution of paper.

[Response to something mentioned in the rebuttal: your points about the derivative needing to be > 1 seem to assume that the parameters a Glorot-style covariance.  For derivatives less than one, you'd just have to have larger parameter values.]
","I appreciate the amount of work that enter in this paper. I am however not convinced about the effects of SELU activations in practice. I think the theory build behind them is nice, but, as it happens a lot of times, I’m not sure how much bearing it has in practice. I think the construction here is unstable because of the initial assumptions. Some more detailed comments:

1. Batch norm and other normalization for CNN and RNN vs MLPs. I am not convinced by the authors arguments. One can easily increase the minibatch size to to reduce variance, not? And then they can apply batch norm. Why is this not a solution?  

2. Based on the theory we know that, under some constraints on the weights, using SELU you have a fixed point attractor where the activation are normalized. How fast do you converge to this attractor? How many layers? I could easily imagine that you need to have 100 layers to see the activation converge to 0 mean and variance. I might be wrong, but I didn’t felt like we have  a bounds or any guarantee of seeing the activations be normalized after just 2 layers or something small like this. 

3. Also while the layers on top of such a deep MLP are normalized, the ones on the bottom are not (because you haven’t applied the map sufficiently many times). This means that while the gradients are well behaved through the top layers, they most likely are not on the bottom few layers and can still vanish/explode.

4. This might not be the authors fault, but I’m not familiar with any of the datasets (UCI are a bit toyish to be fair). The numbers don’t seem that impressive to me (e.g. table 2) though I’m not sure I’m reading the numbers correctly. Unfortunately this doesn’t help with my confidence in the approach.

Minor detail. Another approach for normalizing activation is the one given by https://arxiv.org/abs/1507.00210, which I feel will be more stable regardless to minibatch size. 

","
        The paper proposes a new (class of) activation function f to more efficiently train very deep feed-forward neural networks (networks with f are called SNNs). f is a combination of scaled ReLU (positive x) and an exponential (negative x). The authors argue that 1) SNNs converge towards normalized activation distributions 2) SGD is more stable as the SNN approx preserves variance from layer to layer. In fact, f is part of a family of activation functions, for which theoretical guarantees for fixed-point convergence exist. These functions are contraction mappings and characterized by mean/variance preservation across layers at the fixed point -- solving these constraints allows finding other ""self-normalizing"" f, in principle.

        Whether f converges to a fixed point, is sensitive to the choice of hyper-parameters: the authors demonstrate certain weight initializations and parameters settings that give the fixed-point behavior. Also, authors analyze the performance of SNNs with a modified version of dropout, which preserves mean/variance while using f.

        In support, authors provide theoretical proofs and experimental results on many datasets, although these vary in size (and quality?).

        As such, I think the paper warrants an accept -- although I have questions / concerns that I hope the authors can address.

        Questions / concerns:
        1. Most exps are run on small datasets. This is understandable since only FNNs are considered, but do we get similar results on networks that also include e.g. convolutional / recurrent layers? How can the theoretical analysis work / apply in that case (e.g. convolutions / pooling / memory cells are not always contraction mappings)?
        2. Ad point 1. What are the results for SNNs on MNIST / Cifar-10?
        3. The authors only considered layers without bias units. What happens when these are included? (This likely make it harder to preserve domains, for instance)
        4. During training, the mean/var of the weights do change, and are not guaranteed to be in a basin of attraction (see Thm 1) anymore. Why is there still convergence to a fixed-point, even though no e.g. explicit weight clipping / normalization is applied (to make Thm 1 apply)?
        5. How does this analysis interact with different optimizers? What is the difference between using momentum-based / Adagrad etc for SNNs?
        6. Hyper-parameters of f need to be set precisely, otherwise f is not domain-preserving / contraction mapping anymore. How sensitive is the model to this choice?
        7. It is unclear how the fixed-point analysis theoretically relates to the generalization performance of the found solution. Is there a theoretical connection?
      "
Fast amortized inference of neural activity from calcium imaging data with variational autoencoders,"Artur Speiser, Jinyao Yan, Evan W. Archer, Lars Buesing, Srinivas C. Turaga, Jakob H. Macke",https://proceedings.neurips.cc/paper/2017/hash/5d6646aad9bcc0be55b2c82f69750387-Abstract.html,"This paper presents a variational auto-encoder-style method for extracting spike-trains from two-photon fluorescence microscopy time traces. This is a task that scientists are interested in for the purposes of interpreting one- two- or three- photon microscopy data, which are becoming a staple method in neuroscience. The authors introduce 3 different spike-to-fluorescence models and use one of two neural networks to create an decoder to recover the (discritized) spike trains from the fluorescence. Overall, the method does show some promise. That said, my biggest concern with this work lies with the characterization of the method. Deep-networks just like any other algorithm, need to have an understanding of their trade-offs and limitations. It is especially important in an applications that will be used to inform scientific conclusion to understand aspects of the method such as the the bias and sensitivity to model mis-match. Unfortunately, I feel that the results section fall short of such a characterization and additional simulations or theory needs to be established to instill confidence in this method. I have detailed below specific comments that I feel the authors should address to improve the manuscript.

Comments:

1) Is this problem even solvable in general? In the linear case (the first, most basic model) it seems that spikes can be inferred, however the nonlinearities used in the following models can make spikes indistinguishable without adequate time between spikes. Is there a limit to the inter-spike times that this method can distinguish in the non-linear case? Is this  a reasonable limit for neural data? 

2) In a related note, all the data (and simulations) in this work use 60Hz sampling rates for the fluorescence traces. Much of the fluorescence calcium literature (especially 2-photon imaging, which seems to be becoming dominant) sampling rates as low as 10-15Hz are not unheard of. How does this method perform when the sampling rates fall to 30Hz or 10Hz, when spikes may truly be indistinguishable? Additionally, to have the discrete spike-train output retain a sampling rate on the order of a refractory period, would the proposed networks have an a-symmetry between the input and output rates, or would the network need to be changed to allow for multiple spikes per bins?

3) On line 66, the authors claim to use a ""powerful recurrent neural network"" model. I assume the authors don't mean 'powerful' in the statistical testing sense, so such embellishment is unnecessary. 

4) The authors often claim that the network learned in one sense can generalize well to other settings (e.g. line 61). All the testing and training were done with i.i.d. spikes constant rates chosen from one of three values. I did not see in the results any testing from the network trained with this constrained data-set on fluorescence traces generated from more complex spike-rates. Do the authors have evidence of this generalization, or is this a statement provided based on the use of VAEs in other contexts? If it is the former, the authors should provide these results to validate their claim. If it is the latter, the authors should verify that this generalization holds true for their application, as properties of VAEs do not necessarily translate across domains. 

5) The authors present three different models with various levels of complexity, however I do not see any results as to how the proposed network inference methodology changes with respect to each of these networks. It would be informative to see how the results, say on a single V1 trace, change with respect to the model used to train the network. 

6) Related to the previous point, sometimes it is unclear which model is being used in which networks for the various results (e.g. what model is used for the V1 results?).

7) In Figure 3, the reconstruction base on the true spike train using (was this using the SCF or SCDF model?) is not as accurate as the results of the spikes inferred using the network trained on that model. How is this possible? This discrepancy warrants at least some discussion. Clearly there is a model-mismatch of some kind. Is the network's ability to achieve better prediction mean that the method has a built-in bias in the estimated spikes that stems from the model mismatch? This seems like an important point that is glossed over. A systematic bias in the inferred spikes can create errors in scientific analyses applied to the resulting spike trains. 

8) The work ""amortize"" is useless in the context of machine learning and only serves to further obfuscate our field further with respect to outside audiences that may seek to use these methods. I very highly recommend omitting this word from the paper. 

2) Acronyms should be defined where first used (e.g. in all the sub-sub-section titles for the various models)

3) Most of the figures could do with bigger font sizes
","The paper described a variational autoencoder (VAE) framework for spike time inference from calcium imaging data. The authors propose 2 recognition models, one with a recurrent network component that accounts for temporal correlations in spike times, and another that is purely feed-forward but that nonetheless accounts temporal correlations in the calcium signal by convolving over a window.  The authors also use a principled model for the generative model of the calcium imaging data, conditioned on spike times.

The premise of the paper is summarized in lines 60-61, “Once trained to perform spike inference on one dataset, the recognition model can efficiently generalize to other similar datasets without retraining.” 

The written quality of the paper is mostly exceptional with complex ideas being described very fluidly with the appropriate degree of background and appropriate references to relevant literature. The idea is appropriately novel and the authors are using state of the art inference and modeling approaches.

I would have liked to score this paper higher as I appreciate the idea, the written quality, and the technical quality of the paper. However, there is at least one very troubling element to this paper.  Specifically, it is not clear precisely what the authors are proposing practitioners should do with this method.  Should a scientist go out and find some instantiation of this model and use it directly on their data without retraining for their specific setup? Should they not retrain their network for each new dataset, assuming that each new dataset will have a different class of cell being imaged and a different version of GcAMP? Wouldn’t the model have to be retrained when the training set has a different dynamic range of firing rates than the test set? It is difficult to believe, and the authors have not demonstrated, that the method is transferrable across datasets, only that it has good predictive performance on a holdout set.  This is not to say that the method is worthless.  I see clear value in the method. I only think that the “amortized” part of this method is unsubstantiated.

I also have two complaints regarding specific points in the paper. For one, the authors propose four different generative models (equations (2)-(5)) but appear only to have used one of them in their simulation and real data experiments.

Also, Figure 3 indicates that the prediction from the inferred spike train is considerably better than the prediction from the true spike train, especially at high spike rates, suggesting some kind of bias in the spike inference. Could the authors address this?
","This paper describes an innovative DNN-based approach for inferring spikes from calcium imaging measurements. This is an extremely important problem, as calcium imaging has rapidly become one of the standard tools for assessing activity in large neural populations. While a number of approaches exist to solve the problem, mostly based on generative models, the problem is far from solved (see e.g. recent quantitative evaluation by ref. 15 in the paper). The authors propose a creative solution based on a combination of a generative model and a recognition model based on convolutional and recurrent neural network architectures. Conceptually, this is a very strong contribution and their quantitative evaluation indicates that their approach achieves state-of-the-art performance on simulated data and a small set of actual recordings. The authors also outline interesting future possibilities for separating backpropagating spikes and dendritic signals from one another using the model.

I only have a few minor comments:
The authors understate the amount of data available on crcns.org – by now there are additional datasets. I think it is fine for this paper to focus on one of them, the relatively straightforward case of recordings from individual neurons at large magnification. How performance transfers to the case of population recordings is a different question. The authors should briefly note that somewhere.
I am not sure I understand the setup of the simulations described in lines 2017ff – if you train your model on 50 cells from one model and evaluate it one 30 others simulated from the same model, how is it surprising that performance transfers? Rather, why can it even be a little lower? Is there variability in the model parameters between the cells? I feel like I am missing something here.

Table 1 should note that values are correlation coefficients. 

Figure 4 could be cleaned up – e.g. for placement of the legends and maybe avoiding bar plots in B.
"
Asynchronous Parallel Coordinate Minimization for MAP Inference,"Ofer Meshi, Alexander Schwing",https://proceedings.neurips.cc/paper/2017/hash/5dc126b503e374b0e08231344a7f493f-Abstract.html,"Summary:
This paper proposes an asynchronous parallel approximate algorithm for MAP inference in graphical models represented as factor graphs. The proposed method is based on dual decomposition which breaks the model into overlapping pieces and adds penalty terms to enforce agreement between the overlapping portions. Whereas, HOGWILD performs asynchronous gradient updates at each factor, the proposed method performs full coordinate ascent at each iteration. The main concern is that updates based on stale values will be invalid, however, the authors show results that bound expected errors of this type. The authors also provide some methods for adaptively choosing the number of worker nodes to further minimize this error.

Overall, I found the paper reasonably clear and the methods well justified. Conceptually, the difference between the proposed method and HOGWILD is relatively small, but the analysis is well done and the experiments (which I found well constructed and convincing) demonstrate that this change can lead to large performance gains.

Comments:

- I found portions of the notation to be a bit unclear. I think that the authors could spend a bit more time introducing and describing equation (2) which would seem to be the crux of the method. I suggest adding a few steps of the derivation of (2) as it is not that difficult to show and would make the paper much more accessible. Barring that, explaining what is meant by a ""containment relationship"" and the intuition behind the Lagrange multipliers (which are just called ""agreement constraints"") would help.

- How is the MAP assignment recovered from a smoothed objective? (I would add this step to the text as it is not obvious)

- The authors should either show the proof that the updates in (4) are the coordinate maximizers or cite the specific source that proves this.

- I would consider making it more clear that the error considered in section 4 is the only type of possible error accrued by the parallelism. A simple statement such as ""If there is no delay, then the algorithm is performing exact coordinate ascent"" near the beginning of this section would clear this up.","SUMMARY:
========
The authors propose a parallel MAP inference algorithm based on a dual decomposition of the MAP objective.  Updates are performed without synchronization and locally enforce consistency between overlapping regions.  A convergence analysis establishes a linear rate of convergence and experiments are performed on synthetic MRFs and image disparity estimation.

PROS:
=====
The paper is well-rounded and provides algorithmic development, theoretical analysis in terms of convergence rate results, and experimental validation.  Clarity is good overall and the work is interesting, novel, and well-motivated.

CONS:
=====
Experimental validation in the current paper is inadequate in several respects.  First, since the approach is posed as a general MAP inference it would be ideal to see validation in more than one synthetic and one real world model (modulo variations in network topology).  Second, the size of problem instances is much too small to validate a ""large-scale"" inference algorithm.  Indeed, as the authors note (L:20-21) a motivation of the proposed approach is that ""modern applications give rise to very large instances"".  To be clear, the ""size"" of instances in this setting is the number of regions in Eq. (1).  The largest example is on image disparity, which contains less than 27K regions.  By contrast, the main comparison, HOGWILD, was originally validated on three standard ""real world"" instances with a number of objective terms reaching into the millions.  Note that even single-threaded inference converges within a few hundred seconds in most cases.  Finally, the authors do not state which disparity estimation dataset and model are used.  This reviewer assumes the standard Middlebury dataset it used, but the reported number of regions is much smaller than the typical image size in that dataset, please clarify.

Results of the convergence analysis are not sufficient to justify the lack of experimental validation.  In particular, Theorem 1 provides a linear rate of convergence, but this does not establish global linear rate of convergence as the bound is only valid away from a stationary point (e.g. when the gradient norm is above threshold).  It is also unclear to what extent assumptions of the convergence rate can be evaluated since they must be such that the delay rate, which is unknown and not controlled, is bounded above as a function of c and M.  As a secondary issue, this reviewer is unclear about the sequential results.  Specifically, it is not clear why the upper bound in Prop. 2 is necessary when an exact calculation in Prop. 1 has been established, and as the authors point out this bound can be quite loose.  Also, it is not clear what the authors mean when they state Prop. 2 accounts for the sparsity of updates.  Please clarify.

"
Inductive Representation Learning on Large Graphs,"Will Hamilton, Zhitao Ying, Jure Leskovec",https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html,"The authors introduce GraphSAGE, an inductive learning representation learning method for graph-structured data.  Unlike previous transductive methods, GraphSAGE is able to generalize the representation to previously unseen nodes.  The representation is learned through a recursive process that samples from a node's neighborhood and aggregates the results.  GraphSAGE outperforms other popular embedding techniques at three node classification tasks.

Quality:
The quality of the paper is very high.  The framework has several attractive qualities: a representation that is invariant with respect to node index, strong performance at three inductive node classification tasks, and fast training and inference in practice.  The authors include code that they intend to release to the public, which is likely to increase the impact of the work.

Clarity:
The paper is very well-written, well-organized, and enjoyable to read.  

Originality:
The idea is incremental but creative and useful.

Significance:
This work is likely to be impactful on the NIPS community due to the strong results as well as the fast and publicly available implementation.  The work builds on recent developments to offer a tangible improvement to the community.

Overall impression: A high-quality paper that is likely to be of interest to the NIPS community and impactful in the field.  Clear accept.","The paper proposes an approach for generating node embeddings in large graphs, with the important property that it is applicable to nodes previously not seen (for example, to a node that was created just recently in a dynamic graph). I am not an expert in this area, so I am afraid this review is not particularly well informed. In my view, the problem is important and the approach has many potential applications. The authors provide an extensive review of related work and adequately explain how their approach is different (and the differences are indeed important). Experimental evaluation is rigorous; the results are very positive, clearly demonstrating the utility of the approach. 	

Section 5 is a very nice addition to the paper. 

It was a welcome relief to see plots where the text is clearly legible (this was in fact the only paper in my batch that did so).","The manuscript deals with converting high-dimensional nodes in a graph into low dimensional representations, where it is not necessary to use the whole network (e.g. when nodes are added or removed) to calculate the low dimension representation of a given node. 

1. The introduction is not written very clearly, and it takes several ""read-through""s to understand the author's goal. For example, they use the term transductive without explaining what it means. It is left to the reader to deduce that they probably mean the setting presented in the first sentence of the previous paragraph.

2. The fact that the manuscript is far from the standard introduction, methods, results framework further detracts from its readability.

3. The relation between the proposed algorithm and the Weisfeiler-Lehman isomorphism test is very interesting and should be explained in more details, and possibly in the background, and not as an afterthought as it is now. It provides both motivation and an explanation as to why there is a chance that the unsupervised embedding may be useful in some context.

4. The extension to the minibatch setting is not clear enough. In a highly connected graph (such as a PPI), the neighborhood set of a small K may already be the whole network, making the task computationally expensive. 

"
Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs,"Rowan McAllister, Carl Edward Rasmussen",https://proceedings.neurips.cc/paper/2017/hash/5eac43aceba42c8757b54003a58277b5-Abstract.html,"This paper describes an extension to the PILCO algorithm (Probabilistic Inference and Learning for COntrol, a data-efficient reinforcement algorithm). The proposed algorithm applies a measurement filtering algorithm during the actual experiment and explicitly takes this measurement filtering algorithm into account during the policy learning step, which uses data from the experiment. This is an important practical extension addressing the fact that measurements are often very noisy. My intuitive explanation for this approach is that the proposed approach makes the overall feedback system more ""repeatable"" (noise is mostly filtered out) and therefore learning is faster (given that the filtering is effective, see last sentence of the conclusion).

The paper presents detailed mathematical derivations and strong simulation results that highlight the properties of the proposed algorithm. 

Comments/Suggestions: 
- A discussion of the limitations of the work would be useful. It is only briefly mentioned in the Future Work section. E.g., how do you handle unobserved states? I think in practice this is a real limitation as we often do not know some of the underlying dependencies (e.g., the behavior of a self-driving car depends on weather conditions, road conditions etc). 
- The paper needs more attention to detail. There are various typos and inconsistencies in terms of spelling throughout the paper. Reference style within the paper is inconsistent (using author names and not using it). Examples: L19 account FOR (instead of ""of""), L6/L29 observation space is spelled with and without hyphen, L68 into THE state, L68 reducing the learning task TO, L71 The original PILCO FRAMEWORK does, L92 over AN, L103 policy IS initialised, etc etc
- I would appreciate if you provide/emphasize more intuitive explanations regarding the practical implications/interpretations of the algorithms (similar to my intuitive explanation above?)
- Real experiments would be important to show the usefulness in practice. Should be part of future work?!
- References need to be cleaned up. The reference style is not consistent. Refs. [10] and [11] are identical.
 ","SUMMARY

In this paper, the authors propose to learn and plan with a model that represents uncertainty in a specific case of partial observability - that of noisy observations. Their method the analogue of a known model based method - PILCO - for this noisy observation case. It handles the noise by assuming a joint Gaussian distribution over the latent states and observations at each time step when making predictions (at execution time, observations are known and a filtered latent state is used). On the kind of dynamical system that these method are commonly evaluated on - the cart-pole swing-up in this case - the proposed methods seems to be better than relevant baselines at dealing with noise. 


TECHNICAL QUALITY

The proposed algorithm is supported by a derivation of the update equation and experimental validation on the cart-pole swing-up task. The derivation seems technically solid. I just wondered why in (5) the covariance between M_t|t-1 and Z_t is 0, since in Fig. 2 it seems that Z_t is a noisy version of B_t|t-1 of which M_t|t-1 is the mean. 

Although the experiments only consider a single system, the results seem quite convincing. The empirical loss per episode is significantly lower, and predictions of the cost per time step are much better than for the compared methods. I just wondered what is meant by the ""empirical loss distribution"", and whether the 100 independent runs that the error bars are based on are 100 evaluations of the same learned controller; or 100 independently trained controllers. In Figs. 7 and 8 I would be interested to see the n=0 case (or, if that causes numerical problems, n=0.01 or similar). 

In terms of related work, the authors mainly restrict themselves to discussing Bayesian model learning and Bayesian RL techniques. It might be interesting to also discuss other styles of policy search methods for the partially observable setting. 

It might be insightful to describe what about the current method limits us to the noisy observation case. If we observed positions but not velocities, coulnd't we filter that, too?


NOVELTY
I haven't seen earlier extensions of PILCO-type methods to the partially observable case. So I would consider the method quite novel.

SIGNIFICANCE AND RELEVANCE
Noisy dynamical systems occur in many (potential) application domain of reinforcement learning (robotics, smart grid, autonomous driving), so, this is a relevant research topic for the RL community. On the investigated system, the improvement relative to the state of the art seems quite significant, however, the method is only evaluated on a single system. 

CLARITY

The paper is mostly well-written. However, in some part the clarity could be improved. For example, the paper does not mention explicitly they assume the reward function to be known. The policy improvement step (8 in the algorithm) is not explained at all. For readers already familiar with PILCO this is quite obvious but it would be good to give a short introduction from the others. 

The technical section in 4.1 - 4.2 is very dense. I understand space is rather limited but it could be good to explain this part in more detail in the appendix. For example, the paper states V=E[V'] is assumed fixed, but doesn't state how this fixed value is computed or set.

Figures 1 and 2 are presented as analogues, yet the latent state X is not shown in the right image. It would be easier to understand the relationship if the underlying latent process was shown in both images. 

There are quite many grammar issues. I'll point some out under minor comments. 

MINOR COMMENTS

Grammar - please rephrase sentences in lines 7, 52, 86-87, 134,209,250, 283, 299-300
Contractions and abbreviations - please write out (doesn't, etc.) 
Some sentence start with note but don't seem grammatical. I suggest writing ""Note that, ..."" or similar.
Line 55: Greater -> Larger
Line 65: Seems like the author name shouldn't appear here
Line 99: Less -> fewer
line 112: ""Overfitting leads to bias"". Overcapacity generally causes a high variance, not necessarily bias. Or is the bias here a result of optimizing the policy on the erroneous model? (similar in line 256)
line 118: I guess k(x_i, x_j) should have a superscript a.
line 118: are the phi_a additional hyperparameters? How are they set?
line 147, 148 ""detailed section"" -> detailed in Section""
line 162: it's not clear to me what you mean by ""hierarchically random""
line 174: it seems you switched x and z in p(x)=N(z, Sigma). 
line 177: Gaussian conditioning is not a product.
line 181: ""the distribution f(b)"". This seems a bit odd. Maybe assign f(b) to a random variable?
line 189: not sure what you mean by ""in principle"".
- you might want to introduce notation \mathbb{C} and \mathbb{V}
line 211: ""a mixture of basis functions"" -> do you mean it is a linear combination of basis functions?
line 245: ""physically"" -> ""to physically""
line 273  ""collect"" -> to collect
line 274: trails -> trials
line 283: this seems a strong claim, you might want to rephrase (fPILCO doesn't predict how well other methods will do as is implied here )
line 290 - you might want to choose another symbol for noise factor than n (often number of data points).
297 : data - > data points ?
315: Give greater performance than otherwise -> improved performance with respect to the baselines ?
References: I would consistently use either full names or initials. Some references like 11 are quite messy (mentioning the year 3 times in this case). The words Gaussian and POMDPs in paper titles should be capitalized correctly.","** Summary of Paper
The paper proposes an extension to PILCO to improve performance in the presence of noisy observations. 
The reasoning is that when observations are noisy, the policy should act on the filtered state, not the observations directly.
The main contribution of the paper is to not only follow this reasoning during execution of the policy, but also during the prediction/policy evaluation phase of the PILCO algorithm. They argue that this better adapts the policy to acting on the filtered state, allowing it to take more decisive actions.

The main challenge that they tackle lies in the fact that they replace observations (i.e. point values) with the filtering distribution. This also means that during prediction, they not only have to predict one distribution over possible future states (due to the probabilistic forward model), but a distribution over possible future noisy observations and the corresponding distribution over future filtering distributions.

They evaluate their resulting algorithm on the cartpole swing-up environment.

I would recommend this paper for acceptance. 
It is for the most part well written (see below for details) and proposes a potentially useful extension to a widely used algorithm, even though the set of problems it is applicable to is restricted. 
While the paper restricts itself to only one environment for experiments, it has two different setups to better show the characteristics of its method.
This, together with a clear discussion of strengths and weaknesses of the method, makes it easy to understand whether the proposed algorithm can be useful for one's problem setting.
My reasons for not giving a higher rating are
1. The restricted applicability
3. I'm not entirely sure about the fit to NIPS in terms of originality. On the one hand it is 'only' as an extension to an existing algorithm, but on the other hand this extension is non-trivial and requires a considerable amount of thought.

** Quality
The paper is technically sound with both a theoretical analysis and convincing experimental results. Strengths and also restrictions of the algorithm are clearly stated and discussed, giving a good understanding of when the algorithm could be useful. 

** Clarity
The overall quality of the paper is good. For the most part it is well written and easy to follow due to good explanations.
However, I think section 4.2 ('Filtered-System Prediction Phase') could be a bit more detailed. While it is possible to understand the explanation, I think it would be easier to do so if it were more explicit in terms of how various random variables influence each other.
Some examples/ideas (please correct me if necessary):
- In equation (5), I assume \mu^m_{t|t-1} is as known from the previous iteration step?
- Also in equation (5), is mu^x_t computed according to the 'standard' PILCO algorithm? The only other mentioning of mu^x is somewhere in equation (1) without explicitly discussing it. Also, I guess instead of /tilde{mu}^x_{t-1} (like in PILCO), one computes it based on mu^{\tilde{m}}?

Overall I just think that this section should be at least slightly longer and more detailed as it is describing one of the two core parts of the new method. 

The following is a very minor point as it's fairly clear what is meant:
I am unsure about the notation of the believe b (for example in line 174) as b is sometimes used as if it is a random variable (e.g. line 156) and sometimes as a distribution (e.g. line 175). I think it boils down to (for me), that to me the equation in line 174, namely p(x|z,u) \sim N(m,V), should use an equal sign instead of \sim. A clearer notation here might also resolve that between equations (21) and (22) p(M) and M are used interchangeably?

A few small typos I spotted:
- Line 7: There's 'with' missing
- Line 19: of => for (I think)
- Line 127: and => in
- Line 147: detailed _in_ section 4.1

** Originality
The proposed algorithm is an extension to a widely used algorithm, building upon several earlier works with a similar goal. It does not propose entirely new concepts (i.e. both filtering and PILCO are widely known) put combines them in a new and non-trivial way. 

** Significance
The proposed algorithm is useful in a restricted set of problem that are suitable for PILCO but additionally have (close to?) gaussian observation noise. However, given that PILCO is widely used and quite powerful and that the authors of this paper clearly state the strengths and weaknesses of their approach, I believe this to be a valuable contribution. 

"
Coded Distributed Computing for Inverse Problems,"Yaoqing Yang, Pulkit Grover, Soummya Kar",https://proceedings.neurips.cc/paper/2017/hash/5ef0b4eba35ab2d6180b0bca7e46b6f9-Abstract.html,"The paper considers a distributed computing scenario where a bunch of processors have different delays in computing. A job (here finding a solution of a system of linear equations) is distributed among the processors. After a certain deadline the output of the processors are combined and the result is produced.

Previous literature mainly neglect the slower processors and work with the best k out of n processors' outputs. Usually an MDS code is used to distribute the task among the processors. I understand the main contribution is to include the stragglers computations and not treat them as mere erasures.

I have few critical comments regarding the work.

1. Correctness of results:  I think there are serious problems in the assumptions (see line 116-117). If x_i s are all iid, it is not necessary that r_i s are all iid as well. This only happens when M is square invertible matrix. For example, if a row of M is zero then the corresponding entry of r_i is going to be always zero. The main results all depend on this assumption, which makes me doubtful about the veracity of results.

2. Assumptions without justification: From the introduction it seems like the main result is contained in Theorem 4.4. This theorem is true under two very specific assumptions.  One, the solutions of a systems of linear equations are all iid variables, and the number of iterations of each processor within a deadline are all iid random variables. These two seem like very stringent condition and I see no reason for these to be true in real systems. The authors also do not provide any motivation for these assumptions to be true either. Given this, I am unable to judge whether the theoretical results are of any value here.

Another key condition for the results to be true is n-k = o(\sqrt(n). Again it was not explained why this should be the case even asymptotically. 

Also, I do not understand the implication of the random variable C(l) satisfying the ratio of variance and mean-square to be large. Why would this assumption make any sense?

Overall lack of motivations for these assumptions seem concerning.

3. Unfair comparison:  The replication scheme, that the paper compare their coding scheme with, has several problems. And I do not think that it is a good comparison. The assumption is that n -k is very small, i.e., o(\sqrt{n}). So computation of only o(\sqrt{n}) elements are computed twice. The rest of the processors are uncoded. This is a very specific coding scheme, and it is obvious that the Fourier matrix based scheme (which is an MDS code) will beat this scheme.

In the introduction it was claimed that: “To compare with the existing coding technique in [6], ..” But the theoretical comparison to the method of [6] is missing. 


4. Novelty: I do not think the presented scheme is new. It is the same MDS code based scheme that was proposed by Lee et al (in ref [6]). 

Overall the same encoding method has been used in both [6] and in this paper. The only difference is that the results of the stragglers are also used in this paper while decoding - so it is expected to get a better error rate. But in general, the claim of introduction, the prevention of ill-conditioning of MDS matrices in the erasure channel, remains unmitigated. It is just that a particular problem is considered in the paper.


5. Experiments: 9. The figures are not properly labeled. In the third plot of fig 2, ref [12] is mentioned, which should be [6] (I think). This is quite confusing.

I am further skeptical about this plot. At certain value of the T_deadline, the method of [6], should result in absolute 0 error. But this is not observed in the plot. It is also not clear why in the extension of the coded method the mean square error would increase.

6. More minor comments:
 The authors say “iterative methods for linear inverse problem” as if it is a very standard problem: however it is not to me. A more understandable term would be `finding solutions of a linear system'. So for quite long time I could not figure out what the exact problem they are trying to solve. This create difficulty reading the entirety of the introduction/abstract.

\rho has been used as spectral radius, and also in the heavy tail distributions.

=============
After the rebuttal:
I am able to clarify my confusion about independence of the random variables. So I do not have about the concern with correctness anymore. The point that I am still worried about is the unfair comparison with repetition codes! I do not think n-k =o(n) is the correct regime to use repetition code.

It seems to me from the rebuttal is, if T_deadline is large enough, then coding does not help (which makes sense). Is it clear what is the crossover point of T_deadline?  

I am satisfied with the rebuttal in other points. 


 ","The paper presents an error correcting based technique to speedup iterative linear solvers. The authors follow a new line of work that uses ideas from coding theory to alleviate the effect of straggler (eg slower than average) nodes in distributed computation, where in many works so far the computational result of straggler nodes is considered as erased information. The main idea of coding in the context of distributed computation is the following (a high-level, 3 node example): if a worker is computing task1, a second worker is computing task2, and a third is computing  a “linear combination” of task1+task2, then from any 2 workers a master node could recover  both computational tasks. This idea can both provably and in practice improve the performance of distributed algorithms for simple linear problems like matrix multiplication, or linear regression.

In this work, the authors present an interesting new idea: for linear iterative solvers where we are optimizing with respect to some accuracy, instead of treating straggler nodes as erasures, we could still use some partial results from them. The algorithmic flow is as follow: the tasks are encoded to add redundancy, and the workers start their local computation. Then, the master node sets a deadline, after which it collects any results computed so far by the workers. Unlike, the “erasures” case, here partial results are used from all workers, and this non-discarded computational effort can provably help.

The authors compare the coding technique with no coding and task replication, and establish that in theory when coding is used, then the accuracy of the distributed iterative linear solvers can be unboundedly better, than both replication or uncoded computation.

The authors finally present experiments on HT-Condor with 120 worker nodes, where the coded scheme outperforms both the uncoded and replication schemes significantly. My only concern with the experiments is that the size of the data set to be parallelized is actually small enough to fit in a single machine, so it is unclear if any speedups can be observed in this case. It would be interesting to see how a larger data set would perform for these problems.

Overall, the paper is well-written, it presents an interesting idea and analysis, and has novel and technically sound results.  On the experimental side it would be interesting to see how things scale beyond medium sized data sets, and for the case where faster machines than Condor-HT (eg amazon ec2 gpu instances) are used.","The authors propose a new coded computation scheme for solving “many” linear inverse problems in parallel. While the existing works on coded computation mostly view the stragglers as ‘erasures’, the authors view the outputs of each processor, which are intermediate results of the iterative algorithms, as ‘noisy’ computation results. Under this new view, the authors propose a new encoding/decoding scheme, analyze the performance of their scheme, and compare it with other schemes based on the erasure view.

The paper is concrete, and I believe that this paper brings a new (and useful) angle to the community. I have two minor concerns. One concern is that some parts of the paper are hard to follow, which I think make the paper hard to follow and grasp the key idea of it. Further, one key reference is missing: “Anytime Coding for Distributed Computation” by Ferdinand and Draper, Allerton 2016. To the best of my knowledge, although they are still under the umbrella of the erasure view, they are the first ones who study the performance of coded computation in terms of computing approximate computation results as a function of “stopping time”. "
"Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions",Ryan J. Tibshirani,https://proceedings.neurips.cc/paper/2017/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html,"This paper studies connections/equivalences between Dykstra's algorithm, coordinate descent (à la Gauss-Seidel block alternating minimization). Many of these equivalences were already known 
      (indeed essentially in the optimization literature as stated by the author). The authors investigates this for the soft feasibility (best approximation) problem and block-seperable regularized regression,
      where the regularizers are positively homogeneous (i.e. supports of closed convex sets containing the origin). 
      The author claims that this is the first work to investigate this for the casen where the design is not unitary.
      Actually, the extension to arbitrary design is very straightforward (almost trivial) through Fenchel-Rockafellar duality.
      In fact, positive homogeneity is not even needed as Dykstra's algorithm has been extended to tje proximal setting (beyond indicator functions of closed convex sets). 
      Having said this, I have several concerns on this paper.
      + My first concern pertains to the actual novelty of the work, which I beleive is quite limited. The manuscript has  a flavour of a review paper with some incremental extensions.
      + The paper contains some important inaccuracies that should be addressed.
      Other detailed comments are as follows:
      + (1): intersection of sets should be non-empty. Otherwise many of the statements made do not rigorously hold.
      + What the author is calling seminorms are actually support functions (symmetry is NOT needed), or equivalently gauges of polar sets.
      + Extension to the non-euclidean case: several key details are missing for all this to make sense. In particular the notion of a Legendre function. Qualification conditions for strong duality to hold are not stated either.
","The paper studies the relationship between different splitting algorithms like Dykstra, CD and ADMM. They prove an equivalence between Dykstra and CD that (to the best of my knowledge) is novel in its general form.

Clarity: the paper is beautifully written, it is a pleasure to read.

Content: While not particularly surprising (this equivalence was already known for a more restricted class of problems, which is acknowledged in the paper), the paper make interesting contributions besides this connection, such as:

    * Adaptation of Dykstra convergence results (Theorem 2 and 3) to coordinate-descent problems (Lasso).
    * Parallel CD algorithm.
    * An extension of this connection to non-quadratic loss functions via Bregman divergences.

The paper is correct as far as I can tell, although I only skimmed through the proofs in the appendix.

The paper does not contain any experiments, which I didn't see as an issue given that most of the discussed algorithms are well established. However, I would have given a higher rating if it had convincing experiments e.g., on the parallel-Dykstra-CD algorithm.

I would be very interested to know from the authors if they are aware of the Dykstra-like proximal splitting method described in [X1] and whether there would be any benefit or any generalization that could come from using this more general algorithm instead of the projection-only Dykstra algorithm that the authors mention.

[X1] Combettes, Patrick L., and Jean-Christophe Pesquet. “Proximal splitting methods in signal processing.” https://arxiv.org/pdf/0912.3522.pdf , Algorithm 7.6 

","This is a interesting and well written paper. It provides important insights into coordinate descent and its connections with Dykstra's algorithm. After reading this paper, I agree with the author's sentiment that ""the connection between Dykstra's algorithm and coordinate descent is not well-known enough and should be better explored"". This paper makes a nice contribution, tying together results sprinkled throughout the optimization literature, and also presents several new additions, putting them together in one cohesive piece of work. I was pleased that the authors included proofs in the supplement, and it was also great that there was a small numerical experiment (again in the supplement)."
Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems,"Ingmar Kanitscheider, Ila Fiete",https://proceedings.neurips.cc/paper/2017/hash/5f14615696649541a025d3d0f8e0447f-Abstract.html,"
			An LSTM recurrent neural network is trained to perform simultaneous localization and mapping (SLAM) tasks,
			given noisy odometry data and occasional input from contacts with walls.

			The paper is well written and I find it interesting to study recurrent neural network solutions of SLAM.
			
			But the idea of using LSTM-RNNs for SLAM problems is not new (as the authors mention in the discussion, line 256)
			and the results do not really surprise me. Given the amount of training trials, the network has presumably seen
			sufficiently many trajectories (and variations of the environment) to perform well on most test examples
			(and if the difference between training and testing error is too large, follow the standard recipes 
			and tune the number of hidden neurons and the number of training trials). A more interesting result would be
			it the LSTM-RNN would generalize to out of distribution samples, i.e. if its performance would be comparable 
			to that of a PF SLAM in totally different environments than the ones seen during training.
			
			It is also not clear to me, what we learn about neural recordings. Given that the task of the specialized networks is
			to output the position (independent of head direction) and the identity of the current environment (independent of current location)
			it does not suprise to see head-direction independent tuning and cells with low spatial selectivity.

			More comments and questions:
			1. line 5: ""We generate the first neural solution to the SLAM problem"": This sounds too strong given papers like
			   Blum and Abbott 1996 http://dx.doi.org/10.1162/neco.1996.8.1.85
			   Foster et al. 2000 dx.doi.org/10.1002/(SICI)1098-1063(2000)10:1 < 1::AID-HIPO1 > 3.0.CO;2-1
			   or the one already cited by Förster et al. 2007
			2. line 46-48: I don't see why the network must have created a map. Isn't it sufficient to extract 
			   some features that identify each environment?
			3. The font size in the figures is a bit too small for my taste; especially Figure 3.
			4. Caption 4D: I guess it should read ""specialized (top) and generalized (bottom)""
			5. line 208: How is the spatial selectivity measured?
			6. Figure 5B: How is the time in seconds related to the (discrete) simulation steps?
			7. line 242: What is the fractal manifold dimension?
			8. Suppl. line 19: I was curious how rare the events are actually. 
			   Is it something like on average every 10th step for the small environments (task 2 and 3)
			   and every 30th step for the larger environments?
			9. Suppl. line 101: Why do the tiles extend up to 35 cm? 
			   Isn't it extremely unlikely to trigger a boundary event with this distance?

======

I read the author’s rebuttal and my colleagues’ review. My main concerns remain:

1. A powerfull off-the-shelf function approximator (LSTM) is applied to artificial data. This is a setting where one can always achieve a small test error. One can play with the difficulty of the task, the amount of data and the meta-parameters of the function approximator. It would be a different thing if tested on completely different environments where the network would truely have to learn to solve the SLAM problem independently of the environment.

2. As mentioned above, given the tasks it does not surprise to see head-direction independent tuning and cells with low spatial selectivity. Any model that solves these tasks has to represent head-direction independent and position independent information. It is not clear to me what we learn on top of this with this specific model.

		","The authors train a recurrent neural network with LSTMs to perform navigation tasks. The network is provided with the speed and heading direction of the simulated rat, along with boundary events. Using this information, the network has to estimate the current position of the rat, and (in some cases) the identity of the environment. The network is able to perform the tasks, with an accuracy similar to that of a particle filter. Neurons in the network exhibit spatial selectivity and remapping.
This is an important and impressive work. 
Unlike vision, there are very few works using end to end learning of navigation tasks. This is despite the fact that there is ample biological information on the neural activity during navigation, and it has many real world applications (e.g. robotics).
The framework used allows the authors to study both the recognition of an environment and navigation within it. This is particularly important when contrasting with the usual neuroscience experiments in which the environments are very limited. Real world environments are, of course, much more variable, but this framework can chart a path towards their study.
Limitations and comments:
1.	The trajectory is randomly generated, as opposed to an actual rat that does not necessarily care about its absolute position, but rather on where it has to go.
2.	The environments are different, but not very different. The failure of the single hypothesis filter indicates that the variability is enough to affect navigation. But it is unclear whether the network is able to operate when the environments vary greatly in size (implying multiple timescales of path integration).
3.	Fig 3 is impressive, but NN is comparable to 4000, not 10000. Does the PF SLAM have a similar demand (50% of the loss) on classification as the NN? If not, then perhaps perfect classification is not needed to localize well because some environments are similar
4.	Figure 4D legend – should be top/bottom, not left/right
5.	Figure 5A – marking the diagonal would greatly assist here
6.	Figure 2,3 – the choice of colors is not very good. It’s hard to discern the light red.
","In this study, the authors trained recurrent neural network models with LSTM units to solve a well studied problem, namely Simultaneous Location and Mapping (SLAM). The authors trained the network to solve several different version of the problems, and made some qualitative comparison to rodent physiology regard spatial navigation behavior. 

In general, I think the authors' approach follows an interesting idea, namely using recurrent network to study the cognitive functions of the brain. This general idea has recently started to attract substantial attentions in computational neuroscience. 

The authors tested several version of the localization tasks, which is extensive. And the attempt to compare the model performance to the neurophysiology should be appreciated. They also attempted to study the inner working of the training network by looking into the dimensionality of the network, which reveals a relatively low dimensional representation, but higher than the dimension of the physical space.

Concerns-

The algorithms used is standard in machine learning, thus my understanding is that the main contribution of the paper presumably should come from either solving the SLAM problem better or shed some light from the neuroscience perspective. However, I am not sure about either of them.

1) For solving the SLAM problem,  I don't see a comparison to the state-of-the-art algorithms in SLAM, so it is unclear where the performance of this algorithm stands. 
After reading through other reviewers' comments and the rebuttal, I share similar concerns with Reviewer 1- it is unclear whether the networks learn to solve the SLAM generically, and whether the network can perform well in completely different environments. The set-up the authors assumed seem to be restrictive, and it is unclear whether it can apply to realist SLAM problem. I agree with Reviewer 1 that this represents one important limitation of this study I initially overlooked, and it was not satisfyingly addressed in the rebuttal. 


2) From the neuroscience perspective, some of the comparison the authors made in Section 3.2 are potentially interesting. However, my main concern is that the ""neuron-like representations"" shown in Fig.4A,B are not really neuron-like according to the rodent physiology literature. In particular, the place fields shown in Fig4A and 4B do not resemble the place fields reported in rodent physiology, which typically have roughly circular symmetric shape. I'd certainly appreciate these results better if these spatial tuning resembles more to the known rodent physiology.  Can the authors show the place fields of all the neurons in the trained network, something like Fig 1 in Wilson & McNaughton (1993, Science)? That'd be helpful in judging the similarity of the representation in the trained network and the neural representation of location in rodents.

Also, in the rodent literature, the global remapping and rate remapping are often distinguished and the cause for these two types of remapping are not well-understood at this point. Here the authors only focus on global remapping. One potentially interesting question is whether the rate remapping also exist in the trained network. The paper would be stronger if the authors could shed some light on the mechanisms for these two types of remapping from their model. Just finding global remapping across different environment isn't that surprising in my view.

Related to Fig4E, it is known that neurons in hippocampus CA1 exhibit a heavy tail in their firing rate (e.g, discussed in ref[20]). Do the authors see similar distributions of the activity levels in the trained network?

3) The manuscript is likely to be stronger if the authors could emphasize either the performance of the network in performing SLAM or  explaining the neurophysiology. Right now, it feels that the paper puts about equal weights on both, but neither part is strong enough.

According to the introduction and abstract, it seems that the authors want to emphasize the similarity to the rodent physiology. In that regard, Fig. 2 and Fig. 3 are thus not particularly informative, unless the authors show the performance from the recurrent network is similar to the rodent (or maybe even humans, if possible) behavior in some interesting ways. To emphasize the relevance to neuroscience, it is useful to have a more extended and more thorough comparison to the hippocampus neurophysiology, though I am not sure if that's possible given the place fields of the trained network units do not resemble the neurophysiology that well, as I discussed earlier.

Alternatively, to emphasize the computational power of the RNN model in solving SLAM, it would be desirable to compare the RNN to some state-of-the-art algorithms. I don't think the particle filtering approach the authors implemented represents the state-of-the-art in solving SLAM. But I could be wrong on this, as I am not very familiar with that literature.



Finally, a somewhat minor point- 
the authors make the comparison on the capacity of the network to the multi-chart attractor model in the Discussion, but I am not sure how that represents a fair comparison. I'd think one LSTM unit is computational more powerful than a model neuron in a multi-chart attractor model. I'd like to understand why the authors think a comparison can be drawn here. 


"
SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud,"Zahra Ghodsi, Tianyu Gu, Siddharth Garg",https://proceedings.neurips.cc/paper/2017/hash/6048ff4e8cb07aa60b6777b6f7384d52-Abstract.html,"The authors introduce a new neural network model which enables a client to verify that its request to execute inference over a neural network has been done correctly by a server. SafetyNets provide the client a high-probability guarantee of detecting wrong/lazy executions by the server, and incurs only a low overhead for the server while significantly decreasing the client's computation compared to executing the network itself. SafetyNets come with severe restrictions on the network during inference, but experimentally these restrictions are shown to have little impact on network performance.

Quality: The experiments are convincing; the theory is an extension to the specific setting of neural networks of related work on interactive proofs.

Clarity: This paper is very clear and easy to understand.

Originality: The authors describe this as the first neural network model that provides (probabilistic) correctness guarantees. 

Significance: This paper appears to be an interesting contribution to the field of interactive proofs and distributed deep-learning.

Detailed comments:
- The experiments on MNIST and TIMIT show that using quadratic activation functions has a non-trivial, positive impact on training time and, at least for TIMIT, on test error. Have the authors evaluated their methods on more complex datasets such as CIFAR-10 (or even ImageNet?), and can they confirm that this behavior persists?

- How scalable is this approach to very large (wider and/or deeper) neural networks?

- Have the authors considered using regularization during training to force the weights and biases of the network to be closer to integers? It would be interesting to see whether this can further increase the neural network's final accuracy.

- This is perhaps beyond the scope of this work, but providing a measure of the impact of the transformation to a SafetyNet (converting weights to integers) would be a valuable contribution: i.e., can we guarantee that this transformation only impacts the inference by at most x%, depending on the weights and biases?

- SafetyNets do not provide any privacy guarantees; could they be merged with CryptoNets to provide simultaneously privacy and correctness guarantees?","This paper presents an algorithm to verify the integrity of a server that computes neural network predictions. The algorithm is based on the interactive proof scheme of Thaler. It converts the verification of a neural network to the problem of verifying the sum of polynomials, which can be carried out efficiently. The algorithm enables the user to move computation-intensive tasks to the server side, while maintaining reliability of the prediction.

Although the algorithmic idea of interactive proof is not new, this paper does a good job in adapting it to the task of neural network computing, which demonstrates a nice application of the technique. The empirical results are promising, showing that reliability can be achieved with little additional computation cost and little sacrifice of accuracy.

There are several limitations of the proposed algorithm. They have been discussed in the paper. The algorithm doesn't support non-polynomial activation functions, and doesn't support max pooling. However, the high-level idea is interesting enough that it may inspire a new direction of research. Hopefully these limitations can be overcome by future study.

I am curious about the accuracy of baseline predictors without scaling and rounding. It is worth comparing it with accuracy of the proposed algorithm in Table 1. 
","The submission is interested in allowing an entity (""the verifier"") to outsource the training of neural networks to a powerful but untrusted service provider (""the prover""), while obtaining a guarantee that that the prover correctly trained the neural network. Interactive proofs (IPs) are a kind of cryptographic protocol that provide such a guarantee.

The dominant cost in neural network training lies in evaluating the network on many different inputs (at least, this was my understanding -- the submission could be a bit clearer about this point for the benefit of readers who are not experts on how neural networks are trained in practice). Hence, the submission is focused on giving a very efficient IP for evaluating a neural network on many different inputs. 

The IP given in the submission applies to a rather restricted class C of neural networks: those that exclusively use quadratic activation functions and sum-pooling gates. It does not support popular activation functions like ReLU, sigmoid, or softmax, or max-pooling or stochastic pooling. The reason for this restriction is technical: existing IP techniques are well-suited mainly to ""low-degree"" operations, and quadratic activation functions and sum-pooling operations are low-degree.

Despite the fact that the IP applies to a rather restricted class of neural networks, the submission's experimental section states that this class of networks nonetheless achieves  state-of-the-art performance on a few benchmark learning tasks.

The submission represents a quite direct application of existing IP techniques (most notably, a highly efficient IP for matrix multiplication due to Thaler). The main technical contribution of the IP protocol lies in the observation that evaluating a neural network from the class C on many different inputs can be directly implemented via matrix multiplication (this observation exploits the linear nature of neurons), and squaring operations. This allows the authors to make use of the highly efficient IP for matrix multiplication due to Thaler. 

Without this observation, the resulting IP would be much more costly, especially for the prover, as Thaler's IP for matrix multiplication is far more efficient than general IPs for circuit-checking that have appeared in the literature.

In summary, I view the main contributions of the submission as two-fold. First, the authors observe that evaluating any neural network from a certain class (described above) on many different inputs can be reduced to operations for which highly efficient IPs are already known. Second, the submission shows experimentally that although this class is restrictive, it is still powerful enough to achieve state of the art performance on some learning tasks. 

Evaluation: Although the submission contains a careful and thorough application of interactive proof techniques, it has a few weaknesses that I think leave it below the bar for NIPS. First, as a rather direct application of known IP methods, the technical novelty is a bit low. Second, the class of networks supported is restrictive (though, again, the experiments demonstrate that the class performs quite well on some learning tasks).

Third, and perhaps most importantly, is an issue of motivation. The interactive proof in the submission allows the verifier to ensure that the neural network was correctly trained on a specific training set using a specific training algorithm. But people shouldn't care *how* the network was trained -- they should only care that the trained network makes accurate predictions (i.e., has low generalization error). Hence, there is an obvious alternative solution to the real problem that the submission does not mention or compare against: the prover can simply send the final trained-up network to the verifier. The verifier can check that this final network makes accurate predictions simply by evaluating it on a relatively small randomly chosen selection of labeled examples, and confirming that its error on these labeled examples is low. 

Based on the reported numbers for the verifier (i.e., an 8x-80x speedup for the verifier compared to doing the full training itself with no prover) this naive solution is in many cases likely to be more efficient for the verifier than the actual solution described in the paper."
Improved Graph Laplacian via Geometric Self-Consistency,"Dominique Joncas, Marina Meila, James McQueen",https://proceedings.neurips.cc/paper/2017/hash/619205da514e83f869515c782a328d3c-Abstract.html,"This paper proposes a method for improving the graph Laplacian by choosing the bandwidth parameter in the heat kernel for the neighborhood graph.  The method uses geometric consistency in the sense that the Riemannian metric estimated with the graph Laplacian should be close to another estimate of the Riemannian metric based on the estimation of the tangent space.  Some experiments with synthetic and real data demonstrate promising results of the proposed method. 

The proposed method for choosing bandwidth seems novel and interesting.  It is based on the very natural geometric idea for capturing the Riemannian geometry of the manifold.  There are, however, some concerns also as described below. 

- From the experimental results in Table 1, it is not very clear whether the proposed method outperforms the other methods.  In comparison with Rec, the advantage of GC are often minor, and GC^{-1} sometimes gives worse results.  More careful comparison including statistical significance is necessary.  

- The authors say that d=1 is a good choice in the method.  The arguments to support this claim is not convincing, however.  It is not clear whether Proposition 3.1 tells that d=1 is sufficient.  More detailed explanations and discussions would be preferable. 

- The paper is easy to read, but not necessarily well written.  There are many typos in the paper.  To list a few, 
-- l.36: methods are duplicated. 
-- eq.(2): it should be minus in the exponential. 
-- l. 179: on is duplicated. 
-- Caption of Figure 1: have are. 
","General comment:
The idea of optimizing epsilon parameter by comparing the metric induced from Laplacian and the geometry (tangent space) determined without epsilon, is interesting. The approach for realizing this idea is well thought-out.

Concerns:
The empirical result that the working dimension d' can be small, (even one-dimension is nice), is somewhat surprising.
The proposition 3.1 in this paper partly explain the reason, but it is not enough.
It is nice if the author(s) provide theoretical guarantee that considering small dimensional submanifold is enough.

The proof of Proposition 2.1, which gives a method to calculate cometric should be presented (in supplementary material).

eq.(2), the minus ""-"" is missing from exponential function.

The presentation of the experimental results should be improved. Particularly, it is hard to read Fig. 1.


"
Generalization Properties of Learning with Random Features,"Alessandro Rudi, Lorenzo Rosasco",https://proceedings.neurips.cc/paper/2017/hash/61b1fb3f59e28c67f3925f3c79be81a1-Abstract.html,"Comments:
- page 4, 7th line after line 129. ""... of features is order sqrt{n}log{n}, ..."" -> ""... of features is of order sqrt{n}log{n}, ...""
- line 134. ""This is the first and still one the main results providing a statistical analysis..."" -> ""This is the first and still one of the main results providing statistical analysis...""
- lines 206-207. ""It is always satisfied for alpha = 1 ands F = k^2. e.g. considering
any random feature satisfying (6). "" ->  ""It is always satisfied for alpha = 1 and F = k^2  e.g. by considering any random features satisfying (6). "" ?
- line 213, Theorem 3. ""...then a number of random features M_n equal to"" -> ""...then a number of random features M_n is equal to""
- line 256. ""We f compute the KRR..."" -> ""We compute the KRR...""
- line 276. ""... since this require different techniques..."" -> ""... since this requires different techniques""

Conclusion:
- The topic of the paper is important, the results are new, the paper is well-written.
- Still, the method of random features has various hyperparameters, e.g. the kernel width, the regularization parameter, etc. Moreover, results of Thm. 2 and Thm. 3 depend on parameters r, gamma, etc., which are not known. Thus, we need a model selection approach, as well as its theoretical analysis.
- The authors consider problem-specific selection of the sampling distribution. However, it may be even efficient to consider a data-dependent selection of the kernel approximation based on random features. E.g., in the paper ""Data-driven Random Fourier Features using Stein Effect"" (http://www.cs.cmu.edu/~chunlial/docs/17ijcaiRF.pdf) the authors proposed to estimate parameters of a kernel decomposition in random features based on available input sample. Thus, it can be interesting to consider theoretical properties of this approach using the tools, developed by the authors of the reviewed paper.
","I was very interested by the result presented here:

The main result is that, with a data set of n example, can can acheive an error that decay as 1/sqrt(n) (just as in the Ridge regression method) with sqrt(n)log(n) random projection. I believe this is a very useful guaranty that come as a very good news for the use of random projections and random kitchen sinks, as proposed in NIPS in 2007  Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007) Of course, this might be still a lot of random projection, but there are way to obtain them in a fast way by analog means (see for instance the optical device in http://ieeexplore.ieee.org/abstract/document/7472872/ that allows millions of random projections).

The gain is indeed interesting from a computational point of view: from O(n^3) and O(n^2) in time and space for standard kernel ridge regression to O(n^2) and O(n√n)  with random features

Question: I do not see the dimension d playing any role in theorem 1.

Section 4 Numerical results could be made clearer. Also, the author could try with data coming from a standard dataset as well.
","This is in my opinion an excellent paper, a significant theoretical contribution to understanding the role of the well established random feature trick in kernel methods. The authors prove that for a wide range of optimization tasks in machine learning random feature based methods provide algorithms giving results competitive (in terms of accuracy) to standard kernel methods with only \sqrt{n} random features (instead of linear number; this provides scalability). This is according to my knowledge, one of the first result where it is rigorously proven that for downstream applications (such as kernel ridge regression) one can use random feature based kernel methods with relatively small number of random features (the whole point of using the random feature approach is to use significantly fewer random features than the dimensionality of a data). So far most guarantees were of point-wise flavor (there are several papers giving upper bounds on the number of random features needed to approximate the value of the kernel accurately for a given pair of feature vectors x and y but it is not clear at all how these guarantees translate for instance to risk guarantees for downstream applications). The authors however miss one paper with very relevant results that it would be worth to compare with theirs. The paper I talk about is: ""Random Fourier Features for Kernel Ridge Regression: Approxiamation Bounds and Statistica Guarantees"" (ICML'17).
In this paper the authors work on exactly the same problem and derive certain bounds on the number of random features needed, but it is not clear for me how to obtain from these bounds (that rely on certain spectral guarantees derived in that paper) the \sqrt{n} guarantees obtained in this submission. Thus definitely I strongly advice relating to that paper in the final version of this draft. Anyway, the contribution is still very novel. Furthermore, this paper is very nicely written, well-organized and gives an excellent introduction to the problem. What is most important, the theoretical contribution is to my opinion huge. To sum it up, very good work with new theoretical advances in this important field."
Predictive-State Decoders: Encoding the Future into Recurrent Networks,"Arun Venkatraman, Nicholas Rhinehart, Wen Sun, Lerrel Pinto, Martial Hebert, Byron Boots, Kris Kitani, J. Bagnell",https://proceedings.neurips.cc/paper/2017/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html,"The paper proposes an additional loss term for training a RNN that encourages it to capture the latent state. This simple reconstruction loss does help performance on variety of tasks. 

The main message of the paper is that if future observations are predictable from an internal state, then it means it has captured the latent dynamic of the environment. Such an internal state then should be more useful for the actual task. The claim is supported by diverse set of experiments, although the improvements in the RL tasks are not significant. 

But here are my concerns:
- Since \phi is only defined in sec 4.1, I assume identity is used in the all experiments. Although it is motivated by a more general framework, the models in the experiment are simply doing frame prediction, which is not very novel. There are many works that do future frame prediction both in video and RL. I think the paper will be more clear if the introduction has a paragraph explaining what the model is actually end-up doing: predicting future frames.

- Although PSR have nice theoretical guarantees, I don't think they would translate to the proposed model. It would still suffer from non-convexity as the base model RNN is non-convex. 

- Since there are many works that also perform future frame prediction, the paper needs a ""related work"" section, instead of spending so much space on explaining RNNs.

Other comments:
- Sec 4.2 needs more details. It is not clear what is the task, and exactly how the models are trained. [L237] If only the best policy is plotted, why there are error bars in Fig 5?  

- L246: what ""parallelization"" means here?

- What ""5k"" and ""50k"" means in table 1?

- What ""simple"" means in table 2?","The authors present a simple method for regularizing recurrent neural networks (specifically applied to low-dimensional control tasks trained by filtering, imitation, and reinforcement learning) that is inspired by the literature on Predictive State Representations (PSRs). In PSRs, processes with latent states are modeled by directly identifying the latent state with a representation of the sufficient statistics of future observations. This stands in contrast to RNNs, which use an opaque hidden state that is optimized by backpropagation to make the whole system model the target of interest through evolution of the hidden state. They propose a simple regularization method for RNNs inspired by PSRs, called Predictive State Decoders, which maintains the parametric form of a standard RNN, but augments the training objective to make the hidden state more directly predict statistics of distant future observations. While direct training of an RNN by backpropagation implicitly encourages the internal state to be predictive of the future, the authors demonstrate that this explicit additional regularization term gives significant improvements on several control tasks.

The idea specifically is to augment the standard training loss for an RNN controller (either for filtering, imitation learning, or reinforcement learning) with an additional term that maps the hidden state at each timestep through an additional learned parametric function F, and penalizes the difference between that and some statistics of up to 10 future observations. This encourages the internal state of the RNN to more directly capture the future distribution of observations without over-fitting to next-step prediction, and architecturally improves gradient flow through the network similar to skip-connections, attention, or the intermediate classifiers added in FitNets or Inception.

I like the main idea of the paper. It is simple, well-motivated, and connects RNN-based models to a large body of theoretically-grounded research in modeling latent state processes, of which the authors do a good literature review. The experiments and analysis are thorough, and demonstrate significant improvements over the baseline on many of the tasks in the OpenAI gym. The baselines use strong, well-known techniques and optimization methods for imitation learning and RL.

I have some concerns that the paper does not offer enough detail about the exact architectures used. For the filtering experiment, it is stated that phi is the identity function, which I assume is true for the future experiments. Additionally, unless I am missing something, the exact functional form of F, the decoder, is never mentioned in the text. Is this a feedforward neural network? Given that the predicted states can sometimes be 10 times the size of the observation dimension, have the authors thought about how to apply this method to models with very large dimensional observations? More detail would be required on exactly the form of F in a camera-ready.

Overall, I think this is a well-executed paper that demonstrates a nice synergy between the PSR and RNN methods for latent state process modeling. The method seems simple to implement and convincingly improves over the baselines.
","The paper proposes a regularization technique of RNNs that tries to predict future inputs from the current hidden state.

First of all, I think the paper is written in a very confusing manner. The underlying idea is very simple and intuitive, but it is dressed up in an unconvincing theoretical motivation. You have to read to page 5 in order to finally find out the main idea. The PSR connection is a fine side observation but I don't feel that it a strong motivator. The idea should and can be explained in other, more intuitive terms (e.g. self-supervision) on page 1 itself. But this paper confuses with irrelevant terms and definitions in order to maintain a ""theoretical connection"".

I like the idea itself - this fits in with the self-supervision trend. Predicting future inputs as another objective to train hidden states is intuitive. Basically you are adding a language modeling cost to the RNN.

The experiments are decent. I'm surprised that the variance of the PSD is often just as large as without the additional loss (e.g. TRPO), which is contrary to the Unsupervised Auxiliary Tasks paper by Jaderberg et al. 2016. This suggests that the this regularizer is not as strong as other self-supervision tasks. A comparison with the PSR task and other self-supervision tasks would be valuable. Right now, it's hard to tell if this is worth incorporating into self-supervised RL models. Also, it would be very useful to see ablation studies, i.e. the full table for varying the hyperparameter ""k"". This would help characterize the how well these models can predict future, and when noise starts dominating.

To conclude, good simple idea, lacks important ablation experiments, confusingly written."
Federated Multi-Task Learning,"Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S. Talwalkar",https://proceedings.neurips.cc/paper/2017/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html,"This work extends on both federated learning and MTL to present the federated MTL, which considers important practical issues like reliability of computing devices and the communication channel. Overall it’s a well written paper. The challenges and contributions are clearly presented. The proposed algorithm should run reasonably efficiently (although it would be better if the authors can further list the computing and memory complexity overhead, if any, compared to previous work, in the supplementary material). The theoretical analyses are complete and correct as far as I can tell. The experiments are comprehensive and the results are indeed impressive. The only major suggestion I would give is to possibly include discussion (or future work) addressing the limitation of requiring convex loss in real applications, how techniques presented in this work may benefit distributed nonconvex problems, and/or how they may benefit from “convexified” stronger models (e.g. [1-4]).

[1] Convex Deep Learning via Normalized Kernels, NIPS 2014
[2] Convolutional Kernel Networks, NIPS 2014
[3] Tensor Switching Networks, NIPS 2016
[4] Convexified Convolutional Neural Networks, ICML 2017
","The paper generalizes an existing framework for distributed multitask learning (COCOA) and extends it to handle practical systems challenges such as communications cost, stragglers, etc. The proposed technique (MOCHA) makes the framework robust and faster.

Pros:
- Rigorous theoretical analysis
- Sound experiment methodology

Cons:
- Not very novel (compared to [47]) and just a mild variation over [47] including the analysis.

Major Comments:
---------------

1. The Equation 5 depends on \Delta \alpha_t* (minimizer of sub-problem). My understanding is that this minimizer is not known and cannot be computed accurately at every node, and therefore, we need the approximation. If so, then how do the nodes set \theta^h_t? Appendix E.3 states that this parameter is directly tuned via H_i. However, the quality should be determined by the environment, and not supposed to be 'tuned'. If tuned, then it might act as a 'learning rate' -- instead of taking the full step in the direction of the gradient, the algorithm takes a partial step.

2. The main difference between this paper and [47] seems to be that instead of the same \theta in [47], this paper has defined separate \theta^h_t for each node t.

Minor comments / typos:
-----------------------

39: ...that MTL is -as- a natural ....

108: ...of this (12) in our... -- should be (2)","      This work proposes a refinement of multi-task learning for systems that have bandwidth, reliability, and battery constraints (federated learning).
      The extension of CoCoA seem rather direct. The dual, as most duals in this type of optimization problem, is amenable to a distributed computation framework.
      The change in regularization of the primal is a minor, but important point.

      After the author feedback, it became clear that the synchronous updates and the changes to CoCoa have merit. Most of the intellectual contribution is on the convergence analysis rather than the learning algorithm, which is both a strong and a weak point.  

      Overall the paper seems like a good contribution. 

      "
Learning Causal Structures Using Regression Invariance,"AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang",https://proceedings.neurips.cc/paper/2017/hash/62889e73828c756c961c5a6d6c01a463-Abstract.html,"The authors propose using regression systematically in a causal setup where an intervention refers to a change in the distribution of some of the exogenous variables. They propose two algorithms for this, one is complete but computationally intractable, whereas the other is a heuristic. 

One of the most important problems is the motivation for this new type of intervention. Please provide real world examples of when this type of intervention is realistic.

Another important problem is about Assumption 1. This assumption is very convenient for the analysis but it is very unclear what it entails. How should the variances be related to one another for Assumption 1 to hold? And under what perturbation? Authors should provide a more detailed alternate assumption instead that clearly lays out how noise variances relate to each other, while also considering the causal graph. Otherwise, it is very difficult to assess how restrictive this assumption is.

Line 38: ""Note that this model is one of the most problematic models in the literature of causal inference""
-> Additive noise models with Gaussian exogenous variables are arguably one of the most restrictive models. It is not clear if the hardness of identification in this case is relevant for practical questions.

Line 62, 63, 64: ""Also since the exogenous noises of both variables have changed, commonly used interventional tests are also not capable of distinguishing between these two structures [4]""
This is not true. As long as the noise variables are independent (they are always independent in Pearlian setting under causal sufficiency), a simple conditional independence test in the two environments, one pre-interventional and one post-interventional, yields the true causal direction (assuming either X1 -> X2 or X1 <- X2). The fact that exogenous variables have different distributions does not disturb this test: Simply observe X1 and X2 are independent under an intervention on X2 to conclude X1->X2.

Line 169: ""ordinary interventional tests"" are described as the changes observed in the distribution of effect given a perturbation of the cause distribution. However, the more robust interventional test commonly used in the literature checks the conditional independence of the two variables in the post-interventional distribution, as explained in the above comment. Please see ""Characterization and Greedy Learning of Interventional Equivalence Classes"" by Buhlmann for details. 

Based on the two comments above, please do not include ""discovering causal graph where interventional methods cannot"" in the list of contributions, this is incorrect and severely misleading. I believe the authors should simply say that methods used under perfect interventions cannot be used any more under this type of intervention - which changes the distribution of the exogenous variable - since no new conditional independencies are created compared to the observational distribution. This would be the correct statement, but it is not what the paper currently reads. Please edit the misleading parts of the text.

Example 1 is slightly confusing: Authors use I = {X1} which does not have any subscript as the previous notation I_{i,j}. I believe it would be more clear to write it as I = {\emptyset, X1} instead.

It is not clear if the algorithm LRE is complete, I believe not. Please explicitly mention this in the paper. 

The regression invariance set - R - can be indicative of the set of parents of a variable. Authors already make use of this observation in designing the LRE algorithm. What if every exogenous variable changed across every two intervention, i.e., I_{i,j} = [n]. How would you change LRE algorithm for this special case. I would intuitively believe it would greatly simplify it. This may be a useful special case to consider that can boost the usefulness of regression invariant sets.

Another extension idea for the paper is to design the experiments I_{i,j}. What is the minimum number of interventions required?

Simulation results are very preliminary. The results of the real dataset simulations is not clear: Authors have identified a set of possible causes of the number of years of education. Since the true causal graph is unknown it is not clear how to evaluate the results. 
","
The paper studies the task of inferring causal DAGs in a scenario where the structural equations are constant across different data sets, while the unobserved noise terms change. The approach is interesting, relevant and sound. The presentation is ok, but could be improved. For instance,
the description of invariant prediction must be more explicit -- what is assumed to change and what is assumed to remain constant? In particular, I didn't see why it fails to identify the DAG in Fig 1b due to the lack of clarity about the invariant prediction scenario. 


Some remarks / questions:

- Some readers may appreciate a remark about why it is natural that the noise variable changes while the structure coefficient remains the same. 
After all, both are just parameters of the conditional distribution of a variable given its causes. 

- Assumption 1: 'small value \epsilon' is not defined. Maybe say s.th. like 'if there is an epsilon_0 such that R(G,I) is not changed for perturbations \epsilon smaller than \epsilon_0'.

- Ref. [2] appeared at UAI 2010

- In line 56, the authors exclude ""pathological cases"", which seems to exclude that two noise terms change jointly in a way that ensures that non-generic regression invariances occur. It would be nice to be more explicit about this and postulate such an independence assumption rather than stating it implicitly (unless I didn't get you right). 

- I'm wondering to what extent the additional causal information provided by regression invariance could also be captured by a scenario where one 
introduces an additional variable labelling the data set. I'm not saying that this would be equivalent to your scenario, it would just be nice to see a few remarks why yours is superior.  

- I missed the point why it's important that the Jacobian matrix mentioned in line 268 and 272 is a rational function of the entries of B 

- In line 238 'Furthermore, we have \Sigma_i =...' --> this is the *definition* of \Sigma_i, right? Please indicate.

- Appendix A: I didn't get the meaning of the notation C[\beta .... ; ...]. Since there are no space constraints, these statements could be properly explained. 

- Please indicate what F(p,n-p) stands for 

- Would non-linear structural equations yield stronger identifiability results? "
Practical Hash Functions for Similarity Estimation and Dimensionality Reduction,"Søren Dahlgaard, Mathias Knudsen, Mikkel Thorup",https://proceedings.neurips.cc/paper/2017/hash/62dad6e273d32235ae02b7d321578ee8-Abstract.html,"The paper looks into a very basic and important area of the choice of hash functions for feature hashing and densified one permutation hashing. 

The paper empirically shows that mixed tabulation is superior, in terms of speed, over popularly used murmurhash3 and both are better in accuracy that shift-multiply.  

The main contribution of the paper is an important empirical comparison of famous hashing schemes of similarity estimation and retrieval of feature hashing and densified hashing, which will be a good resource for practitioners. 

Even though the contributions are mostly empirical since murmurhash3 is widely popular this paper will have some impact in practice. 

","The main contribution of the paper is the set of empirical comparisons of various hashing functions, mutiply, 2-universal (2U), Murmur, and mixed tabulation, for (1) similarity estimation and (2) LSH. This type of empical evaluations is very important and will benefit practioners. Overall, this is a good paper. For improving the paper, I would suggest the authors to take into consideration of the following comments (some of which might be crucial):

---1. There is already a study of using 2U and 4U hashing for similarity estimation and classification tasks. See
[1] b-Bit Minwise Hashing in Practice. Proceedings of the 5th Asia-Pacific Symposium on Internetware, 2013. 
[2] https://arxiv.org/abs/1205.2958, which is a more detailed version of [1], as far as I can tell. 

[1,2] used 2U and 4U for evalating both b-bit minwise hashing and FH　(although they used VW to refer to FH). For example, [1,2] showed that for dense sets, 2U and 4U had an obvious bias and larger MSE. 

I would suggest the authors to cite one of [1,2] and comment on the additional contributions beyond [1,2]. 

---2. [1,2] already showed that for classification tasks, 2U and 4U seems to be sufficiently accurate compared to using fully random hash functions. This paper ommited classification experiments. It is not at all surprising that mixed tabulation will work as well as fully random hash for classification, which the NIPS community would probably care more (than LSH). 

---3. This paper used 2U instead of 4U. It would be more coninvincing to present the results using 4U (or both 2U and 4U) as it is well-understood that in some cases 4U is indeed better. 

---4. The LSH experiments can have been presented more convincingly. 
a) In practice, we typically must guaranttee a good recall, even at the cost of retrieving more points. therefore, while the presentation of using the ratio # retrieved / recall is useful, it may cover important details. 

b) only mulply hash and mix tabulation results are presented. It is important to also present fully random results and 2U/4U hashing results. 

c) If the results from different hash functions as shown  in Figure 5 really differ that much, then it is not that appropriate to compare the results at a fixed K and L, because each hash function may work better with a particular parameter. 

d) The LSH parameters are probably chosen not so appropriately. The paper says it followed [30] for the parameters K,L by using K ={8,10,12} and L={8,10,12}. However, [30] actually used  for K = {6, 8, 10, 12} and L = {4, 8, 16, 32, 64, 128}

e)  LSH-OPH with K = {8,10,12} is likely too large. L =[8,10,12} is likely too small, according to prior experience. If the parameters are not (close to) optimal, then it is difficult to judge usefulness of the experiment results. 

-----------
In Summary, overall, the topic of this paper is very important. The limitation of the current submission includes i) a lack of citation of the prior work [1,2] and explanation on the additional contribution beyond [1,2]. ii) the classification experiments are missing. It is very possible that no essential difference will be observed for any hash function, as concluded in the prior work [1,2]. iii) the LSH experiments may have issues and the conclusions drawn from the LSH experiments are hard to judge.   
","This paper considers the mixed tabulation hash function of [DKRT] in FOCS 2015 which had been proved to perform like a true random function in some applications (like Jaccard Similarity using One Permutation Hashing). This paper provides similar theoretical results for Feature Hashing and more importantly experiments to support this theoretical results. They use mixed tabulation for the OPH and Feature Hashing and compared its performance to a variety of hash functions. They showed that for the hash functions whose results were comparable, mixed tabulation was at least 40% faster.

I believe this is a nice experimental result supported by a solid theory.  However, the paper is not very well written and it has many typos. There are also certain places that are not clear, for example Figure 1 is not well explained and it is not quite clear what is going on from the text. 

minor comments:
abstract, functions -> function
page 2, fast the -> fast as the
page 2, they -> They
page 3, with k a divisor of -> with k being a divisor of 
page 3, simple -> simply
page 5, some the most -> some of the most
page 11, theorem 2 should be changed to theorem 1
"
Gaussian Quadrature for Kernel Features,"Tri Dao, Christopher M. De Sa, Christopher Ré",https://proceedings.neurips.cc/paper/2017/hash/62f91ce9b820a491ee78c108636db089-Abstract.html,"%%% UPDATE: Thank you for your response, which has been read %%%
%%% I had not appreciated that the experiments don't exactly implement the method for which theoretical results are provided; hence the reduction in my overall score by one level %%%

This was an excellent paper - very careful, well written, impressive and enjoyable. It establishes solid theoretical results for deterministic quadrature alternatives to random Fourier features. I have a few comments, but overall I think this would be a fantastic contribution to NIPS.

- On page 2, lines 72-74, I wonder if the assessment of QMC is a bit pessimistic. Was higher-order QMC used in that reference? Is there some fundamental reason to think that no QMC method at all would outperform the proposed approach? I am wondering about whether higher-order digital nets could be used within QMC; these correspond to weighted RKHS and have something in common with the ANOVA construction. See Dick and Pillichshammer's 2010 book.

- \top for transpose in latex

- define the Cartesian basis e_i before it is used

- On page 4, lines 151-155, the comparison to RFF could be tempered again with a caveat that more assumptions are made to get the novel result, relative to the rate presented for RFF. Put another way, is it clear that RFF would not perform better under this additional assumption being made in the paper?

- Please define \tilde{\Omega} in section 4.


","Post-rebuttal comments: Thank you for the feedback. I have raised my score on overall rating. 

Summary: In my opinion, this is an interesting and promising direction for the use of deterministic quadrature rules in kernel approximation. The weakness of the paper is in the experiments: there should be more complete comparisons in computation time, and comparisons with QMC-based methods of Yang et al (ICML2014). Without this the advantage of the proposed method remains unclear.


- The limitation of the obtained results:
The authors assume that the spectrum of a kernel is sub-gaussian. This is OK, as the popular Gaussian kernels are in this class. However, another popular class of kernels such as Matern kernels are not included, since their spectrum only decay polynomially. In this sense, the results of the paper could be restrictive. 

- Eq. (3):
What is $e_l$? 

Corollaries 1, 2 and 3 and Theorem 4:
All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).

- The equation in Line 170:
What is $e_i$?


- Subsampled dense grid:
This approach is what the authors used in Section 5 on experiments. However, it looks that there is no theoretical guarantee for this method. Those having theoretical guarantees seem not to be practically useful. 


- Reweighted grid quadrature:
(i) It looks that there is no theoretical guarantee with this method.

(ii) The approach reminds me of Bayesian quadrature, which essentially obtains the weights by minimizing the worst case error in the unit ball of an RKHS. I would like to look at comparison with this approach.

(iii) Would it be possible to derive a time complexity?

(iv) How do you chose the regularization parameter $\lambda$ in the case of the $\ell_1$ approach? 

- Experiments in Section 5:

(i) The authors reported the results of computation time very briefly (320 secs vs. 384 seconds for 28800 features in MNIST and ""The quadrature-based features ... are about twice as fast to generate, compared to random Fourier features ..."" in TIMIT). I do not they are not enough: the authors should report the results in the form of Tables, for example, varying the number of features. 

(ii) There should be comparison with the QMC-based methods of Yang et al. (ICML2014, JMLR2016). It is not clear what is the advantage of the proposed method over the QMC-based methods.

(iii) There should be explanation on the settings of the MNIST and TIMIT classification tasks: what classifiers did you use, and how did you determine the hyper-parameters of these methods? At least such explantion should be included in the appendix. 



","I have read the other reviews and the rebuttal.

This is paper is concerned with deterministically constructing a feature map z for the class of subgaussian kernels, which can serve as a substitute for random Fourier transforms. By Bochner's theorem, expressing a kernel via its Fourier transforms amounts to computing an intractable integral. While, classically, this integral is computed by MC integration, the authors propose to use Gaussian quadrature to calculate this Fourier transform. For the special case of the ANOVA kernel, the Gaussian quadrature based Fourier transform is shown to produce good estimates using O(D) instead of O(D^3). The good performance of this method is backed up by experiments for classification problems on MNIST and TIMIT. 

The paper reads very well. The introduction on kernels and quadrature is well-written and serves as a good introduction. I liked the fact that the involved proofs are in the appendix, however I would recommend to mention this in the main text, since I at first thought that they were simply omitted. 

This research is highly significant. Substituting high-dimensional Monte Carlo integration with deterministic ones can---as the paper shows---yield significant speed-ups, if it is done right. Of course this will only work, if one fits the integration rule to the integrand. This case, where the ANOVA kernel is used, is a convincing example, since it encodes sparse dependencies and therefore the 'curse of dimensionality', i.e. the room for a function to wiggle around uncontrollably in unmodelled dimensions, can be avoided. I think this would be very interesting for the community to see.

As an addition I would like to point the authors to an interesting line of research concerning probabilistic versions of Gaussian quadrature rules, about which the authors might not know. Sarkka et al [1] have shown that one can obtain Gaussian quadrature rules (i.e. polynomially-exact quadrature) as a posterior mean for Bayesian quadrature with suitable kernels. Maybe this can be combined with your methods, if the numerical uncertainty over the integration for the Fourier map is important?

As a last point of criticism, I want to point out that one of the fundamental references in this paper---[18] ""How to cale up kernel methods to be as good as neural nets""---is only an arxiv submission and appears not to be peer-reviewed although it was published in 2014. Is this the case or was the wrong reference chosen? For a reference which is cited as a fundamental reason for this paper I would expect something which is peer-reviewed.

Overall, I think it's a very good paper and due to its interesting theoretical and practical implications I vote to 'accept'.

[1] https://arxiv.org/pdf/1504.05994.pdf"
Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets,"Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, Joseph J. Lim",https://proceedings.neurips.cc/paper/2017/hash/632cee946db83e7a52ce5e8d6f0fed35-Abstract.html,"The paper describes a new learning model able to discover 'intentions' from expert policies by using an imitation learning framework. The idea is mainly based on the GAIL  model which aims at learning by imitation a policy using a GAN approach. The main difference in the article is that the learned policy is, in fact, a mixture of sub-policies, each sub-policy aiming at automatically matching a particular intention in the expert behavior. The GAIL algorithm is thus derived with this mixture, resulting in an effective learning technique. Another approach is also proposed where the intention will be captured through a latent vector by derivating the InfoGAN algorithm for this particular case. Experiments are made on 4 different settings and show that the model is able to discover the underlying intentions contained in the demonstration trajectories. 

Comments:
First of all, the task focused in the paper is interesting. Discovering 'intentions' and learning sub-policies is a key RL problem that has been the source of many old and new papers. The proposed models are simple but effective extensions of existing models.  

 Actually, I do not really understand what is the difference made by the authors between 'intentions' and 'options' in the literature. It seems to me that intentions are more restricted than options since there are no natural switching mechanisms in the proposed approach while options models are able to choose which option to use at any time. Moreover, w.r.t options literature, the main originality of the paper is both in how the intentions are discovered (using GAIL), and also in the fact that this is made by imitation while many options learning models are based on a classical RL setting. A discussion on this point is important, both in the 'related work' part, but also in the experimental part (I will go back to this point later in the review).  At the end, it seems that the model discovered sub-policies, but is still unable to know how to assemble these sub-policies in order to solve complex problems. So the proposed model is more a first step to learn an efficient agent than a complete solution.  (This is for example illustrated in Section 6 ""[...]the categorical intention variable is manually changed [...]""). The question is thus what can be done with the learned policy ? How the result of the algorithm will be used ? 

Concerning the presentation of the model, the notations are not clear. Mainly, \pi^i gives the impression that the policy is indexed by the intention i (which is not the case, since, as far as I understand, the indexation by the intention is in fact contained in the notation $\pi(a|s,i)$) . Moreover, the section concerning the definition of the reward is unclear: as far as I understand, in your setting, trajectories are augmented with the intention value at each timestep. But this intention value i_t is not used during learning, but will be used for evaluating the reward value. I think that this has to be made more clear in the paper if accepted. 

Concerning the experimental sections:
* First, the environments are quite simple, and only focused on fully-observable MDP. The extension of the proposed model to PO-MDP could be discussed in the paper (since I suppose it is not trivial). 
* Second, there is no comparison of the proposed approach with options discovery/hierarchical models techniques like ""Imitation Learning with Hierarchical Actions -- Abram L. Friesen and Rajesh P. N. Rao"", or even ""Active Imitation Learning of Hierarchical Policies
Mandana Hamidi, Prasad Tadepalli, Robby Goetschalckx, Alan Fern"". This is a clear weak point of the paper. 
*  I do not really understand how the quantitative evaluation is made (reward). In order to evaluate the quality of each sub-policy w.r.t each intention, I suppose that a manual matching is made between the value of $i$ and the 'real' intention. Could you please explain better explain that point ? 
* The difference between the categorical intentions and continuous one is not well discussed and evaluated. Particularly, on the continuous case, the paper would gain if the interpolation between sub-policies is evaluated as it is suggested in the article. 

Conclusion: 
 An interesting extension of existing models, but with some unclear aspects, and with an experimental section that could be strenghtened 

","This paper proposes to learn stills from demonstrations without any a priori knowledge. Using GAN, it generates trajectories from a mixture of policies, and imitate the demonstration. This idea is very simple and easy to be understood. I accept that the idea could works.

There are however some issues in the details. 

The objective Eq.7 does not show optimizing p(i), which is surely to be optimized.

In Eq.7, should not the laster term E log(p(i)) be expressed as H(p(i)), instead of H(i)? And the former is not a constant.

Figure 5 is too small, and too crowd to read clearly.
","- Summary
This paper considers a multi-task imitation learning problem where the agent should learn to imitate multiple expert policies without having access to the identity of the tasks. The proposed method is based on GAIL (generative adversarial imitation learning) with an additional objective that encourages some of the latent variables (called intention variables) to be easily inferred from the generated trajectories. The results show that the intention variables can captures different modes of the expert behaviors on several Mujoco tasks.

[Pros]
	- The problem considered in this paper is interesting.
	- The proposed objective is novel, though it is very similar to InfoGAN or GAIL + [Florensa et al.]'s idea in the end.
[Cons] 
	- (minor) The experimental results are good but not ""impressive"" because the expert behaviors are quite clearly separated (see Quality for details).
	- (minor) The arguments on hierarchical RL are not much convincing (see Quality for details).

- Quality
The proposed method and its connection to InfoGAN are interesting and sound reasonable.

Some suggestions to improve the paper:
1) The expert behaviors seem quite well-separated in the experiment (e.g., walking forward or backward). It could be much more impressive if the paper showed that the proposed model can capture subtle but distinguishable differences of behaviors (e.g., different styles of walking forward) in separate latent codes.
2) ""Hierarchical RL"" part is not much convincing. The goal of option discovery in HRL is to find useful temporal abstractions from ""temporally unsegmented"" tasks. A similar problem formulation in imitation learning context would be to show two subtasks (grasping -> pushing) in a ""single"" episode and let the agent to figure out how to temporally segment the expert's behavior into two subtasks and learn them. In the experiment, however, the authors gave two subtasks as separate episodes to my understanding, and the agent only needed to correctly model two behaviors into two intention variables, which is a much easier problem than the original problem of ""option discovery"". 
3) It would be interesting to show the model's behaviors from interpolated or extrapolated intention variables.
4) The proposed method is actually quite close to [Florensa et al.]'s method in the sense that the latent variables are encouraged to be inferred from the agent's behavior, though they did not focus on imitation learning. It would be good to make a connection to this work in more detail.

- Clarity
The paper is well-written except for the following: 
1) In Line 25-32, the motivating example can be misleading. The paper claims that it aims to learn skills from unstructured data (e.g., learning to grasp, reach, cut from a video showing cooking). It sounds like this paper aims to discover subtasks from temporally unsegmented data, which is not the case because the paper provided temporally segmented (but unlabeled) data to the model. This is also related to the comment on HRL in the Quality section.
2) In Line 33-35, the term ""automatically segmented tasks"" or ""unstructured demonstrations"" are again a bit misleading as discussed above. Instead, ""unlabeled demonstrations"" is more clear. 

- Originality
The multi-task imitation learning problem is interesting, and the proposed idea of using GAIL with InfoGAN-like objective is novel. 

- Significance
Multi-task imitation learning is an important research direction. This paper shows a nice application of GAIL and InfoGAN-like approach to capture the underlying modes of diverse behaviors into disentangled latent variables. Though some of the statements are a bit misleading, I think this is overall a solid paper."
Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees,"Francesco Locatello, Michael Tschannen, Gunnar Raetsch, Martin Jaggi",https://proceedings.neurips.cc/paper/2017/hash/63538fe6ef330c13a05a3ed7e599d5f7-Abstract.html,"%%%% Summary
The authors consider constrained convex optimization problems where the constraints are conic and given by a bounded set of atoms. The authors propose linear oracle based method to tackle such problems in the spirit of Frank-Wolfe algorithm and Matching Pursuit techniques. The main algorithm is the Non-Negative Matching Pursuit and the authors propose several active set variants. The paper contains convergence analysis for all the algorithms under different scenarios. In a nutshel, the convergence rate is sublinear for general objectives and linear for strongly convex objectives. The linear rates involve a new geometric quantity, the cone width. Finally the authors illustrate the relevance of their algorithm on several machine learning tasks and different datasets.

%%%% Main comments
I did not have time to check the affine invariant algorithms and analyses presented in the appendix.

The papers contains interesting novel ideas and extensions for linear oracle based optimization methods but the technical presentation suffer some weaknesses:

- Theorem 2 which presentation is problematic and does not really provide any convergence guaranty.
- All the linear convergence rates rely on Theorem 8 which is burried at the end of the appendix and which proof is not clear enough.
- Lower bounds on the number of good steps of each algorithm which are not really proved since they rely on an argument of the type ""it works the same as in another close setting"".

The numerical experiments are numerous and convincing, but I think that the authors should provide empirical evidences showing that the computational cost are of the same order of magnitude compared competing methods for the experiments they carried out.


%%%% Details on the main comments

%% Theorem 2

The presention and statement of Theorem 2 (and all the sublinear rates given in the paper) has the following form:
- Given a fixed horizon T
- Consider rho, a bound on the iterates x_0 ... x_T
- Then for all t > 0 the suboptimality is of the order of c / t where c depends on rho.

First, the proof cannot hold for all t > 0 but only for 0 < t <= T. Indeed, in the proof, equation (16) relies on the fact that the rho bound holds for x_t which is only ensured for t <  = T.

Second the numerator actually contains rho^2. When T increases, rho could increase as well and the given bound does not even need to approach 0.

This presentation is problematic. One possible way to fix this would be to provide a priori conditions (such as coercivity) which ensure that the sequence of iterates remain in a compact set, allowing to define an upper bound independantly of the horizon T.

In the proof I did not understand the sentence ""The reason being that f is convex, therefore, for t > 0 we have f (x t ) < = f (0).""



%% Lemma 7 and Theorem 8


I could not understand Lemma 7. 
The equation is given without any comment and I cannot understand its meaning without further explaination. Is this equation defining K'? Or is it the case that K' can be chosen to satisfy this equation? Does it have any other meaning?

Lemma 7 deals only with g-faces which are polytopes. Is it always the case? What happens if K is not a polytope? Can this be done without loss of generality? Is it just a typo?


Theorem 8:

The presentation is problematic. In Lemma 7, r is not a feasible direction. In Theorem 8, it is the gradient of f at x_t. Theorem 8 says ""using the notation from Lemma 7"". The proof of Theorem 8 says ""if r is a feasible direction"". All this makes the work of the reader very hard.

Notations of Lemma 7 are not properly used:
- What is e? e is not fixed by Lemma 7, it is just a variable defining a maximum. This is a recurent mistake in the proofs.
- What is K? K is supposed to be given in Lemma 7 but not in Theorem 8.
- Polytope?
All this could be more explicit.


""As x is not optimal by convexity we have that < r , e >  > 0"". Where is it assumed that $x$ is not optimal? How does this translate in the proposed inequality?


What does the following mean?
""We then project r on the faces of cone(A) containing x until it is a feasible direction""
Do the author project on an intersection of faces or alternatively on each face or something else?
It would be more appropriate to say ""the projection is a feasible direction"" since r is fixed to be the gradient of f. It is very uncomfortable to have the value of r changing within the proof in an ""algorithmic fashion"" and makes it very hard to check accuracy of the arguments.
In any case, I suspect that the resulting r could be 0 in which case the next equation does not make sense. What prevents the resulting r from being null? 

In the next sentences, the authors use Lemma 7 which assumes that r is not a feasible direction. This is contradictory with the preceeding paragraph. At this point I was completely confused and lost hope to understand the details of this proof.

What is r' on line 723 and in the preceeding equation?

I understand that there is a kind of recursive process in the proof. Why should the last sentence be true?

%% Further comments
Line 220, max should be argmax
I did not really understand the non-negative matrix facotrization experiment. Since the resulting approximation is of rank 10, does it mean that the authors ran their algorithm for 10 steps only?

","This paper considers the optimization problems over the convex cone, parameterized as the conic hull of a possibly infinite atom set. The authors established optimization convergence results for the proposed algorithms which apply to general atom sets and objective functions. Overall, the paper is well written and clearly states the contributions and the connections to existing literatures. 

1. The paper targets an interesting topic, namely the parameterization of the optimization problems over the conic hull of a possibly infinite atom set, which is between but different from what matching pursuit over the linear span of a finite atom set and frank-wolfe over the convex hull of a set of atoms. This facilitates the proposal of the algorithms with optimization theoretical results for general objective functions and atom sets. 

2. The paper fills in an empty space of showing the optimization convergence rates of greedy algorithms in matching pursuit and frank wolfe because the atom set is not guaranteed to contain an atom aligned with a descent direction for all possible suboptimal iterates. This shed light on understanding these problems. 

3. The authors also introduced away-step, pairwise and fully corrective MP variants with theoretical guarantees for completeness. ","In this paper, authors target an “intermediate case” between the two domain parameterizations given by the linear span and the convex hull of an atom set, that is the parameterization of the optimization domain as the conic hull of a possibly infinite atom set. This is different from the greedy optimization methods such as matching pursuit and Frank-Wolfe algorithms over the line span and the convex hull of a set of atoms, respectively. They give explicit convergence rates for non-negative MP algorithms.

The alignment assumption of the existing MP method does not hold if the optimization domain is a cone. Authors present modifications of existing non-negative variants of MP so as to corroborate this issue along with the resulting MP-type algorithms for conic problems and corresponding convergence guarantees. Authors proposed three algorithms with different variants. It might be interesting to see which algorithm is the preferred in what conditions. It is useful to participants to apply the best method to their problems.

In lines 202-205,  authors pointed out that one has to take either the risk of choosing tau too small and failing to recover an optimal solution, or rely on too large tau, which can result in slow convergence. Does it mean that a large tau is preferred for the convergence guarantee without considering the time complexity?

To verify the proposed algorithms, authors conducted experiments on three problems. However, the descriptions of the experimental settings and analysis of the results are too rough. It is better to have detailed discussion. Moreover, the computational time might be an interesting evaluation criterion for comparing different methods as well as their variants.
"
Acceleration and Averaging in Stochastic Descent Dynamics,"Walid Krichene, Peter L. Bartlett",https://proceedings.neurips.cc/paper/2017/hash/643de7cf7ba769c7466ccbc4adfd7fac-Abstract.html,"Summary:
The authors consider mirror descent dynamics in continuous time for constrained optimization problems. They work in the context of stochastic differential equations and consider parametrized dynamics which include many of the recently considered parameters for such dynamics. This includes time dependant learning rate, sensitivity and averaging parameters which provide a great flexibility in the analysis. The dynamics also feature acceleration terms which were recently proposed in a deterministic setting. 

In a first step, the authors carry out an asymptotic analysis in the deterministic setting. They then introduce a stochastic variant of the dynamics and describe uniqueness properties of its solutions. Combining properties of the deterministic dynamics and stochastic calculus the authors propose three results: Almost sure convergence of the sample trajectories, convergence rates, both in expectation and for sample trajectories.

Main comments:
First of all, I have a limited knowledge of stochastic differential calculus and do not feel competent to validate all the technical details in the manuscript, although none of them felt particularly awkward.

I believe that this work is of very good quality. Technical elements are clearly presented and the authors do not neglect fundamental aspects of the problem such as existence and uniqueness. Furthermore, the overall exposition is clear and of good quality and the text is well balanced between the main proof arguments and further details given in the appendix. 

To my understanding, the convergence analysis carried out by the authors extends significantly existing results from the litterature. The model is flexible enough to allow many possible choice of the dynamics. For example the authors are able to show that unit learning rate may robustly maintain good properties of accelerated dynamics in the deterministic case provided that suitable averaging is performed. 

Overall, I think that this constitutes valuble work and should be considered for publication.

Additional comments and questions:

1- The authors motivate the introduction of stochastic dynamics by emphasizing on stochastic methods for finite sums. However the additive noise structure is not particularly well suited to model such cases. Indeed, in these settings the variance of the noise usually features a multiplicative term.

2- All the results are presented in abstract form with the addition of an other parameter ""r"" and assumptions on it. It is not really clear how the assumptions on r restrict the choice of the other parameters. As a result, the convergence rates are not easily read or interpreted. Beyon the proposed corollaries, it is not really clear how the given result provide interesting guaranties in terms of function values. For example would the work of the authors allow to get better rates in the small noise regime?

3- Do the authors have any comment on how the analysis which is carried out in a stochastic, continuous time setting could impact the discrete time setting. Is there a clear path to the conception of efficient algorithms and in particular is it possible to provide a discrete time equivalent for the different time dependent parameters in the ODE?
","The paper formulates a stochastic differential equation framework to study the accelerated mirror descent dynamics in the continuous time settings. The main contribution of the paper is to provide convergence analysis of such dynamical system under noisy gradient, where a random Brownian process is presented in the gradient evaluation. 

The presentation of the paper is very clear and well organized and the theoretical result is solid. The only terminology not clear to me is the term averaging formulation, can authors provide more explanations on it? The convergence result is very interesting because it provides intuition about how the error's magnitude influence the choice of parameters. One question I would like to ask is how can we discretize the given continuous dynamics and what will be the representations of different parameters in the discretized setting. Moreover, since the provided stochastic dynamics is about the accelerated mirror descent, I am wondering is there any acceleration in terms of convergence rate is obtained compare to the standard mirror descent dynamics, like the one in Mertikopoulos and Staudigl 2016. Can authors comment on it? 

Overall, I find the paper very clear and it is theoretically solid. I am happy to accept it if authors can address my concerns.  

Remarks: There is a tiny impreciseness in the equation after line 217 in page 6, it should be a limsup instead of limit, it doesn't affect the correctness of the proof. Some other typos appears in the equation after line 147 in page 4. #EDIT Clarified by the authors.

#EDIT Thank you for author's feedback, I believe this is a good paper to be published, I vote for accept.","This paper studies stochastic dynamics for optimizing smooth convex functions beyond the Euclidean framework. The authors use technique introduced by Raginsky (2012) to study a stochastic modification of the accelerated mirror descent studied by Krichene (2015).

The paper is very clear and well written. It is extremely pleasant to read and the related work is seriously done.  

To conclude, this paper takes up the challenge to understand stochastic optimization algorithms thanks to its comparison with its continuous counterpart which satisfies a SDE. This follows recent progress in the understanding of Accelerated gradient descent (see e,g,. Su, 2015). Even if stochastic calculus is an involved field of mathematics, this paper succeeds in proving interesting result. 

Comments:

- Why put so much effort to treat the mirror descent case whereas the Euclidean case is not already done ?

- What does acceleration bring to the story: a clear comparison of the results with the non-accelerated ones would be enlightening. 

- The propositions are not explained and commented enough. 

- The rates and the conditions could be further detailed

- 155: The corollary 1 deserves further comments: What does the condition $\eta \geq \dot r$ mean ? 
Which rates give this result ? What is the dependency on $L$ ? Why this is an accelerated result ? 

- 183: Proposition 2: What are the conditions on the algorithm parameters to have convergence ?


Minor comments:
14: E=\RR^d, E is not necessary, just \RR^d
50: This part is misleading. The true problem of statistical learning is directly to optimize the true expectation with respect to the unknown law of the data. And this is directly what optimize stochastic gradient descent. The finite sum problem is a different and simpler problem (with different same lower bounds).
50-60: What about analysis of regular gradient descent in continuous time ? 
68: Could the authors give some examples where the noise is decreasing like this.  
Sec 2.1 This section is useless. 
128-130: What are the links between s(t) and \eta(t) ?
137: 1992 is not the correct citation date. Maybe 1892. 1992 corresponds to some reissue.
146: Given that $a$ and $\eta$ are fixed by the algorithm and that $r(t)$ parametrized the Lyapunov function, I would rather write $r=\eta/a$."
LightGBM: A Highly Efficient Gradient Boosting Decision Tree,"Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu",https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,"The paper presents two nice ways for improving the usual gradient boosting algorithm where weak classifiers are decision trees. It is a paper oriented towards efficient (less costful)  implementation of the usual algorithm in order to speed up the learning of decision trees by taking into account previous computations and sparse data.

The approaches are interesting and smart. A risk bound is given for one of the improvements (GOSS), which seems sound but still quite loose: according to the experiments, a tighter bound could be obtained, getting rid of the ""max"" sizes of considered sets. No garantee is given for the second improvement (EFB) although is seems to be quite efficient in practice. 

The paper comes with a bunch of interesting experiments, which give good insights and shows a nice study of the pros and vons of the improving approaches, both in accuracy and computational time, in various types of datasets. The experiments lacks of standard deviation indications, for the performances are often very closed from one mathod to another. A suggestion would be to observe the effect of noise in the data on the improvement methods (especially on GOSS, which relies mostly on a greedy selection based on the highest gradients). 

The paper is not easy to read, because the presentation, explanation, experimental study and analysis are spread all over the paper. It would be better to concentrate fully on one improvements, then on the other, and to finish by the study of them together.  Besides, it would be helpfulfor the reader if the main algorithms where written in simplest ways, not in pseudo codes and assumption on the specificities of data structures.
","This paper investigates the gradient boosting decision tree (GBDT) in machine learning. This work focuses on the case when the feature dimension is high and data size is large, and previous studies are not unsatisfactory with respect to efficiency and scalability. In this paper, the authors introduce two techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). 
GOSS excludes a significant proportion of data instances with small gradients, and only uses the rest to estimate the information gain. EFB bundles mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. The authors prove that GOSS can obtain quite accurate estimation of the information gain with a much smaller data size, and finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio. Finally, the some experiments are presented to show that LightGBM speeds up the training process of conventional GBDT while keeping almost the same accuracy.

Good points:
1. This paper is well-motivated, and the proposed algorithm is interesting and effective.
2. The authors present the theoretical analysis for approximation error.
3. The authors provide strong empirical studies to shows the effectiveness of the proposed algorithm, which present good insights in various datasets.

It would be better to provide some details on the datasets and why these datasets are selected. 
It would be better to provide the generalization analysis since the algorithm is measured by test accuracy and AUC.
","This paper gives LightGBM, an improvement over GBDT which is of greate value in practical applicaitons. Two techniques (i.e., GOSS and EFB) are proposed to tackle the computational problem of estimating information gain on large datasets. Specifically, at each step, GOSS excludes a large proportion of instances with small gradients; and EFB bundles mutually exclusive features to reduce the number of features. Theoretical analysis and empirical studies show the effectiveness of the proposed methods. 

Strong points:
1. Overall, the paper is well written and easy to follow. 
2. The proposed studied here is well motivated and of important pratical value, and the proposed methods are smart and effective. 
3. It is nice to see theoretical analysis for the GOSS method. 
4. The experiment results are convincing, eitheri n accuracy or computational time.

It is pity to see that the theoretical study is on the approximation error, it would be better to see generalization error analysis. That is, how the approximation error will be affect the generalization error. 
"
The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process,"Hongyuan Mei, Jason M. Eisner",https://proceedings.neurips.cc/paper/2017/hash/6463c88460bd63bbe256e495c63aa40b-Abstract.html,"The proposed submission deals with an interesting and important problem: how to automatically learn the potentially complex temporal influence structures for the multivariate Hawkes process. The proposed neutrally self-modulating multivariate point process model can capture a range of superadditive, subadditive, or even subtractive influence structures from the historical events on the future event, and the model is quite flexible. Also, the model in evaluated on both the synthetic and the real data, and yields a competitive likelihood and prediction accuracy under missing data. 

Comments:
1.	Compared with existing work, one potential contribution of this submission is in the increased flexibility of the proposed model. First, in modeling the intensity function, a non-linear transfer function is introduced and is applied to the original defined intensity for multivariate Hawkes processes.  Second, it models the hidden state h(t) as a continuous-time LSTM.
2.	Since the model is more flexible and has more parameters to estimate, it will be harder to accurately learn the parameters. The algorithm basically follows the previous Back Propagation Through Time (Du et al, 2016) strategy and proposes using the Monte Carlo trick in evaluating the integral quantify in the likelihood. I think the convergence of the current algorithm might be very slow since they have too many parameters. Since the learning issue in this model is challenging, it is worthwhile discussing more on the computational efficiency. A more tractable and efficient learning algorithm might be needed in dealing with large-scale problems. 
3.	For the numerical section, only the likelihood under missing data is evaluated. It will be more convincing to add the results for the intensity accuracy evaluation. 


","The authors propose the neural Hawkes process whose intensity function is based on  a continuous-time LSTM. The values of LSTM cells decay exponentially until being updated when a new event occurs, which is different from previous work of combining Hawkes process models with RNNs. The authors argue that their model has better expressivity and achieves better performance compared to the vanilla Hawkes process and the Hawkes process without positivity constraints. The overall novelty of the proposed method is not very significant, but it is well motivated and thoroughly analyzed. 

Some comments: 
(1) Why would you prefer an additive intensity function rather than multiplicative (line 229)? I assume that you always want a strictly positive intensity function and therefore choose to use the ""softplus"" function as your transfer function, but it seems somehow unnatural.
(2) For your experiment on simulated data (Appendix C.4), it's good to see your model achieves much better performance on data generated by your model, and performs comparably or slightly better on  data generated by other two models. I am interested to know whether you directly measured how well your model learns the intensity function, instead of the likelihood on held-out dataset? I suppose for your synthetic data, it's not hard to compare to the ground-truth intensity function."
Bayesian Optimization with Gradients,"Jian Wu, Matthias Poloczek, Andrew G. Wilson, Peter Frazier",https://proceedings.neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html,"Summary of the paper:

This paper describes a technique for incorporating gradient information into Bayesian optimization. The proposed method not only considers gradient information in the modelling process. That is, in the GP that is used to model the objective. But also in the computation of the acquisition function. This is done by using the knowledge gradient method, which measures the expected improvement in the posterior mean of the GP at the observed locations. The proposed approach is shown to be Bayes-optimal and asymptotically consistent. Several experiments including synthetic functions, and real-world experiments involving kernel learning and deep learning illustrate the benefits of the proposed method.  

Detailed comments:

Quality: 

I think the quality of the paper is high. It is very well written, with the exception of a few typos, and it is supported by both theoretical and empirical results. The paper also reviews important related methods and has an exhaustive experimental section comparing with them.

Clarity:

The paper is clearly written and important information is included in the supplementary material, including the proofs of the propositions and theorems considered. Something strange is the non-smoothness of the EI function in Figure 1. Why is this so? Is it related to the noisy assumption?

Originality:

As indicated by the authors, considering gradient information is not new. Some authors have already considered this. However, and importantly, it seems that the technique proposed by the authors is the only one that considers this information in the computation of the acquisition function. Previous methods seem to consider this information only in the modeling of the objective. It is also a very interesting idea to use stochastic gradient optimization to maximize the acquisition function. As far as I know this is the first work considering that. 

Significance:

I think this is an interesting paper for the community working on Bayesian optimization. It will certainly receive some attention. The experimental results also suggest important gains by considering gradient information, and they look significant. Something that can be criticized is that obtaining gradient information will probably be more expensive. Thus, gradient free methods will be able to do more evaluations of the objective given a similar computational time. This has not been considered by the authors.
","Major: 

To me this paper is an important one as it presents substantial contributions to Bayesian Optimization that are at the same time elegant and practically efficient. While the paper is well written, it could be formally improved in places; below are 
a few comments questions that might be useful toward such improvements. Besides this, I was a bit puzzled by the fact that Theorem 1 is not very precisely stated; also, given the fact that it is proven in appendix (which is understandable given 
the space limitations) and with a proof that requires substantial time to proofread formally (as it is almost a second paper in itself, with a specific policy formalism/set-up), I had to adapt my confidence score accordingly. Also, I was wondering if the consistency proof could work in the case of noiseless observations, where it does not make sense to replicate evaluations at the same point(s)?
Nevertheless, from the methodological contributions and their practical relevance, I am conviced that the presented approach should be spread to BO/ML researchers and that NIPS is an excellent platform to do so.   


Minor:

* In the abstract: ""most optimization methods"" is a bit strong. Derivative-free optimization methods should not be forgotten. 
Yet, gradients are indeed essential in (differentiable) local optimization methods. 
* In the abstract: syntax issue in ""We show d-KG provides"". 
* Introduction: from ""By contrast"", see comment above regarding *local* optimization.
* Introduction: ""relative value"" does not sound very clear to me. Maybe speak of ""variations"" or so? 
* Introduction: ""we explore the what, when, and why"". In a way yes, but the main emphasis is on d-KG, and even if generalities 
and other approaches are tackled, it is probably a bit strong to claim/imply that *Bayesian Optimization with Gradients* is 
*explored* in a paper subjected to such space restrictions. 
* Introduction: from ""And even"" to ""finite differences""; in such case, why not use this credit to do more criterion-driven 
evaluations? Are there good reasons to believe that two neigbouring points approximating a derivative will be better than 
two (more) distant well-chosen points? [NB: could there exist a connection to ""twin points"", in Gaussian kernel settings?] 
* Section 3.1: ""to find argmin""; so, the set [in case of several minimizers]?
* Section 3.1: ""linear operator"" calls for additional regularity condition(s). By the way, it would be nice to ensure 
differentiability of the GP via the chosen mu and K. 
* Section 3.1: the notation used in equation (3.2) seem to imply that there cannot be multiple evaluations at the same x. 
Notations in terms of y_i (and similar for the gradient), corresponding to points x_i, could accomodate that. 
* Section 3.1: the paragraph from line 121 to 126 sounds a bit confusing to me (I found the writing rather cryptic there)
* Section 3.2: ""expected loss""; one would expect a translation by the actual min there (so that loss=0 for successful optim)
* Section 3.2: in equation (3.3), conditioning on ""z"" is abusive. By the way, why this ""z"" notation (and z^(i)) for ""x"" points? 
* Algorithm 2, lines 2: a) =argmax abusive and b) is theta on the unit sphere? restricted to canonical directions? 
* Section 3.3: in the expression of the d-KG factor, the term \hat{\sigma}^{(n)} appears to be related to (a generalization of) 
kriging update formulas for batch-sequential data assimilation. Maybe it could make sense to connect one result ro the other?
* Section 3.3: the approach with the enveloppe theorem is really nice. However, how are the x^*(W) estimated? Detail needed...
* Section 3.3: ""K samples""; from which distribution? + Be careful, the letter K was already used before, for the covariance. 
* Section 3.3, line 191: ""conditioning"" does not seem right here...conditional? conditionally? 
* Section 3.4: in Prop.1, you mean for all z^{(1:q)}? Globally the propositions lack mathematical precision...
* Section 3.4: ...in particular, what are the assumptions in Theorem 1? E.g., Is f assumed to be drawm from the running GP? 
* Section 4: notation Sigma is used for the covariance kernel while K was used before. 
* Section 4: about ""the number of function evaluations""; do the figures account for the fact that ""d-approaches"" do several f 
evaluations at each iteration?
* Section 5: about ""low dimensional""; not necessarily...See REMBO and also recent attempts to combine BO and kernel (sparse) decompositions.   
","
The paper proposes a novel acquisition function d-KG (for derivative-enabled knowledge-gradient) which is an extension of the knowledge-gradient (KG) by Wu and Frazier 2016. The main difference, to my understanding, is that the possible future gradients (batch as well as online evaluations) also affect the acquisition function, and not just the marginal posterior over the the objective, which the authors argue, should improve the selection of evaluation-locations. The authors also provide a way of estimating the gradients of d-KG in an unbiased way, such that stochastic optimizers can be used to maximize it. The method is evaluated on some classic synthetic benchmarks of BO and real work applications e.g. a logistic regressors and an MLP.

The paper is well written and clear. The novel contributions are clearly marked as such and the connections and distinctions to existing methods that authors point out are very informative. 
I did not entirely follow the proof of Section 3.4, nor the derivation of \nabla d-KG since they were not contained in the main paper, but the line of argument contained in 3.3 sounds plausible to me. 

Some questions:
- why is the noise on the derivative observations independent? likewise the independence between gradient and value observation. Is this purely for computational convenience/easier estimation of \sigma? or is there an underlying reason?
- I am not quite sure how \theta is defined. Is \theta \nabla y (below Eq. 3.5) a projection? If yes, a transpose is missing. Also, must \theta have unit norm? 
- In Section 4.1, why did you observed the fourth derivative? From pseudocode 1 it looks as \theta is optimized for.

A major practical concern:
- You use SGD to optimize d-KG. How do you tune its learning rate? This might be quite tricky in practice, especially since you might need to re-tune it after each update of the GP and not just one time for the whole BO-procedure.

Nitpicky comments:
- line 168: is it `a continuous' instead of `an continuous'?
- y-labels on Figure 2 would be nice.
- line 242: `We also...': word missing?
- line 288: `are can': one word too much?
- please don't call a shallow MLP with 2 hidden layers `deep neural network' (lines 283 and 292 and the conclusion)

(*Edit:* I increased the score since I found the arguments of the authors convincing. But I encourage the authors to comment on the learning rate tuning as they promised in the rebuttal.)
"
Visual Reference Resolution using Attention Memory for Visual Dialog,"Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal",https://proceedings.neurips.cc/paper/2017/hash/654ad60ebd1ae29cedc37da04b6b0672-Abstract.html,"Overall Impressions:

I think this is a solid paper. The problem is timely, the paper is well written, the approach is relatively novel and well motivated, and both qualitative and quantitative results are impressive. There are some things I have questions about but these are largely a matter of curiosity and not critique. 

Strengths:

- Writing is relatively clear and the figures do a good job of supporting the text.
- The approach is well described and motivated.
- The synthetic dataset seems well constructed for this task; the visual perception portion being easy to solve compared to the referential ambiguity. 
- The various ablations presented in the synthetic experiments were interesting.
- Results on the synthetic and Visual Dialog datasets are convincing. 

Weaknesses:
- I would have liked to see some analysis about the distribution of the addressing coefficients (Betas) with and without the bias towards sequential addressing. This difference seems to be very important for the synthetic task (likely because each question is based on the answer set of the previous one). Also I don't think the value of the trade-off parameter (Theta) was ever mentioned. What was it and how was it selected? If instead of a soft attention, the attention from the previous question was simply used, how would that baseline perform?

- Towards the same point, to what degree does the sequential bias affect the VisDial results? 

- Minor complaint. There are a lot of footnotes which can be distracting, although I understand they are good for conserving clarity / space while still providing useful details.

- Does the dynamic weight prediction seem to identify a handful of modes depending on the question being asked? Analysis of these weights (perhaps tSNE colored by question type) would be interesting.

- It would have been interesting to see not only the retrieved and final attentions but also the tentative attention maps in the qualitative figures. 

","This paper proposed a visual reference resolution model for visual dialog. The authors proposed to attentions, 1: tentative attention that only consider current question and history, and 2: relevant attention that retrieved from an associate attention memory. Two attentions are further combined with a dynamic parameter layer from [9] and predict the final attention on image. The authors create MNIST Dialog synthetic dataset which model the visual reference resolution of ambiguous expressions. The proposed method outperform baseline with large margin. The authors also perform experiments on visual dialog dataset, and show improvements over the previous methods. 

[Paper Strengths]
1: Visual reference resolution is a nice and intuitive idea on visual dialog dataset. 
2: MNIST Dialog synthetic dataset is a plus.
3: Paper is well written and easy to follow. 

[Paper Weaknesses]
My major concerns about this paper is the experiment on visual dialog dataset. The authors only show the proposed model's performance on discriminative setting without any ablation studies.  There is not enough experiment result to show how the proposed model works on the real dataset. If possible, please answer my following questions in the rebuttal. 

1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters? 

2: There are two test settings in visual dialog, while the Table 1 only shows the result on discriminative setting. It's known that discriminative setting can not apply on real applications, what is the result on generative setting? 

3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.
"
Straggler Mitigation in Distributed Optimization Through Data Encoding,"Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin",https://proceedings.neurips.cc/paper/2017/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html,"The authors propose a simple scheme for making the distributed solution of large least squares problems more predictable by avoid tail events such as stragglers, slowing down the computation.

While the setup is not as general as that of [19], the results are much stronger.

Empirically the data increases only a factor of 2 while allowing a great amount of leeway on how many machines they are going to wait for in each iteration and still achieve a good final solution.

The authors failed to make connections with Randomized Linear Algebra techniques for solving least squares problems. In particular it is known that to guarrantee a good solution for least squares with sampling data points should be sampled according to their leverage score. One option proposed in the RandLA community is to apply subsampled Hadamard transforms because they uniformize the leverage scores, making the subsequent selection of transformed data points, trivial. I think the authors here might be observing some of the benefits of this. To verify, it would be good to perform an experiment with a datasets where the data points have very non-uniform leverage scores. ","This paper addresses the issue of performing distributed optimization in the presence of straggling/slow computation units. In particular, the paper focuses on the problem of linear regression

min_w \|Xw - y\|_2, ---- (1)

where X = [(X_1)^T, (X_2)^T,..., (X_m)^T]^T and y = [y_1, y_2,..., y_m]^T denote the data points and the corresponding labels. In general, the distributed setup with $m$ worker nodes allocates $i$-th data point $X_i$ and the associated label $y_i$ to $i$-th worker node. The linear regression problem is then solved in an iterative manner where messages/information needs to be communicated among (master) server and the worker nodes. However, in practice, some of the workers (aka stragglers) take longer time to completer their end of processing/communication, which slows down the entire distributed optimization problem.

In order to address the delay caused by the stragglers, this paper proposes to introduce redundancy into the data samples and labels by various encoding methods. The paper utilizes an encoding matrix $S$ to transforms the data points $X$ and labels $y$ into $\tilde{X} = SX$ and $\tilde{y} = Sy$, respectively. The paper then solves the following problem instead of (1). 

min_w \|\tilde{X}w - \tilde{y}\|_2 = min_w \|SXw - Sy}\|_2,

where the optimization setup allocates the $i$-th row of the matrices $\tilde{X}$ and $\tilde{y}$ to the $i$-worker node. During each iteration of the distributed optimization process, the server only waits for a subset of worker nodes to respond and then proceeds with the next iteration of the optimization process based on the received information. In this way, the server ignores the stragglers. Interestingly, the paper shows that the introduced redundancy allows the server to compute an (almost) optimal solution of the original problem as long as  the encoding matrix is chosen to satisfy certain spectral properties and the server does not discard too many worker nodes in each iteration. 

The paper focuses on two particular algorithms for solving the underlying optimization problem: 1) gradient descent and 2) L-BFGS. The paper shows that for suitable encoding matrices the proposed solution provably obtains an almost optimal solution. The paper then presents three different approached to generate the encoding matrices with the desired properties. The authors also present extensive simulation results to corroborate their theoretical findings.

The paper addresses a very interesting and practical problem. The proposed solutions is interesting which builds on a recent trend of using coding ideas to mitigate stragglers during distributed computation. That said, the approaches adopted in the paper and its focus significantly differs from some of the existing papers in this area. The paper is also very well written and addresses the underlying problem in a comprehensive manner.

I have a minor comment for the authors. While discussing the related work, the authors argue that most of the previous works preform redundant computation whereas this paper directly encodes the data. However, the authors do acknowledge that [19] does perform a very simple form of data encoding (i.e., replication). The reviewer would like to point out that short-dot approach in [5] is similar to that in [19], therefore [5] can also be classified as encoding the data. In general, all the previous work including [8] can be viewed as encoding the data (assuming that, in a matrix vector computation, matrix is also part of data). Adopting this general view might be beneficial to develop a unified framework to address the problems in this domain.

"
Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation,"Zhaohan Guo, Philip S. Thomas, Emma Brunskill",https://proceedings.neurips.cc/paper/2017/hash/6786f3c62fbf9021694f6e51cc07fe3c-Abstract.html,"The paper addresses off-policy policy evaluation: determining the value of a policy using experience obtained by following a different policy. The authors build on existing work on importance sampling, focusing on how temporally-extended actions (options) can be useful in this setting, especially when the horizon is long. 

The specific contributions of the paper are as follows: (1) importance-sampling estimator in the presence of options, discussion of how it can reduce estimation error compared to when using only primitive actions, experimental confirmation in the taxi domain, (2) importance sampling in the presence of an option with a fixed distribution of terminal states, discussion of how this can reduce estimation error compared to when such an option is not present, experimental confirmation in the taxi domain, (3) a general-purpose, incremental importance-sampling algorithm that can be used with or without options, along with an experimental evaluation in a simple (but well-motivated) MDP. This last algorithm drops distant experiences, examining covariant structure to determine a numerical value for ""distant"".

The problem addressed in the paper is important, with many real-world application areas. The results are straightforward but potentially useful. How useful they turn out to be depends on how often real-world problems exhibit the various forms of structure exploited by the authors. The domains used in the paper are well motivated but synthetic. Real-world domains and data sets, especially in experiment 3, would go a long way in making a stronger case for the approach. 

The paper is clear and well organized. 

Minor comment: In experiment 1, it would be informative to see additional conditions tested, using for example options that are typical for the taxi domain (for instance, driving to the passenger location). The options tested by the authors are informative but artificial (they simply follow the optimal policy from a given state for a given number of steps). 
","The authors investigate how options influence the variance of importance sampling estimators to increase the length of trajectories that off-policy evaluation approaches can be applied to. They prove that for off-policy evaluation with WIS and single timestep actions it can take an exponential number of trajectories in the horizon to achieve a desired level of accuracy. This is particularly problematic because WIS has much lower variance than IS. Corollaries 1 and 2 prime the reader’s intuition about how options can reduce impact of long horizons by describing two special cases. The authors introduce the notion of stationary options and demonstrate how the trajectory can be decomposed into the importance weighted return up to the first occurrence of the stationary option and the remainder of the trajectory. Unfortunately, the condition for stationary options is difficult to enforce in practice. So the authors introduce a covariance test that helps to relax this assumption (by introducing some bias). INCRIS exploits the covariance test to drop importance sampling weights when doing so minimizes an estimate of the MSE.

The authors should spend more time describing INCRIS. Moving from the covariance test to the algorithm is too abrupt and it is hard to relate this back to the topic of off-policy evaluation over option policies.

It is confusing in Corollaries 1 and 2 that K is used for two different things (number of decision points and total number of changed steps).

Line 66 says ‘ldots’ which should be …
","[I have read the other reviews and the author feedback, and updated my rating to be borderline accept. The paper will be substantially stronger with more realistic expts.
In the absence of such expts, I suspect that the IncrIS estimator will run into substantial computational challenges and sub-optimal MSE in realistic scenarios.]

The paper introduces new off-policy estimators to evaluate MDP policies. The options-based estimator of Theorem3 is a straightforward adaptation of per-step importance sampling. 

Section6 makes an interesting observation: if at any step in the trajectory we know that the induced state distribution is the same between the behavior policy and the evaluation policy, we can partition the trajectory into two sub-parts and conduct per-step importance sampling on each sub-part. This observation may prove to be more generally useful; in this paper, they exploit this observation to propose the IncrIS estimator. 

IncrIS uses sample variance and covariance estimates (presumably; this detail is missing in their experiment description) to detect what prefix of sampled trajectories can be safely ignored [i.e. yields smaller (estimated) MSE of per-step importance sampling on the resulting suffix]. This is an interesting approach to bias-variance trade-offs in importance-sampling-estimator design that can be explored further; e.g. covariance testing requires us to fix the choice of k (ignored prefix length) across the entire set of trajectories. Can we condition on the observed trajectories to adaptively discard importance weights?

In summary, the contributions up to Section 6 seem very incremental. IncrIS introduces an intriguing bias-variance trade-off but the experimental evidence (MSE in a constructed toy MDP)is not very convincing. In realistic OPE settings, it is likely that horizons will be large (so IncrIS can shine), but there may be lots of sampled trajectories (computational challenge for IncrIS to compute \hat{MSE}), and we almost certainly won't be in the asymptotic regime where we can invoke strong consistency of IncrIS (so, \hat{MSE} may yield severely sub-optimal choices, and IncrIS may have poor empirical MSE). Due to these intuitions, a more realistic experiment will be much more informative in exploring the strengths and weaknesses of the proposed estimators.

Minor:
66: \ldots
112: one *can* leverage *the* advantage?"
Attentional Pooling for Action Recognition,"Rohit Girdhar, Deva Ramanan",https://proceedings.neurips.cc/paper/2017/hash/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Abstract.html,"This paper presents an attention-based model for action recognition and human object interaction. The model can be guided by extra supervision or not. It achieves accuracy improvements without increasing the network size and computational cost a lot. Authors provide an extensively empirical and analytical analysis of the attention module, and they further introduce a derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods for fine-grained classification typically. Experiments on three benchmarks have demonstrated the efficacy of the proposed method. 

One key advantage is to learn the attention map in an unsupervised manner. To make a prediction, the attention maps can provide insight into where the network should look in terms of both bottom-up saliency and top-down attention. This allows it to get rid of detecting the bounding box usually required in hard attention.

This is an interesting work, and the idea sounds well motivated. The paper reads well too. One major concern is: 

The fully-connected weight matrix in second-order pooling is approximated by the product of two vectors with rank-1. Then, how about the information loss compared the original one? Does such loss affect the attention map? Some theoretical analysis and more discussions are expected.

Minor issues: 
Page 3, ""network architecture that incorporate(s) this attention module and explore(s) a pose ...""
Page 5, ""It consist(s) of a stack of modules""
Page 8, ""... image resized to 450px at(as) input time did""?
","The paper introduces a simple yet efficient way of incorporating attention modelling in CNN-based action recognition networks.
The paper is clearly written, the contribution is interesting and the experimental validation is convincing.

Minors comments:
Section 2 speaks of ""performance"" without explicitly stating which evaluation metrics are considered, except for [36] where it is said that mean average precision is considered (btw the acronym mAP is not properly defined here), this should be fixed to be clearer.
A few typos were left that need further proofreading.","The authors propose to use a low rank approximation of second order pooling features for action recognition. They show results on three compelling vision benchmarks for action recognition and show improvements over the baseline. 

Pros:
(+) The proposed method is simple and can be applied on top of any network architecture
(+) Clear improvement over baseline 

Cons:
(-) Unclear novelty over well known second order pooling approaches
(-) Fair comparison with other attention based models is missing


It is unclear to me how the proposed approach is novel compared to other low rank second order pooling methods, typically used for fine grained recognition (e.g. Lin et al, ICCV2015), semantic segmentation etc. While the authors do a decent job walking us through the linear algebra and interpreting the variables used, the final approach is merely a classifier on second order features.

In addition, the authors make design choices that are not clearly justified. For multi class recognition, the authors do not justify the choice for a class agnoistic b but for class-specific a. These experiments should be added in order to prove the effectiveness of the proposed design. 

The authors claim that they get rid of the box detection step present at other approaches such as R*CNN or Mallya & Lazebnik. However, they do not discuss how their approach would generalize to instance-specific action recognition. An image can contain many people who perform different actions. In this case, the action labels are not assigned at the frame level, but at an instance level. R*CNN was designed for instance-level recognition, thus the necessity of the person box detection step. The authors should discuss how the proposed approach can handle this more generic setup of instance-level labels? This is a limitation of the current approach, rather than a benefit as implied in the Related Work section.

The authors fail to provide a fair comparison with competing approaches. The authors use a ResNet101 or an Inception-V2 architecture for their base network. They show improvement over their own non-attentive baseline, using the same architecture. However, the competing approaches, such as R*CNN or Mallya & Lazebnik all use a VGG16 network. This makes comparison with these approaches and the current approach unfair and inconclusive on MPII and HICO dataset. On HMDB, the improvement over the TSN BN-inception method is small while comparisons with other methods is unclear due to the varying underlying base architectures. 
"
Testing and Learning on Distributions with Symmetric Noise Invariance,"Ho Chung Law, Christopher Yau, Dino Sejdinovic",https://proceedings.neurips.cc/paper/2017/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html,"The authors present an approach relying on a Fourier feature based
distance between the phase functions of probability
distributions. This type of distance, and the corresponding kernel function  
follow the concept of the Maximum Mean Discrepancy. 
The phase function based approach allows to introduce a statistical
test to compare distributions corrupted by additive noise. The
distribution of the noise is assumed to be unknown except some
properties, e.g. positivity of the characteristic function. The phase
features are also exploitable in learning from samples clustered into
labeled bags. The proposed framework then applied on different data
sets to evaluate non-parametric two-sample tests.       

The approach of the paper is a sounding one. Obviously this technique
is created for statistical problems with a special structure, but that
kind of structure is hard to handle otherwise with elementary approaches. 
The model looks like a certain error correcting mechanism on
measurements perturbed by special type of noise. I think data sources
relating more directly to signal decoding problems, specific
examples could be communication between mobiles or between satellites,
which can provide even more sounding and  practical test
environment. 

The conditions exploited might require additional investigations or relaxation,
e.g. indecomposable random variable, since to test them could be
even harder than the problem presented by the authors.   
","Summary:

The authors proposed phase discrepancy between distributions and phase features for embedding distributions, which are based on the use of phase function that is invariant to additive SPD noise components, aiming respectively at two-sample testing and learning on distributions. The phase discrepancy is similar in mathematical form to MMD but is tailored to incorporate invariance to additive SPD noise. As the phase discrepancy induces a shift-invariant kernel (Proposition 1 and 2), direct application of random Fourier features to such a kernel lead to phase features. Rich experimental results well demonstrate the strength of the proposed methods.

Comments:

- 1 -
Line 120, I think the justification of introducing the subset of indecomposable proba measures for which phase functions are uniquely determined is quite insufficient. For example, how would this lack of identifiability limit the utility of the phase discrepancy? Or does this subset properly include interestingly many distributions for application?

- 2 -
Since MMD is well studied with the RKHS which contributed deeply to the understanding and well guarantees the utility of MMD, could the authors relate the proposed phase discrepancy to some properties of the RKHS?","Overview: In this paper the authors present a method for making nonparametric methods invariant to noise from positive definite distributions by utilizing estimated characteristic functions while disregarding magnitude and only considering the phase. The authors apply the technique to two sample testing and nonparametric regression on measures, ie each ""sample"" is a collection of samples. The authors demonstrate that the techniques introduced behave as expected by performing tests on synthetic data and demonstrate reasonable performance of the new two sample tests and new feature on real and semi-synthetic datasets.

Theory: The fundamental idea behind this paper (disregarding the magnitude of a characteristic function) is not terribly revolutionary, but it is elegant, interesting, and likely to be useful so I think worthy of publication. Though the underlying concept is somewhat technical, the authors manage to keep the exposition clear. The authors explain the concept using population version of estimators and give what seem to be very reasonable finite sample estimators, but it would be nice to see some sort of consistency type result to round things off.

Experiments: The synthetic experiment clearly demonstrates that the new features behave as one might expect in for a two sample test, but it is disappointing that the technique seems to greatly reduce the test power compared to current nonparametric techniques. This difference is pretty striking in the >=2000 sample regime. While the technique is not effective for the Higgs Boson two sample test its ineffectiveness is in itself very intriguing, the difference between the two classes seems to be some sort of psd noise phenomena. For the last two experiments, the proposed technique works well on the semi-synthetic dataset (real data with synthetic noise added), but not particularly better, on the final real world data experiment. Ultimately the proposed technique doesn't demonstrate decidedly superior testing or regression performance on any real world dataset, although it does slightly outperform in certain sample regimes.

Exposition: The paper is a pleasure to read but there are some errors and problems which I will list at the end of this review. 

Final Eval.: Overall I think the idea is interesting enough to be worthy of publication.

--Corrections By Line--

3: ""rarely"" should be ""rare""
14: ""recently"" should be ""which have recently been"" to sound better
53: To be totally mathematically precise perhaps you should state the E is a psd rv again
115: add ""the"" after ""However""
116: I'm assuming that ""characteristic function"" was meant to be included after ""supported everywhere""
119: Should probably mention a bit more or justify why these ""cases appear contrived""
123: The notation P_X and P_Y hasn't been introduced, but perhaps it is obvious enough
123: Here K is a function on a pair of probability measures on line 88 K is a function of two collections of data
134: We need some justification that this is actually an unbiased estimator. A lot of the obvious estimators in the MMD and related techniques literature are not unbiased
208: \chi^2(4)/4 is not technically correct, its the sample that is divided by 4 not the measure
268: I'm not familiar with this dataset. Please mention the space which the labels lie in. I'm assuming it is R.



Update: I've bumped it up a point after reading the other reviews and rebuttal. Perhaps I was too harsh to begin with."
Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,"Antti Tarvainen, Harri Valpola",https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html,"This work describes a simple approach to improve semi-supervised learning by training a student to be consistent with the predictions of a teacher that is simply a moving average of the student. The approach and motivation is easy to understand and the paper clearly written. The results are quite impressive.

Figure 1 is quite unclear. Since in 1(a) the model (gray curve) has fit to the unlabeled target, then the classifier must be an unsupervised classifier but the caption doesn't indicate that.

The \Pi model is also repeatedly mentioned in the text and the results table but there is no clear description of it. In line 74, is the mean square error applied on the logits of the model? The text should also describe the architecture of the CNN used to produce the supervised results. Am I right to assume table 1 doesn't use any unlabelled data? What was the value of \alpha that worked in practice?

Minor comment:
Line 106 is badly phrased.
","The paper proposes a new method for using unlabeled data in semi-supervised learning. The idea is to construct a teacher network from student network during training by using an exponentially decaying moving average of the weights of the student network, updating after each batch. This is inspired by previous work that uses a temporal ensemble of the softmax outputs, and aims to reduce the variance of the targets during training. Noise of various forms is added to both labelled and unlabeled examples, and a L2 penalty is added to encourage the student outputs to be consistent with the teachers. As the authors mention, this acts as a kind of soft adaptive label propagation mechanism. The advantage of their approach over temporal ensembling is that it can be used in the online setting. It also appears to converge faster and gives better results in the settings with very few labels.

Strengths
- Can be used in the online learning scenario.
- Results are comparable with other state of the art on CIFAR-10 and SVNH.
- The approach could easily be combined with other techniques for further improvement.
- Experiment configuration details given in full in the appendix.
- Simple approach and should be widely applicable.

Weaknesses
- More evaluation would have been welcome, especially on CIFAR-10 in the full label and lower label scenarios.
- The CIFAR-10 results are a little disappointing with respect to temporal ensembles (although the results are comparable and the proposed approach has other advantages)
- An evaluation on the more challenging STL-10 dataset would have been welcome.

Comments
- The SVNH evaluation suggests that the model is better than pi an temporal ensembling especially in the low-label scenario. With this in mind, it would have been nice to see if you can confirm this on CIFAR-10 too (i.e. show results on CIFAR-10 with less labels)
- I would would have like to have seen what the CIFAR-10 performance looks like with all labels included.
- It would be good to include in the left graph in fig 3 the learning curve for a model without any mean teacher or pi regularization for comparison, to see if mean teacher accelerates learning or slows it down.
- I'd be interested to see if the exponential moving average of the weights provides any benefit on it's own, without the additional consistency cost.","
Summary-
This paper shows that an exponential moving average (in parameter space) of
models produced during SGD training of neural networks can be used as a good
teacher model for the student model which corresponds to the last (current) SGD
iterate. The teacher model is used to provide a target (softmax probabilities)
which is regressed to by the student model on both labelled and unlabelled
data. This additional loss serves as a regularizer. In other words, the model
is being trained to be consistent with its Polyak-averaged version.
Furthermore, noise is injected into both the student and teacher models to
increase the regularization effect (similar motivation as dropout and other
related methods). This is shown to be more effective than (1) Just using noise
(and no moving average), and (2) using a moving average over softmax
probabilities (not parameters) which is updated once every epoch.

Strengths-
- The proposed teacher model is a convenient one to maintain. It just requires keeping a moving average.
- The additional computational cost is just that of a forward prop and benefits seem to be worth it (at least for small amounts of labelled data).
- The supplementary material provides enough experimental details.

Weakness-
- Comparison to other semi-supervised approaches : Other approaches such as variants of Ladder networks would be relevant models to compare to.

Questions/Comments-
- In Table 3, what is the difference between \Pi and \Pi (ours) ?
- In Table 3, is EMA-weighting used for other baseline models (""Supervised"", \Pi, etc) ? To ensure a fair comparison, it would be good to know that all the models being compared to make use of the EMA benefits.
- The proposed model benefits from two factors : noise and keeping an exponential moving average. It would be good to see how much each factor contributes on its own. The \Pi model captures just the noise part, so it would be useful to know how much gain can be obtained by just using a noise-free exponential moving average.
- If averaging in parameter space is being used, it seems that it should be possible to apply the consistency cost in the intermediate layers of the model as well. That could potentially provide a richer consistency gradient. Was this tried ?

Minor comments and typos-
- In the abstract : ""The recently proposed Temporal Ensembling has  ...  "": Please cite.
- ""when learning large datasets."" -> ""when learning on large datasets.""
- ""zero-dimensional data points of the input space"": It may not be accurate to say that the data points are zero-dimensional.
- ""barely applying"",  "" barely replicating"" : ""barely"" -> ""merely""
- ""softmax output of a model does not provide a good Bayesian approximation outside training data"". Bayesian approximation to what ? Please explain. Any model will have some more generalization error outside training data. Is there another source of error being referred to here ?

Overall-
The paper proposes a simple and effective way of using unlabelled data and
improving generalization with labelled data. The most attractive property is
probably the low overhead of using this in practice, so it is quite likely that
this approach could be impactful and widely used.
        "
Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning,"Liangpeng Zhang, Ke Tang, Xin Yao",https://proceedings.neurips.cc/paper/2017/hash/69a5b5995110b36a9a347898d97a610e-Abstract.html,"I found this to be a very interesting paper, with a nice analysis of an underappreciated property of value functions in reinforcement learning.

Potential points of improvement include the following:
- The theory is quite specific, and it would be nice if this could be generalized (e.g., not focusing as much on NSR-MCs).
- The experiments are very small and contrived.  I'm okay with this, for this paper, but it would of course be interesting to test these ideas in more interesting problems.  For instance, could we measure the skewness of values in a problem of interest?  And if we correct for this, does this help at scale?  Even so, I'm okay with accepting the paper as is and leaving that for future work (potentially by other researchers).","This paper focuses on the problem arising from skewness in the distribution of value estimates, which may result in over- or under-estimation. With careful analysis, the paper shows that a particular model-based value estimate is approximately log-normally distributed, which is skewed and thus leading to the possibility of over- or under-estimation. It is further shown that positive and negative rewards induce opposite sort of skewness. With simple experiments, the problem of over/underestimation is illustrated.

This is an interesting paper with some interesting insights on over/underestimation of values. I would like the reviewers to clarify the following two issues.

The results are implied for a particular kind of model-based value estimation method, more specifically, one which relies on definition 3.1. The implication of this result to other kinds of estimation methods, especially the model-free ones is not clear. A discussion is required on why a similar effect is expected with other kinds of values estimation method.

It is disappointing to have no experiments with the suggested remedies, such as the balancing the impact of positive and negative rewards and reward shaping. Some experiments are expected here to see whether these are indeed helpful. And an elaborate discussion of reward balancing and how to achieve it could also improve the quality of the paper.

","This paper approaches an important and fundamental problem in RL from a new direction. Overall, I found the introductory material did an excellent job of highlighting its importance and that the main results were straightforward to follow. I did feel a bit like I lost some of the grounding intuition during section 3.3.

There is just the single primary contribution of the paper, but it is well done and I think stands on its own well enough to be accepted. The experimental section felt a bit like an afterthought. It does add to the paper, but the authors could have gotten much more out of this section by showing that the log-normality *really* does show up in practice in a larger variety of MDPs. Essentially I would argue for the same sort of experiment, but including more simple MDPs and maybe some non-chain classic RL domain as well.

The work has some interesting potential connections with other topics in RL, although the authors do not make these connections explicit. The distributions considered in the paper are also the focus for those interested in parameter uncertainty, posterior sampling, and exploration."
Bayesian Compression for Deep Learning,"Christos Louizos, Karen Ullrich, Max Welling",https://proceedings.neurips.cc/paper/2017/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html,"This paper approaches model compression using a group sparsity prior, to allow entire columns rather than just individual weights to be dropped out. They also use the variance of the posterior distribution over weights to automatically set the precision for fixed point weight quantization. The underlying ideas seem good, and the experimental results seem promising. However, the paper supports the core idea with a great deal of mathematical complexity. The math was presented in a way that I often found confusing, and in several places seems either wrong or poorly motivated (e.g., KL divergences are negative, right and left side of equations are not equal, primary motivation for model compression given in terms of minimum description length). (the paper would have been much stronger had it stuck with just the core idea and supporting experimental results!)

Section 2 – This seems like a strange primary motivation for model compression. Our goal when using deep networks is (almost) never to transmit a set of conditional labels with the fewest bits, and almost always to accurately assign new labels to new data points. 

115 - its’ -> its

128/129 -  I didn’t understand the sentence. Probably you mean to say that the variance of the spike rather than the variance of the slab is zero?

138 - This is not a fully Bayesian treatment. The posterior is being restricted to a specific approximate hierarchical functional form.

Eq 4 -  We only recover Jeffrey’s prior in the 1D case. We do not recover Jeffrey’s prior e.g. if we compute p(W) from equation 5.

154 -  I don’t understand the correspondence to dropout here. I believe that would require that the distribution included a delta function at zero variance.

Eq 9. -  If alpha_i =1, the KL divergence is negative, and as alpha_i --> 0, the KL goes to -infinity.  KL should be strictly nonnegative, so something is definitely wrong here:.

Eq 10 - The  middle and the rightmost terms are not equal to each other. The middle term sets all off diagonal weights to zero. (Should this be ordinary matrix multiplication)

186 – S has a heavy tail. This will encourage *all* weights to be simultaneously large occasionally.

 197 – what does it mean for the shape hyperprior to go to zero?

Eq 19 - Middle term has the same problem as in equation 10.

(239 - aside, largely tangential to current paper -- it's strange how 5 bits keeps on coming up as a per-weight capacity limit, in a variety of architectures and settings. It seems suggestive of something more fundamental going on. e.g. [Han et al, 2015], [Bartol et al, 2016], [Collins, Sohl-Dickstein, Sussillo, 2016])","This paper describes a method for compressing a neural network: pruning its neurons and reducing the floating point precision of its weights to make it smaller and faster at test time. The method works by training the neural net with SVI using a factored variational approximation that shares a scale parameter between sets of weights. The entropy term of the variational objective incentivizes weights to have a large variance, which leads to compression by two separate mechanisms: neurons with high-variance weights relative to their means can be pruned, and layers can have their floating point precision reduced to a level commensurate with the variance of their weights. It's a fairly principled approach in which compression arises naturally as a consequence of Bayesian uncertainty. It also seems effective in practice.

I have three main reasons for not giving a higher score. First, I'm not convinced that the work is hugely original. It leans very heavily on previous work, particularly Kingma et al. on variational dropout and Molchanov et al. on sparsification arising from variational dropout. The two novelties are (1) hierarchical priors and (2) reduced floating point precision, which are certainly useful but they are unsurprising extensions of the previous work. Additionally, the paper does a poor job of placing itself in the context of these previous papers. Despite repeatedly referencing the variational dropout papers for forms of variational approximations or derivations, the concept of 'variational dropout' is never actually introduced! The paper would be much clearer if it at least offered a few-sentence summary of each of these previous papers, and described where it departs from them.

Second, the technical language is quite sloppy in places. The authors regularly use the phrase ""The KL divergence between X and Y"" instead of ""the KL divergence from X to Y"". ""Between"" just shouldn't be used because it implies symmetry. Even worse, in all cases, the order of ""X"" and ""Y"" is backwards, so that even ""from X to Y"" would be incorrect. Another example of sloppy language is the use of the term ""posterior"" to describe the variational approximation q. This causes genuine confusion, because the actual posterior is a very relevant object. At the very least, the term should be ""posterior approximation"". In the paper's defense, the actual equations are generally clear and unambiguous.

Finally, the experiments are a bit unimaginative. For example, although the authors argue that Bayesian inference and model compression are well aligned, we know they have quite different goals and you would expect to find some situations where they could be in conflict. Why not explore this tradeoff by looking at performance and compression as a function of hyperparameters like $\tau_0$?  How much compression do you get if you select $\tau_0$ by maximizing the marginal likelihood estimate rather than choosing an arbitrary value?
","1) Despite the abstract putting equal weight on pruning and precision reduction, the latter seems to have been relegated to the experiments section like a bit of an afterthought. I understand there are some implementation difficulties, but if it's part of the main thesis it should feature more prominently. Also, the description of how to handle that is very superficial. You refer to the appendix, but I'd generally say that central components should be kept in the main text. (although I can sympathize with the space constraints)

2) You're using variational inference, so I would have liked to see a few words about how mean-field assumptions and variance underestimation (your q is unlikely to be flexible enough to avoid this problem) affect you given that you use variance so explicitly. Especially bit precision inference seems like it could be sensitive to this?

3) what are your justifications for the choice of q approximations? In particular their form and component distributions.

Overall, I think it was an excellent paper, if a bit dense with content.     "
Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?,"Cameron Musco, David Woodruff",https://proceedings.neurips.cc/paper/2017/hash/69dafe8b58066478aea48f3d0f384820-Abstract.html,"
	The authors study the problem of k-rank approximation of
	kernel matrix K has a complexity of at least \Omega(nnz(A)k),
	where nnz(A) is the number of non-zero elements in input
	data. The authors also show that for some classes of kernels
	kernelized dataset can be low-rank-approximated in O(nnz(A))
	time.

	The focuses purely to the theory of kernel approximations and
	it contains no real dataset or éven a short discussion as to
	how this would advance the current state of the art or
	conclusion with prosects of practical implications of the
	results. The paper is written quite concisely. I am not 100%
	confident that I am able to review the significance of this
	theoretical contribution, but I would have liked some
	connection to real applications as well.
	
	The citation style is not pursuant to the NIPS guidelines (the
	manuscript uses citations of type [SS00] while the NIPS style
	would be [1] etc.).
      ","The paper presents negative and positive results to the problem of computing low-rank approximations to the kernel matrix of a given dataset. More specifically, for a fixed kernel function and given some input data and a desired rank k, the authors consider the two related problems of (1) computing a rank k matrix which is relatively close to the optimal rank k approximation of the kernel matrix and of (2) computing an orthonormal basis of a k-dimensional space such that when projecting the data from the feature space (defined by the kernel) down to this k-dimensional space it is relatively close to the optimal such projection. In particular, the paper focuses on the question whether it is possible to solve these problems in a time that is independent of the dimensionality of the input data matrix A (say n times d) and instead depends on the number of non-zero entries in A.

Regarding problem (1) the paper provides a negative answer for the linear kernel, all polynomial kernels, and the Gaussian kernel by showing that the problem is as hard as computing (exactly) the product of the input matrix with an arbitrary matrix C (of dim d times k). This implies that a solution cannot be achieved without a significant improvement of the state of the art in matrix multiplication. 

Regarding problem (2) the paper provides a positive result for shift-invariant kernels using an algorithm based on random Fourier features. While the practical relevance of the given algorithm is limited due to a poor scaling with the number of data points (n to the 4.5), its existence is theoretically significant because it achieves the desired independence of n times d and, thus, gives hope for further improvement down to a practically relevant dependencies on n.

The given results constitute valuable and technically challenging contributions to a topic of high relevance to the NIPS community. Additionally, the paper is excellently written including a good exposition of the results and high-level outline of the proofs, which makes it easier to follow the detailed steps.","
This paper proposes a method to make a low-rank (rank k) approimation of a (n*n) kernel matrix which is faster when the dimensionality of the dataset d is very high (d >> n) and the input data A is very sparse.

Understanding when and for what goal the contribution is useful is non-trivial.  The paper says high-dimensional data often appears (which is true) and cites genetics data, but often genetics data is not extremely sparse (e.g. lists for every common mutation point the variant observed, where often more than one variant is non-rare).  Even so, if the feature space is high-dimensional, one often will choose algorithms which are only linear in the dimension d (and have a higher complexity in the smaller n).  Most operations are possible in O(n^3) (e.g. matrix multiplication, matrix inversion, ...).  So is low-rank approximation really needed if n is ""not too large"" ?  If one can afford the proposed algorithm with complexity O(n^{4.5}...), I would expect low-rank approximation is not needed in many cases.
Even so, the O(n^{4,5}) complexity seems to imply that the number of data points in the input data should be rather small, which is a clear limitation of the proposed algorithm.

The paper doesn't show experiments.  This is not needed in a lot of analytic work, but here I'm left with the practical question of what would be a concrete task where the proposed approach would be better than ""no low rank approximation"", and would also be better than ""classic low rank approximation strategies"".

The paper doesn't have a conclusions/further work section, which is useful to summarize and evaluate, e.g. list strong / weak points and how one could mitigate the weaknesses.

DETAILS:
Line 89: be extending -> be extended
Line 91: remove duplicate ""achieving""
Line 104: accept -> except

"
Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks,"Ziming Zhang, Matthew Brand",https://proceedings.neurips.cc/paper/2017/hash/6a2feef8ed6a9fe76d6b3f30f02150b4-Abstract.html,"This paper proposes a simple and efficient block coordinate descent (BCD) algorithm with a novel Tikhonov regularization for training both dense and sparse DNNs with ReLU. They show that the proposed BCD algorithm converges globally to a stationary point with an R-linear convergence rate of order one and performs better than all the SGD variants in experiments. However, the motivations of using Tikhonov regularization and block coordinate descent are not very clear.  The technical parts are hard to follow due to the absence of many details.  The presented results are far from state-of-the-art. In this sense, I am not sure whether the proposed method can be applied to real ""DNNs"". My detailed comments are as follows. 


Specific comments: 

1. The motivation of Tikhonov Regularization should be clarified. Why do we need to introduce Tikhonov Regularization instead of others? What is the main advantage of Tikhonov Regularization compared with other Regularizations?  In the second paragraph of Section I, the authors mentioned several issues of DNN, such as the ""highly non-convex"", ""saddle points"" and ""local extrema"", followed by the Tikhonov Regularization. Does it mean that the proposed Tikhonov Regularizationcan address all these issues?
   

2. What is the motivation of the decomposition into three sub-problems? The authors should explain why such decomposition will not suffer from vanishing gradient.

3. From Eqn. (4) to Eqn. (5), it is hard to follow. On line 133, what is the specific formula of matrix Q(\tilde {\mathcal A})? How to confirm matrix Q(\tilde {\mathcal A}) is positive semidefinite?
4. A mathematical definition of Tikhonov regularized inverse problem should be given clearly.

5. There is a quite strong argument: “In fact, the inverse sub-problem resolve the vanishing gradient issue in traditional deep learning, because it tries to obtain the optimal solution for the output feature of each hidden layer, which are dependent on each other through the Tikhonov matrix.” There are no theoretical justifications or empirical backing to verify that such inverse sub-problem can resolve the vanishing gradient issue.

6. The authors should give more details for why the “optimal” output features work similarly as target propagation.

7. On line 246, what is the definition of \mathcal P?

8. In Fig. 5, BCD-S can learn much sparser networks, why weight matrix of the 4th layer is still dense.

9. On line 252, what the prefix “Q” means?

10. On line 284, there is a missing word: “dense”.

11. The format of the reference is confusing.

12. It is unclear why using the DNN structure in Figure 1. The network studied in the experiments has only 3 hidden layers, which is actually not ""deep"". Should we use skip connections here? Moreover, the presented results are far from state-of-the-art. In this sense, I am not sure whether the proposed method can be applied to real ""DNNs"". 
","The paper pulls together a few important recent ideas on optimizing deep neural networks (as an alternative to popular SGD variants).  The  derivation of the Tikhonov regularized problem in Eq. (4) and (5) from the recursive objective in Eq. (2) through relaxing the ReLU outputs as a convex projection is very clean and convincing. The decomposition into an inverse problem (activations), a least-squares problem (weights) and a final classification problem (soft-max weights) is illuminating. This formulation provides a novel perspectives on  deep learning and offers many interesting avenues, e.g. generalization of ReLUs,  more general connectivity patterns through shortcuts (as suggested in the used architecture), sparsity in network connectivity, etc.

When it comes to optimization, the paper points out that it is difficult to guarantee convergence of a naive alternating optimization scheme (e.g. ADMM) and thus resort to an easier to analyze block-coordinate descent algorithm, presented in Algorithm 1 and analyzed in Section 4.  On the plus side: the authors are able to derive a convergent algorithm. On the minus side: it seems that there is a lot more to explore here and that the paper is merely a first (somewhat preliminary) step towards exploiting the derived problem formulation. 

A proof of concept on MNIST data is given in Section 3. One has to say, that the experimental section is relatively weak: one data set, matlab vs. phython code, very limited analysis. This requires a more extensive quantitative (run-time, solution quality) and qualitativ investigation.  Figure 4 - a percentage pie chart is really confusing. For the final version, a more extensive evaluation would greatly improve the quality of the paper.","This manuscript proposes a modification of feed-forward neural network optimization problem when the activation function is the ReLU function.
My major concern is about the convergence proof for the block coordinate descent algorithm. In particular, Theorem 1 is incorrect for the following reason:
A. It assumed that the sequence generated by the algorithm has a limit point.
This may not be the case as the set U is not compact (closed but not bounded). Therefore, the sequence may not converge to a finite point.
B. It stated that the sequence generated by the algorithm has a unique limit point (line 263: ""the"" limit point).
Even the algorithm has limit points, it may not be unique. Consider the sequence x_i = (-1)^n - 1/n, clearly it has 2 limit points: +1 and -1.

For the rest parts of the paper, my comments are as follows.
1. The new formulation seems interesting, and it can be separately discussed from the block coordinate descent part, and it looks to me the major novelty is the new formulation but not the algorithm.
However, the major concern for the new formulation is that it has much more variables, which can potentially lead to spatial unfeasibility when there are more data instances. This problem hinders people from applying the formulation to large-scale problems and thus its usefulness is limited.
2. I do not see that the problem in line 149 is always convex with respect to W. This requires further elaboration.
3. In the introduction, some efforts were put in describing the issue of saddle points. However, this manuscript does not handle this problem as well. I'd suggest the authors to remove the related discussion.
4. The format of paper reference and equation reference is rather non-standard. Usually [1] is used for paper reference and (1) is for equation reference.
5. The comparison with Caffe solvers in term of time is meaningless as the implementation platforms are totally different. I do not understand  the sentence ""using MATLAB still run significantly faster"" especially regarding the word ""still"", as matlab should be expected to be faster than python.
6. The comparison of the objective value in 3(a) is comparing apples to oranges, as the two are solving different problems.

=== After feedback ===
I did notice that in Line 100 the authors mentioned that U,V,W are compact sets, but as we see in line 124, U is set to be the nonnegative half-space, which is clearly unbounded and thus non-compact.
Therefore, indeed when U is compact, this is correct that there will be at least a limit point and thus the convergence analysis may hold true (see details below), it is not the case for the problem (4) or (5).

Regarding the proof, I don't agree with the concept of ""the"" limit point when t = \infty. I don't think such thing exists by nature.
As stated in my original review, there might be several limit points, and by definition we always have all limit points are associated with t = \infty, just in different ways.
The authors said in the rebuttal that because their algorithm converges so ""the"" limit point exists, but this is using the result proved under this assumption to prove that the assumption is true. I don't think that is a valid way of proof.

In addition, in the extremest case, consider that the sets U,V,W all consist of one point, say u,v,w, and the gradient at (u,v,w) is not zero. Clearly in this case there is one and only one limit point (u,v,w), but apparently it is not a stationary point as the gradient is non-zero.

Therefore, I will keep the same score.
"
Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes,"Ahmed M. Alaa, Mihaela van der Schaar",https://proceedings.neurips.cc/paper/2017/hash/6a508a60aa3bf9510ea6acb021c94b48-Abstract.html,"The authors propose a method of estimating treatment effectiveness T(x) from a vector of patient features x.  Treatment effectiveness is defined as  (health outcome with treatment Yw) - (health outcome without treatment Y(1-w)).  Presumably a health outcome might be something like survival time.  If a patient survives 27 months with the treatment and only 9 without then the effectiveness T(x) would be 18 months?

The authors estimate models of ""outcome with treatment"" and ""outcome without treatment"" jointly using RKHS kernel approximations on the whole dataset (I think there is a shared kernel).  

For a specific patient the effectiveness is based on the actual outcome of the patient which will be based on their features and their treatment condition minus the population model for the features of the opposite or counterfactual treatment condition.  

The population model is represented by an RKHS approximation which generates Bayesian posterior confidence bounds allowing the algorithm to integrate over uncertainty in the treatment effectiveness.  I like that the authors call this out in line 139: integrating over the uncertainty in the counterfactual condition improves their results. They employ an empirical Bayes framework in which they optimize for hyperparameters that control distributions over health outcomes. They use a loss function that tries to maximize accuracy of the actual outcome observed for a patient with specific features and minimizing the variance of the counterfactual ... 

This seemed a bit counterintuitive to me - wouldn't you want to maximize the variance of the counterfactual in order to be consistent with the principle of least commitment / maximum entropy? Don't make conclusions that are stronger than the data supports??

They end up using a mixture of two kernels to provide additional flexibility in modeling. 
The algorithm does a matrix inversion which I think is common in RKHS style approximation which may limit its scalability somewhat (also true of Gaussian Process based approaches). The example they show has only 1000 patients which is very small for an EHR dataset generally which can have millions of patients.

The goal of the authors is to be able to handle situations with strong selection bias (that is, the treatment is correlated with the patient features so we don't get an unbiased selection of treatments for any given condition).  They end up having to filter the datasets they work with to create stronger selection bias.  It is not clear whether this was a necessary step for them to demonstrate advantage over the other algorithms they benchmark against.

For their filtered dataset they show metrics with around 1/3 the error rate of competing state of the art methods. They attribute the gains to the ability of their algorithm to compensate for selection bias by integrating over uncertainty in the counterfactual???

The paper has a lot of technical jargon which is not  obviously necessary to explaining the intuitions clearly. Do we really need the details of the representer theorem in the paper? I would prefer using the space to describe intuitions on why it works more clearly.

line 27: ""glints"" is used as a verb in an awkward way

I liked Figure 1, but it took me a while to understand it. It might be worth explicitly calling out the roles of the 3 subfigures in the caption.  If you used fewer data points it might be more obvious that the realized factual outcomes and the counterfatual distributions are supposed to align ... maybe it would make sense to stack these graphs vertically so they share the same x axis?

line 53: ""helps creating"" is an awkward expression

lines 251 ... it would be nice if the authors could describe a bit more about the competing bench mark methods and why they perform the way they do? There is a short related work section but it doesn't use any of the names of the algorithms.  I don't know if these are the same as the comparison benchmarks or not.

Would the ordering be the same with a dataset that has low selection bias?

","In this paper, the authors proposed a novel multi-task learning framework using a nonparametric Bayesian method. Also, they demonstrate that the proposed method significantly outperforms the state-of-the-art.

This reviewer has the following comments:
- Bayesian perspective on the multi-task learning is not a novel concept. The authors need to review other literatures and include them in the introduction.

-  It is mentioned that the computational burden of algorithm 1 is dominated by the O(n^3) matrix inversion in line 13. It would be interesting to compare to the other methods in terms of the computational efficiency. Also, the author mentioned that for large observational studies, it can be ameliorated using sparse approximations. But it would be interesting to see the robustness and computational efficiency for large dataset.


","The authors develop a nonparametric bayesian method in order to infer individualised treatment effects on observational data. In particular they use a multi task gaussian process prior with a linear coregionalization kernel as covariance. They use a regularized risk-based empirical Bayes method to find the optimal set of hyperparameters and interpolant. It jointly minimises the error of factual outcomes and the posterior counterfactual variance. They show that their method outperforms the state of the art in two observational datasets. 


Significance 
I believe that this is an interesting model that infers ITEs on observational data.

Clarity
Some parts of the model proposed by the authors are well-described and the structure of the paper makes it relatively easy to follow.  

However, some sections would benefit from clearer explanations: 
1. the paragraph entitled ""Related works"". The explanation seems to be too concise and sometimes imprecise: for example, the authors do not define \alpha (in the text nor in the Figure), the reference is not accurate: Sec 3.2 [15] instead of Sec 2.3 [15], the response surface f_0 and f_1 are not mathematically defined. 
As a consequence I believe that the feature based interpretation lacks clarity although it is one of the main contribution of the paper. 
2. the choice of the linear model of regularisation could be justified more precisely, for example the choice and influence of the parameters in A_0 and A_1 (equation (9)). 


In the experiment section, the Bayesian coverage value \gamma is not given. 
In the IHDP and the UNOS datasets, could the authors explain how the selection biases that are introduced affect the model? In particular, the selection bias that is introduced in the IHDP dataset is not described. 



Correctness
Equation (10): it seems that Q(\theta) still depends on the whole dataset through the first term (posterior counterfactual variance) although the authors are evaluating the error via LOO-CV. 
In the proof in the supplementary, indexes 'i' are missing on many of the differentials. 

"
Learning Overcomplete HMMs,"Vatsal Sharan, Sham M. Kakade, Percy S. Liang, Gregory Valiant",https://proceedings.neurips.cc/paper/2017/hash/6aca97005c68f1206823815f66102863-Abstract.html,"The paper studies the learnability of HMMs in the setting when the output label size m is smaller than the number of states n. This setting is particularly tricky since the usual full-rank assumption that comes up in tensor decomposition based methods is not valid. This paper gives both algorithms and information-theoretic lower-bounds on the learnability of overcomplete HMMs, which clearly increases our state of understanding. 

1. They show that HMMs with m=polylog(n) even for transition-matrix being a random dense regular graphs, can not be learning using poly(n) samples even when the window size is poly(n). 

2. They show that overcomplete HMMs can be learned efficiently with window size O(log_m n), under some additional four conditions (well-conditioned-ness of T, no-short-cycles, sparsity of graph given by T and random sparse output distributions)
	

Positives:

1. The lower bound is interesting and surprisingly strong. This is unlike the parity-based lower bounds in Mossel-Roch which are more computational by nature, and more similar to moment-based identifiability lower bounds known for Gaussian mixtures and multiview models (see Rabani-Swamy-Schulman). 

2. This paper tries to understand in depth the effect of the structure of the transition matrix on learnability in the overcomplete setting. 

3. The techniques developed in this paper are new and significantly depart from previous works. For instance, the algorithmic result using tensor decompositions departs significantly from previous techniques in analyzing the Khatri-Rao (KR) product. Analyzing overcomplete tensor decompositions and these KR products are challenging, and the only non-generic analysis I am aware of (by Bhaskara et al. for smoothed analysis) can not be used here since the different tensor modes are dependent. At the heart of this argument is an interesting coupling argument that shows that for any two different starting states, the random walk distributions after O(log_m n) steps will be significantly different if the observation matrices are random. This crucially depends on the well-conditioned-ness of T and no-short-cycle condition which they argue is needed for identifiability. 

4. I like the discussion of assumptions section 2.4; while the paper assumes four conditions, and they give qualitative reasons for why most of them are necessary. 

Negatives: 

1. The proofs are fairly dense (especially Theorem 1 , Lemma 3) and the error analysis is hand-wavy at places . Though I have no reason to suspect the correctness, it would be good to improve the readability of these proofs. 

2. It is not clear why some of the conditions are necessary. For instance, the random sparse support assumption seems stronger than necessary. Similarly, while the lower bound example in Proposition 1 explains why an example with just short cycles is not identifiable, this doesn't explain why there should be no short-cycles in a connected, well-conditioned T.   
	
Summary: Overall, the paper presents a nice collection of results  that significantly improves our understanding on overcomplete HMMs. I think the techniques developed here may be of independent interest as well. I think the paper is easily above the bar. 

Comments:

The identifiability results for HMMs using O(log_m n ) should also be attributed to Allman-Mathias-Rhodes'2010. They already show that the necessary conditions are true in a generic sense, and they introduced the algorithm based on tensor decompositions. 

Typo in Section 2.2: ""The techniques of Bhaskara et al. can be applied.."" --> "".. can not be applied ...""

","The paper addresses the problem of learning overcomplete HMM’s with discrete observation space. Main contribution of the paper is a series of necessary conditions that ensure the learnability of the model parameters with time window of finite size. In almost all cases (according to [15]), the length of the minimal time window grows logarithmically with the size of the hidden space. The authors identify a set of transition matrices for which the model is actually learnable with polynomial sample complexity, under certain condition on the observation distributions.

Here are few comments:
- the paper seems to be closely related to [15], where the same technique is used to prove a quite general result. The two main theorems in the paper refine this result in the following ways: Theorem 1 defines a set of models that belong to the measure-zero set of non-learnable HMM mentioned in [15]; Theorem 2 defines a set models that are learnable under certain conditions on the transition and observation matrices.

- the stated necessary conditions are pretty technical and their relevance for practical applications is very shortly discussed. As the major practical application seems to be to language generative models, it would help to see and explicit example or some empirical results on this field. 

- the applicability of the results relies on some general conditions imposed to the observation matrix through the paper.  How `realistic' is assumption 4 on the output distribution? How is it related to the Kruskal rank of the observation matrix?

- all results in the paper are obtained by assuming that the HMM is learnt by the method of moments. What can be said about other approaches? Does the proposed conditions apply if the model parameters are obtained via an alternative method? 

- the paper contains lot of wordy explanations of the technical assumptions and results but any informal description on how the theorems are proven. 

- in theorem 2, it would be nice to see the explicit dependence of the sample complexity in terms of the singular value of the transition matrix. Is this related to epsilon? In condition 1, the smallest singular value of the the transition matrix is required to be larger than an inverse function of the alphabet size. For very large hidden space this requires c >> 1 and makes the conditions quite restrictive. Are the `allowed' transition matrices still nontrivial in the limit n >> m?

- it would be helpful to see a schematic version of the mentioned learning algorithm. 
What happens if the joint diagonalization approach is replaced by a different tensor decomposition scheme? Some of them apply directly to the overcomplete case, may this allow shorter time windows?  

- the authors could also add some more extended experiments to show how sparsity, degree and short cycles effects the recovery of the true parameters via the method of moments.
"
Convolutional Phase Retrieval,"Qing Qu, Yuqian Zhang, Yonina Eldar, John Wright",https://proceedings.neurips.cc/paper/2017/hash/6ad4174eba19ecb5fed17411a34ff5e6-Abstract.html,"The authors of the article consider phase retrieval problems where the measurements have a specific form: they are the convolution of the n-dimensional input signal x with a random filter of length m. The authors show that, provided that m is larger than O(n), up to log factors, and up to a term that is related with the sparsity of x in the Fourier domain, a simple non-convex algorithm can solve such problems with high probability.
The proof is quite technical. Indeed, the measurement vectors are highly correlated with one another. To overcome this difficulty, the authors use ""decoupling techniques"", together with results from [Rauhut, 2010] and [Krahmer, Mendelson, Rauhut, 2014].


I liked this article. I had not realized it was so complex to prove the convergence of a simple non-convex phase retrieval method in a very structured case, and, in my opinion, it is good that it has been done. The result by itself is interesting, and the proof techniques developed by the authors seem general enough to me so that they can be reused in other settings.

As previously said, the proof is quite technical, and long. There are 38 pages in the appendix. I tried to read as much of it as possible, but, given the time constraints of the review process, I certainly did not have the time to deeply understand everything. However, what I read seems both clear and solid.

The organization is also good. However, maybe the authors could discuss in more detail the case of coded diffraction patterns ([CLS15b]). Indeed, the case of phase retrieval with coded diffraction patterns is, at least at first sight, very similar to the case considered in the article: seen in the Fourier domain, coded diffraction patterns consist in convolving the input signal with (m/n) random filters of length n, while, in convolutional phase retrieval, the input signal is convolved with a single random filter of length m. I would appreciate the authors to compare their result and their proof techniques with the ones obtained for coded diffraction patterns.


Minor remarks and typos:
- Before Equation (4): add ""that""?
- l.95 (and also in the ""basic notations"" paragraph of the appendix): "", because"" should be "". Because"".
- Paragraph 3.1: I did not know the name ""alternating direction method"". I think that ""alternating projection method"" is more common.
- Equation (12), and the previous two: ""i\phi"" is missing in the exponential function.
- l.180: "". Controlling"" should be "", controlling"".
- l. 209: ""are not"" should be ""is not"".
- Equation (19) (and the one after Equation (46) in the appendix): it took me a while to understand why this equation was true. Maybe it could be proved?
- l.220: ""this control obtains"" should be ""we obtain this control"".
- Equations (21) and (23): \mathcal{Q} should be \hat\mathcal{L}.
- The paragraph starting at line 244 seems a bit redundant with the explanations around Equation (22).
- Experiments: the main theorem says that, when x is proportional to the all-one vector, the initialization step still works, but the local descent does not. It would be nice to add an experiment that checks whether this is really what happens in numerical simulations.
- [CC15] has been published.
- The accent on ""Candès"" is missing in some of the bibliography entries.
- Appendix, l.58: ""we proof"" should be ""we prove"".
- Proposition 2.2: some ""that"" and ""such that"" could be added.
- l.92: ""be"" should be ""is"".
- l.150: I do not understand ""for \gamma_2 functional of the set \mathcal{A}"".
- Lemma 3.11: the last equation contains a \sqrt{\delta} while, in the statement, it is a \delta (no square root).
- l.202: I think that a ""1/m"" is missing in front of the expectation.
- l.217: ""for w"" should be ""for all w"".
- l.220: ""define"" should be ""be defined"".
- l.222: ""define"" should be ""defined"".
- l.224: ""is constant"" should be ""is a constant"".
- Equation after line 226: I think that there should be no parentheses inside the sup.
- Theorem 4.3: ""are"" should be ""be"".
- l.268: a word is missing before ""that"".
- Second line of the equation after line 278: the upper bound in the second integral should depend on d, not v.
- l. 279: ""where the"" should be ""where in the"", and ""Choose"" should be ""choosing"".
- Equation after line 292: I do not understand why the first inequality is true.
- l. 297: ""so that"" should be removed.
- l.305: ""is complex"" should be ""is a complex"".
- Equation just before line 326: there should be parentheses aroung the logarithms.
- Lemma 5.3: I do not understand the meaning of the first inequality: what is the difference between the first and the second term?
- l.466: ""is close"" should be ""are close"".
- l.481: ""are defined"" should be ""is defined"".
- l.484 and 486: ""first"" and ""second"" have been exchanged.
- I think that ""we have [Equation] holds"" (used multiple times) could be replaced with ""we have that [Equation] holds"" or simply ""[Equation] holds"".
- Equation after line 492: it could be said that it is a consequence of Lemma 6.2.
- Lemma 6.5, main equation: a multiplicative constant is missing before \delta.
- Equation after l.510, sixth line and after, until the end of the proof: the \leq sign should be a \geq sign.
- Equation after l.513: why w^{(r)}? Also, I think that there should maybe be close to 1 multiplicative constants before the last two terms of the sum.
- Equation after l.551: I do not understand the presence of ||x||.
- Lemma 6.3 and lemma 8.1 are not perfectly identical (there is a 2 in 6.3 that becomes a 2+\epsilon in 8.1).
- References: the name of the authors is missing in reference [led07].","This paper considers the phase retrieval problem where the measurement strategy is as follows: convolve the hidden vector z with a random pattern a and observe the amplitude of it.

Similar problems have been considered and analyzed when measurements arise from inner products with i.i.d. random Gaussian vectors (see Candes et al. Wirtinger flow as well as convex optimization based papers).

This paper is a technical and the main contribution is the fact that authors analyze an algorithm which is
a) highly nonconvex (and runtime efficient algorithm). Additionally, the algorithm does not require ""resampling"" which means they can use the same samples for their iterations. This makes the problem more challenging and algorithm more realistic
b) Structured measurements that can be diagonalized by Fourier transform and that are more realistic than Gaussian samples.

I believe the paper will be a nice contribution to NIPS however authors can do a better job motivating the problem they analyze. While Figure 1 and 2 is informative, I don't see why Figure 3 is real data. They might want to find an application where convolutional measurements are directly relevant (in a similar fashion to works in coded diffraction patterns). This would increase the practical motivation of the paper.

They also mention their techniques can be useful in other problems where convolutional measurements exist. They should consider elaborating more on it.

Finally, can the same analysis address the Fourier measurements (where signal is again zero-padded) instead of convolutional. Since convolutional measurements are diagonalized by Fourier transform, they are already fairly related but it is not clear if the same ideas apply or not.

Also they should cite related works such as Non-convex phase retrieval from STFT measurements and mention how their techniques compare.
","This paper studies the convolutional phase retrieval problem.  It can be transformed into standard form for phase retrieval, where the design matrix is partial random circulant. This is a different from most previous work where the design is sub-gaussian. The authors propose to solve a weighted version of the objective for Reshaped WF. Spectral initialization + gradient descent are used to minimize it.

This is a very technical paper, so my questions mainly focuses on this part. I would kindly ask the authors to elaborate the following:

1.   I like the setup where rows of A (design) are dependent. Yet from my understanding, the partial random circulant design still satisfies RIP (Thm 3.10). If you use the normal fourth order objective (maybe with or without weights), would it be possible that the result from Procrustes Flow [1] can be applied here? 

2. The proof follows the idea of recent analysis for alternating minimization. The Reshaped WF paper tries to prove local regularity condition.  Out of my curiosity, why do the authors choose the AltMin route? Would it be easier if you try to prove the latter? It is really nontrivial to use decoupling to bound the phase term Tau_2 in line 177. How does your proof technique relate to geometry, intuitively?

3. By Proposition 2.2, we can see the basin of contraction is shrinking in higher dimension. Is the log factor some artifact from the analysis? I think the rate is acceptable, viewing that the radius of unit n-ball decreases approximately as O(n^{1/2 + 1/2n}).

Thanks!

[1] Low-rank Solutions of Linear Matrix Equations via Procrustes Flow"
Stochastic and Adversarial Online Learning without Hyperparameters,"Ashok Cutkosky, Kwabena A. Boahen",https://proceedings.neurips.cc/paper/2017/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html,"This paper introduces a parameter-free algorithm for online learning that attains non-trivial guarantees in both the stochastic and adversarial settings simultaneously. Previous work in this area includes the MetaGrad algorithm, which was not parameter-free and required knowledge of L_max, but simultaneously attained O(B L_max \sqrt{T}) regret in the adversarial setting and O(dBL_max log(T)) in the stochastic or strongly setting. The key insight by the authors is to interpret an upper bound on the regret as an expectation over the learner's output vectors (scaled by the norm of the gradient at each round), to rewrite this expression in terms of a bias and variance term, and then to shift the regularization component of an existing parameter-free algorithm by a running estimate of the previously mentioned expectation. This results in an algorithm with O(\sqrt{T} \log(T)^2 L_max B) regret. When the loss functions satisfy a certain curvature property called \alpha-acute convexity, the algorithm is able to attain a regret in O(\log(T)^4 L_max B/\alpha). 

This paper introduces a clever method for an interesting area of online learning, that of parameter-free algorithms that work well in multiple environments. The manuscript is well-written and the authors provide the reader with an abundance of both insight into the problem and intuition for their solution. The main concern I have for the paper is the practical utility of their algorithm. The algorithm's adversarial guarantee includes an extra log(T)^2 factor in addition to the usual terms, and, as the authors admit, the stochastic guarantee in the stochastic setting may only be compelling for extraordinarily large values of T. Still, this paper is at the very least an interesting theoretical contribution. 

Some technical questions:
1) Line 107: Can the authors explain how they arrived at the 2nd inequality?
2) Line 184: Should the last expression be \frac{\mu}{2} \|w - w^*\|^2? 
3) Line 187: Can the authors explain how they arrived at the 2nd inequality (i.e. .... \geq \frac{\mur}{2}\|w\|)?
4) As the authors mention, for strongly convex functions, \alpha = \mu/(2 L_max). Thus, the stochastic bound will become O(\log(T)^4B/\mu). However, the standard online convex optimization guarantee for strongly convex loss functions is O(log(T)L^2/\mu). Apart from the additional log(T) terms, can the authors discuss the difference in dependence on B vs L^2?

Post author response: I've read the author response and choose to maintain my score. 
     ","The paper presents a new parameter-free algorithm for online convex optimization. The algorithm guarantees sqrt(T) regret in the adversarial setting while achieving polylog(T) expected regret in certain stochastic settings, referred to as ""acutely convex"" problems, in which the expected gradient at every point has positive correlation with the direction to the global optimum.

The algorithm proposed by the authors is a modification of the parameter-free FreeRex algorithm (Cutkosky & Boahen, COLT'17), which was analyzed only in the adversarial online setting. The algorithmic modification itself is insightful and non-trivial, and the intuition behind it is nicely explained in Sec. 2 of the paper.

That being said, I was not entirely convinced by the potential merits of the new algorithm. The (poly-)logarithmic regret guarantee of the algorithm is valid only for the class of ""acutely convex"" stochastic problems, which is shown to contain strongly convex problems but does not seem to contain much more than that. The authors do give an example of an acutely convex stochastic problem which is not strongly convex, but it is merely a one-dimensional toy example which I found not too convincing.

In the strongly convex case, however, log(T) regret can be easily obtained (in both the adversarial and stochastic settings) using simple online gradient descent. While it is true that the proposed algorithm can automatically tune itself to exploit strong convexity without manually tuning any hyperparameters, simpler algorithms with this property were already suggested in the past (e.g., Bartlett et al, NIPS 2007; Sani et al, NIPS 2014).

On top of the above, the algorithm is not very practical, as it is not at all obvious how to efficiently implement it even on very simple convex domains such as the L2 or L1 unit balls. Also, the log^4(T) regret bound of the algorithm is interesting only for extremely large values of T.","The authors introduce FreeRexMomentum, an online algorithm that shows O(sqrt(T)) complexity in the adversarial setting while remaining logarithmic (log4) in the stochastic setting. The algorithm is an iterative improvement over FreeRex (a Follow-The-Regularized-Leader) approach that requires no prior information on the problem's parameters, by adding a cumulative term to the regularizer. This term includes the past predictions weighted by the corresponding loss gradient. It acts as a momentum that accelerates the learning in the adversarial case. 
The algorithm is a nice contribution to the recently introduced adaptive algorithms in that it unifies the stochastic and adversarial case with a single algorithm without (much) compromising the optimality in the former case. The analyses seem sound even though I cannot  appreciate the extent of the contribution here since I am not knowledgeable as for the classical ""tools"" used in this literature. 
There is one worrying aspect of the algorithm however, it is on the dependency on $d$ the dimensionality of the search space. I am not sure to which extend this makes the algorithm unpractical for some problems
Finally, an experimental section with some practical applications would have greatly enriched the paper, showing that the same algorithm can be applied in different settings. "
Masked Autoregressive Flow for Density Estimation,"George Papamakarios, Theo Pavlakou, Iain Murray",https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html,"This is a really well written paper that presents a unified view of various state-of-the-art techniques for density modeling. More specifically: the authors show that autoregressive models, normalizing flows, inverse autoregressive flows and RealNVP/NICE can be easily understood from a common perspective. Some aspects of this perspective were already known and, for example, described in the IAF paper by Kingma et. al. But this paper describes the connections between these models and techniques in far more detail and in a plausible and easy to follow way. Furthermore, starting from this perspective, the authors derive a new variant and show that they obtain very promising results that compete with (or beat) various state of the art approaches. 
","SUMMARY 
* The paper presents a method for density estimation based on a stack of autoregressive models, exploiting a normalizing flow interpretation of autoregressive models. This is an approach to increase the flexibility of the conditionals in masked autoregressive density estimation in contrast to say mixture models. 

CLARITY 
* The paper is very well written. It does a good job at explaining the relations of existing methods. 
* The experiments are presented in a convincing way and code is provided. 

RELEVANCE
* The proposed approach appears sensible and seems to provide substantial benefits over other approaches, especially in the case of conditional density estimation. 
* The experiments show a good performance of the proposed methods. While other methods have shown very good performance when incorporating knowledge about the problem domain, the proposed methods are demonstrated to provide a competitive alternative for general purpose density estimation. 
* The discussion seems to open promising avenues for further research. 
","This paper is generally well written and I enjoy reading it. It introduces an expressive density model called masked autoregressive flow (MAF) that stacks multiple MADE layers to form a normalizing flow. Although it seems a bit incremental since the techniques involved have been studied in IAF and MADE, this paper does a good job elaborating on different types of generative modeling and providing guidelines for their use cases. It also makes a connection between MAF and IAF. Only a few comments/questions below:

* It'd be helpful to motivate a bit more on the advantage of density models. What are the applications or downstream tasks that make density models more suitable than their alternatives such as VAEs or GANs? For example, does the proposed density model admit efficient algorithms for marginalization or other inference tasks?

* When applying multiple transformations in the normalizing flow, is it implied that the conditionals need not be Gaussian as in eq. 2 because the density is computed according to the density of u? When u is transformed from some lower-level random vectors, it could very likely be non-Gaussian. Do I understand correctly?

* Does the base density of an MAF (line 225) refer to the density of u in eq. 3 or the density of p(x_i|x_{1:i-1}) in eq. 1? I assumed it's the former.

* When comparing the results from MAF (5), MAF (10) and MAF MoG (5) on the data sets POWER and MNIST, MAF MoG outperforms the other two. I have the impression that if using multiple normalizing flow layers where each layer has multiple non-linear MADE layers, MAF should be universal without MoG. What's the authors' opinion on this?
"
QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic",https://proceedings.neurips.cc/paper/2017/hash/6c340f25839e6acdc73414517203f5f0-Abstract.html,"Update: I decrease slightly the grade due to the mismatch between theoretical and practical results that could be better covered. Still this paper has strong experimental results and some theoretical results. I would encourage the authors to improve on the gap between the two.

In this paper the author introduce Quantized SGD (QSGD), a scheme for reducing the communication cost of SGD when performing distributed optimization. The quantization scheme is useful as soon as one has to transmit gradients between different machines. The scheme is inspired by 1 BitSGD where 32 bits are used to transfer the norm of the gradient and then 1 bit per coordinate to encode the sign. The author extends this approach by allowing to send extra bits per coordinate to encode the scale of the coordinate. They obtain an unbiaised stochastic quantized version of the gradient. Therefore, one can still see QSGD as an SGD scheme, but with extra variance due to the quantization. Because of the quantization of the scale of each coordinate, small coordinates will be approximated by 0 and therefore the quantized gradient will be sparse, unlike 1BitSGD which gives a dense gradient. The authors use this property to further compress the gradient by encoding the non zero coordinates and only transfering information for those. In order to encode the position of those coordinates efficiently, the authors uses the Elias code, a variable length integer encoding scheme.
The authors then applies this approach to deep learning model training.
The authors also present how to quantize SVRG, a variance reduced SGD method with linear rate of convergence that can be used for convex optimization.

Strengths
=========
- The introduction is very clear and the authors do a great job at presenting the source of inspiration for this work, 1BitSGD. They also explain the limits of 1BitSGD (namely that there is no convergence guarentees) and clearly state the contribution of their paper (the ability to choose the number of bits and therefore the trade off between precision and communication cost, as well as having convergence guarantees).
- the authors reintroduce the main results on SGD in order to properly understand the convergence of SGD in relation with the variance or second order moment of the gradients. This allow the reader to quickly understand how the quantization will impact convergence without requiring in depth technical knowledge of SGD.
- As a consequence of the above points, this paper is very well self contained and can be read even with little knowledge about quantization or SGD training.
- The authors did a massive investigation work, regarding both synchronous and asynchronous SGD, convex and non convex, as well as variance reduced SGD. Despite all this work, the authors did a good job at keeping the reading complexity low by separating properly each concern, sticking to simple case in the main paper (synchronous SGD) and keeping more complex details for the supplementary materials.
- This paper contains a large amount of experiments, on different datasets, different architecture and comparing with existing methods such as 1bit SGD.
- This paper offers a non trivial extension of 1bit SGD. For instance this method introduce sparsity in the gradient update while keeping convergence guarantees.


Improvements
============
- The result on sparsity in Lemma 3.1, whose proof is in Lemma A.5 in the supplementary material seems wrong to me. Line 514 in the supplementary materials, first `u_i` should be replaced by `|u_i|` but more importantly, according to the definition on line 193 in the main paper, the probability is not `|u_i|` but `s |u_i|`. Thus, the sparsity bound becomes `s^2 + sqrt(n) s`. I would ask the authors to confirm this and update any conclusion that would be invalidated. This will have an impact on Lemma A.2. I would also encourage the authors to move this proof with the rest of the proof of Lemma 3.1.
- Line 204, the description of Elias(k) is not super clear to me, maybe giving a few examples would make it better. The same applies to the description in the supplementary materials. Besides, the authors should cite the original Elias paper [1].
- The authors could comment on how far is the synchronous data-parallel SGD from current state of the art practice in distributed deep learning optimization. My own experience with mini-batches is that although theoretically and asymptotically using a batch size of B can make convergence take B times less iterations, this is not always true especially when far from the optimum, see [2] for instance, in practice the `L R^2/T` can dominate training for a significant amount of time. Other approaches than mini-batch exist, such as EASGD. I am not that familiar with the state of the art in distributed deep learning but I believe this paper could benefit from giving more details on the state of the art techniques and how QSGD can improve them (for instance for EASGD I am guessing that QSGD can allow for more frequent synchronization between the workers). Besides the model presented in this paper requires O(W^2) communication at every iteration where W is the number of worker, while only O(W) could be used with a central parameter server, thus this paper setup is particularly beneficial to QSGD. Such an approach is shortly described in the supplementary materials but as far as I could see there was no experiment with it.

Overall this papers contains novel ideas, is serious and well written and therefore I believe it belongs at NIPS. It has detailed theoretical and experimental arguments in favor of QSGD. As noted above, I would have enjoyed more context on the impact on state of art distributed methods and there is a mistake in one of the proof that needs to be corrected.

References
==========
[1] Elias, P. (1975). Universal codeword sets and representations of the integers. IEEE transactions on information theory.
[2] Sutskever, Ilya, et al. On the importance of initialization and momentum in deep learning. ICML 2013","Summary: This paper proposes a variant of SGD motivated by distributed implementation, where a novel compressing technique was introduced to quantize the stochastic gradient into a predefined range. It also analyzes the variance of the compressed SG and its affect on different stochastic algorithms. 

Strength: The proposed quantization technique is novel and leads to quantifiable trade-off between communication and convergence. Much experimental results on training CNN and RNN justify the proposed method. 

Questions/Comments: 

1. In experiments, ""When quantizing, we scale by the maximum value of the vector, ..."". Is there any theoretical guarantee on using this? If yes, the paper should mention it before when presenting the encoding. 

2. In experiments, ""We do uno quantize very small gradient matrices in QSGD"", how small is very small? 

3. For QSVRG variant, it seems that the proof only holds for a smooth objective without a simple non-smooth regularizer. Can you extend the analysis for that case as well? 


I have read the authors' feedback. It is still not clear to me what guarantee that scaling by the max norm can provide. 
","Paper summary: This paper proposed a quantization based gradient compression approach for accelerating distributed/parallel stochastic gradient descent methods. The main idea is to quantize each component of the gradient vector by randomized rounding to a discrete set of values, followed by an Elias integer encoding scheme to represent and transit the quantized gradient. Theoretical analysis shows that the proposed method can considerably reduce the expected per-iteration bits of communication between machines/processors without sacrificing the convergence guarantee of exact SGD. The practicality of the proposed method is examined in training neural networks for image classification and speech recognition tasks. 

Comments: The paper is generally well organized and clearly presented. The empirical study is extensive and the reported results show some promise in achieving reasonable trade-off between efficiency and accuracy in a number of multi-GPU CNN training tasks. The reviewer has, however, several major concerns regarding its technical contents as in below:

(1) The novelty of the proposed approach is limited. As acknowledged by the authors, the idea of gradient quantization and encoding has been extensively explored in neural networks acceleration and/or compression (see, e.g., [3, 13, 14, 17, 30, 35]). Although it is interesting to employ such a strategy on multi-GPU distributed learning tasks, the overall technical contribution in this line is rather marginal. Particularly, the quantization function Q_s(v) seems to be defined in a quite standard way; it is unclear why such a quantization scheme is more preferable than the state-of-the-arts. The used integer encoding method is also off-the-shelf.  

(2) The theoretical contribution is weak. The most interesting theoretical result of this paper is Theorem 3.2. The main message conveyed by this theorem is that by using the proposed quantization and encoding scheme, the per-iteration communication complexity is, roughly speaking, reduced by $\sqrt{n}$ while the variance of stochastic gradient is increased by $\sqrt{n}$. The reviewer is not convinced that such a result well justifies in theory the efficiency of method, as the improved communication efficiency is achieved at the price of dramatically increased iteration complexity. 


(3) There exists a gap between method description and implementation. The authors mentioned in line #283-284 that “When quantizing, we scale by the maximum value of the vector (as opposed to the 2-norm), as this simplifies computation and slightly reduces variance.”. It is a bit unclear to the reviewer why L2-norm (vector sum) is more computationally intensive than infinity-norm (vector sort) in the considered setting. Even if this is the case, is it possible to modify the theory part so as to justify such an implementation trick?

(4) The paper title can be improved. The current title sounds too general and it should be modified to better reflect the “gradient quantization & encoding” nature of the proposed method. 
"
Deanonymization in the Bitcoin P2P Network,"Giulia Fanti, Pramod Viswanath",https://proceedings.neurips.cc/paper/2017/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html,"The paper explores the effect of modifying the transaction-flooding protocol of the bitcoin network as a response to deanonymisation attacks in the original pre-2015 protocol. The authors model the original protocol (trickle) and the revised variant (diffusion) and suggest that deanonymisation can equally be established with both protocols. The authors further support their claim by exploring both models in simulations. 

Quality of Presentation: 

The paper is well-written (minor aspects: some incomplete sentences, unresolved reference) and provides the reader with the necessary background information to follow. 

The representation of the transaction-flooding protocols is comprehensive and introduced assumptions are highlighted.

Content-related questions:

- The authors refer to probability of detecting participants of 30%, which is relatively low (but indeed considerable). A fundamental assumption of performing the deanonymisation, however, is that the listening supernode would connect to all active bitcoin nodes. How realistic would it be in the future (assuming further growth of the network) that a supernode would be able to do so? -- Edit: The argument for the use of botnets made by the authors is a convincing counterpoint.

- While the overall work is involved, I find that the paper does not comprehensively answer the original claim that both trickle and diffusion perform equally good or bad at maintaining anonymity. Observing Figure 6, we can find that Trickle and Diffusion simulated show differences in probability of around 0.1 (with Diffusion performing worse than Trickle), which is not negligible. Figure 7 shows the operation on a snapshot of the real network, with an inverted observation: Diffusion performs better than Trickle. (Admittedly, they converge for larger number of eavesdropper connections.) The discrepancy and conflicting observations are worth discussing, and the statement that both protocols have `similar probabilities of detection' needs to be clarified (How do you establish 'similarity'?). ","The paper is extremely interesting, very well motivated, and was a pleasure to read! 

1.] The paper is extremely well written and overall very clear.

2.] Can comparisons be made with existing methods? If so, why weren't these made? 

3.] Can the authors discuss robustness to model misspecification in detail (both strengths and weakness)? 

4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)? 

5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data? 

6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss. ","The paper presents an interesting approach towards deanonymizing the Bitcoin network. The paper is presented in a tidy manner and contains enough figures to explain the approach and experiments. Especially the probabilistic analysis is nice and provides stimulating ideas. I did not find any problems and I recommend acceptance. 

Though the paper is well presented, I'm not completely sure that the paper fits to NIPS or could have stimulating impact to the NIPS community"
Learning with Average Top-k Loss,"Yanbo Fan, Siwei Lyu, Yiming Ying, Baogang Hu",https://proceedings.neurips.cc/paper/2017/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html,"This is an interesting paper that introduces and analyses a new way of aggregating individual losses over training examples, being an alternative for the commonly used average loss and the recently introduced maximum loss. The proposed average top-k loss lies in between of those two existing approaches. 

The premises concerning an alternative to the average loss shown in the beginning of the paper seem to be very valid. Indeed the behavior of the average loss in the situations presented in Figure 1 and described in the corresponding paragraphs are a good justification for this research topic. Also the analysis of the behavior of the maximum loss in comparison with average and average top-k for the case of non-separable data is interesting.

Interestingly,  in the experiments on all the datasets the accuracy of the model optimized for average loss is better than the one optimized for max loss. According to [19] there are data sets for which max loss should perform better than the average loss. It would be interesting to include such data sets to the experimental study and see how the average top-k loss performs on them.

One could also try to use other aggregation function over the individual losses to be optimized on a training set (e.g., median, quantiles, OVA, different types of integral? Could you comment on that?

Minor comments:
- Is the name ""ensemble loss"" often used? For me, it sounds somehow confusing.

- Let \hat{k}^* be tuned on a validation set of size \hat{n}. If we use the entire training set for learinng a final model, should not k^* be appropriately adjusted to reflect the ratio \hat{k}^*/\hat{n}?

- line 5 and 307: can combines => can combine

After rebuttal:

I thank the authors for their response. 
","This paper investigates a new learning setting: optimizing the average k largest (top-k) individual functions for supervised learning. This setting is different from the standard Empirical Risk minimization (ERM), which optimize the average loss function over datasets. The proposed setting is also different from maximum loss (Shalev-Shwartz and Wexler 2016), which optimize the maximum loss. This paper tries to optimize the average top-k loss functions. This can be viewed as a natural generalization of the ERM and the maximum loss. The authors summary it as a convex optimization problem, which can be solved with conventional gradient-based method. The authors give some learning theory analyses of setting on the classification calibration of the Top-k loss and the error bounds of ATk-SVM. Finally, the authors present some experiments to verify the effectiveness of the proposed algorithm.

This work is generally well-written with some advantages as follows:
1) The authors introduce a new direction for supervised learning, which is a natural generalization of ERM and the work of (Shalev-Shwartz and Wexler 2016).
2) Some theoretical analyses are presented for the proposed learning setting. 
3) The author present a learning algorithm.

Cons:
1) Some statements are not clear, for example, top-k loss, which is similar to top-k ranking; more important, ensembles loss gives some impressions for ensemble learning whereas they are totally different. 
2) When we used the average top-k loss? I do not think that the authors make clear explanations. Intuitively, the performance (w.r.t. accuracy) of average top-k loss is less than ERM without noise and outliers, while I guess the average too-k loss algorithm may have good performance when deal with noise data and outliers.
3) How to choose the parameter k? The authors use cross-validation in experiments, while there is no some analyses. 
4) The authors should present some t-test on the performance on benchmark datasets. I doubt some experimental results. For example, the accuracy for German is about 0.79 for standard learning algorithms.
5) How about the efficiency in comparison with ERM and the work of (Shalev-Shwartz and Wexler 2016).
","This paper proposed a new ensemble loss (average top-k loss) for supervised learning problems. The average over the k largest individual losses over a training set is used as the objective function for supervised training purpose. The author proposed a convex formulation to implement this idea and formulate the overall problem as a convex optimization which can be solved using gradient methods. The author provides also analysis on how the free parameter k relates to the classification problems and its optimal setting. Similar to standard average loss, the author provide sample complexity bound for ATk based SVM formulation. 

* on experiments: 
- it would be better to provide some details on the datasets and why these datasets are selected
- if 10 random train/test splits are used, it is better to have std in table 1. 
- It would be great to include some more comments on why the K-plots in figure 3 have different trends for the four datasets. Is this connected to some property of the dataset? It is not clear to conclude from these plots how to select k in general. 
- for the regression problem RMSE is used a metric but the author considered both square loss and abs loss. It would be good to use both RMSE and MAE to measure the performance. 

* Figure 1 has four different synthetic datasets but it is really difficult to parse the information without detailed explanation. It would be more helpful to illustrate the key idea in the introduction by explaining what is the key difference of the four synthetic examples and comments on in which cases the ATk loss makes more sense and helped reduce certain errors. 




"
MaskRNN: Instance Level Video Object Segmentation,"Yuan-Ting Hu, Jia-Bin Huang, Alexander Schwing",https://proceedings.neurips.cc/paper/2017/hash/6c9882bbac1c7093bd25041881277658-Abstract.html,"This manuscript introduces an approach for Instance-level video object segmentation based on a recurrent neural net, which can capture the temporal coherence and fuses in each frame the outputs of two deep nets for each object instance: a binary segmentation net providing a mask and a localization net providing a bounding box. The experimental results demonstrate the advantage of the approach. Overall, the approach looks reasonable and the text is well-written. 

However, there are some issues in the manuscript and it needs a careful revision to remove all such issues. To name a few:

- Figure 1 and Figure 2 look a bit redundant and can be fused in some way.

- In line 127-129: it is stated that the input of the flow stream is the concatenation of the magnitude of the flow field from I_{t} to I_{t-1} and I_{t} to I_{t+1} and, again, the warped prediction of the previous frame \phi(t (y_{t-1}). However, this is not consistent with Figure 2.

- The sentence in line 114-115 lacks some text.","Paper summary
----------------
This paper presets a recurrent neural network approach to instance segmentation in videos. The method is a combination of several techniques (RNNs, bounding box proposals, optical flow, binary segmentation), that, in combination, yield best performance on the current standard DAVIS dataset for this task. The techniques itself are known, the novelty lies in the combination into a system that performs well. A well done ablation study gives some insights in what works best for this problem (please include the effect of fine-tuning on first frame as well). 

Review summary
-----------------
While not technically novel, the combined approach gives some insights in what works and why for this problem. This is likely of interest to researchers in this field. The paper is well written and experiments are well done, including a comprehensive ablation study. I have not seen this particular combination of models. In summary, I recommend acceptance of this submission, it is not a technical strong one, but presents an architecture that is among the best for video instance segmentation. I am not excited about this problem and have to admit that I am not fully aware of the literature on this problem. However, the submission is complete in the sense, that it achieves what it sets out to do and the combined system works well.

Details / comments
-------------------
- I recommend to include “fine-tuning on first frame” as a criterion for Table 1 and also for the ablation study. This technique is found to always improve performance but some methods do without. Since this severely effects runtime and the field should strive for instance independent mask propagation, methods that do without should be preferred. 
- What if the object is not moving and the optical flow is zero everywhere?
- Please state the test time per frame / video sequence. 

typo: line 64 the the
","Last year has seen a large progress in video object segmentation, triggered by the release of the DAVIS dataset. In particular, [Khoreva et al., CVPR'17] and [Caelles et al., CVPR'17] have shown that training a fully-convolutional network on the first frame of the video can greatly improve performance in the semi-supervised scenario, where the goal is to, given ground truth annotation in the first frame of a video, segment the annotated object in the remaining frames. In addition, Khoreva et al., demonstrated that feeding the prediction for the previous frame as an additional input to the network and utilising a parallel branch that operates on the optical flow magnitude further improves the performance. These approaches have indeed set the new state-of-the-art on DAVIS, but remained constrained to operating on a single frame and a single object at a time. 

In this work the authors propose to address these limitations as well as integrate some promising techniques from the recent object detection methods into a video object segmentation framework. Starting from the approach of Khoreva et al., which consists of two fully-convolutional networks pertained for mask refinement, taking rgb and flow magnitude as input respectively, as well as an object mask predicted in the previous frame, and fine-tuned on the first frame of the test video, they propose the following extensions:

1. The object mask from the previous frame is warped with optical flow to simplify mask refinement.

2. The model's prediction is ""cleaned up"" by taking a box proposal, refining it with a specialised branch of the network and suppressing all the segmentation predictions outside of the box.

3. A (presumably convolutional) RNN is attached in the end of the pipeline, which enforces temporal consistency on the predictions.

4. The model is extended to the instance-level scenarios, that is, it can segment multiple objects at once, assigning a distinct label to each of them.

A complex system like this is, of course, challenging to present in an 8 page paper. The authors address this issue by simply skipping the description of some of the components. In particular, the RNN is not described at all and the details of the bounding box generation are omitted. Moreover, the ""instance-level"" results are obtained by simply running the model separately for each instance and merging the predictions on the frame level, which can just as well be done for any other method. 

In general, the paper is well written and the evaluation is complete, studying the effect of each extension in separation. The study shows, that in the scenario when the networks are fine-tuned on the first frame of the video, the effect of all the proposed extensions is negligible. This is in part to be expected, since, as was shown in the recent work, training a network on the first frame of a video serves as a very strong baseline in itself. When no fine-tuning on the test sequence is done though, the proposed extensions do improve the method's performance, but the final result in this setting on DAVIS'16 remains more than 13% bellow the rgb-only, no-test-finetuning model of Khoreva et al. Though their model is trained on a larger COCO dataset, this is hardly enough to explain such a large performance gap with a much simpler model.

In the state-of-the-art comparison the full method outperforms the competitors on most of the measures on 3 datasets (DAVIS'16, DAVIS'17 and Seg-Track-v2). That said, the comparison with [Khoreva et al., CVPR'17] on DAVIS'17 is missing and the performance gap is not always significant relatively to the complexity of the proposed solution (0.9% on DAVIS'16). Especially, given the recent extension of Khoreva et al., (see reference [25] in the paper), which shows significantly better results with much simpler tools.

Overall, the authors have done a good job by combining insights from several recent publications into a single framework but failed to achieve convincing results.
"
Max-Margin Invariant Features from Transformed Unlabelled Data,"Dipan Pal, Ashwin Kannan, Gautam Arakalgud, Marios Savvides",https://proceedings.neurips.cc/paper/2017/hash/6d0f846348a856321729a2f36734d1a7-Abstract.html,"The paper proposes a new approach for learning representations that are invariant to unitary transformation. 

I enjoyed reading the paper for its conceptual novelty. The idea is to construct unitary-invariant kernels then use them to construct representations for unlabeled data (the unlabeled data refers to data that are not annotated with the corresponding unitary transformations).

There are a few issues the authors should address:

1.  What is the integration measure in \Psi_H (as defined on line 199. BTW, please define it clearly as a displayed math)?  Why the group (in the RKHS) is locally compact (such that there is a Haar measure)?  I think this point worths some thinking over as the compactness relies on the kernel mapping.

2. l(x) is a high-dimensional vector if the number of unlabeled transform data points is large.  Thus Line270-271 should be carefully examined as constructing those classifiers become non-trivial.

3. The treatment of unsupervised MMIF (Line270-279) is not very satisfying. How to choose K?  What to deal with the randomness of making those binary classifiers?    BTW line 270 uses N (# of classes) to represent the number of training samples.

4. Experiments:  the paper describes only one experiment where there is only one unitary-group (invariant to pose transformation) being considered. It will be helpful if more experiments are reported.  The paper also misses the obvious baseline of not using group-invariant kernels but instead using the same projection step (projecting onto binary SVMs) .

5.  Writing:  the paper has a significant drop in clarity starting section 3.  Other issues should be considered:
(1) An example: use an example to illustrate some the concepts such as group, Haar measure, etc
(2) Implementation details:  many are missing such as how to compute the integration with respect to the Haar measure empirically. Whether the transformation in the experiment study is discrete or not.
(3) Keep audience in mind:  for example,  it is a bit confusing to see Fig3 (and related texts).  Since \omega are linear combinations of l(x), so the MMIF would be linear combinations of l(x) too.  The extra steps could be explained better.
(4) Lack of a discussion section:  while most of them are boilerplates, the paper is lacking it and the paper could use the space to discuss some issues and weakness int he proposed approach. What if there are two or more invariant groups?  Why MMIF-Pixel is better than MMIF-VGG in the experiments?

In general, I like the paper despite above. It is a new way of thinking about representation learning. While a few things still need to be worked out,  there are enough sparkling and different thoughts to warrant the acceptance to NIPS to inspire further investigation.



","#### Paper summary

This paper considers the problem of learning invariant features for
classification under a special setting of the more general semi-supervised
learning; focusing on the face recognition problem. Given a set of unlabeled
points (faces), and a set of labeled points, the goal is to learn to construct
a feature map which is invariant to nuisance transformations (as observed
through the unlabeled set i.e., minor pose variations of the same unlabeled
subjects), while being discriminative to the inter-class transformations (as
observed through the labeled sample). The challenge in constructing such an
invariant feature map is that each face in the labeled set is not associated
with an explicit set of possible facial variations. Rather, these minor facial
variations (to be invariant to) are in the unlabeled set.

The paper addresses the problem by appealing to tools from group theory. Each
nuisance transformation is represented as a unitary transformation, and the
set of all nuisance transformations is treated as a unitary group (i.e., set of
orthogonal matrices). Key in constructing invariant features is the so called
""group integration"" i.e., integral of all the orthogonal matrices in the group,
resulting in a symmetric projection matrix. This matrix is used to project
points to the invariant space. The paper gives a number of theoretical results
regarding this matrix, and extends it to reproducing kernel Hilbert spaces
(RKHSs). The invariant features, termed *max-margin invariant features* (MMIF),
of a point x are given by taking invariant inner products (induced by the
matrix) in an RKHS between x and a set of unlabeled points.

##### Review summary

The paper contributes a number of theorems regarding the group integration, as
well as invariance in the RKHS. While parts of these can be derived easily, the
end result of MMIFs which are provably invariant to nuisance transformations
is theoretically interesting. Some of these results appear to be new. Notations used
and writing in the paper are unfortunately unclear in many parts. The use of
overloaded notation severely hinders understanding (more comments below). A few
important aspects have not been addressed, including why the MMIFs are
constructed the way they are, and the assumption that ""all"" unitary
transformations need to be observed (more details below). Computational
complexity of the new approach is not given.

##### Major comments/questions

I find the MMIFs and the presented theoretical results very interesting.
However, there are still some unclear parts which I hope the authors can
address. In order of priority,

1. What is the relationship of the new invariant features to the mean embedding
features of Raj et al., 2017?

Local Group Invariant Representations via Orbit Embeddings
Raj et al., 2017
AISTATS 2017

2. The central part of the proposal is the group integration, integrating over
""all"" orthogonal matrices. There is no mentioning at all how this is computed
until the very end on the last page (line 329), from which I infer that the
integration is replaced by an empirical average over all observed unlabeled
points. When the integration is not over all orthogonal matrices in the group,
all the given theorems do not hold. What can we say in this case? Also, how do
we know that the observed unlabeled samples are results of some orthogonal
transform? Line 160 does mention that ""small changes in pose can be modeled as
unitary transformations"" according to reference [8]. Could you please point out
where in [8]?

3. I understand how the proposed approach creates invariance to nuisance
transformations. What is unclear to me is the motivation of doing the
operations in Eq. (2) and line 284 to create features which are discriminative
for classification. Why are features constructed in this way a good choice for
being discriminative to between-class transformations? Related, at line 274,
the paper mentions about generating random binary label assignments for
training. Why random? There is not enough explanation in the text. Could
you please explain?

4. The size of the invariant feature map in the end is the same as the number
of classes (line 284). However, before obtaining this, an intermediate step in
Eq. (2) requires computing a feature vector of size M for each x (input point),
where M = number of distinct subjects in the unlabeled set. Presumably M must
be very large in practice. What is the runtime complexity of the full
algorithm?

5. In Theorem 2.4, the paper claims that the newly constructed transformation
g_H is a unitary transformation (preserving inner product) in the RKHS H.
However, it appears that only the range of feature map is considered in the
proof i.e., the set R := {x | \phi(x) for x in domain of X} and \phi is the
feature map associated with the kernel. In general, R is a subset (or equal) of
H. The proof of this theorem in the appendix is not complete unless it mentions
how to extend the results of the group transformation on R to H. 

6. I think Lemma 2.1 requires the group \mathcal{G} to have linear operation.
So, not ""any"" group as stated?

##### Suggestions on writing/notations

* \mathcal{X}, \mathcal{X}_\mathcal{G} are overloaded each with 2-3
definitions. See for example lines 119-125. This is very confusing.

* Line 280: K appears out of nowhere. I feel that it should be the number of
classes, which is denoted by N at line 269. But in the beginning N is used to
denote the cardinality of the labeled set...

* The paper should state from the beginning that the Haar measure in Lemma 2.1
will be replaced with an empirical measure as observed through the unlabeled
sample. It will help the reader.

* Line 155: I suggest that the term ""generalization"" be changed to something
else. As mentioned in the paper itself, the term does not have the usual
meaning.

* There are a few paragraphs that seem to appear out of context. These include
line 85, line 274, line 280 and line 284. Why should the paragraph at line 284
be there? I find it confusing. I feel that lines 311-323 should be in the
introduction.


------- after rebuttal ------
I read the authors' response. The authors partially addressed the issues that 
I raised. I have updated the score accordingly. 
","The authors present a novel technique for constructing an SVM invariant to transformations that can either be known a priori or learned from unlabelled data. They provide a solid theoretical foundation for their method and demonstrate it achieving superior performance on the task of face recognition. The paper is overall well written and tells a coherent story from problem statement to theoretical derivation to experimental validation of the method. The problem of designing kernel methods invariant to specific transformations, in particular unknown transformations learned from data, is an important one that should be of interest to a wide audience in the kernel machine learning community.

One issue I'd like the authors to clarify is that on line 258 they assume that we have unlabelled data transformed by all actions in the unitary group which usually will not be the case if we try to learn those transformations from an unlabelled dataset. To what extent is this important in practice and what kind of problems could it lead to if the unlabelled dataset didn't include sufficiently many instances of transformed inputs for each data point?

A minor problem with notation: line 123 defined X_G as a set {gx,y} while line 125 defines it to be a set {gx}."
Sparse Approximate Conic Hulls,"Greg Van Buskirk, Benjamin Raichel, Nicholas Ruozzi",https://proceedings.neurips.cc/paper/2017/hash/6d9cb7de5e8ac30bd5e8734bc96a35c1-Abstract.html,"The paper ""Sparse approximate conic hulls"" develops conic analogues of approximation problems in convex geometry, hardness results for approximate convex and conic hulls, and considers these in the context of non-negative matrix factorization. The paper also presents numerical results comparing the approximate conic hull and convex hull algorithms, a modified approximate conic hull algorithm (obtained by first translating the data), and other existing algorithms for a feature-selection problem. Numerical results are also presented. 

The first theoretical contribution is a conic variant on the (constructive) approximate Caratheodory theorem devised for the convex setting. This is obtained by transforming the rays (from the conic problem) into a set of vectors by the ""gnomic projection"" applying the approximate Caratheodory theorem in the convex setting, and transforming back. The main effort is to ensure the error behavior can be controlled when the gnomic projection/its inverse are applied. This requires the points to have angle to the ""center"" strictly smaller than pi/2. This kind of condition on the points persists throughout the ""conic"" results in the paper. The second main contribution is establishing hardness results for approximate conic and convex hull problems. Finally, the main point of the paper seems to be that the epsilon-approximate conic hull algorithm can be used to approximately solve NMF under the column subset restriction. 

The paper is quite well written, albeit a little dense, with much of the core technical content relegated to the supplementary material. The results are interesting from a purely convex geometry point of view. Nevertheless, I am somewhat unconvinced about certain issues in the formulation of the problems in the paper (rather than the technical results of the paper, which are nice):
-- the choice of angular metric seems a bit arbitrary (there are other ways to put a distance on the non-negative orthant that is naturally adapted to the conic setting, such as the Hilbert metric. Perhaps the angular metric is best suited to using Frobenius norm error in the matrix factorization problem? If so it would be great if the authors could make this more clear.
-- The gamma-shifted conic version performs well in experiments (and the original conic version does not), which is interesting. How should we choose the shift though? What reason is there not to make the shift very large? Is it possible to ""pull back"" the gamma shift throughout the paper, and formulate a meaningful version of approximate Caratheodory that has a parameter gamma? Perhaps choosing the best-case over gamma is a more interesting formulation (from a practical point of view) than the vanilla conic version studied in this paper.

In addition to these concerns, I'm not sure how much innovation there is over the existing convex method of Blum et al, and over the existing hardness results for NMF in Arora et al. (I am not expert enough to know for sure, but the paper feels a little incremental).  

Minor comments:
-- p3 line 128: this is not the usual definition of ""extreme"" point/ray in convex geometry (although lines 134-136 in some sense deal with the difference between the usual definition and the definition in this paper, via the notion of ""degenerate"")
-- p4 line 149: one could also consider using the ""base"" of the cone defined by intersection with any hyperplane defined by a vector in the interior of the dual cone of the points (not just the all ones vector) instead of using the gnomic projection. This preserves the extreme points, 
 so the basic strategy might work, but perhaps doesn't play nicely with Frobenius norm error in the NMF problem? It would be helpful if the authors could briefly explain why this approach is not favorable, and why the gnomic projection approach makes the most sense.
-- p5 line 216, theorem 3.4: It would be useful if the authors describe the dependence on gamma in the parameters of the theorem. It seems this will be very bad as gamma approaches pi/2, and it would be good to be upfront about this (since the authors do a good job of making clear that for the problem setup in this paper the bounded angle assumption is necessary)
-- p6 line 235, there is some strange typesetting with the reference [16] in the Theorem statement.
-- p6 line 239: again the dependence on gamma would be nice to have here (or in a comment afterwards). 
-- p6 line 253: it may be useful to add a sentence to say how this ""non-standard"" version of d-sum differs from the ""standard"" version, for the non-expert reader.
-- p8 lines 347-348: this is an interesting observation, that the gamma-shifted conic case is a sort of interpolant between the convex and conic cases. It would be interesting to be able to automatically tune gamma for a given scenario.
","This paper provides an approximation algorithm for NMF. Specifically, the algorithm outputs few columns of the data matrix such that the conic hull of those columns is close to the conic hull of the columns of the entire matrix (in an appropriately defined metric). The main difference from existing works is that it does explicitly not assume there is a true model (i.e., few columns such that other columns are generated as combinations of those columns) or the separability assumption made in the existing works. In this sense, the results are model-free. The algorithm is based on gnomic projections and is heavily based on [16], with appropriate modifications. 

It is not clear why the algorithm does not depend explicitly on the dimensionality of the matrices. It would be better if the authors explain why clearly or point out which assumption leads to this effect. The results provided are interesting and would be of interest to the community. Hence I propose to accept the paper.","The paper presents a greedy approximation algorithm for finding a small subset S of columns of X such that conic hull of S approximates the conic hull of X. If k is the smallest number of columns to get the e -approximation, the algorithm produces O(k/e^2/3) columns that give O(e^1/3) approximation. The algorithm is heavily inspired from an earlier work [16] that produces same approximation for convex hull problem. Authors transform conic hull problem to that of convex hull using gnomonic projection (scaling the points so that they lie on a suitable hyperplane at unit distance from the origin). The main contribution is claimed to be the analysis of this which authors say is not immediate from the analysis in [16]. Apart from this, the paper also proves an approximate Caratheodory theorem for conic hulls, and shows that both convex and conic hull versions are d-SUM-hard. Overall, the paper shows that same approximation results hold for conic case as shown earlier for the convex case in [16].

I have not gone through all the proofs in the appendix so cannot comment on the immediacy of the proofs for the conic case from the convex case [16]. Approx Caratheodory theorem (thm 3.4) for conic seems to be not so difficult to obtain from the convex case though, given the monotonicity of the distortion on the hyperplane as a function of distortion on the sphere. 

Here are my other comments: 

1. The paper should also cite earlier work on approximate solution for the conic hull problem where the quality is measured using some other metrics, eg. ""Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization"", Gillis and Luce, 2015). 

2. The paper keeps switching b/w the case when X is nonnegative matrix and the case when X is allowed negative entries. For ex, lines 41-43 talk about the nonnegative X case, whereas Algorithm 1 seems to be taking about general X (P can have points anywhere in R^m). 

3. Algorithm 1: for P anywhere in R^m, the algorithm takes q as any vector in the space. However I think q should be in the dual cone of conic(P) to make the algorithm work. For nonnegative data, the dual cone is positive orthant itself. 

4. Lines 148-150: the statement about earlier work [11] is not right -- they also allow any vector q in the positive orthant. See Algorithm 1 in [11] ""detection step"" and ""Remarks (3)"" in the same paper. 

5. What optimization problem is solved to project points onto the current convex hull at step k? I didn't see the paper talking about this. 

6. How is mutant X-RAY (line 316) is related to Algorithm 1 in the paper? Essentially if denominator (p^T X_j) is removed in the ""detection step"" in [11] and ""max"" variant is used, this is ""mutant X-RAY"" as called in the paper. 

7. Will Algorithm 1 correctly recover the set of columns S if the data is generated using conic combinations of X, i.e., for the case when X = conic(S)? It doesn't look like so. A formal proof or comment would be good. 
 

"
Label Distribution Learning Forests,"Wei Shen, KAI ZHAO, Yilu Guo, Alan L. Yuille",https://proceedings.neurips.cc/paper/2017/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html,"The authors describe a method for label distribution learning based on differentiable decision trees. The authors use differentiable sigmoid units to estimate a label distribution using leaf nodes of trees. Learning in split nodes is done via backprop. whereas for leaf nodes the authors propose a methodology based on variational bounding. The authors compare their work with relevant methods on learning label distributions and show the competitiveness of their method. 

I think this is a good paper, providing a sound methodology for learning LD. The main weakness of this paper is that it is somewhat difficult for the reader to differentiate the actual contribution of this paper with respecto to reference [20]. I would like to see an answer to this in the rebuttal phase. 

detailed comments: 


typo: The best parameters ARE determined by: 

It is not clear to me whether the way split-nodes are learned is a contribution of this paper, or if it is described in [20], please clarify

Do you have any plans of releasing the code of the LDF method with caffe? 

The numbers of trees and deep of trees is quite small, the authors make an effort to analyze the performance of their method under different parameter settings, but, the number of trees and depth is still small, what are author thoughts in much larger ensembles and tree depths? Have the authors performed experiments evaluating this setting? Please elaborate on this

For the experimental results from Section 4.1, are the results from Table 1 comparable to each other? How do you guarantee this? Please provide more details

For the problem approached in Section 4.2, can the authors please report the state of the art performance? (e.g., not only based on label distribution learning)

What is the complexity of the model?, also, can the authors report runtime?

","The paper describes a new label distribution learning algorithms based
on differentiable decision trees. It can be seen as a natural
extension to reference [20] (in the paper) where deep neural decision
forests are introduced. To deal with label distribution learning
problems the authors define an update function derive from variational
bounding, define a loss function for forests, connect the split nodes
from different trees to the same output, and proposed an approach
where all the trees can be learned jointly.

The authors followed an iterative process fixing alternatively the
distribution at the leaf nodes and the split function.

The authors tested their proposed method with three datasets and compared
it against other state-of-the-art label distribution learning
algorithms with very promising results.

A limitation of the approach is that the number of trees in the
ensemble and the depth of the trees need to be defined in
advance. It also seems that the approach only works for binary trees,
which again limits its applicability or require artificially deep trees.

The authors should clarify how the index function works, which
determines which output units from ""f"" are used for constructing the tree.

The paper is well written, with clear results and sufficient
contributions to be accepted at NIPS.

Typo:
- "" ... update rule for q and .""
"
Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Akihiro Yabe, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi",https://proceedings.neurips.cc/paper/2017/hash/6e5025ccc7d638ae4e724da8938450a6-Abstract.html,"The paper considers the online sparse regression problem introduced by Kale (COLT'14), in which the online algorithm can only observe a subset of k features of each data point and has to sequentially predict a label based only on this limited observation (it can thus only use a sparse predictor for each prediction). Without further assumptions, this problem has been recently shown to be computationally hard by Foster et al (ALT'16). To circumvent this hardness, the authors assume a stochastic i.i.d. setting, and under two additional distributional assumptions they give efficient algorithms that achieve sqrt(T) regret. 

The results are not particularly exciting, but they do give a nice counter to the recent computational impossibility of Foster et al, in a setting where the data is i.i.d. and well-specified by a k-sparse vector. One of the main things I was missing in the paper is a proper discussion relating its setup, assumptions and results to the literature on sparse recovery / compressed sensing / sparse linear regression. Currently, the paper only discusses the literature and previous work on attribute efficient learning.

In particular, I was not entirely convinced that the main results of the paper are not simple consequences of known results in sparse recovery: since the data is stochastic i.i.d., one could implement a simple follow-the-leader style algorithm by solving the ERM problem at each step; this could be done efficiently by applying a standard sparse recovery/regression algorithm under a RIP condition. It seems that this approach might give sqrt(T)-regret bounds similar to the one established in the paper, perhaps up to log factors, under the authors' Assumption (a) (which is much stronger than RIP: it implies that the data covariance matrix is well conditioned, so the unique linear regression solution is the sparse one).

While the authors do consider an additional assumption which is somewhat weaker than RIP (Assumption b), it is again not properly discussed and related to more standard assumptions in sparse recovery (e.g, RIP), and it was not clear to me how reasonable and meaningful this assumption is. 


Questions / comments:

* The ""Online"" in the title of the paper is somewhat misleading: the setting of the paper is actually a stochastic i.i.d. setting (where the objective is still the regret), whereas ""online"" typically refers to the worst-case non-statistical online setting.

* The description of Alg 1 is quite messy and has to be cleaned up. For example:
	- line 192, ""computing S_t"": the entire paragraph was not clear to me. What are the ""largest features with respect to w_t""?
	- line 196: what are i,j?
	- line 199, ""computing g_t"": many t subscripts are missing in the text.

* Thm 2: Why k_1 is arbitrary? Why not simply set it to k'-2 (which seems to give the best regret bound)?

* Can you give a simpler algorithm under Assumption (b) for the easier case where k <= k'+2 (analogous to Alg 1)?


Minors / typos:

* line 14: the references to [11,10] seems out of context.
* line 156, Assumption (a): use an explicit constant in the assumption, rather than O(1); in the analysis you actually implicitly assume this constant is 1.
* line 188: ""dual average"" => ""dual averaging"".
* Lem 3 is central to the paper and should be proven in the main text.
* Lem 4 is a standard bound for dual averaging / online gradient descent, and a proper reference should be given. Also, it seems that a simpler, fixed step-size version of the bound suffices for your analysis.
* line 226: ""form"" => ""from"".
* line 246: ""Rougly"" => ""Roughly"".
","While I think the question considered is an important one, I feel the results are weak compared to the strong assumptions made. In particular, I'm most skeptical about 2 points:

1. The regret bound depends on d, the ambient dimension of the feature vectors. This is unacceptable in high-dimensional regimes, where dimension d can be much larger than T. In fact, in order for the regret bound to make sense T has to be much larger than d, which makes the sparsity assumption vacuous.

2. The assumption made in this paper is the strongest possible: random design with least eigenvalue bounded away from below. One cannot make the even stronger beta-min assumption here because it trivializes the problem. Under such strong assumptions one wishes to see the equivalent of fast rate in Lasso (which would translate to log T regret in online learning). This is not the case, as only sqrt{T} slow rate is proved and it is unclear whether it is tight (I would be really surprised if sqrt{T} is optimal, as similar problems in online convex optimization / Lasso shows that with (restricted) strong convexity like the assumptions made in this paper, log T is what to be expected).

Minor comments:

3. I suggest emphasizing ""with limited/partial observations"" in the title and the problem formulation. Online sparse regression refers to the problem where full observations of the features are always available.

4. I do not understand Theorem 1 at all. Assumption (a) implies (b) so why would the problem be difficult with (a)? In fact I suggest not to state (b) as a assumption/condition, but rather a corollary/lemma as it is implied by (a)."
Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds,"Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, Licheng Jiao",https://proceedings.neurips.cc/paper/2017/hash/6ef80bb237adf4b6f77d0700e1255907-Abstract.html,"The paper generalizes the Nesterov’s method to geodesically convex problems on Riemannian manifolds. This is an important theoretical contribution to the field, especially it shows that such a construction is possible. Furthermore, the work on deriving the quantities for the Karcher mean problem is really interesting. ","Summary of the Paper
====================

The paper considers a geodesic generalization of Nesterov's accelerated gradient descent (AGD) algorithm for Riemannian spaces. Two versions are presented: geodesic convex case and geodesic strongly convex smooth case. The proposed algorithms are instantiated for Karcher mean problems, and are shown to outperform two previous algorithms (RGD, RSGD), which address the same setting, with randomized data. 

Evaluation
==========

From theoretical point of view, finding a proper generalization for the momentum term (so as to be able to implement AGD) which maintains the same convergence rate for any Riemannian space is novel and very interesting. From practical point of view, it is not altogether clear when the overall running time reduces. Indeed, although this algorithm requires significantly smaller number of iterations, implementing the momentum term can be very costly (as opposed to Euclidean spaces). That said, the wide range of settings to which this algorithm potentially applies makes it appealing as a general mechanism, and may encourage further development in this direction. The paper is well-written and relatively easy-to-follow.




General Comments
================
- The differential geometrical notions and and other definitions used intensively throughout this paper may not be familiar for the typical NIPS reader. I would suggest making the definition section more tight and clean. In particular, the following are not seemed to be defined in the text: star-concave, star-convex, grad f(x), intrinsic inner-product, diameter of X, conic geometric optimization, retractions.

- Equations 4 and 5 are given without any intuition as how should one derive them. They seem to be placed somewhat out of the blue, and I feel like the nice figure provided by the authors, which could potentially explain them, is not addressed appropriately in the text.




Minor Comments
==============
L52 - redundant 'The'
L52+L101 - There is a great line of work which tries to give a more satisfying interpretation for AGD. Stating that the proximal interpretation as the main interpretation seems to me somewhat misleading.
L59 - Can you elaborate more on the computational complexity required for implementing the exponent function and the nonlinear momentum operator S. 
L70 - why linearization of gradient-like updates are contributions by themselves.
L72 - classes?
L90 + L93 - The sentence 'we denote' is not clear.
L113 - the sentence 'In addition, different..' is a bit unclear..
L126 - besides the constraint on alpha, what other considerations are needed to be taken in order to set its value optimally?
L139 + L144 - Exp seems to be written in the wrong font.
L151 - The wording used in Lemma 3 is a bit unclear.
L155 - in Thm 1, consider restating the value of beta.
L165 - How does the upper bound depend on D? Also, here again, how is alpha should be set?
L177 - maybe geometrically -> geodesically?
L180 - In section 5, not sure I understand how the definition of S is instantiated for this case.
L181 - 'For the accelerated scheme in (4)' do you mean algorithm 1?
L183 - Y_k or y_k?
L193 - 'with the geometry' is not clear.
L201 - Redundant 'The'. Also, (2) should be (3)?
L219 - Can you explain more rigorously (or provide relevant pointers) why RGD is a good proxy for all the algorithm stated above?
Page 8 - Consider providing the actual time as well, as this benchmark does not take into account the per-iteration cost.
L225 - Why is C defined explicitly?


","
This paper gives an accelerated first-order methods for geodesically convex optimization on Riemannian manifold. It is proven tha the proposed method converge linear, i.e., O((1-\sqrt{mu/L})^k), for mu-strongly G-convex and L-smooth function and O(1/k^2) for only G-L-smooth function. This results generalize the Euclidean method: the Nesterov's accelerated method. Numerical experiments report the performance of the proposed method compared to RGD and RSGD methods.

The main concern for this paper is the applications of the proposed method. The experimental result is not convincing. For Karcher mean on SPD matrices, the cost function is smooth and the Hessian always has good condition number. Methods that explore higher order information have better performance. The authors claims that Bini's method, Riemannian GD method and limited-memory Riemanian BFGS have similar performance. However, it is not the case, especially when BB step size instead of a constant step size is used as the initial step size in line search algorthm. Usually 30 passes can reduce the objective gap by a factor more than 10^10 rather than only 10^3 in this paper.

This application seems not to be a good example for the proposed method. Since the propose method does not require the function to be C^2, an application with C^1 cost function may be more suitable to show the performance of the proposed method.
"
Hierarchical Implicit Models and Likelihood-Free Variational Inference,"Dustin Tran, Rajesh Ranganath, David Blei",https://proceedings.neurips.cc/paper/2017/hash/6f1d0705c91c2145201df18a1a0c7345-Abstract.html,"The paper presents an implicit variational inference method for likelihood-free inference. This approach builds on previous work and particularly on Hierarchical Variational Inference and Implicit Variational Bayes.   

The key trick used in the paper is the subtraction of the log empirical distribution log q(xn) and the transformation of the ELBO in the form given by eq. 4, which suggests the use of log density ratio estimation as a tool for likelihood-free variational inference. The rest methodological details of the papers are based on standard tools, such as log density 
ratio estimation, reparametrization and hierarchical variational distributions. 

While I found the trick to deal with likelihood intractability very interesting, it requires log density ratio estimation in high-dimensional spaces (in the joint space of data x_n and latent variable z_n). This is very challenging since log density ratio estimation in high dimensions is an extremely  difficult problem  and there is no clear evidence that the authors provide a stable algorithm to deal that. For instance, the fact that the authors have not applied their method to a standard GAN (for generating high dimensional data such as images) but instead they have constructed this rather weird Bayesian GAN for classification (see page 7) indicates that the current algorithm is very unstable. In fact it is hard to see how to stabilize the proposed  
algorithm since initially the “variational joint” will be very different from the ""real joint"" and it is precisely this situation that makes log density ratio estimation completely unreliable, leading to very  biased gradients in the early crucial iterations of the optimization.     

","The paper defines a class of probability models -- hierarchical
implicit models -- consisting of observations with associated 'local'
latent variables that are conditionally independent given a set of
'global' latent variables, and in which the observation likelihood is
not assumed to be tractable. It describes an approach for KL-based
variational inference in such 'likelihood-free' models, using a
GAN-style discriminator to estimate the log ratio between a
'variational joint' q(x, z), constructed using the empirical
distribution on observations, and the true model joint density. This
approach has the side benefit of supporting implicit variational
models ('variational programs'). Proof-of-concept applications are
demonstrated to ecological simulation, a Bayesian GAN, and sequence
modeling with a stochastic RNN.

The exposition is very clear, well cited, and the technical machinery
is carefully explained. Although the the application of density ratio
estimation to variational inference seems to be an idea 'in the air'
and building blocks of this paper have appeared elsewhere (for example
the Adversarial VB paper), I found this synthesis to be cleaner,
easier to follow, and more general (supporting implicit models) than
any of the similar papers I've read so far.

The definition of hierarchical implicit models is a useful point in
theoretical space, and serves to introduce the setup for inference
in section 3. However the factorization (1), which assumes iid
observations, is quite restrictive -- I don't believe it technically
even includes the Lotka-Volterra or stochastic RNN models explored in
the paper itself! (since both have temporal dependence). It seems
worth acknowledging that the inference approach in this paper is more
general, and perhaps discussing how it could be adapted to problems
and models with more structured (time series, text, graph)
observations and/or latents.

Experiments are probably the weakest point of this paper. The 'Bayesian GAN' 
is a toy and the classification setup is artificial; supervised learning is
not why people care about GANs. The symbol generation RNN is not
evaluated against any other methods and it's not clear it works
particularly well. The Lotka-Volterra simulation is the most
compelling; although the model has few parameters and no latent
variables, it nicely motivates the notion of implicit models and shows
clear improvement on the (ABC) state of the art.

Overall there are no groundbreaking results, and much of this
machinery could be quite tricky to get working in practice (as with
vanilla GANS). I wish the experiments were more compelling. 
But the approach seems general and powerful, with the
potential to open up entire new classes of models to effective
Bayesian inference, and the formulation in this paper will likely be
useful to many reasearchers as they begin to flesh it out. For that
reason I think this paper is a valuable contribution.

Misc comments and questions:

Lotka-Volterra model: I'm not sure the given eqns (ln 103) are
correct. Shouldn't the Beta_3 be added, not subtracted, to model the
predator birth rate? As written, dx_2/dt is always negative in
expectation which seems wrong. Also Beta_2 is serving double duty as
the predator *and* prey death rate, is this intentional? Most sources
(including the cited Papamakarios and Murray paper) seem to use four
independent coefficients.

line 118: ""We described two classes of implicit models"" but I only see
one? (HIMs)
line 146: ""log empirical log q(x_n)"" is redundant

Suppose we have an implicit model, but want to use an explicit
variational approximation (for example the mean-field Gaussian in the
Lotka-Volterra experiment). Is there any natural way to exploit the
explicit variational density for faster inference?

Subtracting the constant log q(x) from the ELBO means the ratio
objective (4) no longer yields a lower bound to the true model
evidence; this should probably be noted somewhere. Is there an
principled interpretation of the quantity (4)? It is a lower bound on
log p(x)/q(x), which (waving hands) looks like an estimate of the
negative KL divergence between the model and empirical distribution --
maybe this is useful for model criticism?
","Thank you for an interesting read.

This paper proposed a hierarchical probabilistic model using implicit distributions. To perform posterior inference the authors also proposed a variational method based on GAN-related density ratio estimation techniques. The proposed method is evaluated with a number of different tasks including ABC, supervised learning and generative modeling.

I like the idea in general but I think there are a few points that need to be made clearer.

1. How is your method related to AVB [36] and ALI [13]? I can see these connections, but not all the readers you're targeting could see it easily.

2. In the AVB paper they mentioned a crucial trick (adaptive contrast) to improve the density ratio estimations in high dimensions. You only did a toy case (2D linear regression) to demonstrate the stability of your method, and your findings are essentially the same as in the toy example in the AVB paper (naive density ratio estimation works well in low dimensional case). It would be better if you could provide an analysis in high dimensional case, e.g. your BNN example.

3. Hinge loss: why the optimal r is the log ratio?

4. Generating discrete data: yes using r(x, w) instead of r(x) could provide gradients, however this means you need to input w to the discriminator network as well. Usually you need quite a deep (and wide) neural network to generate realistic data so I presume w could be of very high dimensions. How scalable is your approach here?

5. I'm a bit worried about no quantitative results for the sequence generation part. I think it's not a good practice for just including generative samples and letting the readers judge the fidelity.

In summary I think this paper is borderline. I would be happy to see clarifications if the authors think I've missed some important points."
Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding,"Mainak Jas, Tom Dupré la Tour, Umut Simsekli, Alexandre Gramfort",https://proceedings.neurips.cc/paper/2017/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html,"In this paper, the authors propose a novel probabilistic convolutional sparse coding model (alpha-stable CSC) for learning shift-invariant atoms from raw neural signals. They propose an inference strategy based on Monte Carlo EM to deal efficiently with heavy tailed noise and take into account the polarity of neural activations with a positivity constraint. The formulation allows the use of fast quasi-Newton methods for the M-step which outperform previously proposed state-of-the-art ADMM based algorithms. The experiment results on LFP data demonstrate that such algorithms can be robust to the presence of transient artifacts in data and reveal insights on neural time-series without supervision.

The main contribution of this study is extending a general (alpha-stable distribution) framework, comparing to the Gaussian distribution. From experiment point of view, the proposed approach is very useful. However, there are a few comments and suggestions for the authors to consider:
(1)	Figure 1(a) only demonstrates the PDFs of alpha-stable distributions characterized by alpha and beta. Please try to describe the characters of the scale parameter and location parameter. 
(2)	As written in the paper, “As an important special case of the alpha-stable distributions, we obtain the Gaussian distribution when alpha= 2 and beta = 0”, the authors emphasize that the current CSC models might be limited as they consider an L2 reconstruction error. While the extending model is more robust to corrupted data than the Gaussian models. However, I wonder if the experiments should include Gaussian Mixture Model as a fair comparison.
(3)	How to determine the parameters (i.e., lambda, N, T, L, K), respectively, in synthetic and LEP data experiments?
","The paper proposes a probabilistic variant of Convolutional Sparse Coding using alpha Stable distributions. Overall the paper is well written, and the method appears to be novel. The authors develop an EM approach to solving the problem, where the E step corresponds to finding the distributional parameters and an M step that corresponds to standard (non-probabilistic) CSC. This appears to improve robustness. Another plus for this paper is that source code has been provided.

- L62:  ""l2 reconstruction error, which corresponds to assuming an additive Gaussian noise distribution"". I don't think this is true: l2 reconstruction error corresponds to a Gaussian likelihood. A Gaussian noise distribution corresponds to a l2 regulariser
- L115-7: ""It is easy to verify that the MAP estimate for this probabilistic model, i.e. maxd,z log p(d, z|x), is identical to the original optimization problem defined in (1)."" I've Not come across the 1 norm being the MAP estimate of the exponential distribution before (normally Laplace (double exponential)). Could you explain how this is the case?
- eq 3: Should mention that only symmetric stable distributions are being considered
- L139-140: ""It can be easily shown that both formulations of the CSC model are identical by marginalizing the joint distribution"". Not immediately obvious - can this be included in the supplemental?
- L157 not obvious where the weights come from
","The paper presents a new sparse coding method by using alpha-stable sampling distributions (instead of Gaussians) when sampling convolutional sparse codes. This proposed model is shown to approximate the heavy-tailed neural data distributions. A sampling based inference algorithm is presented and evaluated on neural time-series data. All told this is a good contribution. The necessity of modeling heavy-tailed data and noise is critical in neuroscience applications where, in certain cases, these tails contain the discriminatory signals among different groups/subsets/classes of the data. Below are some pointers to improve the paper. 

1. The authors propose a EM and MH based sampling learning/inference for the proposed model. However, CSC can be posed as efficient bi-convex problem which gives guarantees (unlike the EM-style local minima). Does the proposed model entail a more biconvex type learning? If not, this needs to explained clearly (and related work needs to be improved). If so, then why is that modeling not considered? Does taking the sampling route tie-up to some properties of alpha-stable distributions? These modeling aspects needs to be explained.
  
2. It seems that the influence/choice of alpha is not explicit in the sampling scheme. Since the basis of the paper is that alpha is not going to be equal to 2 (i.e., gaussian is not the correct choice), some evaluations justifying this in practice needs to be reported (e.g., estimating the alpha that gave rise to the correct bases from Fig 5; beyond simply setting it to some fixed value like 1.2). More generally influence of all hyper-parameters needs to be addressed. "
Modulating early visual processing by language,"Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, Aaron C. Courville",https://proceedings.neurips.cc/paper/2017/hash/6fab6e3aa34248ec1e34a4aeedecddc8-Abstract.html,"Overall Impression:

I think this paper introduces a novel and interesting idea that is likely to spark future experimentation towards multi-modal early-fusion methods. However, the presentation and the writing could use additional attention. The experiments demonstrate the effectiveness of the approach on multiple tasks though they are a bit narrow to justify the proposed method outside of the application domain of vision + language. I think further iterations on the text and additional experiments with other model architectures or different types of multi-modal data would strengthen this submission.


Strengths:

+ I like the neurological motivations for the CBN approach and appreciate its simplicity.

+ Comparing fine-tuning batch norm parameters (Ft BN) vs the question-conditioned batch norm predictions provided an interesting ablation. It seems like adjusting to the new image statistics (Ft BN) results in significant improvement (~1% VQA, 2% Crop GuessWhich) which is then doubled by conditioned on question (~2% VQA, 4% Crop GuessWhich).

+ I appreciate the promise of public code to reproduce the experimental results.

+ The tSNE plots are quite interesting and show that the language conditional modulation seems to have a significant effect on the visual features. 

Weaknesses:

- I don't understand why Section 2.1 is included. Batch Normalization is a general technique as is the proposed Conditional Batch Normalization (CBN). The description of the proposed methodology seems independent of the choice of model and the time spent describing the ResNet architecture could be better used to provide greater motivation and intuition for the proposed CBN approach. 

- On that note, I understand the neurological motivation for why early vision may benefit from language modulation, but the argument for why this should be done through the normalization parameters is less well argued (especially in Section 3). The intro mentions the proposed approach reduces over-fitting compared to fine-tuning but doesn't discuss CBN in the context of alternative early-fusion strategies. 

- As CBN is a general method, I would have been more convinced by improvements in performance across multiple model architectures for vision + language tasks. For instance, CBN seems directly applicable to the MCB architecture. I acknowledge that needing to backprop through the CNN causes memory concerns which might be limiting.

- Given the argument for early modulation of vision, it is a bit surprising that applying CBN to Stage 4 (the highest level stage) accounts for majority of the improvement in both the VQA and GuessWhat tasks. Some added discussion in this section might be useful. The supplementary figures are also interesting, showing that question conditioned separations in image space only occur after later stages.

- Figures 2 and 3 seem somewhat redundant. 

Minor things:
- I would have liked to see how different questions change the feature representation of a single image. Perhaps by applying some gradient visualization method to the visual features when changing the question?
- Consider adding a space before citation brackets. 
- Bolding of the baseline models is inconsistent. 
- Eq 2 has a gamma_j rather than gamma_c
L34 'to let the question to attend' -> 'to let the question attend'
L42 missing citation
L53 first discussion of batch norm missing citation
L58 ""to which we refer as"" -> ""which we refer to as""
L89 ""is achieved a"" -> ""is achieved through a""","The paper proposes a novel method called conditional batch normalization (CBN) to be applied on top of existing visual question answering models in order to modulate the visual processing with language information from the question in the early stages. In the proposed method, only the parameters of the batch norm layer of a pre-trained CNN are updated with the VQA loss by conditioning them on the LSTM embedding of the input question.

The paper evaluates the effectiveness of CBN on two VQA datasets – the VQA dataset from Antol et al., ICCV15 and the GuessWhat?! dataset from Vries et al., CVPR17. The experimental results show that CBN helps improve the performance on VQA by significant amount. The paper also studies the effectiveness of adding CBN to different layers and shows that adding CBN to last (top) 2 layers of CNN helps the most. The paper also shows quantitatively that the improvements in VQA performance are not merely due to fine-tuning of CNN by showing that the proposed model performs better than a model in which the Batch Norm parameters are fine-tuned but without conditioning on the language. Hence demonstrating that modulating with language helps. 

Strengths:

1.	The paper is well-motivated and the idea of modulating early visual processing by language is novel and interesting for VQA task. 

2.	The proposed contribution (CBN) can be added on top of any existing VQA model, hence making it widely applicable.

3.	The ablation studies are meaningful and are informative about how much of early modulation by language helps.

4.	The paper provides the details of the hyper-parameters, hence making the work reproducible.

Weaknesses:

1.	The main contribution of the paper is CBN. But the experimental results in the paper are not advancing the state-of-art in VQA (on the VQA dataset which has been out for a while and a lot of advancement has been made on this dataset), perhaps because the VQA model used in the paper on top of which CBN is applied is not the best one out there. But in order to claim that CBN should help even the more powerful VQA models, I would like the authors to conduct experiments on more than one VQA model – favorably the ones which are closer to state-of-art (and whose codes are publicly available) such as MCB (Fukui et al., EMNLP16), HieCoAtt (Lu et al., NIPS16). It could be the case that these more powerful VQA models are already so powerful that the proposed early modulating does not help. So, it is good to know if the proposed conditional batch norm can advance the state-of-art in VQA or not.

2.	L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to? 

3.	In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.

4.	Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?

5.	Figure 4 visualization: the visualization in figure (a) is from ResNet which is not finetuned at all. So, it is not very surprising to see that there are not clear clusters for answer types. However, the visualization in figure (b) is using ResNet whose batch norm parameters have been finetuned with question information. So, I think a more meaningful comparison of figure (b) would be with the visualization from Ft BN ResNet in figure (a).

6.	The first two bullets about contributions (at the end of the intro) can be combined together.

7.	Other errors/typos:
a.	L14 and 15: repetition of word “imagine”
b.	L42: missing reference
c.	L56: impact -> impacts 

Post-rebuttal comments:

The new results of applying CBN on the MRN model are interesting and convincing that CBN helps fairly developed VQA models as well (the results have not been reported on state-of-art VQA model). So, I would like to recommend acceptance of the paper. 

However I still have few comments --

1. It seems that there is still some confusion about test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the performance of the MCB model to be 62.5% on test-standard split. However, 62.5% seems to be the performance of the MCB model on the test-dev split as per table 1 in the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).

2. The reproduced performance reported on MRN model seems close to that reported in the MRN paper when the model is trained using VQA train + val data. I would like the authors to clarify in the final version if they used train + val or just train to train the MRN and MRN + CBN models. And if train + val is being used, the performance can't be compared with 62.5% of MCB because that is when MCB is trained on train only. When MCB is trained on train + val, the performance is around 64% (table 4 in MCB paper).

3. The citation for the MRN model (in the rebuttal) is incorrect. It should be --
@inproceedings{kim2016multimodal,
  title={Multimodal residual learning for visual qa},
  author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  booktitle={Advances in Neural Information Processing Systems},
  pages={361--369},
  year={2016}
}

4. As AR2 and AR3, I would be interested in seeing if the findings from ResNet carry over to other CNN architectures such as VGGNet as well.","Summary:
The current work takes a different approach to fusing modalities for tasks in the intersection of language and vision, than most of the other works. Instead of fusing these modalities after extracting independent representations, the authors aim to modulate the visual features using the text. In particular, they aim to learn the batch normalization parameters of ResNet, which is typically used to extract visual representation, conditioned on the text. They call such it ‘Conditional Batch Normalization’ (CBN) and the resultant network as ModRNet. Experiments on two tasks shows the effectiveness of the proposed approach.

Strengths:
(a) Fusing both the modalities as early as the visual feature extraction is novel idea, to the best of my knowledge. This is well motivated by the paper, drawing parallels to findings in neuroscience on how language can influence the response to visual stimulus in human brain.

(b) Batch normalization parameters as a means to fuse modalities is a good choice as it scales only with the number of channels, thus greatly limiting the number of additional parameters to be learnt. Further, other parameters of the network being frozen exemplifies the simplicity of the approach.

(c) The paper contains good ablation studies along with analyzing finetuning ResNet on the task vs ModRNet. I thoroughly enjoyed the experiments and the discussions that followed.

Weaknesses:
(a) Experiments in the paper are limited to ResNet architecture for feature extraction. Though not a strict weakness, one does wonder about the generalizability of CBN on other networks, perhaps batch normalized version of VGG? Evidence that this is somehow a property of ResNet, perhaps due to the residual connection, would also suffice.

(b) Highly recommend the authors to carefully read the manuscript and resolve typos/grammatical issues that hinder the reading and understanding of the proposed approach. Few of the errors are listed towards the end.

Comments:
(a) L13-16 sentences don’t flow well. While the initial sentences suggest our capability to model such tasks, the later sentences state that these remain a long-standing challenge.

(b) L185 - The task of oracle has not been described (with the input - outputs clearly mentioned) till this point. This makes the understanding of specifics that follow difficult.

(c) L212 - Retrain their model with the same visual features on same sized images as the current work, for perfect comparison ?

Typos:
(a) L4 - inputs
(b) L17 - towards
(c) L42 - Missing reference or a typo?
(d) L150 - latex typos
(e) L190 - ReLU
(f) L218, L267 - typos

Post discussion:
The authors seemed to have mostly done a good job with the rebuttal. After looking at other reviewer's comments and the rebuttal, I continue to feel that the paper has novel contributions bringing new insights which future works can build on. However, I am still curious about the generalization to other networks like my other reviewers. Showing some evidence on other networks (batch normalized version of VGG) would make the paper stronger, even if the findings suggest that it is not useful for other architectures. The authors do mention the lack of pretrained batch norm VGG models on tensorflow, but would be great if they could use pytorch pretrained models in time for the camera ready. 
"
Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols,"Serhii Havrylov, Ivan Titov",https://proceedings.neurips.cc/paper/2017/hash/70222949cc0db89ab32c9969754d4758-Abstract.html,"Increasing my score based on the authors rebuttal. The argument that the proposed method can complement human-bot training makes sense. Also, it seems RL baseline experiments were exhaustive. But the argument about the learnt language being compositional should be toned down since there is not enough evidence to support it.

Old reviews:
The paper proposes to use Gumbel-softmax for training sender and receiver agents in a referential game like Lazaridou (2016). Unlike the previous work, large number of distraction images and variable length messages are allowed. The results suggest the proposed method outperforms Reinforce baseline. In addition, the paper proposes two way to force the communication to be more like natural language.

My first concern is that the paper disregards continuous communication for being unlike natural language, and insists on using discrete symbols. But during training, continuous communication is allowed transmit gradients between the agents. This use of back-propagation makes it impossible to learn from interactions with human, which is probably vital for learning natural language. 

Secondly, I think baseline experiments are not exhaustive and can be improved. Here are my points:
- L109: Learnt baseline (network itself predicts baseline values) is pretty standard in deep RL works for it is efficiency over moving average. Why it hasn't tried? Also it is common to add entropy regularization term to the loss to encourage exploration in deep RL. Has it tried? 
- Appropriate baseline here continuous communication during training and discretization during testing as done in (Foerster, 2016) 
- L135: a simple solution would be to decrease the temperature to zero during training. Has it tried?
- L163: why not hyper-parameters are not tuned? At least the learning rate should be tuned, especially in Reinforce baseline because the default values are tuned for supervised training.

At last, the claim of compositionally (L11) of the learnt communication is not supported by any evidence. Also, L196 suggests that word 5747 represents animals. Then, it should still represent the same thing even at different position as in natural language word ""animal"". The fact that it is not suggest that the learnt communication is actually unlike natural language. Why not similar analysis on reinforce baseline?

Other comments:
- L59: It is not a non-stationary environment. Since the sender and the receiver have the same reward, they can be considered as a single RL agent. Then, this agent writes words into the environment as ""sender"", then observes them at the next step as ""receiver"". 
- How KL is optimized in sec 3.3? It can't be computed directly because there are exponentially many ""m""s.
- Why there are two reinforce versions in figure 2 left. What is greedy argmax reinforce?
- It would be good to include a histogram of message lengths
- Why such large vocabulary size? What is the effect of smaller vocabulary?
","The authors present a sender/receiver model as an encoder/decoder based on stacked LSTM's and trained straight through with Gumbel-softmax estimations to allow variable-length messages from sender to receiver. The sender encodes information about a target image, which the receiver decodes to select one image out over a hundred distracting images.

The paper is well-written and the methodology proposed is novel and works well against a reinforcement-learning-based approach baseline. The qualitative experiments seemed a little unstructured, and the paper wandered for a while in section 3 before wrapping up.

Specific comments:

-In the abstract, you claim that yours is the first work to require sequences of discrete symbols, but Mordatch and Abbeel also have this requirement (just by time step, rather than one utterance forming to completion before being sent to a receiving agent).

-Main contribution's third bullet in the Introduction is vague. This would be a good place to foreshadow the minor, semi-supervised follow-up experiments' results.

-Figure 1's caption should have the text explaining what the various arrows mean, not the body of the text. Also, adding captions for image ""t"", images ""d1"", ""d2"", ""d3"" and the start symbol ""< s >"" would be helpful to connect the figure to the text.

-Why not use the penultimate FC-4096 layer of the VGG network, as is more common practice, to represent images?

-I found parts of section 3.1 hard to follow. Did you do your hyperparameter settings based on performance on the validation data you selected from the MSCOCO training data, or on the MCOCO validation data you use as testing data? Which datasets were Figure 2 trained and tested on?

-Also in 3.1, it would be good to redefine ""success rate"" here as the accuracy of picking the correct image, since it could be something like a top-k measure for where the correct image landed among distracting images.

-In 3.2, you note as the last sentence that the methodology you describe was used to find the ""animal"" word as well. Restructuring this section to talk about the selection methodology first will make the whole thing seem stronger and less like cherry-picking.

-I couldn't follow section 3.3's experiment well. In Table 1, the final row ""Imaginet"" seems from the caption to be ""the natural language,"" but from the body of the text I could not figure out what this method represented.

-Section 3.5 doesn't contribute much, as a negative result, and could become a paragraph discussing the possibility and how it didn't work, probably.

-Section 4 seemed out of place, and could have served as motivation for trusting ST-GS before the experiments were carried out.
","In this paper, two agents are trained to “communicate” using variable-length strings. The sender encodes a target image as a sequence of symbols. The receiver selects the target image from a set of images based on the symbols.  The models are trained using REINFORCE or straight-through Gumbel-softmax, and evaluated on MSCOCO images. Additional experiments attempt to make the symbols closer to natural language.

This is an interesting idea, but it might not be ready for publication yet. Some comments:
- This model basically produces an image embedding (as a sequence of symbols) which is discriminative enough to select the image out of a batch of 128 images. The motivation is similarity to human language, but it's unclear that any similarities exist. It might be useful to compute some statistics of the learned sequences and compare them to captions.
- The experiments in generating a more “natural” language are a bit hard to interpret:
-- In the first experiment, the sender is regularized using KL divergence to a language model (trained on image captions). The communication performance drops, and is equal to that of just the receiver trained on captions. It’s hard to conclude much from this. Perhaps the authors could vary the regularization scale and show a trade-off between “naturalness” and discriminative ability of the representation. 
 -- In the second experiment, the sender is trained to both produce captions and communicate with the receiver. This does not make a difference on captioning performance (compared to a model trained just to produce captions), and communication performance is not discussed. It’s again hard to get much insight from this experiment.
- The paper would benefit from better organization; e.g. sections on the VAE interpretation and ST-GS as a pseudo-gradient seem a bit out of place.

"
"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning","Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber",https://proceedings.neurips.cc/paper/2017/hash/70efdf2ec9b086079795c442636b55fb-Abstract.html,"The paper explores a new way to share parameters in an RNN by using convolution inside an LSTM cell for an unstructured input sequence, and using tensors as convolution kernels and feature maps. The method also adds depth to the model by delaying the output target for a specified number of steps.

The idea is quite interesting, and is novel as far as I can tell. The authors provide clear formulation (although a rather complicated one) and provide some experimental results. 
On a real world dataset (wikipedia LM) the method seems very close to SOA, with about half the parameters.

The problems I see with this approach are:
- I find it hard to believe that meaningful high dimensional feature maps can be created for most problems, thus scaling to high dimensional tensors is questionable (The authors only try up to dimension 3)
- Using “depth in time” introduces a delay and is not suitable for streaming applications (e.g. speech)
- For high dimensional tensors the number of hyper parameters can become quite large.

Minor nit:
- Line 242, it says “Fig.3” and should be “Table 3”
","The paper introduces a tensorized version of LSTM that allows for implicitly adding depth and width to the network while controlling the computational runtime.
The paper is clearly written, the contribution is interesting and the experimental validation is convincing.","
This paper proposes Tensorized LSTMs for efficient sequence learning. It represents hidden layers as tensors, and employs cross-layer memory cell convolution for efficiency and effectiveness. The model is clearly formulated. Experimental results show the utility of the proposed method.

Although the paper is well written, I still have some questions/confusion as follows. I would re-consider my final decision if the authors address these points in rebuttal.

1. My biggest confusion comes from Sec 2.1, when the authors describe how to widen the network with convolution (lines 65-73). As mentioned in text, ""P is akin to the number of stacked hidden layers"", and the model ""locally-connects"" along the P direction to share parameters. I think it is a strategy to deepen the network instead of widening it, since increasing P (the number of hidden layers) won't incur more parameters in the convolution. Similarly, as mentioned in lines 103-104, tRNN can be ""widened without additional parameters by increasing the tensor size P"". It does not make sense, as increasing P is conceptually equivalent to increasing the number of hidden layers in sRNN. This is to deepen the network, not to widen it.

2. The authors claim to deepen the network with delayed outputs (Sec 2.2). They use the parameter L to set up the network depth. However, as shown in Eq. 9, L is determined by P and K, meaning that we can not really set up the network depth as a free parameter. I guess in practice, we would always pre-set P and K before experiments, and then derive L from Eq. 9. It seems over-claimed in lines 6-10, which reads like ""we could freely set up the width and depth of the network"".

3. The authors claims that the proposed memory cell convolution is able to prevent gradient vanishing/exploding (line 36). This is not verified theoretically or empirically. The words ""gradient vanishing/exploding"" are even not mentioned in the later text.

4. In the experiments, the authors compared tLSTM variants in the following dimentions: tensor shape (2D or 3D), normalization (no normalization, LN, CN), memory cell convolution (yes or no), and feedback connections (yes or no). There are 2x3x2x2=24 combinations in total. Why just pick up the six combinations in lines 166-171? I understand it become messy when comparing too many methods, but there are some interesting variants like 2D tLSTM+CN. Also, it might help to split the experiments in groups, like one for normalization strategy, one for memory cell convolution, one for feedback connections, etc.
      "
Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback,"Zheng Wen, Branislav Kveton, Michal Valko, Sharan Vaswani",https://proceedings.neurips.cc/paper/2017/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html,"
	This paper studies an influence maximization problem of a semi-bandit setting in which a player (agent) selects a set of initial active nodes and receives the ids of all the edges that was used to activate one of the end nodes. They consider a model that there is a linear dependence between an edge weight and an edge feature and propose an algorithm called IMLinUCB which uses (alpha,gamma)-approximation oracle as a subroutine. 
They show an upper bound of (alpha)(gamma)-scaled regret by introducing a new complexity metric called maximum observed relevance that measures a complexity of a given diffusion network in terms of both network topology and edge weights. They also conduct some experiments that support their theoretical result.

Nicely written paper.
The idea of using the metric called maximum observed relevance to characterize the regret bound is very interesting.
Online learning of influence maximization problem itself is interesting but considering a long time of an influence diffusion process, I wonder if it is valuable as a real application.
      ","The paper studies the following bandit model: actions are nodes in a known directed graph and at each time step t the following happens: (1) hidden from the player, each edge (i,j) is independently declared ""open"" or ""closed"" with fixed but unknown probability w(i,j); (2) the player chooses a subset S_t of nodes of cardinality at most K; (3) all directed paths originated from nodes in S_t that go through open edges are revealed to the player; (4) the player's reward is the number of nodes in these paths. The regret of the player is measured with respect to the expected reward obtained by consistently playing the K-sized subset S of nodes that maximizes the expected reward. Since this set is NP-hard to compute, but easy to approximate, it is assumed that the player has access to an oracle that, given G and estimates of w(i,j) for each edge (i,j), returns the K-sized subset S_t that maximizes the expected reward according to the estimates. The probabilities w(i,j) are assumed to follow a linear model, w(i,j) = x(i,j)*theta, where x(i,j) is the known d-dimensional feature vector associated with edge (i,j) and theta is an unknown parameter vector. A special case is the ""tabular model"" where all the feature vectors are orthogonal. The paper proposes a natural variant of LinUCB to learn the vector theta using the oracle to compute the maximizing subset. The regret scales with dC_*\sqrt{|E|T} where d is the dimension of theta, E is the edge set, and C_* is a parameter that depends on the interaction between G and w. The experimental section is properly done.

The paper builds on a series of works that study influence maximization in the bandit model. The main new ideas are the linear model for probabilities, which provides a key to scalability, and the introduction of C_* as a natural scaling parameter for the regret. The results are not extremely strong, but they are nevertheless interesting and technically solid.

The parameter sigma plays no role and could perhaps be omitted from the analysis."
Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization,"Ahmet Alacaoglu, Quoc Tran Dinh, Olivier Fercoq, Volkan Cevher",https://proceedings.neurips.cc/paper/2017/hash/71887f62f073a78511cbac56f8cab53f-Abstract.html,"This paper combines several techniques in large-scale optimization: smoothing, acceleration, homotopy, and non-uniform sampling.  It solves the problem with three convex functions and a linear operator f(x)+g(x)+h(Ax), where f is smooth. The smoothing technique from Nesterov [14] is applied to smooth the last term h(Ax), then a method based on block-wise forward-backward splitting is used to update one randomly chosen block at every iteration. Then acceleration and homotopy is applied to obtain a faster convergent algorithm. In fact this algorithm is a primal algorithm using the proximal gradient on the smoothed function. 
The numerical results show the performance of the proposed algorithm compared with some existing algorithms. 
L165, the reference is missing. 
","The authors propose a new smooth primal-dual randomized coordinate descent method for the three composite problem F(x) = f(x) + g(x) + h(Ax), where f is smooth, g is non-smooth, separable and has a block-wise proximal operator, and h is a general nonsmooth function. The method is basically a generalization of the accelerated, parallel, and proximal coordinate descent method of Fercoq and Richtarik, and the coordinate descent method with arbitrary sampling of Qu and Richtarik to the three composite case. The difficulty of this extension lies in the non-smoothness of the function h composed with a linear operator A. The authors provide an efficient implementation by breaking up full vector updates. They show that the algorithm achieves the best known O(n/k) convergence rate. Restart schemes are also used to improve practical performance. Special cases of the three composite problem are considered and analyzed. 

The contributions are clearly stated and overall, this is a nice paper. It is applicable to many different objective functions and employs all the ""tricks"" to make this a fast and efficient method.

Comments:
- The smoothing and non-uniform sampling techniques of Algorithm 1 are clearly described in Section 2, but the homotopy and acceleration techniques are not described anywhere, they are just directly used in Alg 1. This makes Alg. 1 a little cryptic if you are unfamiliar with the structure of these methods -- I think Alg. 1 needs to be better explained either in words or indicating what method/approach each step corresponds to.
- Following from the previous comment, ll 9 of Algorithm 1 - where does this formula for \tau come from?, ll 153 - where does the adapted \tau formula come from for the strongly convex case? This is not clear.
- What are the values of \hat{x}, \tilde{x} and \bar{x} initialized to in Alg 1?
- What is the ""given"" value of \dot{y} (re: line 73 and equation (6))?
- Unknown reference in line 165.

============ 
POST REBUTTAL
============ 
I have read the author rebuttal and the other reviews. I thank the authors for addressing my comments/questions, as well as the comments/questions of the other reviewers. My recommendation stands, I think this paper should be accepted.","This paper combines several existing techniques to extend the applicability of coordinate descent method to a larger class of problem. I think this is a nice and solid work but just not has much novelty. The proof can be derived by modifying the proofs in the previous papers and the convergence rate is expected. I found the following issues which I wish the authors can fix. 

1. (Major) The Assumption 1 (c) says h^* should have a bounded domain. However, this is not satisfied in (9). In fact, when h is an indicator function of a singleton set {c}, h^*(u)=u^Tc whose domain is R^n. Therefore, all results in Section 3.4, including Theorem 3.5, cannot be justified. I wonder if there is a mistake in the proof of Theorem 3.5. For example, how can D_h^* be finite when the domain is not bounded? If D_h^* is unbounded, how can the right hand side of (12) depends on D_h^*?

(Minor)
2.  I found both y^* and y^\star in Theorem 3.5. Please make the notation consistent. Also I did not find their definitions, although I can guess they are dual optimal solution.

3. There is a missing reference [?] in line 165."
Linearly constrained Gaussian processes,"Carl Jidling, Niklas Wahlström, Adrian Wills, Thomas B. Schön",https://proceedings.neurips.cc/paper/2017/hash/71ad16ad2c4d81f348082ff6c4b20768-Abstract.html,"%%% UPDATE: Thank you for your response, which has been read %%%

This paper addresses an interesting topic - how to encode symmetries or invariances into kernels. The principal application area is in Bayesian nonparametric regression, where constraints may be presented as prior information but the form of the regression function is not fully specified. An important example of such a constraint is when the field being modelled is known to be of the divergence kind, so that a certain physical quantity is conserved. Related work in this area has included both ANOVA kernels and Stein's method-based kernel, which are each constrained to have vanishing integrals. The present work differs, in that it attempts to consider more general forms of constraint. This effort is commendable, as generic kernels continue to be widely used in machine learning applications without strong physical justification.

I think the proposed method is not general and there is no theory to suggest that it will work in anything other than some special cases, but nevertheless it is interesting and could reasonably be in the proceedings for NIPS.

- The authors ought to discuss their work in relation to ""Learning with Algebraic Invariances, and the Invariant Kernel Trick"" by Franz J. Király, Andreas Ziehe, Klaus-Robert Müller.

- On page 4 it is incorrectly claimed that the prior for f will ""inherit"" the properties of the prior for g. This is of course not true - if g has a prior GP(0,k) where k generates a Sobolev space of order b and if \mathcal{F} is a differential operator of order c, then the prior on g will be something like a Sobolev space of order b - c. So the smoothness properties of f and g differ.

- On page 5, it reads as though the equation ""f_i = \Phi_i \xi f"" is without loss of generality, but it is of course an ansatz. This could be better emphasised by clearly labelling this (and the equation ""g = \Gamma \xi^g"") as an ansatz.

- The limited generality of the method could be better acknowledged. Perhaps an example where the method fails could be included.","Summary of the Paper:

This paper describes a mechanism to incorporate linear operator constraints in the framework of Gaussian process regression. For this, the mean function and the covariance function of the Gaussian processes are changed. The aim of this transformation is to guarantee that samples from the GP posterior distribution satisfy the constraints indicated. These constraints are typically in the form of partial derivatives, although any linear operator can be considered in practice, e.g., integration too. Traditional methods incorporated these constraints by introducing additional observations. This has the limitation that is more expensive and restricted to the observations made. The framework proposed is evaluated in a synthetic problem and in a real problem, showing benefits with respect to the data augmentation strategy.

Detailed comments:

Quality:

I think the quality of the paper is good in general. It is a very well written paper. Furthermore, all the key points are carefully described. It also has a strong related work section. The weakest point is however, the section on experiments in which only a synthetic dataset and a real dataset is considered.

Clarity:

The clarity of the paper is high.

Originality:

As far as I know the paper seems original. There are some related methods in the literature. However, they simply augment the observed data with virtual instances that have the goal of guaranteeing the constraints imposed.

Significance:

I think the problem addressed by the authors is relevant and important for the machine learning community. However, I have the feeling that the authors have not success in noting this. The examples used by the authors are a bit simple. For example they only consider a single real example and only the linear operator of derivatives. I have the feeling that this paper may have potential applications in probabilistic numeric methods, in which often a GP is used. 

Summing up, I think that this is a good paper. However, the weak experimental section questions its significance. Furthermore, I find difficult to find practical applications within the machine learning community. The authors should have made a bigger effort on showing this. I would consider it hence a borderline paper.



","The authors present a novel method for inference in Gaussian processes subject to linear equality constraints.  In contrast to previous approaches, which used techniques such as data augmentation with artificial observations, the proposed method incorporates the linear constraints directly into the GP kernel such that all draws from the GP satisfy the constraints.   This is an elegant solution for linearly constrained GP regression problems. 

The principal drawback of this approach appears to be the non-trivial task of finding an operator G_x that spans the nullspace of F_x.  Algorithm 1 suggests an iterative approach to constructing such an operator, but little guidance is given for the crucial step of selecting a set of scalar operators (\xi_g).  For the running example, the differential operators are natural guesses given the form of F_x, but how might this be done more generally?  Moreover, is it guaranteed that such an operator exists?  

Minor comments:
- Section 3.1 seems out of place.  It would naturally fall under ""Related Work,"" but that is currently deferred to the end.  Personally, I think Related Work should come before Section 3.
- The discussion of ""interpreting F_x and G_x as matrices"" and ""thinking of F_x[f] as matrix-vector multiplications"" is a bit informal and could be made more rigorous. As written, the reader is left wondering if/when this is warranted. 

"
Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities,"Michael Eickenberg, Georgios Exarchakis, Matthew Hirn, Stephane Mallat",https://proceedings.neurips.cc/paper/2017/hash/72b386224056bf940cd5b01341f65e9d-Abstract.html,"The paper presents the solid harmonic scattering, which creates a rotation invariant representation of 2D and 3D structures. The paper presents the details of the proposed transformation and derives its properties. The solid harmonic scattering is then used to predict the energy of molecules given the positions of individual atoms and their charges. A permutation invariant embedding of a molecule is first computed from positions and charges, and then the scattering transform is applied to obtain a rotation and translation invariance representation. This representation is used to predict the total energy with a linear regressor and a neural net with multiplicative gates. Experiments in the GDB7-12 dataset are performed and the results are competitive with other machine learning based approaches.

The problem of energy prediction is important, and the proposed transformation is interesting. The introduction makes the case of learning from data using the right operators (such as convolutions for images), and motivates the exploration of special operators to analyze other types of data, such as molecule structures. The authors implemented the solution in GPUs to accelerate computations. The formulation of the method seems interesting, but the following questions need to be answered to frame this work with the current research in machine learning:

* Although an elegant design, the proposed module does not have learnable parameters, and thus the contribution to machine learning appears limited. Convolutional filters have the capacity to adapt weights for each problem, while the proposed transform seems to be fixed. Even though it has connections to neural networks, the static nature of the solution goes in the opposite direction of designing machines that learn.

* The transform seems to be very specific for molecule representation. It would be interesting to see applications in other domains that would benefit from this transform. If the value of this work has more impact in the quantum physics community, perhaps NIPS is the wrong venue to discuss its merit? 

","This paper presents a method for computing rotation and translation-invariant filters for 2D and 3D signals. Invariance is acheived by integrating over the (modulus of) solid harmonic wavelet coefficients. The utility of solid harmonic wavelets to achieve transformation invariance seems like a novel contribution to this application (e.g. molecular energy regression). However, the proposed invariant coefficients are calculated in a fixed way. The neural network is actually applied only for regression on the invariant descriptors for example.  Thus, this proposed work has a much in common with the long line of literature that has explored rotation invariance for 3D signal representations using Spherical Harmonic expansions (e.g. Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors 2003 just to list one of many). Given the close tie between the wavelet construction and spherical/solid harmonics, it would be easier to track the background of this paper by exploring the relationship to the existing literature on invariance through spherical harmonics.

The contribution to the molecular energy prediction seems interesting and novel, and the impact of the paper may depend on the value of that contribution (this review is not an expert in that topic). Based on the invariant descriptors alone however, the paper is borderline (experimental impact would be necessary).","This paper presents a novel representation learning approach for 2D and 3D images, based on solid harmonic wavelet scattering. The representations are built to be invariant to rotation and translation. The proposed approach is applied to quantum molecular energy prediction and is shown to reach state-of-the-art performance.

The paper is well written. The solid harmonic wavelet scattering approach is a significant contribution and the application to quantum energy regression is very interesting. The experiments are convincing and shows promising results. 

I am a bit concerned by the potential applications of the proposed approach: The wavelets seem to be specifically designed for the task at hand, the authors should discuss other potential applications in more details.

Minor comments:
- References should be added for the claims in Section 3.1
- Please add references to Table 1, either directly in the table or in the caption."
On Frank-Wolfe and Equilibrium Computation,"Jacob D. Abernethy, Jun-Kun Wang",https://proceedings.neurips.cc/paper/2017/hash/7371364b3d72ac9a3ed8638e6f0be2c9-Abstract.html,"The paper draws a connection between the classical Frank-Wolfe algorithm for constrained smooth & convex optimization (aka conditional gradient method) and using online learning algorithms to solve zero-sum games. This connection is made by casting the constrained convex optimization problem as a convex-concave saddle point problem between a player that takes actions in the feasible set and another player that takes actions in the gradient space of the objective function. This saddle point problem is derived using the  Flenchel conjugate of the objective. Once this is achieved, a known and well explored paradigm of using online learning algorithms can be applied to solving this saddle point problem (where each player applies its own online algorithm to either minimize or maximize), and the average regret bounds obtained by the algorithms translate back to the approximation error with respect to the objective on the original offline convex optimization problem.
The authors show that by applying this paradigam with different kinds of online learning algorithms, they can recover the original Frank-Wolfe algorithm (though with a slightly different step size and rate worse by a factor of log(T)) and several other variants, including one that uses the averaged gradient, using stochastic smoothing for non-smooth objectives and even a new variant that converges for non-smooth objectives (without smoothing), when the feasible set is strongly convex.

***Criticism***
Pros:
I think this is a really nice paper, and I enjoyed reading it. While, in hindsight, the connection between offline optimization and zero-sum games via Fenchel conjugates is not surprising, I find this perspective very refreshing and original, in particular in light of the connection with online learning. I think NIPS is the best place for works making such a connection between offline optimizaiton and online learning. I truly believe this might open a window to obtain new and improved projection-free algorithms via the lens of online learning, and this is the main and worthy contribution of this paper, in my opinion. This view already allows the authors to give a FW-like algorithm for non-smooth convex optimization over strongly convex sets which overcomes the difficulties of the original method. There is an easy example of minimizing a piece-wise linear function over the euclidean unit ball for which FW fails, but this new variant works (the trick is to indeed use the averaged gradient). Unfortunately, as I discuss next, the authors do a somewhat sloppy work presenting this result.

Cons:
- I find Section 4.3 to be very misleading. Algorithm 5 is a first-order method for convex non-smooth optimization. We know that in this setting the best possible (worst-case) rate is 1/\eps^2. The construction is simply minimization of a piece-wise linear function over a unit Euclidean ball (which is certainly strongly convex with a constant strong convexity parameter). Hence, claiming Algorithm 5 obtains a log(T)/T rate is simply not true and completely misleading. Indeed, when examining the details of the above ``bad"" construction, we have that L_T will be of order \eps when aiming for \eps error. The authors should do a better job in correctly placing the result in the context of offline optimization, and not simply doing plug&play from the online world. Again, stating the rate as log(T)/T is simply misleading. In the same vain, I find the claims on lines 288-289 to be completely false.
It is also disappointing and in a sense unsatisfactory that the algorithm completely fails when gradient=0 is achievable for the problem at hand. I do hope the authors will be able to squeeze more out of this very nice result (as I said, it overcomes a standard lower bound for original FW I had in mind).

Minor things:
- When referring to previous works [13,15] the authors wrongly claim that they rely on a stronger oracle. They also require the standard linear oracle but use it to construct a stronger oracle. Hence, they work for every polytope.
- In Definition 2 is better to state it with gradient instead of subgradient since we assume the function is smooth.
-Line 12: replace an with a
- on several occasions when using ``...=argmin"", note in general the argmin is a set, not a singleton. Same goes for the subdifferential.
-Line 103: I believe ``often times"" should be a single word
- Theorem 3: should state that there exists a choice for the sequence \gamma_t such that...
- Theorem 5: should also state the gamma_t in the corresponding Algorithm 2
- Supp material Definition on line 82: centered equation is missing >=0. Also, ``properly contains"" in line 84 not defined.
- Supp material: Section 4 is missing (though would love to hear the details (: )

*** post rebuttal ***
I've read the rebuttal and overall satisfied with the authors response.
","This paper looks at how Frank Wolfe algorithm can be cast as a minimax optimization problem which can be cast as an equilibrium computation of a zero-sum game, where two players both play optimally. The paper shows that the standard Frank Wolfe method can be cast as an example of a minimax game, where the first player plays the Follow the Leader algorithm while the second player plays Best Response algorithm. They show that the regret of various online algorithms including FTL in a smooth setting can be obtained from the analysis of this framework. The same sort of analysis for the second player playing Best Leader gives an analysis of cumulative conjugate gradient algorithm analysis. The meta-algorithm and Frank wolfe algorithm can also be extended to the setting when f is not smooth, so as to match the FTPL algorithm. In this case, the two players are switched with the second player playing FTL and the first player playing Best Response. which ends up providing the Boundary Frank Wolfe algorithm with O(1/T) rates of convergence with a strongly convex boundary.
The paper is well written and is easy to read with the connection shown nicely in a mathematical format. However it is not very clear how many novel results fall under the purview of the paper's method. The connection between FW and the minimax optimization is pretty well known [17] in the paper's references talk about it in detail. It would be interesting to know what allows the analyses to work for the non-smooth case, also it would be interesting to get some perspective if any of the analyses can match the best rates of batch methods as well. "
Generalizing GANs: A Turing Perspective,"Roderich Gross, Yue Gu, Wei Li, Melvin Gauci",https://proceedings.neurips.cc/paper/2017/hash/73e5080f0f3804cb9cf470a8ce895dac-Abstract.html,"GANs are a very interesting idea, which I haven't heard of so far. Using them for control is very appealing.

The paper is very well written, easy to understand, high in quality: both in methods and results.

There are just two comments. Co-evolution and resilience are much older than the authors assume. Co-evolution has been studied e.g. in 


Stefano Nolfi and Dario Floreano. 1998. Coevolving Predator and Prey Robots: Do ""Arms Races"" Arise in Artificial Evolution?. Artif. Life 4, 4 (October 1998), 311-335. DOI=http://dx.doi.org/10.1162/106454698568620

and more can be found in Nolfi & Floreano, Evolutionary Robotics, 2000

Resilience, e.g. learning the own body model also under variations of the body has been investigated in

J. Bongard, V. Zykov, and H. Lipson. Resilient machines through continuous self-modeling. Science, 314(5802):1118–1121, 2006.

It would be great if the authors could discuss their approach/results in the context of more related work.","This is a very well-written paper and the problem / approach is very interesting. I wonder though how the authors would propose addressing this problem when the action space is discrete?

In addition, Figure 5 is a bit hard to parse. Can the authors summarize some of the statistics (such as correlation) to better demonstrate what the robot is doing?","This paper proposes a generalization of GAN, which the authors refer to as Turing Learning. The idea is to let the discriminator to interact with the generator, and thus work as an active interrogator that can influence the sampling behavior of the generator. To show that this strategy is superior to existing GAN which has the discriminator to function passive responder, the authors perform two case studies of the proposed (high-level) model (interactive), comparing against models with passive discriminators. The results show that the interactive model largely outperforms the passive models, with much less variances across multiple runs.

Pros
1) The paper is written well, with clear motivation. The idea of having the discriminator to work as active interrogator to influence the generator is novel and makes perfect sense. 
2) The results show that the proposed addition of discriminator-generator interaction actually helps the model work more accurately. 

Cons
3) The proposed learning strategy is too general while the implemented models are too specific to each problem. I was hoping to see a specific GAN learning framework that implements the idea, but no concrete model is proposed and the interaction between the discriminator and the generator is implemented differently from model to model, in the two case studies. This severely limits the application of the model to new problems. 
4) The two case studies consider toy problems, and it is not clear how this model will work in real-world problems. This gives me the impression that the work is still preliminary. 

Summary
In sum, the paper presents a novel and  interesting idea that can generalize and potentially improve the existing generative adversarial learning. However, the idea is not implemented into a concrete model that can generalize to general learning cases, and is only validated with two proof-of-concept case studies on toy problems. This limits the applicability of the model to new problems. Thus, despite its novelty and the potential impact, I think this is a preliminary work at its current status, and do not strongly support for its acceptance."
Predicting Scene Parsing and Motion Dynamics in the Future,"Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng Chen, Zequn Jie, Jiashi Feng, Shuicheng Yan",https://proceedings.neurips.cc/paper/2017/hash/73fed7fd472e502d8908794430511f4d-Abstract.html,"The paper proposes a deep-learning-based approach to joint prediction of future optical flow and semantic segmentation in videos. The authors evaluate the approach in a driving scenario and show that the two components - flow prediction and semantic segmentation prediction - benefit from each other. moreover, they show a proof-of-concept experiment on using the predictions for steering angle prediction.

The paper is related to works of Jin et al. and Neverova et al. However, as far as I understand, both of these have not been officially published at the time of submission (and the work of Neverova et al. only appeared on arxiv 2 months before the deadline), so they should not be considered prior art.

Detailed comment:

Pros:
1) The idea seems sound: predicting segmentation and optical flow are both important tasks, and they should be mutually beneficial.
2) The architecture is clean and seems reasonable
3) Experimental evaluation on Citiscapes is reasonable, and some relevant baselines are evaluated: copying previous frame, warping with the estimated flow, predicting segmentation without flow, predicting flow without segmentation, the proposed model without the “transformation layer”. The improvements brought by the modifications are not always huge, but are at least noticeable.

Cons:
1) Steering angle prediction experiments are not convincing at all. The baseline is very unclear, and it is unclear what does the performance of “~4” mean. There is no obvious baseline - training a network with the same architecture as used by the authors, but trained from scratch. The idea of the section is good, but currently it’s not satisfactory at all.
2) I am confused by the loss function design. First, the weighting of the loss component is not mentioned at all - is there indeed no weighting? Second, the semantic segmentation loss is different for human-annotated and non-human-annotated frames - why is that? I understand why human-annotated loss should have a higher weight, but why use a different function - L1+GDL instead of cross-entropy? Have you experimented with other variants?
3) Figures 3 and 4 are not particularly enlightening. Honestly, they do not tell too much. Of course it is always possible to find a sample on which the proposed method works better. The authors could find a better way to represent the results - perhaps with a video, or with better-designed figures.
4) Writing looks rushed and unpolished.  For instance, caption of Table 2 is wrong.
Minor:
5) I’m somewhat surprised the authors use EpicFlow - it’s not quite state of the art nowadays
6) Missing relevant citation: M. Bai, W. Luo, K. Kundu and R. Urtasun: Exploiting Semantic Information and Deep Matching for Optical Flow. ECCV 2016.
7) Many officially published papers are cited as arxiv, to mention a few: Cityscapes, FlowNet, ResNet, Deep multi-scale video prediction, Context encoders

Overall, the paper is interesting and novel, but seems somewhat rushed, especially in some sections. Still, I believe it could be accepted if the authors fix some most embarrassing problems mentioned above, in particular, the part on steering angle prediction.","This paper combines two ideas: 1) predicting optical flow and 2) image segmentation, both using convolutional architectures on a multi-step context, into a joint architecture that both predicts optical flow using semantic labels and predicts the semantic labels S_{t+1} on the next frame using previous optical flow. The paper thereby introduces the idea of predicting the semantic label on the next image rather than on the current image. Specifically, the flow predictor outputs one flow map for foreground moving objects, one for static backgrounds and one for other (e.g., vegetation), and the pixel assignment to these 3 classes depends on the semantic segmentation. The flow features (before upsampling and up-convolution) are used as extra inputs, through a transform layer, along with previous semantic maps, to predict the future semantic map. Finally, the k-step context CNN can be used as an auto-regressive model on S_t and flow to generate iterated predictions, and be trained like an unrolled RNN, using BPTT.

The model is trained on automatically generated labels, provided by state-of-the-art semantic image segmenters and flow estimators.

The results of the joint model outperform both epicflow [21] and S2S [18].

The application to steering angle prediction uses a single baseline. As this seems not to be a common research problem in the community, these experimental results seem anecdotical and would be enhanced by comparisons to more methods.

The paper could benefit from some rewriting. The introduction is very verbose, and that space could be used for a more detailed figure of the full architecture. Are S_{t-1}, ..., S_{t-4} produced by the same epicflow model that is used for distillation, or is the flow predictor retrained? The abstract also contains grammatical errors, e.g., ""later"" -> ""latter"", ""arbitrary time steps ahead"".","The paper introduces a joint system for predicting motion flow and future scene segmentations from frames of video. The motivation is that flow and segmentation are co-dependent problems, and might mutually benefit from each others' solutions.

The authors use two CNN architectures, one which targets motion flow prediction and the other which targets future scene parsing, with a mid-level connection from the motion predictor to the scene parser. They are trained jointly. Their results show their trained system outperforms PredFlow on flow prediction and outperforms S2S on future scene parsing.

In general the paper is interesting but has several problems. 

1. The writing and presentation are not very clear in many place. There was little intuition for why this should or shouldn't work. I had an especially hard time understanding details of the proposed architecture, such as what the inputs are. For example, how exactly does the flow network benefit from the parsing network? Is it just through their joint training? The Intro says: ""In the motion prediction task, we utilize the semantic segmentations produced by predictive parsing to separately predict motion for pixels with different categories."" And section 3.1 says: ""First, the last segmentations St−1 produced by the parsing anticipating network convey pixel-wise class labels, using which the flow anticipating network can predict optical flow values for each pixel according to its belonging object group, e.g. moving objects or static objects."" However, I do not see this made explicit anywhere. How does this work? 

2. The novelty is relatively small given previous work. The paper focuses on joining two strong approaches together and showing that modest improvements are possible in their respective tasks. The improvement in anticipatory scene parsing isn't that surprising - if the motion prediction features weren't useful, the scene parser should just learn to ignore them, and of course motion flow is very sensitive to object boundaries. I'm less sure I understand the reason for the improvement in motion prediction due to my point above about not understanding how the motion predictor uses the parser's output. But again, because both tasks depend on common underlying spatial structure in scenes, it isn't too surprising that parsing features can improve motion prediction. 

3. Isn't this an example of multi-task learning, or at least a close variant? It would be nice to acknowledge that literature and situate this work in the context of what's done there.


One suggestion for making this work more relevant to those who don't necessarily work in this specific visual domain would be to further develop this as a general purpose framework for joining architectures that specialize in different tasks, so a user can take two good architectures and have a good chance of their hybrid learning both tasks better. I think this work moves in that direction, but I don't think it explores this sufficiently (e.g. the transform layer)."
A Screening Rule for l1-Regularized Ising Model Estimation,"Zhaobin Kuang, Sinong Geng, David Page",https://proceedings.neurips.cc/paper/2017/hash/74071a673307ca7459bcf75fbd024e09-Abstract.html,"The author(s) present a screening rule for block structure identification in Ising model. This screening rule combined with exact or inexact optimization procedure can lead to scalebale parameter estimation in Ising model. I have following thoughts on the proposed approach-

(1) Finding a block structure can also be done by using spectral clustering or regularized spectral clustering. Once the block structure is recovered then the exact or inexact optimization can be carried out there as well. How different (or may be effective) the screening procedure compared to a eigenvalue decomposition on a graph Laplacian?

(2) The second moment screening rule procedure proposed here is similar to the screening rule proposed in Witten et al. (2011) and Mazumder & Hastie (2012). How different/similar is the current approach with the existing ones in the literature?

(3) How robust is the block wise parametrization of $theta$ is? If the true $\theta$ does not have a block structure is the initial screening rule could lead to inaccurate parameter estimation?
","SUMMARY:
========
The authors propose an efficient screening for sparse Ising model structure estimation.  The screening rule is shown to be optimal for locating non-interacting variables in an L1 regularized MLE.  Combining this screening rule with approximate optimization allows efficient parallel computation of the full model structure, as demonstrated on several synthetic graph problems.

PROS:
=====
The screening rule can be calculated fairly efficiently, in time quadratic in the number of data elements, and as such could be easily adopted in practice.  The authors also clearly show that the method is optimal in a regularized MLE sense, and computation of the screening rule also provides a range of valid regularization parameters including those that yield a fully disconnected graph.

CONS:
=====
The authors do not discuss the relationship between the proposed screening rule and the *nearly* identical ""correlation decay"" property considered in Loh and Wainwright (2013) and references therein.  The work of Loh and Wainwright is referenced but not cited in the text.  This oversight is curious since Loh and Wainwright provide a probabilistic bound of correct reconstruction using the nodewise method considered in the present paper.  

This reviewer feels that the experimental results are lacking and too much material is referenced in the supplement.  In particular, the authors claim experiments on real world data (L:228).  However, the real world experiment is a single gene interaction network on which only the node-wise method is performed, and is entirely contained in the supplement (B.6).  Moreover, the real world experiment is a small graph for which screening only yields a 20% speed improvement.  This is an unfortunate omission as more detailed experimentation of real world data would have been beneficial.

In addition to the real world experiment, other experimental details are deferred to the supplement or not discussed at all.  Discussion of model selection is omitted and references B.4, which in turn provides only a few sentences and references Liu et al. (2010).  Approximations are only compared to exact inference in a single experiment (also in the supplement) for which exact computation is computed in 90s.  Finally, inclusion of the ""mixed"" approach which uses NW for model selection and PL for estimation is both slower than PL and less accurate than NW, it is unclear why this approach would be considered.
","Review of paper 477 ""A Screening Rule for l1-Regularized Ising Model Estimation""

This paper presents a simple closed-from screening for
l1-regularized Ising model estimation. Specifically, the authors
prove (in Theorem 1) a necessary and sufficient condition that
ensures that blockwise solution of the l1-regularized maximum
likelihood estimation is identical the complete l1-regularized
maximum likelihood solution. This result is based on the KKT
conditions for the l1-regularized MLE and is elegant both in terms
of its simplicity and the simplicity of its proof. The authors also
discuss the challenges remaining, i.e., that the NP-hard problem is
now reduced to a set of lower dimension yet still NP-hard problems.
Hence, making progress in the right direction.

Overall, an elegant result worthy of publication in NIPS.

A thought:

1. How does the hyper parameter tuning (finding \lambda) affect
this problem? If one scans through lambda, then for low enough
values of \lambda only one block can be found and the computation
time will not be reduced. Some clarification would be helpful.
"
A Minimax Optimal Algorithm for Crowdsourcing,"Thomas Bonald, Richard Combes",https://proceedings.neurips.cc/paper/2017/hash/743394beff4b1282ba735e5e3723ed74-Abstract.html,"The authors propose a new lower bound and an efficient method to evaluate the reliability of worker in crowdsourcing scenario. The proposed bound is stricter than the previous ones. The method can also be used in the streaming setting and do not need any EM step to reach the optimal bound as previous work did. 
This paper proposes sound theoretical proof. It clearly points out the advantage comparing to the previous work. The paper conducted experiments on real and simulated datasets. The results do not show overwhelming out-performance (only better in some settings), which indicates certain room for improvement. Nevertheless, it is s solid paper that suggests an interesting research direction.","Summary of paper:
The authors derive a lower bound on estimating crowdsource worker reliability in labeling data.  They then propose an algorithm to estimate this reliability and analyze the algorithm’s performance.

Summary of review:
This submission presents a solid piece of research that is well-grounded in both theory and application.

Detailed review:
—
Quality:
This paper is thorough.  The lower bound and relevant proofs are outlined cleanly.  The algorithm is presented in conjunction with time and memory complexity analysis, and the authors compare to related work nicely.  Both simulated and real data sets are used; the simulated data have multiple settings and six sources of real-world data are used.  The experiments do not demonstrate that the proposed algorithms outperforms all competing methods, but these results are refreshingly realistic and provide a compelling argument for the use of the proposed algorithm.

—
Clarity:
The paper is well-written; the problem and contributions are outlined clearly.  The paper would benefit from more description regarding the intuitions of TE and perhaps a high-level summary paragraph in the numerical experiments section.

—
Originality: 
The proposed algorithm is novel and the derived lower bound is tied nicely to the problem, bringing a rare balance of theory and application.  

—
Significance:
While the problem addressed is very specific, it is important.  The proposed algorithm does well—even if it doesn’t beat all others in every instance, it’s more consistent in its performance.  This paper will be a solid contribution to the crowdsourcing literature, and could easily impact the use of crowdsourced labels as well.","The results presented in the paper seem correct. I did not find any flaws in the analysis. The empirical results seem good but not a big improvement over other methods. However the significance of the results presented escapes me, as I am not familiar with the literature or have any practical experience in crowdsourcing.

"
Communication-Efficient Distributed Learning of Discrete Distributions,"Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, Ludwig Schmidt",https://proceedings.neurips.cc/paper/2017/hash/7486cef2522ee03547cfb970a404a874-Abstract.html,"This paper studies the problem of learning discrete distributions in the distributed setting, where random samples from an unknown discrete distribution are evenly distributed over machines, and the machines can communicate with a referee that at the end needs to output an estimation of the distribution. This problem is important across various applications and also theoretically interesting, but has been studied systematically. This paper provides a set of results that cover the cases of estimating unstructured and structured distributions in l1 and l2 distances.

For unstructured distributions, if each machine has only one sample, then the trivial communication protocal of sending all the data to the referee has optimal communication up to constant factors. Lower and upper bounds are also provided when the machines have more than one sample though with a gap.
For structured distributions, the paper studies k-histogram and monotone distributions. In different regimes about the number of samples on each machine, protocols are proposed that achieve better communication guarantees than the trivial ones. In some regimes the bounds are tight. It is impressive that the study is quite complete, covering almost all regimes (except the lower bounds for learning of monotone distributions in l1). The tables in the supplementary materials are informative and I suggest moving some of them to the main text. 

The presentation is clear and related works are properly cited. 

Overall, the paper proposes a direction that is both empirically important and theoretically interesting, and provides many highly non-trival results, and insightful conceptual messages. 
There are a lot of interesting open questions, like improving the bounds in the paper, considering other important structured distributions, considering other metrics besides l1 and l2, etc. 

-- Line 349: learnking -> learning","Summary; The paper studies the classical problem of estimating the probability mass function (pmf) of a discrete random variable given iid samples, but distributed among different nodes. The key quantity of interest is how much communication must be expended by each node (in a broadcast, but perhaps interactive, setting) to a central observer which then outputs the estimate of the underlying pmf. The main results of the paper areclearly stated and are  easy to follow.  The results mostly point out that in the worst case (i.e., no assumptions on the underlying pmf) there is nothing better for each node to do than to communicate its sample to the central observer. 

The paper addresses a central topic of a long line of recent works on distributed parameter estimation. The distinguishing feature here is to have as few parametric assumptions on the underlying distribution as possible. 

1. In the case when no assumption at all is made on the underlying distribution (other than the alphabet size), the main result is to show that essentially nothing better can be achieved than when all nodes communicate their raw samples (noninteractively) to the central observer. The upper bound is hence trivial and doesnt take much to analyze. The key innovation is in the lower bound where the authors consider a specific near-uniform distribution (where neighboring letters have probabilities that are parametrically (\delta_i in the notation of the paper) close to each other). Then the authors show that any (interactive) protocol allows the central observer to learn these \delta_i parameters and which itself is tantamount to learning the sample itself. The key calculation is to bound the mutual information between the transcripts and the \delta_i parameters which is then fed into a standard hypothesis testing framework (Fano's inequality). Although this machinery is standard, I found the setup of the hypothesis test creative and interesting and novel. Overall, this part of the paper is of fundamental interest (no assumptions at all on the underlying hypothesis) and nicely carried out. 

2. I am less sure about the two `structured' settings considered in this paper: k-histograms and monotone pmfs. I can imagine monotone pmfs coming up in some application domain, but far less sure about why k-histograms makes sense.  The main challenging aspect of this setting is that only k intervals are allowed, but their position is arbitrary. I understand the formulation makes for pretty math (and fits in nicely with a bunch of existing literature), but cannot  see any canonical/practical setting where allowing this flexibility makes sense. I would appreciate it if the authors can elaborate on this point in the rebuttal stage. 

3. In the context of the last sentence of the previous comment, I would also like to see some discussion by authors on any practical implications of this work. For instance the authors mention that the problem is very well motivated and cite some works from the literature [44, 40, 30, 54, 47]. Of these, I would guess that [47] is the only recent practical work (the others are either more than a decade old or also correspond to an already stylized setting) -- on this topic, can the authors please fix the reference [40] so that it includes its title? I would like to see concrete settings where the structured settings are motivated properly and the authors actually try out their algorithms (along with competing baselines) in that setting. In the context of this paper, I would like to see a (brief) discussion of how the settings might be related to the distributed computing revolution underlying modern data centers. Most NIPS papers  have a section where they discuss their  algorithms in the context of some concrete practical setting and report empirical results. 

3. The schemes proposed in the two structured settings are more sophisticated than the baseline one (where the nodes simply communicate their samples directly), but are directly motivated from their centralized counterparts. In particular, the k-histogram setting uses ideas from [3] and approximation in the \A_k norm and the monotone setting from the classical work of Birge. Overall, the proofs are correct and represent a reasonable innovation over a long line of work on this topic. 

Summary: I recommend this submission as far better suited to COLT or even SODA. ","This paper considers the problem of learning a discrete distribution in the distributed model. The samples from the distribution is partitioned into a number of machines, and the goal is to efficiently communicate to a central server that will learn the distribution. They show that if the distribution is unstructured, the naive protocol (of each machine just sending over its entire sample) is the best one can do. On the other hand, the authors consider certain families of structured distributions where it is possible to non-trivially improve upon this naive strategy. 

The paper is very well written. In particular, I commend the authors' simplified theorem statements in the main text, while leaving more precise versions to the appendices. The proof sketches in the main text are illuminating and sufficient to understand the ideas. 

There are significant gaps in the upper and lower bounds in Theorems 2.2, 2.3, and 2.4, especially in terms of epsilon. The authors should explain this, preferably right after the theorem statements. Its not clear to me if these are inevitable or if they are simply artifacts of the analysis. 

That said, this is a solid submission, and I recommend its acceptance.  

Minor: 
- The sentence starting in Line 139 is incomplete. 
- Line 349: learnking-->learning"
VAIN: Attentional Multi-agent Predictive Modeling,Yedid Hoshen,https://proceedings.neurips.cc/paper/2017/hash/748ba69d3e8d1af87f84fee909eef339-Abstract.html,"This paper extends interaction networks (INs) with an attentional mechanism so that it scales linearly (as opposed to quadratically in vanilla INs) with the number of agents in a multi-agent predictive modeling setting: the embedding network is evaluated once per agent rather than once for every interaction. This allows to model higher-order interactions between agents in a computationally efficient way. The method is evaluated on two new non-physical tasks of predicting chess piece selection and soccer player movements.

The paper proposes a simple and elegant attentional extension of Interaction Networks, and convincingly shows the benefit of the approach with two interesting experiments. The idea is not groundbreaking but seems sufficiently novel, especially in light of its effectiveness. The paper is clear and well written. I appreciate that the authors provide an intuitive yet precise explanation (in words) of their mechanism instead of resorting to unnecessarily complicated notation and formulas.

Some confusion comes from the discussions about single-hop and multi-hop, terms that should probably be defined somewhere. On line 105, the authors write: ""The architecture can be described as a single-hop CommNet with attention. Although CommNets typically use more hops for learning communications, more than one hop did not help on our predictive modeling tasks"", but on lines 136 and 262 it says that VAIN uses multiple hops.

The experiments are well thought-out and interesting. I'm only missing some more insights or visualizations on what kind of interactions the attention mechanism learns to focus on or ignore, as the attention mechanism is the main contribution of the paper.

Further, I wonder if a disadvantage of the proposed attention mechanism (with the fixed-length softmax layer depending on the number of agents) does not take away one of the benefits of vanilla INs, namely that it is straightforward to dynamically add or remove agents during inference. If so, perhaps this can be mentioned.

Some minor remarks and typo's:
- typo in sentence on line 103-104: two times ""be""
- equation (3) should be C = (P_i, F_i), or the preceding sentence should be changed
- line 138: sentence should start with ""*While* the original formulation ...""
- line 284: generally *fares* poorly.

Overall, I believe this work would be a valuable contribution to NIPS.","It's not clear to me what the utility of the claimed predictive modeling is. The prediction task in the chess domain of predicting the piece that will move next (MPP) and the next piece that will be taken (TPP) seems strange to me. I don't understand the connection with reinforcement learning that the authors were describing in line 90. The caveat here is that I'm not familiar with CommNets, which the current work is supposed to based on. However, independent from that, I think the paper will benefit from a clearer exposition of the particular tasks and evaluation metrics. Similar to the chess domain, I also don't fully understand what the model is supposed to predict for the soccer domain. If the task is to predict the positions of all players for the next 4 seconds based on the current frame, it's not clear to me how meaningful the task is. What role to other contexts, such as the positions of the opposing team, for example, play in this prediction model?  ","Summary: The authors propose to model predictive tasks in board games via multi-agent interactions. The presented architecture is a CommNet where the interactions P_i affecting actor i are computed by a weighted average instead of a uniform average. The weights w_{i,j} are produced by a softmax over all pair-wise products between the attention vector of actor i with another actor. After learning, the weights encode the graph structure, i.e. which agents affect each other. The interactions P_i are then concatenated with the input features and processed by an MLP for regression (e.g. prediction of a soccer player’s field position in the near future) or classification (e.g. which chess piece moves next or is taken in the next move).
The authors’ premise that the model can learn interaction patterns although evaluating the embedding network per agent (instead of per interaction), is validated in the experimental section.

Qualitative Assessment: 
The paper reads well and is easy to follow. The experimental setup is clear and provides almost enough details for replication. The following points are missing, as far as I can tell:
- I can’t seem to find the number of steps or epochs, the model’s were trained for. There is also no mention of early stopping. How did you decide to stop the training? Was there a threshold for convergence of the training accuracy?
- How did you decide not to use BatchNorm in the soccer experiments, but to add it in the chess experiments?
- Are the numbers reported in Table 1 and 2 single runs or averaged over multiple runs? It is often advisable to display the mean and standard deviation or the min and max over multiple runs, especially since the differences between some rows in the soccer table are very small. 

Another point I would like to see clarified is the claim in line 99 that modeling board games as interactions between agents is novel. Isn’t that what most of game theory is about?

In line 272, I would change the sentence to ""It is clear that VAIN and CommNet perform…"" (as CommNet without pretraining performs better than VAIN in the middle column of table 1)
"
Hierarchical Attentive Recurrent Tracking,"Adam Kosiorek, Alex Bewley, Ingmar Posner",https://proceedings.neurips.cc/paper/2017/hash/752d25a1f8dbfb2d656bac3094bfb81c-Abstract.html,"The paper aims to track a single object in videos. The objective is to maximize the Intersection-over-Union of the predicted box with respect to the ground truth bounding box. The paper uses attention from DRAW to focus only on a subset of the screen.
The paper adds extra losses to use the available ground truth boxes as attention targets during training.

The network may be a good approach for tracking.
I am mainly missing a comparison to the state of the art on a standard benchmark.
It would be nice to see improvements on a practically important task.

The paper provides a comparison only to a method designed for a different objective (next step prediction).
As expected, looking at the current frame improves the produced bounding box for the current frame. If looking at future frames is allowed, another method can look at multiple future frames.

The dataset with 21 videos seems too small. If training on a bigger dataset, a less regularized net may be better.

Clarification questions:
- How are the logarithmic arguments clipped? Is the gradient zero, if a value is clipped?

Typos:
- Line 38: s/prefrontal/primary visual/
- Equation (1) has two = =.
- Line 67: ImageNet video dataset is not present in experiments.
- Line 172: s/a:/: a/
- Footnote 2: s/vecorisation/vectorisation/
- Citation [14]: The paper title should be ""RATM: Recurrent Attentive Tracking Model"".

Update:
I have read the rebuttal.
It would be good to use a standard benchmark or define a new reproducible benchmark.","Summary: The authors present a framework for class-agnostic object tracking. In each time step, the presented model extracts a glimpse from the current frame using the DRAW-style attention mechanism also employed by Kahoue et al. Low level features are then extracted from this glimpse and fed into a two-stream architecture, which is inspired by the two-stream hypothesis. The dorsal stream constructs convolutional filters, which are used to extract a spatial Bernoulli distribution, each element expressing the probability of the object occupying the corresponding locations in the feature map. This location map is then combined with the appearance-based feature map computed by the ventral stream, removing the distracting features, which are not considered to be part of the object.
An LSTM is used for tracking the state of the object, with an MLP transforming the output of the LSTM into parameters for both attention mechanisms and an offset to the bounding box parameters in the previous time step.
The results on KTH and the more challenging KITTI dataset are significantly closer to the state of the art than previous attention based approaches to tracking.

Qualitative Assessment:
The paper reads well and is easy to follow. The description of the model is detailed enough for replication and experiments are performed providing evidence for some of the premises stated in the description of the model.
The results are impressive compared to previous work on attention based tracking. Since, you used a standard tracking benchmark, I think more performance numbers from the tracking community could have been included to show how close the presented method is to the state of the art. (example: Re3: Real-Time Recurrent Regression Networks for Object Tracking
D Gordon, A Farhadi, D Fox, arXiv preprint arXiv:1705.06368)","The paper presents a single object tracking framework based on attention mechanisms. The key insights are (1) learning spatial attention glimpse for coarse search and (2) using appearance attention to extract features without distractors.

The whole pipeline incorporated with recurrent connections is novel for exploiting both motion and appearance information. Each part of the framework is reasonable and well designed. The spatial attention and appearnce attention parts are hierarchically stacked and incorporate global context with top-down signals. All the components are integrated into one compact framwork and can be trained end-to-end.

As pointed in the abstract and intraduction, target-specific model cannot be learnt a priori. From what I understand, the CNN and DFN in the proposed method are used for learning target-specific apperance feature, and the spatial attention model exploits the class-agnostic information. The insights for each part in the framework are well explained, but it's hard to follow the motivation of the whole story.

The experiments and analysis are sufficient and favorable. It achieves significant improvements with meaningful visual results. I tend to vote for an acceptance."
Sobolev Training for Neural Networks,"Wojciech M. Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, Razvan Pascanu",https://proceedings.neurips.cc/paper/2017/hash/758a06618c69880a6cee5314ee42d52f-Abstract.html,"I have enjoyed reading this paper. The idea is clear and very intuitive. Figure 1 is very well conceived.

Now, I must say that, throughout my reading, I was looking for the authors to ""confess"" that the first-order derivatives might be entirely useless in certain situations. But in a situation where both f(x) and f'(x) are given, taken as ground truth, then it is certainly a wise idea to use the information about f'(x) in order to train a new network that approximates f(x) to solve a task.

However the problem is that, when a certain model M1 is trained to minimize a loss on a training set, then there is nothing that guarantees the sanity/usefulness of the gradients of M1. The derivatives depend a lot on the activation functions being used, for example. Then I don't see at all why there would be any general principle that would encourage us to imitate those gradients in order to learn another model M2 based on M1.

It is very possible that this doesn't happen in practice, on common datasets and reasonable models, and that's interesting. I think that it's partly what the paper studies : does this thing work in practice ? Okay, great ! Worth discussing in a paper with good experiments. Accept !


In the past, I have had a conversation with one of the great ML authorities in which he/she casually claimed that when two functions (f1, f2) are close, then their derivatives are also close. This doesn't sound unreasonable as a mental shortcut, but it's ABSOLUTELY INCORRECT in the mathematical sense. Yet people use that shortcut.

The authors refer to this indirectly, but I would really like to see the authors be more explicit about that fact, and I think it would make the paper slightly better. Just to be sure that nobody makes that mental shortcut when they read this stimulating paper.


I had a bit more difficulty understanding the subtleties of the Synthetic Gradient portion of the paper. I trust that it's sound.

Also, in Figure 4 I'm looking at certain plots and it seems like ""regular distillation"" very often gets worse with more iterations. (The x-axis are iterations, right?) Sobolev training looks better, but it's hard to say if it's just because the hyperparameters were not selected correctly. They kinda make regular distillation look bad. Moreover, the y-axis really plays with the scales and makes the differences look way bigger than they are. It's hard to know how meaningful those differences are.

To sum it up, I'm glad that I got to read this paper. I would like to see better, and more meaningful experiments, in a later paper, but I'm happy about what I see now.
","
This paper proposes a new kind of objective function to train neural networks when the gradient of f(x) w.r.t. x is known, called Sobolev training. In addition to providing some theory on sample complexity, the paper provides meaningful empirical evidence of their method.

Sobolev Training consists in training some model m(x;theta) to not only match f(x) but also the first Kth order derivatives of the output w.r.t to the input, i.e. train all grad^k_x m(x) to match grad^k_x f(x).

This paper is very well structured: it proposes an idea, motivates some nice theoretical properties behind it, then demonstrates its utility in some machine learning scenarios. It's clearly written and everything seems to make sense.

One downside of this paper is that it doesn't spend too much time motivating its theoretical approach. The polynomial approximation example is a good way to motivate this in 2D, but does this intuition hold when dealing with neural networks? Evidently something good is going on empirically, but looking at figure 4, I'm wondering if ""sample efficiency"" is the right way to look at this (unless zooming in early in these curves would reveal something else).
"
Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization,"Tomoya Murata, Taiji Suzuki",https://proceedings.neurips.cc/paper/2017/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html,"The paper proposes a doubly accelerated incremental method in mini-batch setting called DASVRDA, where on one hand it accelerates in the sense of Nesterov and on the other hand it accelerates by achieving a better dependency on the size of mini-batches. Convergence result has been provided for both strongly convex and convex objectives. 

I find the result very interesting, it is nice to see an acceleration occurs respect to the size of mini-batches both in theory and in the experiment. One important step in the algorithm is the restarting scheme which it is stated only require the optimal strong convexity of the objective instead of the ordinary strong convexity, can authors provide more details on it because I am confused by the terminology. 

In the experiment, an algorithm applying the UC[8](universal catalyst) on AccProxSVRG[12] (the accelerated mini-batch proximal stochastic variance reduce gradient) has been implemented but its theoretical guarantee is not mentioned in the paper. However, this method may be the real competitor to be compared with. Because it also uses an inner-outer loop construction as DASVRDA and the resulting algorithm will potentially enjoy the double acceleration property. Moreover, it can be applied on both strongly convex and convex settings. Can authors comment on it? 

Overall, the paper provides interesting result but some details need to be clarified. 

#EDIT Authors' feedback have addressed and answered my concerns. 
","Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized  Empirical Risk Minimization 

This paper introduce a new accelerated version of SVRG algorithm with mini batching. They consider two type of objective functions: strongly convex and non strongly convex functions. Their assumptions for both settings are fairly weak and for strongly convex functions, they assume weaker condition than the vanilla SVRG algorithm. They add the momentum terms in inner loop and outer loop of their algorithm. Their proves show that their methods improves the current state of the art methods. They test their methods against some real world data sets and show empirically their methods outperform other methods in mini_batch settings.   

Positive Points: 
Using mini_batch in stochastic optimization of ML models could help with reducing variance and also it is parallelizable. They considered mini_batch for accelerated SVRG for convex and non convex function and improved the convergence bounds in this setting. They showed that to get optimal cost, the good option for mini batch size is square root of n. They computes the efficient value for all the parameter of the algorithms and make the implementation of the algorithm easy. The paper's writing is clear and easy to follow which making the reading of the paper enjoyable. 
 
Minor comments: 
-The only part that I think could be improved is the experiment section. I would rather to see the result of method on bigger data set with bigger than 1M samples with different mini batch size. Beside comparing test error is also useful for ML community.    

- When you refer to supp. mat. in main paper, it'd be better if you mention the section of supp. mat. in main paper. It makes reaching the related part in supp. faster. ","The paper proposes a novel doubly accelerated variance reduced dual averaging method for solving the convex regularized empirical risk minimization problem in mini batch settings. The method essentially can be interpreted as replacing the proximal gradient update of APG method with the inner SVRG loop and then introducing momentum updates in inner SVRG loops. Finally to allow lazy updated, primal SVRG is replaced with variance reduce dual averaging. The main difference from AccProxSVRG is the introduction of momentum term at the outer iteration level also. The method requires only O(sqrt{n}) sized mini batches to achieve optimal iteration complexities for both convex and non-convex functions when the problem is badly conditioned or require high accuracy. Experimental results show substantial improvements over state of the art under the above scenario.

Overall, given the theoretical complexity of the paper, it is very well written and explained. Relation with the previous work and differences are clear and elaborative. The reviewer thinks that the paper makes substantial theoretical and algorithmic advances. However, I am giving paper relatively lower rating due to following reasons:

a)  I would like to see more experiments on different datasets especially the effect of conditioning and regularizer.
b)  The accuracy curves are not shown in the experiments. This is important in this case because it seems that major differences between methods are visible only at lower values of the gap and it would be interesting to see if accuracy has saturated by then which will make the practical utility limited.
c) Instead of x axis as gradient evaluations, I would like to see the actual wall clock time there because the constants or O(1) computation in inner loop matters a lot. Especially, the double accelerated equations look computationally more tedious and hence I would like to see the time axis there.
d) Looking at lazy updates in supplementary, I am not sure why accelerated SVRG cannot have lazy updates. Can authors exactly point out the step in detail which will have the issues."
Learning with Feature Evolvable Streams,"Bo-Jian Hou, Lijun Zhang, Zhi-Hua Zhou",https://proceedings.neurips.cc/paper/2017/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html,"This paper formalizes a new problem setting, Feature Evolvable Streaming Learning. Sensors or other devices to extract feature values have the limited lifespans; therefore, these devices have been periodically replaced and the associated feature space changes. This learning paradigm prepares the overlapping period to adapt to the new feature space. In this overlapping period, learning algorithms receive features from both the old devices and the new devices simultaneously to capture the relationship between two feature spaces.

This paper develops two learning algorithms to efficiently use previous experiences extracted from old training data to train/predict in the new feature space: 1) the weighted combination based predictor ensemble method, 2) the dynamic classifier selection. These two algorithms are proven to have theoretical performance guarantees respectively compared with the naive methods. Derived algorithms are simple for overcoming the difficulty in the feature evolvable streaming setting. 

Overall, this paper is interesting and well-written. The motivation is clear. The discussion is easy to follow. Besides, experiments include the real data setting, in which the RFID technique is used to gather data. This experiment demonstrates the usefulness of the proposed FESL problem setting.

I would like to ask authors about the effectiveness of combining different predictors is. In the real-data experiment, ROGD-u (a baseline method) is comparable to the proposed methods. This result implies that sophisticated feature transformations between two feature spaces are keys in the FESL setting, not how to combine two different predictors intelligently. We can see the combining two predictors have relatively small effects. Is there any good realistic example to show the intelligent combination of predictors works significantly better than the naive method? Is it necessary if we could improve the transformation method between two feature spaces?

As a minor note, if my understanding is correct, ROGD-u is also the proposed method of this paper, although this paper introduced this method as the baseline method. I think authors should insist on the importance of feature transformation and introduce these ROGDs as the proposed methods even without theoretical guarantees.","This paper presents two new methods for learning with streaming data under the assumption that feature spaces may evolve so old features vanish and new features occur. Examples of this scenario are environmental monitoring sensors or RFIDs for moving goods detection. Two different approaches are proposed, both based on the combination of two models: one trained with the old features and the other one based on the new features. Depending on how these models are combined, the authors propose two different methods for Feature Evolvable Streaming Learning (FESL): FESL-c that combines the predictions of both models, and FESL-s that selects the best model each time. Theoretical bounds on the cumulative loss of the both FESL methods as a function of the cumulative loss of the two models trained over different feature spaces are also provided. Experiments on datasets from the UCI Machine Learning Repository, Reuter’s data, and RFID real data have been conducted, and results show that the proposed models are able to provide competitive results in all cases when compared to three baseline methods.

Overall, I think this is a very interesting paper that addresses a real-world problem that is quite common, for example, in environmental sensoring applications. I think that the paper is well-written and well-organized, and the experiments show the significance of the proposed methodology. Additionally, I positively value the use of RFID real data. 

* Contribution with respect to transfer learning. My main concern is about the quantification of the degree of contribution of this work, as I cannot see clearly the difference what is the difference between the proposed approach and the use of transfer learning in two different feature sets (of course, taking into account that transfer learning cannot be applied from t=1,…, T1-B). I would like the authors to clarify this point. 

* Theorems 1 and 2. I would also like the authors to discuss about the practical implications of Theorem 1 and Theorem 2, as in both cases convergence to 0 is obtained when T2 is large enough but that is not the case in Figures 1 and 2. Additionally, it seems to me that although the upper bound for FESL-s algorithm is tighter than for the FESL-c method, the convergence is slower. Please, clarify this point.

* Optimal value for s. I do not understand why there is an inequality in Equation 9 instead of an equality according to the assumptions made (the first model is better than the second one for t smaller than s, and the second model is better than the first one for t larger than s). In this regard, I can see in Figure 3 that this assumption holds in practice, but the value for s depends on the problem you are working on. Providing some intuition about the range of values for s would be valuable.

* Datasets and experimental setup. I have some questions:
  - I would like the authors to clarify why the Gaussian noise was added to the UCI datasets instead of considering two different subsets of features. I guess that this is because it is assumed that the features spaces are not so different, but I would like the authors to discuss this point.
  - Please, provide more details about how batch datasets was transformed into streaming data. I suppose it was done randomly, but then I am wondering whether results on 10 independent rounds are enough to have reliable results and I would like to see the variability of the results provided in terms of the cumulative loss.
  - It is not clear to me how the hyperparameters of the methods (delta and eta) were adjusted. Were they set according to Theorems 1 and 2? What about the hyperparameters in the baseline methods?
  - I do not understand the sentence (page 7): “So for all baseline methods and our methods, we use the same parameters and the comparison is fair”.

* Results. It would be desirable to provide similar tables to Table 1 for the Reuter’s and RFID data.
","In this work, the authors propose a new setting in online learning area. They assume the old feature vould vanish and new feature could occur and there exist an overlapping period that contains both feature space. The authors propose a mapping strategy to recover the vanished feature and exploit it to improve performance. Besides, the authors propsoed two ensembele methhods with performance guarantee. The idea is interesting and straitforward, the paper is easy to follow.

My major concerns are:
1. The key idea is similar with [15], both of them design a constraint between vanished feature and new feature. However, i think the assumption and solution of this work is a special case of feature evolvable streams problem, especially for the assumption about overlapping period. That is, the titile is not appropriate for this work.

2. The theoretical analysis is a directly extension of learning with expert advice [5,28]. Thus, the theoretical result is not novel. "
Safe Model-based Reinforcement Learning with Stability Guarantees,"Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, Andreas Krause",https://proceedings.neurips.cc/paper/2017/hash/766ebcd59621e305170616ba3d3dac32-Abstract.html,"My understanding of the paper:

This paper describes a novel algorithm for safe model-based control of a unknown system.  This is an important problem space, and I am happy to see new contributions.  The proposed approach uses a learnt model of the system, and constrains the policy to avoid actions that could bring the system to un-safe states.  Additionally, both policy actions and exploratory actions are affected by this safety constraint.  The proposed algorithm is based on reasonable assumptions of Lipschitz continuity within the system dynamics as well as the presence of a Lyapunov function, which provides some quantification of risk-related cost of a state. One 'trick' the paper uses is to conflate the Lyapunov 'risk' function with the reward function, thus assuming that cost and 'risk' are antipodal. To be able to find a tractable policy, a discretization step is necessary, but the experimental section at the end of the article shows that the final algorithm is indeed able to perform in a toy environment.

My comments:
I find the hypotheses chosen to be slightly restrictive relative to many real-world scenarios, but acceptable in the case of a more theoretical paper.  More specifically, in the case of the Lyapunov function, many real-world systems have discrete safety functions over their state space (""everything is fine as long as you don't push /this/ button"").  Additionally, as I have stated above, IIUC, the cost function and Lyapunov function are the same, thus making scenarios where the high-reward states are very close to the high-risk states difficult to describe in this framework.  I would be curious to hear you out on this point in particular.

Other comments/questions in a list form:
Can you detail more what the discretization step in your theoretical analysis does to the guarantees?
What is your mixture between explorative and exploitative actions?  Algorithm 1 seems to only ever choose explorative actions.
What is an example of a real-world system you believe this would be useful on?  Especially because safety is a concern brought about mainly by applications of RL, it would have been interesting to see your algorithm applied to a more concrete scenario.

Overall opinion:
I think the analysis and the algorithm are interesting, and as I stated previously, I am very happy to see safety-related literature.  I don't believe this algorithm to be all that useful in application exactly as it is presented, but it provides a good framework for thinking about safety and a nice first pass at attempting to solve it in a less ad-hoc method.","Safe Model-based Reinforcement Learning with Stability Guarantees
==================================================================

This paper presents SafeLyapunovLearning, an algorithm for ""safe"" reinforcement learning.
This algorithm uses an initial safe policy and dynamics model and then sucessively gathers data within a ""safe"" region (determined by Lyapunov stability verification).
The authors establish some theoretical results that show this method is provably-efficient with high probability under certain assumptions and these results are supported by some toy experiment simulations.


There are several things to like about this paper:

- The problem of safe RL is very important, of great interest to the community and without too much in the way of high quality solutions.
- The authors make good use of the developed tools in model-based control and provide some bridge between developmenets across sub-fields.
- The simulations support the insight from the main theoretical analysis, and the algorithm seems to outperform its baseline.

However, I found that there were several shortcomings:

- I found the paper as a whole a little hard to follow and even poorly written as a whole. For a specific example of this see the paragraph beginning 197.
- The treatment of prior work and especially the ""exploration/exploitation"" problem is inadequate and seems to be treated as an afterthought: but of course it is totally central to the problem! Prior work such as [34] deserve a much more detailed discussion and comparison so that the reader can understand how/why this method is different.
- Something is confusing (or perhaps even wrong) about the way that Figure 1 is presented. In an RL problem you cannot just ""sample"" state-actions, but instead you may need to plan ahead over multiple timesteps for efficient exploration.
- The main theorems are hard to really internalize in any practical way, would something like a ""regret bound"" be possible instead? I'm not sure that these types of guarantees are that useful.
- The experiments are really on quite a simple toy domain that didn't really enthuse me.

Overall, I think that there may be a good paper in here - but that in its current form it's not up to the high standard of NIPS and so I would not recommend acceptance.
If I had to pinpoint my biggest concern it is that this paper really doesn't place itself properly in the context of related literature - particularly the line of work from Andreas Krause et al.
","This paper addresses the problem of safe exploration for reinforcement learning. Safety is defined as the learner never entering a state where it can't return to a low cost part of the state space. The proposed method learns a Gaussian Process model of the system dynamics and uses Lyapunov functions to determine if a state-action pair is recoverable from. Theoretical results are given for safety and exploration quality under idealized conditions. A practical method is introduced and an experiment shows it explores much of the recoverable state-space without entering an unrecoverable state.

Overall, this paper addresses an important problem for real world RL problems. Being able to explore without risking entering a hazardous state would greatly enhance the applicability of RL. This problem is challenging and this work makes a nice step towards a solution. I have several comments for improvement but overall I think the work will have a good impact on the RL community and is a good fit for the NIPS conference.

One weakness of the paper is that empirical validation is confined to a single domain. It is nice to see the method works well on this domain but it would have been good to have tried it a more challenging domain. I suspect there are scalability issues and I think a clear discussion of what is preventing application to larger problems would benefit people wishing to build on this work. For example, the authors somewhat brush the issue of the curse of dimensionality under the rug by saying the policy can always be computed offline. But the runtime could be exponential in dimension and it would be preferable to just acknowledge the difficulty.

The authors should also give a clear definition of safety and the goal of the work early on in the text. The definition is somewhat buried in lines 91-92 and it wasn't until I looked for this definition that I found it. Emphasizing this would make the goal of the work much clearer.

Similarly, this paper could really benefit from paragraphs that clearly define notation. As is, it takes a lot of searching backwards in the text to make sense of theoretical results. New notation is often buried in paragraphs which makes it hard to access.

Updated post author feedback:
While I still think this is a good paper and recommend acceptance, after reflecting on the lack of clarity in the theoretical results, I've revised my recommendation to just accept. Theorem 4 seems of critical importance but its hard to see that the result maps to the description given after it. Clarifying the theoretical results would make this a much better paper than it already is.

Minor comments:

How is the full region of attraction computed in Figure 2A?

Line 89: What do you mean by state divergence?

Line 148: This section title could have a name that describes what the theoretical results will be, e.g., theoretical results on exploration and policy improvement

Line 308: Could footnote this statement.

Line 320: ""as to"" -> to

Theorem 4: How is n* used?"
"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis","Kristjan Greenewald, Seyoung Park, Shuheng Zhou, Alexander Giessing",https://proceedings.neurips.cc/paper/2017/hash/769675d7c11f336ae6573e7e533570ec-Abstract.html,"This paper presents an elegant theoretical framework that enables estimating a covariance on both time and features. The framework is based on decomposing the covariance in two additive terms, a temporal and a spatial one, with temporal variations.

It seems that the authors uploaded a second copy of the manuscript as the supplementary materials. This is a pity.

The projection performed in eq (10) seems ad hoc; necessary undoubtedly, but I do not see what it corresponds to in terms of statistical model.


While the model is interesting, the validation on real data is a bit convoluted.
With regards to fMRI, could the authors give insight on the structure of B^-1 estimated? I am not sure that the general Markov model for the time domain is useful.

The validation is very indirect: a increase in brain connectivity is interesting, but it would be of more neuroscientific interest to quantify where it happens or what is its nature. Also, it is unclear to me why this is related to estimating time-changing connectivity.

It is a pity that the authors chose to work on the ADHD 200 data, which is one of most noisy large resting-state dataset. Little solid results have come out of this dataset. To study development, I would have trusted more the Philadelphia Neurodevelopmental Cohort.

Sparse inverse covariance techniques have been used in fMRI since at least 2010. I find it surprising that the authors are citing only papers from 2015 and after.

The fonts are tiny on figure 2 and 3. They are very hard to read.

The conclusion mentions validation on two fMRI data, but I see only one: ADHD 200.

When the authors write \|A\|_max, I believe that they have in mind the max norm, often written \|A\|_\infty. It would have been good to make this explicite.
","This paper studied the graphical structure in spatiotemporal data through transferring the time series data into an additive model, by assuming stationary temporal correlation structure and time-varying undirected Gaussian graphical model for spatial correlation structure. With the assumption that the spatial correlations change smoothly with time, they proposed estimators for both spatial and temporal structures based on kernel method and GLasso approach. The statistical convergence property of the estimators was provided under certain assumptions. The approach presented good performance in both simulation and fMRI data application studies.

This paper is overall clearly written, with solid theoretical support and interesting application. However, some assumptions, such as a common temporal correlation matrix for different spatial locations, may be not valid in application. It may worth further discussion. There are also some other points may need further improvement.

1.	The authors didn’t provide correct supplementary, but another version of the main text.
2.	In the estimation section, tr(A) is supposed to be known. This is not so obviously satisfied in real data application. It is better to provide more discussions on this issue.
3.	Line 178 is unclear. Which lemma? It should be the inverse of B, instead of B itself, exploiting sparsity.
4.	As m / n increases, the bound in Theorem 2 / Theorem 4 converges. However, the regularization parameter \lambda_m / \lambda_n also converges to 0 at the same time, which is a sacrifice of sparsity. In simulation, the authors used a variety of regularization parameters, instead of the proposed one in the theorem. More discussions are expected on this issue.
5.	In simulation, it is recommended to compare with other methods, such as Qiu (2016). Also the authors mentioned that some previous methods were limited to AR(1) model, while their method in this paper can handle more complex models. But they only provide simulation results for AR(1) case, as shown in Fig. 2 & 3. So it is expected to see the simulation results on more general models.
6.	In simulation, it would be better to use the inverse of B(t) instead of B(t) for evaluation, as the focus is the structure of the underlying undirected graphs instead of the edge weights / strength of the undirected graphs.
7.	The assumption that all spatial points follow the same temporal structure is a bit strong. In application, its validity should be discussed. For example, in the fMRI data, different brain regions may present different developing pattern. The visual cortex may develop fast in childhood, while the dorsal prefrontal cortex related to decision and attention usually has a development peak in a later age. It would be better to check the validity of these assumptions before application.
8.	Assuming a time series follows a joint normal distribution (equation (2)) is also not very common. In the fMRI data application, its validity should also be discussed. Each of the 90 regions of interest generally has hundreds to thousands voxels, which should enable the test for normality.
9.	In Line 118, it assumes that spatial structure is sparse and m much larger than n. However, the brain connectivity is usually not sparse. In the meantime, in this resting-state fMRI dataset, some subjects has less time points (m larger than or equal to 76) than the number of ROIs (n = 90).
10.	In the application, the dataset include both healthy group and ADHD group. Did the authors use them all together for analysis? It might be better to apply the method to each group separately and compare the results. Different developing trends are expected. And if such a result is obtained, it should be more powerful to illustrate the effectiveness of this method.
11.	In consistency statements for ‘signal’ and ‘noise’, in sense of their correspondence with ‘temporal’ and ‘spatial’ structure. In abstract (line 3-4), they mentioned ‘temporally correlated signal’ and ‘spatially correlated noise’. However, in 2.1 estimator section (line 133 – 134), they mentioned ‘spatial signal variance’ and ‘noise variance … corresponding to tr(A)’, while A corresponds to temporal structure as indicated in equation (2).
12.	In the paragraph from line 44 to 51, it is better to add comparison with Qiu (2016) and Monti (2014), in sense of methodology. 
13.	Some minor points: (i) It is better to provide the definition of e_i after equation (4), as it is provided after equation (5). (ii) The subscripts in equation (8) are not consistent, where ‘t’ should be ‘i’. (iii) In line 170, B to B(t). (iv) In line196, m to n.

Update: I am satisfied with the authors' replies to my comments. Hence I would like to raise my grade.","This work considers a time-varying Gaussian graphical model for spatio-temporal data. A nonparametric method is used to estimate both time-scale and spatial-scale covariance matrix. My comments are as follows:

1.	In equations (2) and (3), the normality assumption is a strong one since the spatio-temporal data may not follow multivariate normal in both rows and columns. 

2.	The notation in this work is somehow confusing. The X is sometimes denoted as random variable, and sometimes denoted as random matrix.

3.	One major concern is that the estimation procedure of B relies on the estimation of A, while the estimation of A also replies on the estimation of B. But the author does not use the iterative procedure to enable the close loop. Also it is not clear how is the convergence of such an iterative procedure. Another concern is that there are quite a few tuning parameters in the proposed method. It is not clear how the authors choose the optimal values of the multiple tuning parameters. 

4.	In this simulation study, the authors only show the performance of the proposed method in terms of the tuning parameter. But it is unclear what is the performance of the proposed method with the optimally-chosen lambda.

5.	The real application is not very convincing to show the advantage of the proposed method.
"
Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling,"Andrei-Cristian Barbos, Francois Caron, Jean-François Giovannelli, Arnaud Doucet",https://proceedings.neurips.cc/paper/2017/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html,"The paper introduce a novel MCMC method for drawing from high dimensional Gaussian distributions.
Clearly, there already exist direct methods for sampling a Gaussian density (involving Cholesky decomposition). The authors affirm that in an high dimensional framework is better to use of the proposed MCMC. 

- Is there, in the literature and applications, a clear demand for sampling high dimensional Gaussians (where Cholesky fails, for instance)?   I believe few cases. 
 
- In these cases, perhaps a Sigma  Points-type approach could be used avoiding any numerical problem (it is a deterministic approach but is able to summarize the Gaussian moments). Please, discuss.



 







","This paper proposes a new parallel approximate sampler for high-dimensional Gaussian distributions. The algorithm is a special case of a larger class of iterative samplers based on a transition equation (2) and matrix splitting that is analysed in [9]. The algorithm is similar to the Hogwild sampler in term of the update formula and the way of bias analysing, but it is more flexible in the sense that there is a scalar parameter to trade-off the bias and variance of the proposed sampler.

I appreciate the detailed introduction about the mathematical background of the family of sampling algorithms and related works. It is also easy to follow the paper and understand the merit of the proposed algorithm. The illustration of the decomposition of the variance and bias in Figure 1 gives a clear explanation about the role of \eta.

Experiments on two covariance matrix shows that the proposed algorithm achieves lower error than Hogwild for some range of the \eta value. However, the range changes with the computation budget as well as the value of the covariance matrix. It is not clear to me if the proposed algorithm is better than Hogwild in practice if we do not have a good method/heuristic to choose a proper value for \eta.

Another problem with the experiment evaluation is that there is no comparison in the only real applications, image pinpointing-deconvolution. Does the clone MCMC give a better image recovery than other competing methods?","The authors propose a novel Gibbs sampling approach for approximately drawing from high dimensional Gaussian distributions.

I very much enjoyed reading this paper, which has wide applicability.  I thought it was nicely written, both the introduction and the explanation of the proposed method were very clear, and the empirical results show clear benefits over existing Hogwild-type algorithms.

My only comment would be for the authors to provide some more practical information regarding the choice of the tuning parameter - how might this be chosen for particular applications?  Is there some way to learn an appropriate bias/variance trade-off in real time?"
Context Selection for Embedding Models,"Liping Liu, Francisco Ruiz, Susan Athey, David Blei",https://proceedings.neurips.cc/paper/2017/hash/7884a9652e94555c70f96b6be63be216-Abstract.html,"The authors propose an extension to the Exponential Family Embeddings (EFE) model for producing low dimensional representations of graph data based on its context (EFE extends word2vec-style word embedding models to other data types such as counts or real number by using embedding-context scores to produce the natural parameters of various exponential family distributions). They note that while context-based embedding models have been extensively researched, some contexts are more relevant than others for predicting a given target and informing its embedding. 

This observation has been made for word embeddings in prior work, with [1] using a learned attention mechanism to form a weighted average of predictive token contexts and [2] learning part-of-speech-specific classifiers to produce context weights. Citations to this related work should be added to the paper. There has also been prior work that learns fixed position-dependent weights for each word embedding context, but I am not able to recall the exact citation.

The authors propose a significantly different model to this prior work, however, and use a Bayesian model with latent binary masks to choose the relevant context vectors, which they infer with an amortized neural variational inference method. Several technical novelties are introduced to deal with the sparsity of relevant contexts and the difficulties with discrete latent variables and variably-sized element selection. 

They show that their method gives significant improvements in held-out pseudolikelihood on several exponential family embedding tasks, as well as small improvements in unsupervised discovery of movie genres. They also demonstrate qualitatively that the learned variational posterior over relevant contexts makes sensible predictions with an examination of chosen contexts on a dataset of Safeway food products.

The idea of the paper is a straightforward, intuitively appealing, clear improvement over the standard EFE model, and the technical challenges presented by inference and their solutions will be interesting to practitioners using large-scale variational models.

One interesting technical contribution is the architecture of the amortized inference network, which must deal with variable-sized contexts to predict a posterior over each latent Bernoulli masking variable, and solves the problem of variable-sized contexts using soft binning with Gaussian kernels.

Another is the use of posterior regularization to enforce a sparsity penalty on the selected variables. Presumably, because the Bernoulli priors on the mask variables are independent, they are insufficient to enforce the sort of ""spiky"" sparsity that we want in context selection, and they show significant improvements to held out likelihood by varying the posterior regularization on effectively the average number of selected contexts. Perhaps the authors would like to more explicitly comment on the trade-off between posterior regularization and prior tuning to obtain the desired sparsity.

Overall, while the evaluation in the paper is fairly limited, the paper presents a clear improvement to the Exponential Family Embeddings model with a sound probabilistic model and inference techniques that will be of interest to practitioners.

1. Ling et al. Not All Contexts Are Created Equal: Better Word Representations with Variable Attention. 2015
2. Liu et al. Part-of-Speech Relevance Weights for Learning Word Embeddings. 2016","Summary: The authors extend the framework of exponential family embeddings by Rudolph et. al. (2016). The framework of Rudolph et. al. (2016) starts by a probabilistic model over dataset. Each element in the dataset has a context which is pre-selected and the conditional probability of this element given the context is a distribution from the exponential family with hyperparameters \alpha, \rho where \rho contains the embeddings. Thus the likelihood of the hyperparameters is the product of these conditional probabilities over all elements in the dataset which can be maximized. The authors of this paper introduce an additional binary latent variables in the model, b, which indicate the inclusion of an element in the context. Due to this, we are required to maximize the marginal likelihood (not only likelihood) which requires a marginalization over all values of b. This is done by optimizing the evidence lower bound using an auxiliary posterior approximation q(b | stuff). Further, the authors introduce a neural net that amortizes the calculation of q: they introduce a neural net which maps from stuff to q's parameters, i.e. q(b | nn(stuff)). Weights of this neural net is found using stochastic gradient descent.

The explanations are clear and the experiments are convincing though I must say that haven't worked embedding models a lot. Also, I was a bit confused about lines 177-185: why do we need to optimize W and (\alpha, \rho, \pi) in two different steps and not jointly in the style of variational autoencoders.","This paper is an straightforward extension of exponential family embeddings (EFE), which employs context selection that uses a subset of the elements in the context for learning the embeddings. Variational method is applied to carry out the inference for the hidden binary selection variable b. The paper is generally interesting. The intuition is clear and the writing makes the paper easy to follow.

However, the posterior regularization seems to overkill the problem, which also introduces an extra beta hyperparameter. Model ablation on the inference network would be more interesting than the investigation of beta. Besides, the experiments are not convincing enough to me. Some other baselines rather than only EFE (for example deterministic gated selection) are welcome. 
Though this is meant to be in a neat probabilistic model framework, there are a lot of heuristics for regularizing the learning of embeddings which makes the model a bit too complex. More importantly, I would at least expect some downstream tasks applying the learned embeddings by CS-EFE. Log-likelihood seems to be not very good for evaluating embeddings.


"
Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction,"Kristofer Bouchard, Alejandro Bujan, Fred Roosta, Shashanka Ubaru, Mr. Prabhat, Antoine Snijders, Jian-Hua Mao, Edward Chang, Michael W. Mahoney, Sharmodeep Bhattacharya",https://proceedings.neurips.cc/paper/2017/hash/788d986905533aba051261497ecffcbb-Abstract.html,"The paper proposes a new model selection and estimation method for problems with underlying sparsity. The selection stage involves intersections of model support resulting from several bootstrap samples.  Estimation involves bagging estimate of best models corresponding to different regularization parameters.  Overall the model is both intuitively appealing, and shows good performance in important applications (eg. estimating complex phenotype from genes). Would recommend for publication.","This paper focuses on model selection and, to some extent, feature selection in large datasets with many features, of which only a small subset are assumed to be necessary for accurate prediction.  The authors propose a general method by which model selection is performed by way of feature compression performed by taking the intersection of a multiple regularization parameters in an ensemble method, and then model estimation by taking a union over multiple outputs.  (This is also the major contribution of the paper.)  A second contribution is found in the union operation in a model averaging step with a boosting/bagging flavor.  Overall, I found the paper's method section well written and the idea proposed to be complete.  The paper's experimental section was difficult to follow, but the results do seem to support the framework.  One major missing part of the paper is a reasonable discussion of using the framework beyond a Lasso base.   Are there reasons why this method would not work for classification?  Are there potential hitches to using this method with already-ensemble-based methods like random forests?  While there are many uses for the UoI with a Lasso base already, it was increase the general interest of the framework if UoI could be placed in the more general ML space.","This paper proposes a general framework for interpretable prediction. The authors consider a two-stage (intersection + union) approach to achieve the goal of guaranteeing both interpretability(sparsity) and accuracy. The authors applied the method to synthetic datasets and some biomedical datasets.

The paper is well written with good demonstrations and easy to understand. 

The main algorithm/framework is not well explained in the main context. 

Theorem 1 for UoI_Lasso in the appendix should be presented more rigorously by listing the assumptions and the exact formulas for constants C and C_1. 

The authors claimed the union of intersections as the major innovation. However, this approach lacks novelty since similar techniques are widely used in the data mining community such as different forms of filter + wrapper + embedded methods. The UoI can be viewed as a special case of forward selection + backward elimination.

The extensive experiments are convincing. But the authors should compare with existing methods with forward selection + backward elimination structures."
Good Semi-supervised Learning That Requires a Bad GAN,"Zihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Russ R. Salakhutdinov",https://proceedings.neurips.cc/paper/2017/hash/79514e888b8f2acacc68738d0cbb803e-Abstract.html,"This work extends and improves the performance of GAN based approaches to semi-supervised learning as explored in both ""Improved techniques for training gans"" (Salimans 2016) and ""Unsupervised and semi-supervised learning with categorical generative adversarial networks"" (Springenberg 2015). 

The paper introduces the notion of a complement generator which tries to sample from low-density areas of the data distribution (in feature space) and explores a variety of objective terms motivated/connected to this analysis. It is difficult to exactly match the motivated objective, due to various hard to estimate quantities like density and entropy, the paper uses a variety of approximations in their place.

In addition to an illustrative case study on synthetic data, the paper has a suite of experiments on standardized semi-supervised tests including ablation studies on the various terms proposed. The overall empirical results are a significant improvement over the Feature Matching criteria proposed in ""Improved techniques for training gans"".

Given the variety of objective terms suggested it is important and appreciated that the authors included ablation studies. It is unfortunate that they are not completely thorough, however. For instance, why does SVHN have a 5 different experiments but MNIST 2 and CIFAR-10 3? Why are the Approximate Entropy Maximization terms only tested on SVHN? How do they perform on CIFAR-10? Could the author(s) comment on why a fuller suite of comparisons was not completed?

The benefits of the various terms are not always consistent across datasets. The text mentions and discusses this briefly but a more thorough investigation by completing the ablation studies would be helpful for people wishing to extend/improve upon the ideas presented in this paper.

","In this paper, the authors proposed a novel semi-supervised learning algorithm based on GANs. It demonstrated that given the discriminator objective, good semi-supervised learning indeed requires a bad generator. They derive a novel formulation that substantially improves over feature matching GANs. Experiments demonstrate the state-of-the-art results on multiple benchmark datasets. The paper is interesting and well-written, which is important for the family of GANs algorithm. I have several concerns as follows:
1. As is demonstrate in W-GAN, the GANs is not easy to converge when we minimizing the KL divergence between the generator distribution and the target distribution. It would be nice if the authors could demonstrate whether the algorithms can be applied to more advanced GANs. 

2. As is well-known, it is not easy to generate the general samples based on GANs, such as the images with high resolutions. It is expected the author could demonstrate how to use the proposed models when sample generation is not easy. 

3. It would be nice if the authors could demonstrate how “bad” a generator should be for a semi-supervised classifier. 

4. It is interesting if the authors could demonstrate whether the human-specified labels can improve the performance of the generator, such that some satisfactory images can be generated. 
","After reading the rebuttal I changed my score to 7.  Overall it is an interesting paper with an interesting idea. Although the theoretical contributions are emphasized I find the empirical findings more appealing. The theory presented in the paper is not convincing (input versus feature, convexity etc). 

I think the link to classical semi-supervised learning and the cluster assumption should be emphasized, and the * low density assumption on the boundary* as explained in this paper :

Semi-Supervised Classification by Low Density Separation
Olivier Chapelle, Alexander Zien

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5826&rep=rep1&type=pdf
 
I am changing my review to 7, and I hope that the authors will put their contribution in the context of known work in semi-supervised learning , that the boundary of separation  should lie in the low density regions . This will put the paper better in context.

-----
This paper provides an analysis of how GAN helps in  semi supervised learning (in the “K+1” setting of [15]). The paper outlines some assumptions under which it is shown that a complement generator is needed to improve the accuracy of the supervised task at hand. Meaning that the generator needs to  target low densities areas in the input space. 

Using this idea an algorithm is given that combines that feature matching criterium , with a density estimation (using a pixel CNN ) under which the generator targets low density areas of the fixed estimated model (for a given threshold of log proba,). 
Other entropy regularizers are added to encourage diversity in the generator. Positive empirical results are reported.
 

Understanding GANs in the semi supervised setting and improving  it  is  an important problem, however the paper has many caveats:

- while the analysis is done in the feature space under a lot of assumptions, the method proposed is in the input space , which gives a big mismatch between the analysis and the proposed method . Convexity that is used all over the proofs is not any more valid.

-  the KL expression (line 238 ) is wrong:  the term assumed to be constant is not constant. It is equal to ‘-log(C) P(x sim p_g, p(x)<=epsilon)’, this term should be optimized as well.  Some other way to come up at the objective presented need to be developed. 
Maybe just motivating the minimization of the cross entropy term, and adding a cross entropy regularization?

- From the analysis and the  2D example  with uniform sampling off the manifold, the paper seems to suggest that the generator should supply samples outside the manifold, in a sense it reinforces the boundaries of the classifier by providing only negative samples. The truth is in between: the generator should not provides too strong samples (very realistic, easy to classify as a class ) , nor too weak samples that   are easily labeled as fake. it should be portion on the manifolds to reinforce the positive , and portions outside  to reinforce the negative. 
A more realistic setup may be probabilistic where the assumption are assumed to hold with probability 1- delta as off the manifold, and delta on the manifold, although corollary 1 is not proven and hard to understand how it could hold , it seems not realistic to me .
Balancing this delta seem to be crucial and seems inline with the analysis of https://arxiv.org/pdf/1705.08850.pdf 
 

- Ablation study:  Performance of the method  in section 6 for many values of epsilon values would illustrate the discussion above, and should be reported, also a justification of the 10 quantile should be provided.

-Overall using a density estimation (Pixel CNN) and entropic regularizer with another estimator seems a bit adhoc and not satisfying, wonder if the authors have other thoughts to avoid those estimators while encouraging a form of  'low density sampling’ while using only implicit modeling. or  hybrid implicit/ prescribed modeling while maintaining an end to end training?

Minor comments:

- the loss is L_supervised + L_unsupervised have you experimented with balancing the two terms with a penalty term lambda, how does this  balancing interact with the threshold epsilon?

- In lines '162 and 163'  This builds a connection with ... [2,25] which leverage ""the low density boundary assumption"". I don't understand this comment , can you elaborate more on what this assumption in laplacian regularization for instance?

- line 178 typo : 'the a  bounded' 

- corollary one : the proof is not provided. I don't understand how those claims are true uniformly over all X, and what does it means |G| goes to infinity, you may need  stronger assumptions here....
  "
Targeting EEG/LFP Synchrony with Neural Nets,"Yitong Li, michael Murias, samantha Major, geraldine Dawson, Kafui Dzirasa, Lawrence Carin, David E. Carlson",https://proceedings.neurips.cc/paper/2017/hash/7993e11204b215b27694b6f139e34ce8-Abstract.html,"Dear authors,

I am not familiar with the methods you present in this paper and the methodological part is beyond my understanding.
Therefore, I cannot say much about your paper apart from:

- The problem you pose is interesting. Extracting synchrony features is not an easy problem. However, I am not familiar with DNNs. I am not able to evaluate whether CNN or another type should be used.

- It is interesting that you checked for features used in other related fields like BCIs. You say that he recordings you use are too long. How long are they? For LFP you seem to use 5 seconds, this is not too different from BCI features.

- Could the GP adapter work separately from the NN? It would be interesting in general if it could help reducing the number of electrodes when the analyses should be done in pairs of sensors.

In general, the paper is very dense. Maybe this is because it is hard to follow for me.","The manuscript introduces the novel CNN named SyncNet, which was designed to detect synchrony/spectral coherence in brain electrophysiological signals. SyncNet uses parameterized convolutional filters. To overcome common issues in EEG (varying setup configuration, correlated channels, ...) a Gaussian Process Adaptor is used to transform EEG data into a pseudo input space. The design of SyncNet is chosen to allow interpretation of learned featruers. The method was applied to several publicly available datasets and its performance compared to state of the art algorithms. Results suggest that SyncNet yields competitive results.

The topic of the paper is timely and important and the presented approach very interesting. The paper is well organized and mostly clear, and the reviewer agrees with the authors reasoning. There are, however, some issues that need addressing:

- Are the computed SyncNet results significantly better than the results obtained with state of the art methods?

- One key feature of SynchNet is interpretability of features. Are the results summarized in Fig.4 meaningful?

- LFPs are only mentioned in the title and in lines 319-330. Please provide more details on LFP data and analysis or remove LFPs from the manuscript! Currently, LFP does not contribute to enhancing clarity of the paper.

","This contribution introduces SyncNet, a framework combining surrogate electrode positioning via GP adaptater and convolution network with contraint convolution parameters. SyncNet is evaluated on synthetic dataset and on real datasets, with both EEG and LFP data. It compares very favorably with state-of-the-art approaches and the GP adapter plays an important role. The results are interpretable, which is of prime importance."
Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes,"Anton Mallasto, Aasa Feragen",https://proceedings.neurips.cc/paper/2017/hash/7a006957be65e608e863301eb98e1808-Abstract.html,"The authors introduce a new framework for the incorporation of uncertainty in population analysis. They study populations of Gaussian Processes and define an approximation of the Wasserstein metric, showing that the barycenter exists and is unique. All the findings are well justified.

The proposed method is validated by experimental results. Two interesting applications are presented: (1) the processing of white-matter trajectories in the brain, from diffusion weighted imaging; (2) the analysis of climate evolution, from data collected by Russian metereologiacal stations. The authors are able to succesfully estimate the uncertainty in the evolution of the learned curves.
The experimental set up is well described and the computational cost of the method discussed.

In the final discussion, it is interesting to see how the authors connect their work to the recent findings in [37] and [38], envisioning the potential for future work. Could you add few more details about your vision?

Quality
This paper is pleasant to read. The results presented are well supported by theoretical analysis as well as by experimental results.

Clarity
The paper is clear, well organised and tidy. All the figures are well presented. The experimental setup is well described and the author state that they will release the code upon publication.

Originality
The theoretical framework introduced in the paper is novel.

Significance
The method presented in the paper can be defined significant since the authors provide interpretable results for critical application domains. Moreover, this method is general and can be applied to different problems.","This paper proposes an optimal transport-based metric on Gaussian processes.  The formula itself (equation 5) is a direct consequence of the 2-Wasserstein metric on Gaussian measures, which is well-studied.  The authors propose this formula and derive a fixed-point iteration for computing the barycenter of a set of Gaussian processes under this metric, whose convergence appears to be a consequence of [26] (a bit more detail in this part of the discussion would be appreciated).  The authors prove nice mathematical properties for the barycenter/transport problems in this metric in a sequence of theorems (continuity wrt operator norm, convergence of distance as basis size increases), and some experiments in diffusion tractography and climate change.

My main question after reading this document is whether there is a nice intuition for the GP transportation metric.  In particular, Wasserstein metrics are derived using a linear program (or convex optimization problem for measures, in the continuum case).  The paper submission jumps directly to a formula (5) without any optimization-based interpretation of optimal transport for GPs.  What quantity are we transporting?  How should I think about this metric specifically in the context of GPs?

The theoretical properties of this metric are nice and not too difficult to prove as consequences of properties of optimal transport between Gaussian measures.  The experiments are less convincing --- in what circumstances does using this geometrization of the set of GPs really help?  The discussion in this section just sort of indicates that the authors computed the Wasserstein barycenter/distance in GP space, but doesn't indicate strongly if/why it provided any benefit for the climate change or tractography applications over existing models.

Other comments:

* l.62:  Capitalize Procrustes

* It might be worth noting that optimal transport between Gaussian measures is also known as the ""Bures metric.""  See e.g. ""On the Geometry of Covariance Matrices"" (Ning, Jiang, Georgiou).

* Not sure if it's relevant, but it might be worth noting any relationship to the method in ""Gromov-Wasserstein Averaging of Kernel and Distance Matrices"" near eq (2).

* Is it possible to provide a transportation distance on GPs for other values of p?

* The notation in the equation after l. 130 caught me by surprise --- in particular, the xi's are input to the problem rather than optimization variables so I might recommend moving the 0 < xi_i and sum_i xi_i = 1 to be inline with the following paragraph.  Also note that there's a typo under the inf.

* What do you mean by ""indexed"" on l.114?  

* l.178:  GDs

* Please expand section 4 to include more detailed **interpretation** of the results rather than just telling us how they were generated.  Was the new distance on GPs worth it?  What improved?  This wasn't clear to me from the text.

* Note for future submissions that section 6 should be omitted for anonymity...

* Several typos in the references (capitalization, missing info in BibTeX) should be fixed.","The paper extends the Wasserstein (earth-mover's) metric from
finite-dimensional Gaussians to GPs, defining a geometry on spaces of
GPs and in particular the notion of a barycenter, or 'average' GP. It
establishes that 2-Wasserstein distances, and barycenters, between GPs
can be computed as the limit of equivalent computations on finite
Gaussians. These results are applied to analysis of uncertain
white-matter tract data and annual temperature curves from multiple
stations.

The writing is serviceable and relevant background is presented
clearly. I found no errors in the results themselves, though did not
check all of them thoroughly. My main concerns are with motivation and
significance. I agree that it is in some sense natural to extend the
Wasserstein geometry on Gaussians to GPs, and it's reassuring to see
that this can be done and (as I understand the results of the paper)
basically nothing surprising happens -- this is definitely a
contribution. But there is not much motivation for why this is a
useful thing to do. The experiments in the paper are (as is standard 
for theory papers) fairly contrived, and the conclusions they arrive 
at (the earth is getting warmer) could likely be gained by many other
forms of analysis --- what makes the Wasserstein machinery
particularly meaningful here?

For example: from a hierarchical Bayesian perspective it would seem
reasonable to model the climate data using a latent, unobserved global
temperature curve f_global ~ GP(m_global, k_global), and then assume
that each station i observes that curve with additional local GP noise,
f_i ~ GP(f_global + mi, ki). Then the latent global temperature curve
could be analytically computed through simple Gaussian conditioning,
would be interpretable in terms of this simple generative model, and
would (correctly) become more certain as we gathered evidence from
additional stations. By contrast I don't understand how to
statistically interpret the barycenter of independently estimated GPs,
or what meaning to assign to its uncertainty (am I correct in
understanding that uncertainty would *not* decrease given additional
observations?  what are the circumstances in which this is desirable
behavior?).

Perhaps other reviewers with a more theoretical bent will find this
work exciting. But in the absence of any really surprising results or
novel theoretical tools, I would like to see evidence that the machinery 
it builds is well-motivated and has real advantages for practical data 
analysis -- can it reach conclusions more efficiently or uncover structure 
more effectively than existing techniques? As currently written the paper
doesn't cross my personal NIPS significance bar. 

misc notes:

- Fig 1: the 'naive mean' column claims to average the pointwise
standard deviations, but the stddevs shown are smaller than *any* of
the example curves: how can this be true? Also why is averaging 
pointwise stddevs, rather than (co)variances or precisions, the relevant
comparison? 

- I wonder if the Wasserstein metric could be useful for variational
fitting of sparse/approximate GP models? Currently this is mostly done
by minimizing KL divergence (as in Matthews et al. 2015,
https://arxiv.org/abs/1504.07027) but one could imagine minimizing the
Wasserstein distance between a target GP and a fast approximating
GP. If that turned out to yield a practical method with statistical or
computational advantages over KL divergence for fitting large-scale
GPs, that'd be a noteworthy contribution.

130-131: missing close-paren in inf_\mu\inP2 (H
152: 'whe' -> 'we'
155: 'This set is a convex'
170: why is 'Covariance' capitalized?
178: 'GDss' -> 'GDs'?"
Online Dynamic Programming,"Holakou Rahmanian, Manfred K. K. Warmuth",https://proceedings.neurips.cc/paper/2017/hash/7a6a74cbe87bc60030a4bd041dd47b78-Abstract.html,"The paper extends the celebrated multiplicative update algorithm to combinatorial structured problems whose offline versions can be solved some kind of DP algorithms. More precisely, the authors extend the Expanded Hedge and the Component Hedge algorithms to k-multipaths and applications thereof. The extension is well conducted by inspecting the various steps of these algorithms. The extension is made so that similar type of regret guarantees are achieved.

The technical contribution of the paper lies in the simplification of the characterization of the convex hull of the decision space due to the underlying DP structure. 

Except for the introduction, the paper is well presented and easy to follow. The authors describe more and more involved models and algorithms, which help a lot understanding the contributions. The introduction focuses on BST, which is only one application of the proposed framework, and confusing actually …

The main concerns about the paper is (i) the lack of reference to existing work on online Markov Decision Processes (arguably one of the main applications of the proposed framework), and (ii) the lack of numerical experiments.  (i) For example, it seems that similar setting as the one described in Section 4 has been investigated in Even-Dar et al. in “Online markov decision processes”, Maths of OR, 2009. How do your results compare to those of the latter paper? There have been plenty of papers on this topic since 2009, and some of them with full information feedback as that considered here. 

Overall the paper is interesting and provides a nice way of generalizing Hedge algorithms to combinatorial problems whose offline version can be solved using DP. However, this does not seem to be a new topic, and the lack of comparison with existing work is regrettable.
 
","The aim of this paper is to extend results in online combinatorial optimization to new combinatorial objects such as $k$-multipaths and binary search trees. Specifically, the authors are interested in extending Expanded-Hedge (EH) and Component Hedge (CH) to online combinatorial games, for which the offline problem can be solved in a bottom-up way using Dynamic Programming. 

Though the results about online optimization with $k$-multipaths are interesting, the overall paper could benefit from more polishing: some definitions and notations are not clearly defined, and some assertions look incorrect. Namely:

In Section 4, the set (of subsets) $\mathcal H_v$ is not clearly defined, so it is very difficult for the reader to capture the family of DP tasks defined according to the recurrence relation (3). In this section, some concrete examples of online optimization problems characterized by this DP family would be helpful.

In Section 5, the notion of Binary Search Tree should be clearly defined. Notably, if the authors are interested by the set of all rooted binary trees with $n + 1$ leaves, the cardinality of this set is indeed the Catalan number of order $n$ (as indicated in Line 268). But, contrary to what is claimed in Lines 250-251, there is a well-known characterization of the polytope specified by the convex hull of all rooted binary trees of order $n$: this is the $n$th dimensional associahedron.  Since this polytope is integral, we can use (at least theoretically) the Component Hedge algorithm for projecting points onto this polytope, and decomposing them (by Caratheodory's theorem) into convex combinations of binary trees.  

Section 5.2 is of particular importance, as it explains how the online BST problem can be solved using previous results obtained for $k$-multipaths. But the translation of this problem into an online $2$-multipath learning task is not clear at all. To this very point, it is important to explain in unambiguous terms how we can learn BSTs using the CH (or EH) technique used for $k$-multipaths. According to the sets $V$, $\Omega$, and $F_{ij}$ defined in Lines 265-266, are we guaranteed that (i) every $k$-multipath of the induced graph encodes a rooted binary tree with $n+1$ leaves (correctness), and (ii) every possible rooted binary tree with $n+1$ leaves can be encoded by a $k$-multipath of this induced graph? 

","This work proposes an online dynamic programming method for a number of problems which involve an exponential number of combinatorial objects. The idea is to reduce the subproblems in dynamic programming algorithms to vertices in k-regular DAG, so that a concise representation of the combinatorial objects can be constructed. The paper is focused on the optimal binary search tree problem but the authors also show how other problems can be addressed by this new technique in the appendix.

Overall, I think this is a solid work. A question is, can the authors explicitly discuss what kinds of problems on which this algorithm can be applied? Some notations are not well explained, e.g. some notations in Eq.(3). There are some language errors in the paper, such as “can also visited” in line 119 and a plural issue in line 211.
"
Neural Discrete Representation Learning,"Aaron van den Oord, Oriol Vinyals, koray kavukcuoglu",https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html,"Abstract: second sentence doesn’t follow from first one - did authors mean to learn “discrete” instead of “useful” representations?

Aside from that first sentence, this is one of the most intriguing and exciting NIPS submissions I’ve ever reviewed. My only question is why this simple-looking approach works so well, when the authors allude to the difficulty others have had in training discrete VAEs. I don’t have direct experience training this sort of model. Is the non-linearity associated with nearest-neighbor lookup so important, as opposed to e.g. greatest vector inner product that would be more typical of neural networks? Also, some discussion of the training time for each data set would have been interesting.
","
Summary-
This paper describes a VAE model in which the latent space consists of
discrete-valued (multinomial) random variables. VAEs with multinomial latent
variables have been proposed before. This model differs in the way it computes
which state to pick and how it sends gradients back. Previous work has used
sampled softmaxes, and other softmax-based ideas (for example, trying to start
with something more continuous and anneal to hard). In this work, a VQ approach
is used, where a hard decision is made to pick the closest embedding space
vector. The gradients are sent into both into the embedding space and the
encoder.

Strengths
- The model is tested on many different datasets including images, audio, and video datasets.
- The model gets good results which are comparable to those obtained with continuous latent variables.
- The paper is well-written and easy to follow.

Weaknesses-
- Experimental comparisons with other discrete latent variable VAE approaches
are not reported. It is mentioned that the proposed model is the first to
achieve comparable performance with continuous latent variable models, but it
would be useful to provide a quantitative assessment as well.
- The motivation for having discrete latent variables is not entirely
convincing. Even if the input space is discrete, it is not obvious why the
latent space should be. For example, the paper argues that ""Language is
inherently discrete, similarly speech is typically represented as a sequence of
symbols"" and that this makes discrete representations a natural fit for such
domains. However, the latent semantic representation of words or speech could
still be continuous, which would make it possible to capture subtle inflections
in meaning or tone which might otherwise be lost in a discretized or quantized
representation. The applications to compression are definitely more convincing.

Overall-
The use of VQ to work with discrete latent variables in VAEs is a novel
contribution. The model is shown to work well on a multiple domains and
datasets. While the qualitative results are convincing, the paper can be made
more complete by including some more comparisons, especially with other ways of
producing discrete latent representations.
      ","The paper introduces a way of doing discrete representation via some discrete transformation in the latent space of a commonly used VAE, and use straight-through estimator to backprop the gradients for learning. 

The main issue with the paper is its clarity; many notations are used without explanation or used abusively, hence I cannot understand their main equations:
In equation (1), could you explain what do you mean by q(z|x) is defined as one hot? Is z a scalar or a one-hot vector? How is it equal to k? 
In equation (3), what is sg (you never defined it)?
Because of the above confusions, I cannot understand what author is proposing.

Another problem is the paper has no comparison with any other method in the experimental section. Therefore it’s not really clear whether the technique has any improvement.

I will potentially change my score if the author can clearly explain what they’re doing in the rebuttal.
"
Probabilistic Rule Realization and Selection,"Haizi Yu, Tianxi Li, Lav R. Varshney",https://proceedings.neurips.cc/paper/2017/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html,"This paper describes a method for selecting and weighting ""rules"" (equivalence class partitions of a discrete/binary feature space), and its application to music composition for 4-part chorales. 
Given a set of candidate rules, the proposed method learns a selection weighting to determine which rules to include, and a weighting over the selected rules to maximize consistency.

Both problems are modeled as least-squares regression over the probability simplex (with differing group-sparsity penalties for the different variables), and the authors provide some efficiency improvements to keep the problem tractable for their application.

Overall, the paper is well-written, and the design choices are well-motivated.  That said, it's not obvious that NIPS is an appropriate venue for this work, for a couple of reasons:

1. There is only one application considered here, which is fairly niche, and maybe not of broad interest to this community.

2. The experimental evaluation focuses entirely on the properties of the solution vectors (e.g., solution paths as a function of regularization factors), but there is no empirical validation on the motivating application, so it's hard to view this as an ""applications paper"".

3. The algorithmic techniques used here are all standard and well-known (though their specific combination might be novel).  The various solver optimizations (group de-overlap, column reduction) might be of broader interest, but it's not clear that they have broader applications than considered here.  It doesn't seem likely that NIPS readers will take much away on the algorithm design front.","The paper tackles rules realisation and selection, given a specific set of given rules that derives from an abstraction process, in the specific case of music compositional rules. The novelty of the paper resides in a new bi-convex optimisation model for rules selection and rule realisation. The challenge posed by rules realisation is due to the high dimensionality of the sample space. The challenge posed by rule selection appears in the case when all rules cannot be simultaneously realised, or not in their entirety. In this case one has to choose which rules to respect and in which amount of components. The strengths of the paper are:
- the goal and challenge of the model are very clearly stated,
- the paper is extremely well written with all notations defined and intuitive explanations of the theoretical results. I especially liked the interpretation of lines 285 - 296. 
- experimental results support the claim of the proposed innovations, as the model (i) behaves as expected when varying the hyper parameters (ii) reduces computation time.

Questions / comments :
- line 118 : can the author explain why the solution is expected to be ""unique"" on p thanks to group penalty ? If there is no group penalty (\lambda_p = 0) I would expect the solution to be unique, which is also what the authors state in line 131 ""in setting \lambda_p sufficiently small, we secure a unique solution for the rule realisation part"". The two statements seem unconsistent.
- line 148 : Theorem 1, claim (2) : am I right to understand that all rules are selected because for all rules the L2 norm converges to a non-zero value ?
- Group De-overlap : am I right to understand that this is not an additional dimensionality reduction but only an interpretation of the grouping presented in Section (3), while screening is an additional step to further reduce dimensionality ?
- line 304 : what do the authors mean by ""failures in group selection"" ? Does it mean that no rule is entirely selected ?
- line 305 : am I right to understand that a simple lasso on the number of rules is impossible because it makes no sense to weight the rules as the selection process is a 0/1 process ?
- I am not familiar with the literature in automatic music composition, and I understand that there is no ""ground-truth"" solution as creativity can be a desired behaviour. I like the fact that the author mention the gain in computational efficiency.  If that exists / is possible in the time of rebuttal I would like to see a baseline model to compare the behaviour in terms of selection / realisation of the proposed model.
- Can the author think of any other possible domain to apply their method ?","The authors present a solution to the problem of jointly selecting and realizing rules which they formulate as a biconvex optimization problem. The optimization exploits group structures in the discrete input space p and rule selection weights w. A group ridge penalty on the L1 norm is used on p to encourage diversity and a group elastic net is used on w with a hyperparameter alpha that controls the effect of the L1 and L2 penalties. Since the objective is biconvex (i.e) freezing p makes the objective convex in w and vice-versa, solvers for p and w are fairly straightforward. The authors also perform further dimensionality reduction by identifying and removing zero components in q during the optimization process. The p-solver exploits dimensionality reduction as a result of group structures. The rule selection (w-problem) is solved using ADMM. Experiments are carried out using an artificial rule set and data from [11]. 

Although paper presents a well motivated and scalable solution to the problem, the model parameterization is somewhat simple and uses fairly straightforward extensions of existing optimization techniques to learn the model parameters. The introduction is well written and the need for having interpretable realization models is well motivated. The simple linear parameterization of the model, while being able to offer interpretability, limits its expressiveness and applicability to more complicated application domains.

I got the impression that the ideas expressed in Section 3 (until the Model Interpretation subsection) were well presented but at the cost of being a little too verbose, specifically the intuitions behind the hyperparameters lambda_p, lambda_w and alpha. I didn’t quite understand the motivation for the group elastic net formulation for rule selection (w), specifically about how it provides “more control over the number of selections”. Would it be interesting to consider sparsity within w groups using something like a sparse group elastic net? - https://arxiv.org/abs/1001.0736.

While this is largely a theoretical paper, I found the experiments (especially 5.2) somewhat lacking more discussion. I would have liked to see some more qualitative analysis about realization process since a major motivation for this paper is interpretability.

In conclusion, I think that while the paper provides and clean, scalable and easy to understand solution to an important problem, it is somewhat lacking in novelty in that it uses a fairly straightforward model parameterization and optimization solution without strong experimental backing.
"
A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning,"Marco Fraccaro, Simon Kamronn, Ulrich Paquet, Ole Winther",https://proceedings.neurips.cc/paper/2017/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html,"This paper proposes to tackle disentanglement of videos into an object's representation and its dynamics, using a combination of Variational AutoEncoders and Kalman filters. The advantages of the method are:
- The use of Linear Gaussian State Space Models allows for the exact inference of the posterior distribution on the spaces z
- Encoding the observed video with VAE allows reasoning about the object's dynamics in low dimensional space
- Finally, by making the LGSSM parameters non stationary, the state space transition can remain linear without loss of modelling ability

The strengths of the paper are:
- The paper is extremely clearly written, among the most well written the reviewer has read. 
- The problem to tackle and its challenge are clearly set
- The proposed model gives better performance than compared models

The weaknesses of the paper are mainly on the experiments:
- While not familiar with the compared models DMM and DVBF in details, the reviewer understood from the paper their differences with KVAE. However, the reviewer would appreciate a little bit more detailed presentation of the compared models. Specifically, the KVAE is simpler as the state space transition are linear, but it requires the computation of the time-dependant LGSSM parameters \gamma. Can the authors comment on the computation requirements of the 3 methods compared in Table 1 ?
- Why the authors did not test DMM and DVBF on the task of imputing missing data ?","The paper presents a time-series model for high dimensional data by combining variational auto-encoder (VAE) with linear Gaussian state space model (LGSSM). The proposed model takes the latent repressentation from VAE as the output of LGSSM. The exact inference of linear Gaussian state space model via Kalman smoothing enables efficient and accurate variational inference for the overall model. To extend the temporal dynamics beyond linear dependency, the authors use a LSTM to parameterize the matrices in LGSSM. The performance of the proposed model is evaluated through bouncing ball and Pendulum experiments.

LGSSM with dynamic parameterization is a nice trick to learn complex temporal dynamics through a simple probabilistic model, but has been exploited in the literature, e.g., [M. Seeger, D. Salinas, V. Flunkert NIPS 2016]. The proposed method does not really disentangle the object representation from the temporal dynamic, as the model is unlikely to infer the correct dynamics of an unseen object. The proposed method only models the temporal dynamic on top of the low dimensional latent representation from VAE.

The proposed method is evaluted on some simple examples, where the object representation is simple (mostly single object with low resolution image) and the dynamic is also simple (bouncing of a ball or Pendulum). It would be more convencing to show experiments with either complex data representation or complex dynamics (e.g., human motion). In Pendulum experiment, the proposed method is compared with other two methods in terms of ELBO. However, it is questionable how to compare ELBO from different models (e.g., the numbers of parameters are probably different.).","This paper seems like a natural combination of VAEs to model image generation and Kalman filters for modeling dynamics. It seems to fill a missing area in the space of models.

Comments:
The notation $p_{\theta\gamma}$ on L101 is confusing
It would be more clear to include \theta, \gamma, and \phi in Figure 1, since otherwise it is easy to forget which is which
It would be nice if Table 1 also included some confidence intervals

Questions:
How necessary is the dynamics parameter network in Sec 3.3? What happens if the nonlinearity is handled using traditional EKF or UKF techniques?

Latex/Formatting:
Appendix, Table, Figure should be capitalized
see citep vs citet
see \times for L204"
Stabilizing Training of Generative Adversarial Networks through Regularization,"Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann",https://proceedings.neurips.cc/paper/2017/hash/7bccfde7714a1ebadf06c5f4cea752c1-Abstract.html,"In this paper, the authors have introduced a regularization scheme to train generative models with GANs. They added a penalty on the weighted gradient norm to address the severe failure modes caused by dimensional misspecification between the true and model distributions. In general, this paper is well written and easy to follow. 
(1) The implementation details of the proposed algorithm are missing. The authors should discuss the implementation details in the main paper. Also, please discuss the trick while training the proposed model. GAN is really hard to train and also difficult to reproduce the results reported by the authors. It will be easier if the authors can include the training tricks in their paper.
(2) The training time is not discussed in this paper. There is always a balance between the performance gain and the training time.
(3) The experimental part is the weakest part of this paper. The authors may want to include more exciting experimental results in the main paper.
I would like to accept this paper if the authors can address the above comments seriously.","This paper proposed to stabilize the training of GAN using proposed gradient-norm regularizer.
This regularization is designed for conventional GAN, or more general f-GAN proposed last year.
The idea is interesting but the justification is a little bit coarse.

1. Eq. (15) is only correct for the maximizer \psi^{\star}. However, the regularizer defined in (21) is for arbitrary \psi, which contradicts this assumption.

2. It seems that increasing the weighting of the regularization term, i.e., \gamma does not affect much the training in Fig. 2.
The authors claim that 'the results clearly demonstrate the robustness of the regularizer w.r.t. the various regularization bandwidths'.
This is a little bit strange and it will be necessary to show how this regularization can affect the training. 

3. For the comparison in the experiments, it is not clear how the explicit noise approach was done. What is the power of the noise? How is the batch-size-to-noise-ratios defined? What's relation between it and the conventional SNR?

4. The results in Section 4.3 make no much sense. From the reported table, the diagonal elements are all quite close to one, which can hardly justifies the advantage of regularized GAN. 

5. It will be convincing to compare the proposed method with some other stabilizing training techniques, like Salimans et al 2016 or WGAN technique.

","In order to solve the problem of requiring careful choice of architecture and parameters in Generative Adversarial Networks (GANs) based deep generative models.
The author proposed a new regularization approach which can sovle the problem cause by the dimensional mismatch between the model distribution and the true distribution.
By analysis noise covolution, conboblved discriminants and the efficient gradient norm-based regularization the author propose a Gradient-Norm Regularizer
for f-GAN. The experiment on MNIST and CIFAR-10 also should that the proposed regularizer is useful.
This work is very interesting and enlightening which can help people to build more stable GAN.
"
Training Deep Networks without Learning Rates Through Coin Betting,"Francesco Orabona, Tatiana Tommasi",https://proceedings.neurips.cc/paper/2017/hash/7c82fab8c8f89124e2ce92984e04fb40-Abstract.html,"This paper extends work by Orabona and Pal on coin-flipping for optimal learning rate dynamics.

I like the idea of a betting framework to tune learning rates over the course of training. My comments are as follows:

1) The abstract sounds as if the main contribution is the idea of coin-flipping for automatic learning rate tuning. However, this idea was introduced before as mentioned in the main text. What I am missing is a clear description (and quantitative demonstration) of the advantages of the proposed algorithm compared to previous work. This should also be more clearly stated in the abstract.

2) There is no quantitative comparison with the algorithm proposed in Orabona and Pal.

3) Do Orabona and Pal need the exact Lipschitz constant or just some bound? In the latter case their algorithm would also be applicable to deep neural networks.

4) I did not really get the intuition behind the coin flipping. In section 4 you introduce a very simple example |x - 10|. But here we only ever evaluate F(w_t), so we are never making a real step? In a learning scenario there would be an x_t that tracks the current state, and then w_t would be the learning rate? I am fairly confused, and I think it would be helpful to write one paragraph in which you walk through the first five steps of the algorithm.

5) Why is the performance of Adam so bumpy in Figure 2/3? I have a lot of experience with this optimiser and I have never seen such curves (unless the learning rates are chosen close to the upper limit). I’d suggest some tuning of the learning rates of all optimisation methods, otherwise the results are very hard to compare.","Summary:
This paper is based on the notion (established in existing works) that subgradient descent can be interpreted as betting on a coin. It generalizes existing results to data-dependent bets and, consequently, data-dependent convergence bounds that improve upon the existing ones. The algorithm is then adapted for the training of neural networks and evaluated on this task experimentally.

Quality:
The derivations are mathematically sound. I verified the proof of Theorem 1. The changes made to adapt COCOB for the training of neural networks (Algo 1 --> Algo 2) are sensible and intuitive.
However, the experimental evaluation of COCOB as a deep learning optimizer is insufficient in my opinion. The lack of theoretical guarantees for DL optimization makes a thorough experimental evaluation on a range of realistic problems indispensable. Small CNNs/MLPs on MNIST/CIFAR-10 are weak test problems. Furthermore, I don't agree with the premise of the comparison to other methods. It is stated that ""default hyperparameters"" are used for the competing methods. I don't think that something like a default learning rate really exists for Adam/RMSprop/etc. Even though COCOB does not require a learning rate, I think it is a somewhat unfair assessment to just compare against Adam/RMSprop/etc with a single learning rate (that might be considered ""default"" by some arbitrary standards). It would be much more convincing to compare against multiple learning rates to judge the performance of the learning-rate-free COCOB algorithm in light of the learning rate sensitivity of other methods.

Clarity:
The paper is well-written. All relevant concepts/definitions are properly introduced. Section 4 is very helpful in that it gently introduces the reader to the interpretation of subgradient descent as coin betting.
To clarify the contribution of this paper, the authors could do a better job in explaining how their algorithm (and its convergence guarantee) is different from that in [Orabona & Pal, 2016].

Originality:
This paper adopts the ""coin-betting"" paradigm of [Orabona & Pal, 2016] but presents improved theoretical results. At the same time, a variant of their algorithm is proposed as a practical method for deep learning optimization, which is a very novel, original approach compared to existing methods like Adam/RMSpop/AdaDelta/etc.

Significance:
This paper proposes a hyperparameter-free optimization algorithm for deep learning, which is a highly significant research area with a big potential impact on many deep learning researchers and practitioners.

Final Evaluation:
As explained above, I have my reservations about the experimental evaluation. Given the sizeable theoretical contribution of the paper, the novelty of the approach (in the deep learning context), and the big potential impact, I argue for acceptance in spite of that.

Proposed Improvements:
Add a thorough experimental evaluation on realistic deep learning optimization problems. For the competing methods, report results for various different learning rates.

Minor Comments / Typos:
- Typos
  * Line 32: should be ""at least one of the hyperparameters of deep learning models""
  * Line 167: should be ""a better understanding of the differences""
- Line 103: F(x) is not differentiable at x=10
- Appendix Eq. (3), second line, should be g_{t, i} instead of g_{t, 1}
- Appendix Eq. (5): there a some subscripts i missing in the g's","This paper presents an optimization strategy using coin betting, and a variant which works well for training neural networks.  The optimizer is tested and compared to various stochastic optimization routines on some simple problems.

I like this paper because it's an unusual optimization method which surprisingly seems to work reasonably well.  It also has fewer tunable parameters than existing stochastic optimizers, which is nice.  However, I'm giving it a marginal accept for the following reasons:
- The experiments aren't very convincing.  They're training some very simple models on some very small datasets, and results in this regime do not necessarily translate over to ""real"" problems/models.  Further, success on these problems mostly revolves around preventing overfitting.  I'd actually suggest additional experiments in _both_ directions - additional simple unit test-like experiments (like the |x - 10| experiment, but more involved - see e.g. the ""unit tests for stochastic optimization"" paper), and experiments on realistically large models on realistically large datasets.
- I don't 100% buy the ""without learning rates"" premise, for the following reason:  If you divide the right-most term of line 10 of Algorithm 2 by L_{t, i}, then the denominator simplifies to max(G_{t, i}/L_{t, i} + 1, \alpha).  So, provided that \alpha is bigger than G_{t, i}/L_{t, i}, the updates effectively are getting scaled by \alpha.  In general I would expect G_{t, i}/L_{t, i} to be smaller than \alpha = 100 particularly at the beginning of training, and as a result I'd expect the setting of \alpha to have a considerable effect on training.  Of course, if \alpha = 100 well in any experiment of interest, we can ignore it, but arguably the default learning rate setting of Adam works reasonably well in most problems of interest too -but of course, we wouldn't call it  ""without learning rates"" method.
- Overall, this is an unusual and unconventional idea (which is great!), but it is frankly not presented in a clear way.  I do not get a lot of intuition from the paper about _why_ this works, how it is similar/different from SGD, how the different quantities being updated (G, L, reward, \theta, w, etc) evolve over the course of a typical training run, etc. despite spending considerable time with the paper.  I would suggest adding an additional section to the appendix, or something, which gives a lot more intuition about how and why this actually works.

More specific comments:

- Very high level: ""backprop without learning rates"" is a strange phrase.  Backprop has no learning rate.  It's an efficient algorithm for finding the gradients with respect to all parameters in your model with respect to the output.  SGD has a learning rate.  SGD is often used for training neural networks.  In neural networks backprop is often used for finding the gradients necessary for SGD.  But you don't need a learning rate to backprop; they are disjoint concepts.

- In your introduction, it might be worth mentioning results that show that the learning rate is one of the most ""important"" hyperparameters, in the sense that if it's set wrong the model may not work at all, so its correct setting can have a strong effect on the outcome.  

- The relation of the second and third inequality in the proof after line 112 took me about 15 minutes to figure out/verify.  It would be helpful if you broke this into a few steps.

- Algorithm 1 is missing some input; e.g. it does not take ""T"" or ""F"" as input.

- Calling \beta_{t, i} the ""fraction to bet"" is odd because it can be negative, e.g. if the gradients are consistently negative then \theta_{t, i} will be negative and 2\sigma(...) - 1 will be close to -1.  So you are allowed to bet a negative amount?  Further, it seems that in general 2\theta_{t, i} can be substantially smaller than G_{t, i} + L_i

- I think you have redefined w_t.  When defining coin betting you use w_t to refer to the bet at round t.  In COCOB w_t are the model parameters, and the bet at round t is (I think) \beta_t, i (L_i + Reward_t, i).

- I think most readers will be most familiar with (S)GD as an optimization workhorse.  Further presentation of COCOB vs. variants of SGD would be helpful, e.g. drawing specific analogies between each step of COCOB and each step of some SGD-based optimizer.  Or, perhaps showing the behavior/updates of COCOB vs. SGD on additional simple examples.

- ""Rather, it is big when we are far from the optimum and small when close to it.""  Not necessarily - for example, in your |x - 10| example, at the beginning you are quite far from the optimum but \sum_i g_i^2 is quite small.  Really the only thing you can say about this term is that it grows with iteration t, I think.

- In Algorithm 2, the loop is just ""repeat"", so there is no variable T defined anywhere.  So you can't ""Return w_T"".  I think you mean ""return w_t"".  You also never increment, or initialize, t.

- Setting L_{t, i} to the running max of the gradients seems like a bad idea in practice with neural networks (particularly for RNNs) because if at some iteration gradients explode in a transient manner (a common occurrence), then for all subsequent iterations the update will be very small due to the quadratic L_{t, i} term in the denominator of line 10 of algorithm 2.  It seems like you would want to set an explicit limit as to the values of |g_{t, i}| you consider, e.g. setting L_{t, i} <- max(L_{t - 1, i}, min(|g_{t, i}|, 10)) or something.

- What version of MNIST are you using that has 55k training samples?  It technically has 60k training images, typically split into 50k for training and 10k for validation.

- I would suggest using an off-the-shelf classifier for your MNIST experiments too, since you are missing some experiment details (how were the weights initialized, etc).

- You say your convolutional kernels are of shape 5 x 5 x 3 and your pooling is 3 x 3 x 3, I think you mean 5 x 5 and 3 x 3 respectively.

- What learning rates did you use for the different optimizers and did you do any kind of search over learning rate values?  This is absolutely critical!"
Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis,"Jian Zhao, Lin Xiong, Panasonic Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Panasonic Sugiri Pranata, Panasonic Shengmei Shen, Shuicheng Yan, Jiashi Feng",https://proceedings.neurips.cc/paper/2017/hash/7cbbc409ec990f19c78c75bd1e06f215-Abstract.html,"This work uses GANs to generate synthetic data to use for supervised training of facial recognition systems. More specifically, they use an image-to-image GAN to improve the  quality of faces generated by a face simulator. The simulator is able to produce a wider range of face poses for a given face, and the GAN is able to refine the simulators output such that it is more closely aligned with the true distribution of faces (i.e. improve the realism of the generated face) while maintaining the facial identity and pose the simulator outputted. They show that by fine tuning a facial recognition system on this additional synthetic data they are able to improve performance and outperform previous state of the art methods. 

Pros:
- This method is simple, apparently effective and is a nice use of GANs for a practical task. The paper is clearly written

Cons:
- My main concern with this paper is regarding the way in which the method is presented. The authors term their approach ""Dual Agent GANS"" and seem to claim a novel GAN architecture. However, it is not clear to me what aspect of their GAN is particularly new. The ""dual agent""aspect of their GAN comes from the fact that they have a standard adversarial term (in their case the BE-GAN formulation) plus a cross entropy  term to ensure the facial identity is preserved. But previous work (e.g. InfoGAN, Auxiliary classifier GAN) have both also utilized a combination of ""heads"". So it seems odd to me that the authors are pushing this work as a new GAN architecture/method. I realize it's very trendy these days to come up with a slightly new GAN architecture and give it a new cool name, but this obfuscates the contributions. I think this is an interesting paper from perspective of using GANs in a data augmentation pipeline (and certainly their particular formulation is tailored to the task at hand) but I do not like that the authors appear to be claiming a new GAN method. 
- Since I think the main contribution of this paper is a data augmentation technique for facial recognition systems, it'd be good to see > 1 dataset explored. 

Some additional comments/questions:
- In eq. 8, do you mean to have a minus sign in the L_G term?
- What was the performance of the network you trained before fine tuning? i.e. how much improvement comes from this technique vs. different/better architectures/hyper-parameters/etc. compared to other methods","
	  
This paper presents a method for augmenting natural face data by 3D synthesis which does not suffer from overfitting on artifacts. The approach uses a GAN network to filter synthesized images so as to automatically remove artifacts. The paper shows that the approach provides a significant boost over a state-of-the-art model on the IJB 'faces in the wild' dataset: reducing the errors by about 25%. 
The idea of augmenting natural images using 3D models is not new. However, real gains over state-of-the-art performance have not materialized due to the models overfitting on the artifacts of the 3D synthesis process. The authors prove that argument by showing that adding unfiltered augmented data to the baseline model actually degrades performance. I believe this paper shows a promising approach to solve this issue that I have not seen elsewhere so far. 
The GAN filter uses an original dual-agent discriminator trained with a loss combining the Wasserstein distance with boundary equilibrium regularization and an identity preserving loss.
The paper is written clearly, the math is well laid out and the English is fine. I think it makes a clear contribution to the field and should be accepted.
	  
	  
	  ","This paper proposed a architecture called DA-GAN, where the dual agents focus on discriminating the realism of synthetic profile face images and perceiving the identity information.

This work is technically sound and the novelty of this work lies in two points. Firstly, the simulator is designed for realism refinement in DA-GAN inputs. Secondly, the dual agents are combined for embedding data distribution and domain prior of faces. Unfortunately, the overall framework is over-reliant on BEGAN, which leads to much room for further improvement. I have few reasons to believe that most of performance improvement comes from the simulator's face ROI extraction and face landmark detection processing. Correspondingly, the adversarial training part focuses more on integrating identity consistency loss on standard generative authenticity loss by the training method of BEGAN. I am highly recommended the authors conduct experiments to compare the impact of each part whether exist or not. In this way, the advantages of this method can be better identified by the reviewers.

The article structure is clear, writing is good and no grammatical errors are found. This method gets good performance improvement in both comparable datasets. However, the whole paper still gives me a faint feeling that the core ideas are not very original and not very solid, so it is safe to say that this paper can but not to a large extent promote more innovative ideas."
Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation,"Christian Borgs, Jennifer Chayes, Christina E. Lee, Devavrat Shah",https://proceedings.neurips.cc/paper/2017/hash/7cc234202e98d2722580858573fd0817-Abstract.html,"The paper is about link prediction in graphs. A highly relevant problem in graph / network analysis problems that arise in recommender systems and drug-protein interaction prediction for example. The paper is extremely well written. It was a pleasure for me to read the very well documented state of the art and related work section. I was able to find only a single typo: l.210 r-top and not r-hop. 
Technically the paper's models and contributions are involved. The authors use graphon models which are limit objects of sequences of graphs and propose to study a nearest neighbors scheme under the model that is a sequence of graphs with n nodes where n-- > infinity but with a fixed sparsity level p = omega( d^2 / n), where d is the rank of the underlying latent factor space. The authors prove that the proposed approach 's Mean squared error goes to zero when omega(d^2 n) entries are observed and the Max squared error goes to zero when Omega (d^2 n log n) entries are observed. Compared to existing results these results remove a log factor, while relaxing the assumption on the noise. The paper uses a graph that has weighted edges, not a standard binary graph.  

The model is not entirely new. The algorithm probably requires a more compact presentation. The algorithm is presented in pages 5 and 6 and some of the notations are a little confusing: it took me time to find the definitions of M,M_1,M_2,M_3","This paper studies a rather generic form of sparse matrix estimation problem. Inspired by [1-3], it suggests an algorithm that is roughly based on averaging matrix elements that are r-hop common neighbours of the two of nodes for which the estimate is being computed. 
The authors then prove sample complexity bounds for the MSE to go to zero. 

While there is a lot of related work in this direction the present paper discussed this nicely and treats the many cases in a unified framework and allows for generic noise function and sparser matrices. The paper provides (as far as I know) best bounds that hold in this generality. I think this is a very good paper that should be accepted. 

Some comments suggesting improvements follow: 

While in the sparse regime treating the generality of the noise is interesting and as far as I know original. It might be worth mentioning that in the denser case results on universality with respect to the noise are known for the matrix estimation problem (""MMSE of probabilistic low-rank matrix estimation: Universality with respect to the output channel"" by Lesieur et al, and ""Asymptotic mutual information for the two-groups stochastic block model"" by Deshpande, Abbe, Montanari). 

The algorithm seems related to a spectral algorithms designed for sparse graphs, based on the non-backtracking matrix. While the introduction mentions in only one sentence relation to only standard spectral algorithm, the relation to belief propagation and non-backtracking operator is discussed on page 6. At the same time the original works suggesting and analyzing those approaches are not cited there (i.e. ""Spectral redemption in clustering sparse networks"" by Krzakala et al, or ""Non-backtracking spectrum of random graphs: community detection and non-regular Ramanujan graphs"" by Bordenave et al.). This relation should perhaps be discussed in the section of related work or in the introduction and not in the section describing in details of the algorithm. 

The introduction discusses an unpublished work on mixed membership SBM, in particular a gap between information theoretical lower bound and a conjectures algorithmic threshold in that case. Perhaps it should be mentioned that such results originate from the normal SBM where both the information-theoretic threshold for detection, and the conjectured algorithmic threshold were studied in detail, e.g. in ""Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications"" by Decelle et al.  Also in that case the gap between the two threshold is d (for large d). 

While the main contribution of the paper is theoretical, it would have been nice to see some practical demonstration of the algorithm, comparison to other algorithms (at the same time this should not be used as an argument for rejection). Evidence of the scalability of the algorithm should be presented. 

Minor points: 
While the o(), O(), \Omega() notations are rather standard I was not very familiar with the \omega() and had to look it up to be sure. Perhaps more of NIPS audience would not be familiar with those and the definition could be shortly reminded. 

I've read the author's feedback and took it into account in my score. 
","This paper proposed an improved iterative algorithm for filling missing entries in a very sparse matrix, typically known as matrix completion and has wide applications in link prediction and recommendation systems. The main ides is to enlarge the choice of the neighborhood samples in case two nodes in the graph do not have a direct linkage, (for example, two nodes can be connected by a series of intermediate nodes on a path) so that the resultant estimation is expected to converge faster. It is shown that the mean squared error of estimation converges to zeros given sufficient samples of O(d^2nlog(n)). No empirical evaluations nor comparisons are provided to support the theoretical analysis. 

I have several concerns with the proposed method. First, as the authors summarized in Section 3, that existing methods for local approximation typically use two steps, namely, distance computation and averaging. To me such algorithms are quite unstable and depends heavily on the distance measure, while the ground truth distance is usually unknown and hard to estimate if the matrix/graph is very sparse. In (3), the distance between two nodes are expressed by the averaged difference of their neighbors's values. Therefore the distance measure and the neighborhood depends heavily on each other, which are hard to optimize.

The theoretic analysis of the paper  does not show clearly why the proposed scheme leads to a higher convergence rate. In particular while it may be helpful to increase the radius of the neighborhood size, what is a qualitative criteria to avoid too large neighborhood? Also, there lacks empirical results demonstrating the usefulness of the proposed method, therefore it is difficult to judge the practical applicability of the method. It is highly preferred that more experiments and comparisons be provided to make this a solid work."
Positive-Unlabeled Learning with Non-Negative Risk Estimator,"Ryuichi Kiryo, Gang Niu, Marthinus C. du Plessis, Masashi Sugiyama",https://proceedings.neurips.cc/paper/2017/hash/7cce53cf90577442771720a370c3c723-Abstract.html,"This paper presents a new risk estimator aiming to solve the problem that the unbiased PU learning risk estimator is unbounded from below such that the model can overfitt easily especially  when the model is complex. Theoretical analysis and empricial studies are conducted and results are reported. 

In PU learning, unbiased PU risk estimator is not bounded below and might lead to serious overfitting . This paper tackles this problem by slightly adjust the original estimators by setting the a cut off at zero. Such modification is simple, but effective. The authors studies the theoretical properties of the revised estimators and showed that this estimator would be effective especially when the number of unlabeled data is abundant.  The experiments on several datasets shows that performance of PU learning improves after using the new risk estimator.

Minor issues: the presentation of this paper can be further improved.
","Summary

The paper builds on the literature of Positive and Unlabeled (PU) learning and formulates a new risk objective that is negatively bounded. The need is mainly motivated by the fact that the state of the art risk formulation for PU can be unbounded from below. This is a serious issue when dealing with models with high capacity (such as deep neural networks), because the model can overfit.

A new lower-bounded formulation is provided. Despite not being unbiased, the new risk is consistent and its bias decreases exponentially with the sample size. Generalization bounds are also proven.

Experiments confirm that state of the art cannot be used with high-capacity models, while the new risk is robust to overfitting and can be applied for classification with simple deep neural networks. 
 

Comments

The paper is very well written, easy to follow and tells a coherent story.

The theoretical results are flawless from my point of view; I have checked the proofs in the supplementary material. The theory is also supported by many comments helping intuition of the arguments behind the proofs. This is very good. I am only confused by the meaning of those two sentences in Section 4:
- Theorem 3 is not a necessary condition even for meeting (3): what is the condition the authors refer to?
- In the proof, the reduction in MSE was expressed as a Lebesgue-Stieltjes integral [29]: why is this relevant?

The authors successfully show that optimisation of this new risk formulation may be effective in practice, on deep neural networks, with only a few (positively) labeled data points. From this, it could help to see what is the point of view of the authors in comparing PU with current trends in deep learning, such as few-shots learning.

The dataset `epsilon` is never mentioned before talking about the model trained on it; I find that confusing for the reader.

Experimental results well address the main point of the paper (i.e. loweboundness/overfitting), but could be expanded. Fig. 1 shows two graphs with a very similar information: PU overfits when training a deep net. A curious reader who does not know about [16] could wonder whether the previous approach actually works at all. Why not showing the same graph (for one among the 2 losses) on a scenario where [16] is effective? For example, given the thesis of the paper, [16] should work fine if we train a linear model on MNIST, right? 

Additionally, it would be interesting to see what is the effect of estimating the class-prior on performance. Experiments in Fig. 2 are designed to show that NNPU can be even better than PN. But what happens if the class priors are estimated by a state of the art method? Since the paper is mostly focus on the theory side, I think this lack is not a major issue for acceptance. Although it will be for any realistic implementation of the proposed method.


Minor

53: Menon et al. ICML 2015 is another valid method for estimating class priors other than [22, 23, 24]

56: Curiosity on notation: why not denoting R_n(g) as R_n^-(g) ? That would be symmetric with R_n^+ (same for the empirical risks)

237: performance of [35] on CIFAR10 has been surpassed, see
https://www.eff.org/ai/metrics or
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130
I think it might be said that [35] is ""among the best performing architectures for classifying on CIFAR10""","This paper considers PU learning, and particularly, unbiased risk estimators for PU learning.  The authors convincingly show that because the unbiased risk estimators can be negative, that this leads to poor test errors.  Figure 1 is very illustrative, and demonstrates that the test error tends to diverge when the training error drops below zero.  This fact is surprising when considering lemma 1, which shows that for a fixed classifier, the unbiased risk estimator is positive.  Apparently, for the rare classifiers when this is not true the test error is very poor.

The correction is very simple in that they do not allow a weighted difference between the unlabeled risk and the false positive risk to exceed 0.  The authors propose a stochastic gradient procedure based on this non-negative risk estimator.  Lemma 1 implies that the bias and mean square error of the NNRE is controlled.  Theorem 4 explores the estimation error based on Rademacher complexity of the classifiers.  The experiments conclusively show that this methodology improves pre-existing PU learning methods.  I have gone over the proofs in the supplement and cannot find faults."
Gradient descent GAN optimization is locally stable,"Vaishnavh Nagarajan, J. Zico Kolter",https://proceedings.neurips.cc/paper/2017/hash/7e0a0209b929d097bd3e8ef30567a5c1-Abstract.html,"The authors present a dynamical system based analysis of simultaneous gradient descent updates for GANs, by considering the limit dynamical system that corresponds to the discrete updates. They show that under a series of assumptions, an equilibrium point of the dynamical system is locally asymptotically stable, implying convergence to the equilibrium if the system is initialized in a close neighborhood of it. Then they show how some types of GANs fail to satisfy some of their conditions and propose a fix to the gradient updates that re-instate local stability. They give experimental evidence that the local-stability inspired fix yields improvements in practice on MNIST digit generation and simple multi-modal distributions. 

However, I do think that these drawbacks are remedied by the fact that their modification, based on local asymptotic theory, did give noticeable improvements. Moreover, the local stability proof is a bit non-trivial from theoretical point of view.
I think this paper provides some nice theoretical results in GAN training and gives interesting insights to GAN training through the dynamical system lens. The most promising part of the paper is the fact that their theory inspired fix to gradient descent yields experimental improvements. 

The theory part of the paper has some drawbacks: the assumptions that under equilibrium the discriminator outputs always zero is fairly strong and hard to believe in practice. In particular, it's hard to believe that the generator will be outputting a completely indistinguishable distribution from the real data, which is the only case where this could be an equilibrium for the discriminator. Also the fact that this is only a local stability without any arguments about the size of the region of attraction, might make the result irrelevant in practical training, where initializing in a tiny ball around the equilibrium would be impossible. Finally, it is not clear that local stability of the limit dynamical system will also imply local stability of the discrete system with stochastic gradients: if the variance in the stochastic gradient is large and the region of attraction is tiny, then a tiny step outside of the equilibrium together with a high variance gradient step could push the system outside of the region of attraction. ","The authors show that the gradient descent method for training GANs is locally asymptotically stable in a more realistic setting than prior theoretical work. The proof idea is straightforward and accessible: the authors cast (stochastic) gradient descent as a stochastic dynamical system and appeal to standard results to conclude that the stability of gradient descent is equivalent to the stability of a deterministic dynamical system, for which a sufficient condition is having all negative eigenvalues in its Jacobian matrix. The rest of the paper states some technical assumptions, and uses them to compute bounds on the Jacobian of the deterministic dynamical system corresponding to the gradient descent method for training GANs. Using these bounds, the author conclude that ordinary GANs are locally stable while Wasserstein GANs are not. Finally, the authors apply their eigenvalue bounds to construct a new regularizer, which can make Wasserstein GANs stable. They present a few preliminary, but intuitively compelling experimental results with their regularizer.

The paper is generally easy to read and the logic is clear. It clearly relates its result to that of recent related work. The implications of the theorems are well explained. One weakness of this paper seems to be an essential and restrictive dependence on realizability and the generator and the true distribution having the same support. While authors do allude to alternative results that relax these assumptions (at the cost of other restrictive ones), I think the implications and dependences on these assumptions should be discussed more prominently, and the results obtained using alternative assumptions better compared.","This paper presents a detailed theoretical analysis of the local asymptotic stability of GAN optimizations. The analysis provides insights into the nature of GAN optimizations. It highlights that the GAN objective can be a concave-concave objective with respect to both the discriminator and the generator parameters, especially at the regions close to the equilibrium. It later shows that the LQ WGAN system for learning a zero mean Gaussian distribution is not asymptotically stable at its equilibria. Motivated by this analysis, the authors propose a gradient-based regularization to stabilize both traditional GAN and the WGANs and improves convergence speed.

It was not fully clear until much later (Section 4) about the effect these analyses have on the optimization convergence speed. Showing some more experiments with some quantification of why the optimization convergence is slow would be very useful (might be hard to show, though). Also, as the authors noted in Section 5, there are some limitations to the current analysis. Some empirical analysis (using different datasets and models) might shed light on the impact such limitations have on practical situations. 
 
Overall, this is a well written paper. The background description, however, can be improved to describe the main problem and issues with the existing work such that the reader does not have to review prior papers.

Please fix the following typos in the final version:
 - of""foresight"" -> of ""foresight""
 - we use the analyze -> we use to analyze
 - guarantee locally stability -> guarantee local stability
 - is locally stable. provided f is -> is locally stable provided f is
"
Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers,"Cong Fang, Feng Cheng, Zhouchen Lin",https://proceedings.neurips.cc/paper/2017/hash/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Abstract.html,"The paper considers the acceleration of stochastic ADMM. The paper basically combines existing techniques like Nesterov's acceleration technique, and increasing penalty parameter and variance reduction technique. The clearness and readability of the paper needs to be improved. The algorithm is designed for purpose of the theoretical bounds but not for a practically faster algorithm. Overall, the paper needs to be improved. 

1. In the abstract and introduction, the paper does not clearly  point out the result is based on the average of the last m iterations when state non-ergodic convergence rate. The paper also does not point out the use of increasing penalty parameter which is an important trick to establish the theoretical result. 

2. Algorithm 2 looks like a warm start algorithm by increasing the penalty parameter (viz. step size). For a warm start algorithm, the theoretical result is expected for the average of the last loop iterates and outer-loop iterates.

3.  If the theoretical result could be established without increasing penalty parameter, it is desirable. Trading penalty parameter (speed) for theoretical bound is not a significant contribution. ","This paper proposes Accelerated Stochastic ADMM(ACC-SADMM) for large scale general convex finite-sum problems with linear functions. In the inner loop, they do extrapolation and update primal and dual variables which ensures they can obtain a non-ergodic results and also accelerate the process and reduce the variance. For outer loop, they preserve snapshot vectors and resets the initial value. The authors may describe the update rule in a more intuitive way since updating formula is a little complicated. The main contribution is that this algorithm has a better convergence rate.","Summary: This paper develops a sophisticated stochastic ADMM method that integrates Nesterov acceleration and variance reduction techniques so as to obtain an optimal non-ergodic O(1/K) convergence rate. The experimental results appear to be very compelling.

Some of the choices in the design of the algorithm appear to be unintuitive (Algorithm 1 and 2). What is the basis of choosing c=tau=2? How does the penalty factor adjustment proposed in the paper compare with other ""adaptive-rho"" heuristics such as based on balancing primal/dual tolerances. Are there any differences among stochastic ADMM variants in terms of ability to parallelize?

Section 3.3: ""we preserves"" --> ""we preserve""
"
PixelGAN Autoencoders,"Alireza Makhzani, Brendan J. Frey",https://proceedings.neurips.cc/paper/2017/hash/7e7e69ea3384874304911625ac34321c-Abstract.html,"Update after rebuttal: I believe the paper can be accepted as a poster. I advise the authors to polish the writing to better highlight their contributions, motivation and design choices. This could make the work attractive and rememberable, not ""yet another hybrid generative model"". 

-----
The paper proposes PixelGAN autocencoder - a generative model which is a hybrid of an adversarial autoencoder and a PixelCNN autoencoder. The authors provide a theoretical justification of the approach based on a decomposition of variational evidence lower bound (ELBO). The authors provide qualitative results with different priors on the hidden distribution, and quantitative results on semi-supervised learning on MNIST, SVHN and NORB.

The paper is closely related to Adversarial autoencoders (Makhzani et al. ICLR 2016 Workshop), which, as far as I know, have only been published on arxiv and as a Workshop contribution to ICLR. Yet, the work is well known and widely cited. The novelty of the present submission significantly depends on Adversarial autoencoders being considered existing approach or not. This is a complicated situation, which, I assume, ACs and PCs are better qualified to judge about.

Now to some more detailed comments.

Pros:
1) Good results on semi-supervised learning on MNIST, SVHN and NORB, and unsupervised clustering on MNIST. It is difficult to say if the results are state-of-the-art since many numbers for the baselines are missing, but at least they are close to the state of the art.
2) A clear discussion of an ELBO decomposition and related architectural choices.

Cons:
1) If Adversarial autoencoders are considered existing work, then novelty is somewhat limited - it’s yet another paper which combines two existing generative models. What makes exactly this combination especially interesting?
2) Lack of results on actual image generation. I understand that image generation is difficult to evaluate, but still some likelihood bounds (are these even computable?), Inception scores and images would be nice to see, at least in the Appendix.
3) It is somewhat confusing that two versions of the approach - with location-dependent and location-independent biases - are used in experiments interchangeably, but are not directly compared to each other. I appreciate the authors mentioning this in lines 153-157, but a more in-depth analysis would be useful.
4) A more detailed discussion of relation to existing approaches, such as VLAE and PixelVAE (both published at ICLR 2017), would be helpful. Lines  158-163 are helpful, but do not quite clear highlight the differences, and strengths and weaknesses of different approaches.
5) Some formulations are not quite clear, such as “limited stochasticity” vs “powerful decoder” in lines 88 and 96. Also the statement in line 111 about “approximately optimizing the KL divergence” and the corresponding footnote looks a bit too abstract - so do the authors optimize it or not?
6) In the bibliography the authors tend to ignore the ICLR conference and list many officially published papers as arxiv.
7) Putting a whole section on cross-domain relations to the appendix is not good practice at all. I realize it’s difficult to fit all content to 8 pages, but it’s the job of the authors to organize the paper in such a way that all important contributions fit into the main paper.

Overall, I am in the borderline mode. The results are quite good, but the novelty seems limited.","The paper build and auto-encoder with pixelCNN decoder and adversarial cost on latent between uniform prior and inference distribution. With the right network design the networks separate global input information stored in the latent and local one captured by pixelCNN when trained on MNIST dataset. With categorical distribution of latents the network learns to capture very close to class information in unsupervised way. The networks perform well in semisupervised settings. The paper is yet another combination of VAE/AdvNet/PixelCNN. The paper has a nice set of experiments and discussion. The model is most closely related to VAE-pixelCNN combination with VAE loss (KL) on latents replaced by adversarial loss (even though they discuss the mathematical difference) and it would be good to run the same experiments (scaling latent loss) with that and compare.

More details: 
- In the Figure 2c I would rather see VAE-pixelCNN combination with the same networks and different scaling of KL term. While there is a mathematical description of the difference, in the end both are some terms penalizing latents, both wanting latents to be unit gaussians. 

- Line 98: That’s not necessarily the case, depending on function approximations. The network can decide to put information into latents or keep them in the input. But yes, it is harder for it be in the latents since latents are noisy and the posterior is approximate.

- Do you get any clear unsupervised separation as in Figure 6 for SVHN and NORB?","The paper describes an adversarial autoencoder model (i.e., an autoencoder where the latents are encouraged to match a prior distribution through a GAN-like training scheme), where the decoder is autoregressive. This model can use both its latent representation and the autoregressive connections to model the data, and which part of the model learns what can be manipulated by slightly modifying its architecture and the prior distribution used for the latents.

The experiment showcased in Figure 2 is great, it nicely demonstrates the benefits of combining both autoregressive and latent variable based modelling. It's also cool to see that the model learns to encode the digit label despite the continuity of the prior (Figure 3).

One thing I'm not sure about is how much the different decompositions (global - local structure vs digit identity - details) are due to the different choice of priors. This is not the only thing that changes between the two types of models. The model with a categorical prior also has a different biasing mechanism, and is much deeper (end of Section 2.1). So I think the paper is a bit quick to attribute the learning of a different decomposition entirely to the change of prior. It would be interesting to take the model with the categorical prior (and all other modifications), and change only the prior back to Gaussian. My guess is it would still learn largely the same decomposition (as in Figure 3(a)), because by my intuition, the architectural changes are much more influential in this than the choice of prior. It would be great to see this addressed in the paper.

The experiment in Figure 5 is also very nice as it clearly demonstrates the ""discretizing"" effect of the GAN loss on the latent code, and the semi-supervised classification experiment is also a great addition.

Overall, the model is a natural extension of both adversarial autoencoders and autoregressive models, and the paper presents a nice follow-up to VLAE and PixelVAE (which study VAEs with autoregressive decoders). The experimental section is well thought-out and the results are convincing, although as stated before I'm not convinced that the choice of prior deserves as much credit as it gets for the different decompositions that are learned.


Remarks:

- Line 45: ""the global latent code no longer has to model all the irrelevant and fine details"", whether this is useful is very task-dependent, imagine a texture classification task for example. So this statement is a bit overgeneralised.

- L187-188: the adversarial net gets continuous inputs (probabilities) from the softmax layer. What about the decoder? I'm guessing this gets discrete input, but this should probably be mentioned explicitly for clarity's sake.

- Although scaling up is clearly not the focus of the paper, an experiment on a larger dataset would also be nice. My intuition is that this model will scale beyond regular PixelCNNs because it can effectively use the latents to code for global structure."
Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models,"Rishit Sheth, Roni Khardon",https://proceedings.neurips.cc/paper/2017/hash/7edccc661418aeb5761dbcdc06ad490c-Abstract.html,"This work proposes a risk bound for various variational bayes algorithms by leveraging the work of [5, 14, 9]. Those bounds show that the variational bayes posterior is competitive with any point estimate of the same class. In section 4, the author specialize the bound to variational algorithms which have a Normal Distribution as the prior and posterior. To avoid the max_q term diverging to infinity, the posterior family is limited to covariance matrices with their smallest eigenvalue above a threshold.

At line 230, the bounded loss is considered and a quick analysis is made. However, having an explicit writeup of the bound would be useful. Also, calculating real values of the bound in the proposed experiment would be useful.

The main result is not trivial and somewhat useful in a theoretical sense.  However, the main result is buried behind maths. Stating more clearly the final bound and resolving the \psy function and \lambda to their actual values according to the choice of either bounded loss or sub-gaussian would be important. Also discussing the fact that bounded loss is unrealistic in the case of NLL and that sub-gaussian bounds don’t converge to 0 as the number of samples increase as stated in [5].

I would also like the author to discuss more in details the usefulness of this result.

Minors:
* Line 73: p(y|w,x) instead of p(y|w) ?
* Between 120 and 130 there are a few places where it feels like it should be conditioned on x but it’s not. Can we be more consistent?
","General comments: 
Overall this is an interesting paper with some nice theoretical results, though the techniques are rather standard. As far as I know, the results on risk bounds for Bayes predictor are novel. It provides theoretical evidence for Bayesian treating of parameters (at least for Gaussian priors) and the generalization guarantee of variational approximations (1-layer, also 2-layer by collapsing the second), which should be of wide interest to the Bayesian community. The experiments on CTMs demonstrate the risk bounds of different variational approximations, though I find the direct loss minimization case less significant because usually it’s hard to compute the marginal log likelihoods. 
The main concern that I have is the presentation of the results. It seems that the authors induce unnecessary complexity during the writing. The three kinds of risks induced in the paper ($r_{Bay}, r_{Gib}, r_{2A}$) are just negative expectations of marginal log    likelihood $\log p(y|x)$, the first term in elbo for w, and the first term in elbo for w and f (using terminology from variational inference). In Section 2.2, a 2-layer Bayesian model is introduced. I understand the authors’ intention to align with real model examples used in the paper after I finish later sections. But I still feel it’s better to derive the main results (Th.2, 6, 8) first on the 1-layer model $p(w)p(y|x,w)$ and later extend them into 2-layer settings. In fact the main theoretical result is on collapsed variational approximations, which can be seen as a 1-layer model (This is also how the proof of Corollary 3 comes).

Minor: In Section 2.2, if the 2-layer model is defined as $p(w)p(f)\prod_i p(y_i|f_i, x_i)$, should the risks be $...E_{p(f|w)} p(y|f, x)$ instead of $...E_{p(f|w,x)}p(y|f)$? I’m not sure why there is a dependence of f on x.","This paper considers several versions of variational inference in one-level and two-level latent variable models: pointing out the loss functions they implicitly minimize, proving risk bounds, and testing their performance empirically. The exposition is clear and the contributions seem significant, though I do not have the background knowledge to evaluate them in more detail.

Typographical comments:
l53: define -> defines
l58: yet -> yet have
l127: comma belongs on previous line
l134: desribed -> described
l145: remove colon after footnote
l164: comma missing at end of line
References: please add {}'s to ensure capitalization of PAC, Bayes, Gibbs, ...
"
Online control of the false discovery rate with decaying memory,"Aaditya Ramdas, Fanny Yang, Martin J. Wainwright, Michael I. Jordan",https://proceedings.neurips.cc/paper/2017/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html,"A very well written contribution on an important topic. 

I find that each of the 3 contributions in the paper could justify a paper for themselves:
The GAI++ algorithm is a simple generalization of the GAI algorithms, that settle an interesting open question regarding FDR versus mFDR control.

The weighted-online FDR control is a nice extension of online-FDR.

Most importantly, in my opinion, is the introduction of the mem-FDR which is a natural adaptation of the exponentially weighted moving average to FDR control. I am surprised this has not been suggested previously, and is a natural and important error criterion. 
It is for this last contribution that I expect this paper to have a high impact. 
","This paper unifies and extends concepts related to the online false discovery rate (FDR) control. This is a recent trendy setting where null hypotheses are coming in a sequential manner over time and the user should make an (irrevocable) decision about rejecting (or not) the null coming at each time. The aim is to make a sequential inference so that the expected proportion of errors among the rejected nulls is bounded by a prescribed value $\alpha$ (at any stopping time).

My opinion is that this paper is of very high quality: the writing is good and easy to follow, the motivation is clear and interesting, the new procedures and concepts are clever and the proofs are really nicely written.

My only concern is that the strongest innovation (decaying memory procedure) is maybe not 
sufficiently well put forward in the main paper. It only comes at the last 1.5 pages and there is no illustration for Alpha-death recovery. This concept seems however more important than the double-weighting of Section 4 that can be put in appendix safely (more standard). I would also suggest to put Section D.3 of the supplement in the main paper (illustration of Piggybacking).

Here are some other comments and typos : 
 line 58 ""will equal 1"" (only with high probability);
 line 131 ""mst"";
 line 143 ""keeping with past work"";
 Notation ""W(t)"" I suggest to use W_t (if you agree);
 Lemma 1 f_i is not F_i measurable but just measurable (right?);
 Section D.1, line 469-470 :"" mu_j follows 0"" should read ""mu_j=0"";
 line 528 : equation (13) should read equation (10);
 line 529-530 : -phi_t (R_t-1) should read +phi_t (R_t-1).

 


"
Safe and Nested Subgame Solving for Imperfect-Information Games,"Noam Brown, Tuomas Sandholm",https://proceedings.neurips.cc/paper/2017/hash/7fe1f8abaad094e0b5cb1b01d712f708-Abstract.html,"Summary: Considers solving two-player zero-sum games of imperfect 
information, like poker. Proposes, I think, two new ideas. (1) ""Reach"" allows the algorithm to make use of any bonus or ""gift"" that it got at prior steps in the game where the opponent made a mistake. (2) ""Nested"" proposes to re-solve subgames ""on the fly"" using more detailed information. That is, the originally solved game may be an abstraction, so when we reach a particular small subgame, maybe we can solve it more exactly in real time.

Opinion: I don't know the game-solving and poker-playing type literature very well. The paper seems to me well-written and to advance some interesting and important ideas for that literature. A weakness is relatively little explanation and discussion of the contributions of new techniques except at a technical level, which made it hard for me to evaluate.
The ""ace in the hole"" for the paper is that these techniques recently contributed to the first defeating of top human players in no-limit hold-'em, which is a pretty big landmark for AI in general.

Comments: I didn't really get an intuition for why ""Reach"" helps, i.e. why knowing that P1 made a mistake earlier in the game tree allows us to improve in a certain subgame. I would love to get intuition for this since it seems like a key point.


----
After author response: I found the intuition for ""reach"" helpful, thanks.","This work proposes a new technique ""reach subgame solving"" to better inform decision making in an imperfect information, two player zero-sum game setting. The main idea is to exploit the knowledge about the difference in payoffs received by the opponent  when following paths leading into and away from the information set.  The authors also suggest an adaptation that extends the method to multiple independent subgames. 

The paper is generally well-written, and results have been proved about the low exploitability of the method, or the discrepancy relative to the Nash equilibrium value (in a zero-sum game this value is unique, so the issue of which Nash equilibrium to compare to does not arise). Empirical results have also been provided to substantiate the gains over existing methods - a standard baseline, the unsafe subgame solving method, seems to be performing quite well on all but one benchmarks, but does not come with any theoretical guarantees. The paper also claims to be the first method to have bested four human subjects in heads-up no limit Texas hold'em poker.  

I wonder if the ideas introduced here could be useful if exploitability were defined not with respect to Nash equilibrium, but some other notions such as (coarse) correlated equilibria? Also, do you think the method could be adapted to non-zero sum (bimatrix games)?     ","The authors present algorithms and experimental results for solving zero-sum imperfect information extensive-form games, with heads-up no-limit Texas Hold'em poker as the motivating application.  

They begin by reviewing the distinction between ""unsafe"" and ""safe"" subgame solving.  In unsafe subgame solving, the opponent's strategy outside the subgame, and hence the distribution of the player's state inside the initial information set, is held fixed.  In safe subgame solving, the opponent's strategy in the subgame's head is allowed to vary as well, and hence the distribution of true states is responsive to the player's strategy.

They then introduce ""reach"" subgame solving, in which a ""gift"" (the additional value that the opponent could have obtained by choosing a different subgame) is computed for each path that leads to the subgame.  These gifts are added to the margin for each information set.  

Finally, they introduce nested subgame solving as a method of dealing with large action spaces: the trunk strategy is computed for an abstracted version of the game, but any time the opponent plays an action that is not in the abstraction, a new subgame is generated and solved on-the-fly that contains the action.

Experimental results suggest that the two methods combined are extremely effective in practice.

Overall I found the exposition well organized and easy to follow, and the performance of the algorithm seems impressive.  I have only minor comments about the presentation.

1. Gifts are motivated as ""allowing P2 to be less concerned with P1 reaching the subgame along that path"" (L.173-174).  My guess is that the gift can be added to the margin because it's a measure of how unattractive a branch is to the opponent, and the goal of maxmargin solving is to make the subgame as unattractive as possible to the opponent; but a little more concreteness / hand-holding would be helpful here.

2. At L.297-303, the authors note that in spite of having better theoretical guarantees, the no-split variants seem to always outperform the split variants.  They note that this is not necessarily the case if the gifts are scaled up too aggressively; however, that does not seem directly relevant to the point (I don't believe the experimental results include scaling rather than just failure to divide).

   If the full gift is simply awarded to every subgame, with no division but also no scaling, is exploitability ever worse in practice?  If so, can the authors provide an example?  If not, do the authors have an intuition for why that might be?
   
   At L.210 the authors say that any split is sufficient for the theoretical guarantees, but it seems clear that some splits will leave more on the table than others (as an extreme example, imagine awarding the entire gift to the lowest-probability subgame; this might be close to not using Reach at all). 

3. At L.88: ""Figure 4a"" probably should be ""Figure 1"".

"
A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent,Ben London,https://proceedings.neurips.cc/paper/2017/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html,"This work analyzes the stability of SGD based learning algorithms. Both the stability of the algorithm with respect to small changes on the training set and small changes on the sequence of samples are analyzed. Those two terms take place in a PAC Bayes bound which suggests that sampling from a posterior distribution over the training set can be more efficient than sampling from a uniform distribution. On the light of this result, they develop Algorithm1, which dynamically produces a posterior focusing on harder examples during training. Experimental results on CIFAR-10 shows faster convergence rate. 

I did not verified the proofs, but the paper is well written and sound. The end algorithm is easy to implement and can be added to many deep learning algorithms with no effort.

The end algorithm is analogous to active learning and the proposed utility function may not work well for some dataset. For example some sample might have high aleatoric uncertainty and the current algorithm would mainly be “obsessed” by unsolvable examples. Using a utility function based on epistemic uncertainty instead would solve this issue. (See Yarin Gal’s work on active learning).

I believe that this algorithm would be good at solving the class imbalance issue, e.g., class A has 10000 samples and class B has 100 samples. Having an experiment highlighting this would add great value to this work (this is not a request, just a suggestion)

Minors:
* Line 88: the indicator function should have \neq inside and not “=“.
* Line 39: The space of X and the space of H both uses R^d and they are not the same space
* Line 50: Typo as a a.

","
      	The paper is interesting and definitely of interest to the NIPS community. In particular, the idea of coupling PAC-Bayes and stability is appealing. Overall, I stand for acceptance, although I have some remarks:

      	* Lines 69-81: I am afraid I have not understood the message here. Too subtle indeed.
      	* The authors assume that the loss is bounded. Could this stringent assumption be relaxed? Some effort has been put lately to derive PAC-Bayesian bounds for unbounded losses and/or heavy-tailed distributions, see for example Catoni (2004, St Flour lecture notes), Audibert and Catoni (2011, Annals of Statistics), Grünwald and Mehta (2016, arXiv preprint), Alquier and Guedj (2016, arXiv preprint).
      	* Most serious concern: in Th. 2 and 3, do the risk bounds hold for the SGD (alg 1)? If not, what is $T$ standing for? This section of the paper is not clear enough. I feel that a very specific risk bound for $\beta$-{uniformly,hypothesis}-stable SGD with the actual $\beta$ terms would be much more clearer to readers.
      	* Th4: to my opinion this result is the most interesting of the paper but curiously this is not put forward. Could a similar result be derived for non-uniform priors (e.g., enforcing sparsity)?
      	* Some typos (lines 110, 170).
      ","The paper opens the way to a new use of PAC-Bayesian theory, by combining PAC-Bayes with algorithmic stability to study stochastic optimization algorithms. The obtained probabilistic bounds are then used to inspire adaptive sampling strategies, studied empirically in a deep learning scenario. 

The  paper is well written, and the proofs are non-trivial. It contains several clever ideas, namely the use of algorithmic stability to bound the complexity term inside PAC-Bayesian bounds. It's also fruitful to express the prior and posterior distributions over the sequences of indexes used by a stochastic gradient descent algorithm. Up to my knowledge, this is very different than any other previous PAC-Bayes theorems, and I think it can inspire others in the future.

The experiments using a (moderately) deep network, trained by both SGD and AdaGrad, are convincing enough; they show that the proposed adaptive sampling strategy can benefit to existing optimization methods simply by selecting the samples during training time.

Small comments:
- Line 178: In order to sell their result, the authors claim that ""high-probability bounds are usually favored"" over bounds in expectation. I don't think the community is unanimous about this, and I would like the authors to convince me that I should prefer high-probability bounds.
- Section 5 and Algorithm 1: I suggest to explicitly express the utility function f as a function of a model h.

Typos:
- Line 110: van -> can 
- Supplemental, Line 183: he -> the


** UPDATE **
I encourage the authors to carefully consider the reviewer's comments while preparing the final version of their paper. Concerning the claim that high-probability bound imply expectation bound, I think it's right, but it deserves to be explained properly to convince the readers.

"
Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"El Mahdi El Mhamdi, Rachid Guerraoui, Hadrien Hendrikx, Alexandre Maurer",https://proceedings.neurips.cc/paper/2017/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,"This paper presents an extension of the safe interruptibility (SInt) 
framework to the multi-agent case. The authors argue that the 
original definition of safe interruptibility is difficult to use in 
this case and give a more constrained/informed one called 'dynamic safe 
interruptibility' (DSInt) based on whether the update rule depends 
on the interruption probability. The joint action case is considered 
first and it is shown that DSInt can be achieved. The case of 
independent learners is then considered, with a first result showing 
that independent Q-learners do not satisfy the conditions of the 
definition of DSInt. The authors finally propose a model where the 
agents are aware of each others interruptions, and interrupted 
observations are pruned from the sequence, and claim that this model 
verify the definition of DSInt.


The paper is mostly well-written, well motivated, offers novel ideas 
and appears mostly technically correct. The level of formalism is 
good, emphasizing on the ideas rather than rendering a boring 
sequence of symbols, but this also hurts somewhat the proof reading 
of the appendix, so I can't be 100% confident about the results.

* Main comments:

- What is the relation between the authors' definition of DSInt and 
the original version? In particular, do we have DSInt implies SInt, 
or the converse, or neither?

- In the proof of Lemma 1, p. 12, third equality, a condition on 'a' 
is missing in the second P term. As far as I understand, this 
condition cannot be removed, preventing this term from being pulled 
out of the sum.

* Minor comments:

- The motivating example is pretty good. On a side note however, it 
seems that in this scenario the knowledge of others' interruptions 
may still be inadequate, as in the general case the different 
vehicles may not even be of the same brand and thus may not exchange 
information (unless some global norm is defined?). This is not so 
much of a criticism, as one needs to start somewhere, but a 
discussion about that could go in the conclusion.

- Definition 2: This should define ""dynamic safe interruptibility"", 
not just ""safe interruptibility"" which is already defined in [16].

- The notation used for open intervals is at times standard (in 
the appendix) and at other times non-standard (as in the main text). 
This should be made homogeneous (preferably using the standard 
notation).

- Def 5: ""multi-agent systems"" -> ""multi-agent system""

- Proof of Thm 2: On the first equality in the display, we lost the 
index (i). 

- The second line after the display should probably read  
\hat{Q^m_t} instead of Q^{(m)}_t.

- Missing period at the end of the proof.

- Proof of Thm 3: ""two a and b"" -> ""two agents a and b"" (or learner, 
player?)

- \gamma is not defined AFAICT

- Proof of Thm 4, ""same than"" -> ""same as""

- Does it mean that independent learners are DSInt but not SInt?

- Penultimate sentence of the conclusion, ""amount exploration"" -> +of

- p.12 after Lemma 2 ""each agents do not"" -> ""each agent does"" or 
""no agent learns""

- last line of the display in the proof of Lemma 1: Use the \left[ 
and \right]  commands (assuming LaTeX is being used) to format 
properly.

- proof of Thm 4: q is undefined.","
        Summary：

        The paper proposes a definition of safe interruptibility (SI) in the multi-agent setting. SI means that a given RL algorithm can still learn the optimal policy while experiencing (adversarial) interruptions: instances in which an external policy pi_int is used instead of the policy that is being learned. One interpretation is that agents need to explore all state-actions and converge to the same optimal policies / Q-value fixed points, even when interruptions are present - hence SI comes down to ensuring that learning / exploration is defined in a way that is ""independent enough"" from historically executed actions / interruptions. In the (competitive) multi-agent setting, convergence to optimal Q-values is not guaranteed, hence the paper defines SI dynamically - i.e. learning with SI should preserve dynamics, the probability of seeing a particular Q-value during training should be unchanged.

        The paper then studies DSI for Q-learning with e-greedy style learning: using joint learners (Q-value function of all states and all actions) and independent learners (Q-value depends on all states and only agent's action). In particular, it shows that certain schemes to set epsilon during training are robust to interruptions. The paper then concludes that JAL is DSI with a well-chosen e-scheme, and IDL similarly is DSI, given an additional pruning mechanism that removes experience where an agent was interrupted.

        As such, the paper proposes an interesting theoretical extension of RL and raises new questions to pursue. However, the work only considers a very limited setting and the practical implications of the current work are not illustrated.

        Additional concerns / comments:

        - The paper considers tabular learning only (no function approximations) and so the analysis seemingly breaks down e.g. for infinite # of states. Given function approximators are essential for most applications, what is the biggest obstacle to applying the DSI analysis to that case?
        - The analysis does not apply to on-policy methods, as the paper notes. This seems like a major omission, although it might be out of the scope of this paper. How could we verify SI in the on-policy case (e.g. use a strong form of entropy regularization)?
        - The paper notes that the defined e-scheme is robust to the most adversarial interruption scheme possible. It would be nice to have some (synthetic) experiments to showcase a practical implementation to see how much better it can do in practice.
        - How precisely could learning a model of the interruption process (i.e. learning theta) help?
        - The pruning mechanism for IDL seems overly strong - how does the analysis change if agents learn to communicate / share their model of the interruption process?
      ","The paper proposes a method to extend previously-published framework of safe interruptibility to multi-agent systems. Formal requirements for safe dynamic interruptible systems are provided and theoretical guarantees are given when systems of jointly learning or independently learning agents are safely interruptible. 

The paper is well-motivated and exposition is clear. The concrete example of self-driving cars is valuable and clearly illustrates the need for the research in this area.

The proposed theoretical foundations seem sound, although I am not an expert in this area. The provided proofs are useful as their techniques could be of value to other researchers who would like to continue work in this area. Ideally, I wish the authors would provide the readers with more tools to extend their theoretical work, such as general guidelines on how to prove safe interruptibility for other learning algorithms (approaches that rely on anticipating strategies of other agents, for example, which are mentioned not to be safely interruptible)

I think this paper tackles an important problem. The preliminary paper on safe interruptibility opened many avenues of research to explore. The authors of this paper have picked an important extension of safe interruptibility in multi-agent systems and have done a solid theoretical investigation that I believe will be of value to the machine learning community and I would like to see it published at NIPS."
Toward Multimodal Image-to-Image Translation,"Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman",https://proceedings.neurips.cc/paper/2017/hash/819f46e52c25763a55cc642422644317-Abstract.html,"This paper proposes a novel way to generate diverse samples in a conditional generative modeling setting by enforcing cycle consistency on a latent space. The task is translating an image of domain A to another domain while making the results perceptually realistic and diverse. The image cycle consistency helps the model to generate more realistic images while the latent cycle consistency enforces the latent space and the image space to have one-to-one mapping, which means the latent code can effectively embed diverse color and texture information.

[Paper strengths]
- This paper alleviates the mode collapse problem occurred in the prior work and generates diverse samples with latent codes.
- The proposed method is novel and the results are convincing.
- Supporting experiments shown in Fig. 3 and Fig. 4 explain well how Bicycle consistency improves diversity of results.

[Paper weaknesses]
- The paper contains inaccurate formulas in equation (3), (5), (7) and (8). Since GAN losses in (1) and (4) become small if the discriminator accurately discriminates real and fake images, “arg min(G) max(D)” should be “arg max(G) min(D)”. Also, the equation (5), (7), (8) miss an encoder “E” in “min”.
- It would be better to show that the changes in diversity w.r.t the increasing dimensions of latent variables.

[Clarity]
The paper is clearly written and the Fig 1 is helpful for understanding the overall architecture.

[Reproducibility]
Dataset used for evaluation is publicly available. Even though the paper omits some details about the experiments and contains incorrect equations, the method can be reproduced since the proposed model shares the architecture with prior work. However, it would be better to elaborate missing details (e.g., hyperparameters, training details).

[Overall]
Explain your rating by discussing the strengths and weaknesses of the submission, contributions, and the potential impact of the paper. Include suggestions for improvement and publication alternatives, if appropriate. Be thorough. Be Fair. Be courteous. Your evaluation will be forwarded to the authors during the rebuttal period.

The main contribution of this paper is adding a latent code to generate diverse samples in an image-to-image translation task and using latent cycle-consistency to prevent the model to ignore the latent code. Although there are some incorrect equations, but they are minor. The method is well explained and justified.
","This paper proposes BicycleGAN, a new type of adversarial network for image generation that leverages two types of cycle-consistency. If we want to learn the conditional image distribution P(B | A), the imaget cycle consistency term encourages the network's encoder and generator to be able to yield B -> z -> B. where z are the GAN's latent noise units. The latent cycle consistency term encourages that z -> B ->z , i.e. the network should be able to generate an image conditioned on (z, A), then encode that image to recover z. 

Empirically, the authors show that the proposed setup yields diverse conditional image samples across several popular domains: line drawings -> shoe, line-drawings -> handbag, outdoor photographs night -> day, and building facade -> building photograph. I think this is a very nice result arising from an intuitive method.

I am very curious how important it is for this method to have paired data across domains. Recently there have been many papers about learning conditional image distributions *without* domain alignments, using domain adversarial losses or cycle-consistency terms. I think the paper would be strengthened by carefully relating to this literature. Would it be possible to use the proposed method to learn *diverse* conditional image distributions without alignments?

","Summary:
The paper enforces a cycle consistency between the latent code and the image output of a generative model that is applied to image-image translation problems. The cycle consistency encourages invertibility of a generated output to the latent code, which addresses the popular issue of mode collapse in generative adversarial networks. The paper extends the pix2pix [1] framework of image-image translation by conditioning the generation of an image not only on an input image but also on a randomly sampled latent code. The low dimensional latent code captures the ambiguous aspects of the output image such as (depending on application) color, texture, etc.  While Isola et al. [1] observe that randomly sampling a latent code alone doesn’t help with mode collapse, the proposed approach applies a cycle consistency loss on both the input-output image pairs and the latent code - output pairs via alternating joint optimization. 

Strengths:
* Mode collapse is a commonly occurring issue in generative adversarial networks, and a method to produce more diverse results could be potentially useful to a large community.   
* The approach is a natural extension of the cycle consistency idea of [2] to address the issue of mode collapse.  
* From the examples in the supplementary material it is clear that the proposed approach generates samples that are both realistic and diverse in the edges -> shoes task, when compared with baseline approaches such as pix2pix, pix2pix+noise, and an ablation of the overall approach, latent cycle GAN (described in 3.3). 
* The analyses in Section 4.2 are interesting. The analysis of the latent space reinforces the earlier claim regarding the poor distribution of latent codes when the model is trained using the Image Cycle.  
* The paper is well written and easy to understand.  

Clarification questions: 
* Are there any specific reasons or intuitions behind the decision to use the least squares loss in the experiment 3.1 (Equation (1)) instead of the discriminator loss used in [1]? 
* It is unclear whether the table in Figure 3., reports Google maps → Satellite images or the opposite transformation.  
* Did the authors ensure that the performance of different methods in the table in Figure 3 are comparable with pix2pix? Specifically, are the ground truth images processed identically as [1]? Please note that [2] states that there are a few differences in preprocessing and hence the performance of methods in [2] are not comparable with [1].  
* Have the authors performed human-perception experiments on other datasets such as shoes? As a consequence of their semantic content, these datasets appear to contain more diverse images in terms of colors, textures etc., than aerial photos.  
* It is unclear how the diversity score in the Table in Figure 3 is computed. Is it the cosine distance between vectors of neuron activations in a layer computed for pairs of images randomly sampled from the set B? 

Final recommendation:
The paper proposes an approach to overcome the popular issue of mode collapse in generative adversarial networks which could be useful to further research into generating diverse samples in conditional generative models. 

References:
[1] Isola et al.:	Image-to-image translation with conditional adversarial networks. CVPR, 2017. 
[2] Zhu et al.: Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint 2017. 
				
			
		
		"
The Marginal Value of Adaptive Gradient Methods in Machine Learning,"Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, Benjamin Recht",https://proceedings.neurips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html,"// Summary:
In an optimization and a learning context, the authors compare recently introduced adaptive gradient methods and more traditional gradient descent methods (with potential momentum). Adaptive methods are based on metrics which evolve along the optimization process. Contrary to what happens for gradient descent, Nesterov's method or the heavy ball method, this may result in estimates which are outside of the linear span of past visited points and estimated gradients. These methods became very popular recently in a deep learning context. The main question adressed by the authors is to compare both categories of method. First the authors construct an easy classification example for which they prove that adaptive methods behave very badly while non adaptive methods achieve perfect accuracy. Second the authors report extensive numerical comparisons of the different classes of algorithms showing consistent superiority of non adaptive methods.

// Main comments:
I do not consider myself knowledgable enough to question the relevance and correctness of the numerical section.

This paper asks a very relevant question about adaptive methods. Giving a definitive answer is a complicated task and the authors provide elements illustrating that adaptive methods do not necessarily have a systematic advantage over non adaptive ones and that non adaptive optimization methods could be considered as serious challengers. The content of the paper is simple and really acessible, hilighting the simplicity of the question. Although simple, this remains extremely relevant and goes beyond the optimization litterature, questioning the reasons why certain methods reach very high popularity in certain communities. The elements given illustrate the message of the authors very clearly.


The main points I would like to raise:
- The artificial example given by the authors is only relevant for illustrative purposes. Comparison of algorithmic performances should be based on the ability to solve classes of problems rather than individual instances. Therefore, it is not really possible to draw any deep conclusion from this. For example, a bad choice of the starting point for non adaptive methods would make them equally bad as adaptive methods on this specific problem. I am not sure that the authors would consider this being a good argument against methods such as gradient descent.
- Popularity of adaptive methods should be related to some practical advantage from a numerical point of view. I guess that positive results and comparison with non adaptive methods have already been reported in the litterature. Do the authors have a comment on this?","Summary
This papers examines similarities and differences of adaptive and non-adaptive stochastic gradient methods in terms of their ability to generalize to unseen data.
The authors find that adaptive methods generally generalize worse than non-adaptive ones both in handcrafted as well as in real world application scenarios.
In addition to this, the authors give some advice as to how to tune non-adaptive methods and report that, despite being ""adaptive"", adaptive methods hugely benefit from tuning their hyperparameters.

Comments

Quality/Clarity
This is a well done paper. The experiments presented confirm their theoretical implications and cover several popular practical applications. 
One question that comes up here though is, how general the applicability of the demonstrated behavior is on other applications and whether it also affects other adaptive methods such as natural stochastic gradient descent and its adaptive counterpart (natural stochastic gradient descent implicitly preconditions with the inverse of the Fisher information and its adaptive counterpart additionally steers the learning rate according to the empirical variance of the noisy gradients).
Some minor issues:
Eq. 6: one (probably the right one) should be \alpha_{-}
l. 191 Optimized hyperparameters... or Optimization of hyperparameters has...
Fig. 2 (a) and (b) How is it that errors drop for RMSProp and SGD on the one side and HB and Adam on the other at the very same moment? Why does this not apply to Adam (default) or AdaGrad?
Further elaboration in the caption would be useful here.


Originality/Significance
The main finding, that adaptive methods fail to achieve better results in terms of test set error than non-adaptive ones, is of great use for practitioners and encourages the use of simpler, albeit more robust methods.
The insight that adaptive methods are far from appropriately adapting to all hyperparameters should also be a hint when analyzing results of one such method.
","The authors question the value of the adaptive gradient methods in machine learning. They claim that the adaptive methods like AdaGrad, RMSProp, Adam find different solutions than the non-adaptive variants like the Heavy Ball method or the Accelerated Gradient method.  Their numerical experiments demonstrate that although the performances of the adaptive methods improve the objective function value quickly, particularly for the training set, their performances on test sets are worse than those of non-adaptive methods. Thus, the adaptive methods do not generalize well.

To explain this poor performance of the adaptive methods, the authors construct a very smart and informative linear regression example and show that the adaptive method AdaGrad fails terribly for the unseen data, whereas the stochastic gradient descent method works as expected. The authors carefully setup a nice experimental study and demonstrate on a set of different machine learning problems that non-adaptive methods in general outperform the adaptive ones. 

This is a nice work. Especially the part in Section 3, where the authors construct their least-squares example. However, I am not sure whether their findings in this section may generalize to other machine learning problems including the ones in Section 4. Though, the empirical study does indeed show that adaptive methods perform worse than the non-adaptive methods, the link between Section 4 and Section 3 is not clear. I find it surprising that the practitioners have not observed the poor performances of the adaptive methods. The authors also share this feeling as they state in their conclusion.

Overall, this work can be considered as a first step to look deeper into the generalization problems with the adaptive methods. Moreover, the practitioners can benefit from the nice empirical study."
Mean Field Residual Networks: On the Edge of Chaos,"Ge Yang, Samuel Schoenholz",t,"This paper analytically investigates the properties of Resnets with random weights using a mean field approximation. The approach used is an extension of previous analysis of feed forward neural networks. The authors show that in contrast to feed forward networks Resnets do exhibit a sub-exponential behavior (polynomial or logarithmic) when inputs are propagated forward or gradients are propagated backwards through the layers.

The results are very interesting because they give an analytic justification and intuition of why Resnets with a large number of layers can be trained reliably. The paper also extends the mean field technique for studying neural network properties, which is of value for analyzing other architectures.  The presentation of the results is overall clear and while the paper is well written. 

Overall this is an interesting paper with convincing results."
Non-convex Finite-Sum Optimization Via SCSG Methods,"Lihua Lei, Cheng Ju, Jianbo Chen, Michael I. Jordan",https://proceedings.neurips.cc/paper/2017/hash/81ca0262c82e712e50c580c032d99b60-Abstract.html,"Summary:
This paper proposes a variant of a family of stochastic optimization algorithms SCSG (closely related to SVRG), and analyzes it in the context of non-convex optimization. The main difference is that the outer loop computes a gradient on a random subset of the data, and the stochastic gradients in the inner loop have a random cardinality and are not restricted to those participating in the outer gradient. The analysis of the stochastic gradients despite this mismatch is the main technical contribution of the paper. The result is a set of new bounds on the performance of the algorithm which improve on both sgd and variance reduced algorithms, at least in the low-medium accuracy regimes. Experimental evidence supports the claims of improvement in objective reduction (both training error and test error) per iteration both before the end of the first pass on data and after, both by SCSG over SGD and by varying batch size variant over the fixed size variant, but only on two networks applied to the venerable and small MNIST dataset. 

Pros of acceptance:
- The claimed results are interesting and shed light on an important regime.
- The proof method might be more widely applicable.

Cons:
- The experimental section is insufficient for the claims made.

Quality:
Theoretical contribution: 
The main contribution of the paper seems to be an analysis of the discrepancy due to using partial long-term gradients, using the behavior of sampled means without replacement. If novel, this is a useful contribution to our understanding of a trick that is practically important for SVRG-type algorithms.

I’ve spot checked a few proofs in the supplementary material, and found a minor error:

-	Lemma B.3 assumes eta_j*L < 1, but table 2 does not mention L in giving the step sizes.


The empirical evaluation uses only one small and old dataset, which is bad methodology (there are many old small datasets, any reasonable algorithm will be good at one). Other than that, it is well designed, and the comparison in terms of number of gradients is reasonable, but should be complemented by a wall-time comparison as some algorithms have more overhead than others. The paper claims SCSG is never worse than SVRG and better in early stages; this justifies a direct comparison to SVRG (with B = n) for each part. An added plot in the existing comparison for early stages, and also a separate experiment that compares (on a log-log plot) the rates of local convergence to high precision inside a basin of attraction. 

Clarity:
The paper is written quite well: the algorithm is clear including the comparison to previous variants of SCSG and of SVRG, the summary of bounds is helpful, and the presentation of bounds in appropriate detail in different parts. 

Some minor comments:
-	L.28 table 1: 
o	What is “best achievable” referring to in [26,27]? A lower bound? An algorithm proven optimal in some sense? should that be ""best available"" instead?
-	L.30 - 33: these sentences justify the comparison to SGD in a precision regime, but are not very clear. Substituting at the desired regime (eps = n^-1/2 or some such) would make the bounds directly comparable.
-	L.56 - 59 are confusing. Applying variance reduction in the non-convex is not novel. 
-	Algorithm 1, L.2: \subset instead of \in, for consistency use the concise notation of 7.
-	L.106: “In order to minimize the amount the tuning works” something is wrong with that sentence.
-	L.116: “and the most of the novelty” first “the” is probably superfluous.  


Supplementary material:
-	L.8 n should be M
-	L.13 B > 0 does not appear in the rest of the lemma or in the proof. Presume it is \gamma?
-	

Originality:
I leave to reviewers more up to date with the non-convex literature to clarify whether the claimed novelties are such.

Significance:
This paper seems to complete the analysis of the existing SCSG, a practical SVRG variant, to the common non-convex regime. Specifically, showing theoretically that it improves on SGD in early stages is quite important.

-----
Updates made after rebuttal phase:
- The authors addressed the issues I'd raised with the proofs. One was a typo, one was a misinterpretation on my part. There may well still be quality issues, but they appear to be minor. I removed the corresponding con. I think the paper could be an 8 if its empirical evaluation was not so flawed.
","Very interesting result. From theoretical perspective the paper is strong, well organized and self-sufficient. 
The only comment that I have is experimental evaluation. MNIST data set is not the best choice for classification problems with Neural Networks. I strongly recommend to try ImageNet. (http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) ","The authors propose a stochastically controlled stochastic gradient method for smooth non-convex optimization. They prove a convergence rate for such method that is superior to the usual SGD and GD methods in terms of the computational complexity. The theoretical results are interesting and relatively novel. But in order to show the benefits of the algorithm, much more extensive experiments are needed than the ones presented in the paper. The biggest concern regarding the experiments in the paper is that they do not include the comparison to other accelerating methods, e.g. momentum based. The authors merely compare their method to SGD and not to any other method. In my opinion, although the other acceleration techniques are different with the proposed method, it is worth understanding how much benefit each one of them brings to the table and at what cost.
It is interesting that the authors have presented experimental results with respect to the number of gradient evaluations as a surrogate for time. But as the authors it is mentioned in the paper, due to the possibility of vectorized computation the gradient computation might not be a suitable surrogate for time. Therefore, I recommend presenting some experiments with wall clock time as a reference for comparison in addition to the current ones with #gradient evaluations. Although using time as a reference can depend on the implementation and platform, but at the end of the day, it is what is important in practice.
"
First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization,"Aryan Mokhtari, Alejandro Ribeiro",https://proceedings.neurips.cc/paper/2017/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html,"This paper presents algorithms for empirical risk minimization on large datasets using an adaptive sample size to improve bounds on convergence.

The work is similar in spirit to DynaSAGA and AdaNewton, although the emphasis is on first order techniques instead of second order ones presented in Ada Newton. The problem formulation also uses a regularization term.

A theoretical bound is provided for an acceptable accuracy between the empirical and statistical solution in Proposition 1. Furthermore, the required number of iterations s_n is lower bounded.

The empirical results present a comparison of gradient descent, accelerated gradient descent and stochastic variance reduction techniques using the adaptive sample size technique. On RCV1 and MNIST datasets, it is seen that the overall computational cost to reach the optimal solution is significantly reduced. 

How does the choice of the learning rate affect s_n i.e. the number of iterations required to obtain convergence?

No description of the execution environment is presented - is the code reusable?

Minor comments:
1. Supplementary Material, Pg 2, Ln 397: bonded -> bounded
2. There appears to be no theorem 1.
","The paper proposes an adaptive sample size strategy applied to first order methods to reduce the complexity of solving the ERM problems. By definition, the ERM problem respect to a given dataset D is to minimize the average loss over it. Instead of handling the full dataset all at once, the proposed algorithm starts with a small subset of D and first minimize the ERM problem respect to this subset. After reaching the desired statistical accuracy on this small problem, it doubles the size of the subset and update the problem with the new subset. The strategy repeats such procedure until the full dataset is included. The paper shows an improvement both in theoretical analysis and in experiments. 

The paper is very clear, the theoretical proof is correct and well presented. I find the paper very interesting and I like the idea of taking into account the statistical problem behind the ERM problem. A question I have is that the algorithm requires to predefine the statistical accuracy and uses it explicitly as a parameter in the construction of the algorithm, is there any way to adapt it by removing its dependency in the algorithm? Because it is demonstrated in the experiments that the choice of such accuracy do influence the performance. (In MNIST, the performance of taking 1/n is better than 1/sqrt{n}) The current algorithm will stop once the predefine accuracy is attained which is eventually improvable by varying it. 

Besides, I am a bit concerned about the novelty of the paper. As mentioned by the author, there is a big overlap with the reference [12]. The main strategy, including the regularized subproblem and the proposition 1, is the same as in [12]. The only difference is to replace the Newton's method by first order methods and provide the analysis of the inner loop complexity. 

Overall, I find the idea interesting but the contribution seems to be limited, therefore I vote for a weakly accept. 
 

","This paper introduce a adaptive method for setting sample size in stochastic methods or converting deterministic methods to stochastic by using mini batch instead of full batch. To compute the mini_batch they consider the estimation error or generalization error in ERM. The main idea of paper comes from ref [6] and [12] of the paper. So the paper tries to generalize these paper's idea. 
The main question I have is, the V_n is upper bound for estimation error and it depends on the model we use for prediction. So why do you think reaching this level of error is good enough for optimization? what if for the given data set with a rich model we can reach to 0 error? I think this part needs more clarification. 
All the proves are clear and easy to follow. 
I believe the experiments do not show the significance of applying your method: 1- For GD or AGD applying your method is like comparing SGD with incremental mini_batch vs GD which is known is SGD with incremental mini-batch is better. 2- based on ref~[1] using growing mini_batch instead of full batch in SVRG helps in convergence. I'd like to see a comparison of your method with other adaptive mini_batch methods to see the actual benefit of applying your method. 

1-https://arxiv.org/pdf/1511.01942v1.pdf      "
Doubly Stochastic Variational Inference for Deep Gaussian Processes,"Hugh Salimbeni, Marc Deisenroth",https://proceedings.neurips.cc/paper/2017/hash/8208974663db80265e9bfe7b222dcb18-Abstract.html,"This paper presents a doubly stochastic variational inference for deep Gaussian processes. The first source of stochasticity comes from posterior sampling, devised to solve the intractability issue of propagating uncertainty through the layers. The second source of stochasticity comes from minibatch learning, allowing the method to scale to big datasets, even with 1 billion instances.

The paper is in general well written and the authors make an effort to explain the intuitions behind the proposed method. However, the paper is not placed well in the related literature. The ""related work"" section is missing reference to [11] (presumably it was meant to be included in line 65) and the paper is completely missing reference to the very related (Hensman and Lawrence, 2014; Nested Variational Compression in DGPs). The idea of using SVI and mini-batch learning for DGPs has also been shown in (Frigola et al. 2014; Variational GP State-Space Models) and (Damianou, 2015; PhD Thesis).

To further expand on the above, it would be easier for the reader if the authors explained modeling and inference choices with respect to other DGP approaches. For example, using the exact conditional p(f|u) to obtain the cancellation is found in a number of approaches, but in the paper this is somehow presented as a new element. On the other hand, absorbing the noise \epsilon in the kernel is (to my knowledge) unique to this approach, however this design choice is underplayed (same holds for the mean function), stating that ""it is for notational convenience"", although in practice it has implications that go beyond that (e.g. no need to have q(h), h being the intermediate noisy layer). This is counter-intuitive, because these are actually neat ideas and I'd expect them to be surfaced more. Therefore, I suggest to the authors to clearly relate these details to other DGP approaches, to facilitate comprehension and theoretical comparison.

Regarding the technical aspect, I find the method sound and significant. The method is not very novel, especially given (Hensman and Lawrence, 2014), but it does introduce some nice new ideas and shows extensive and impressive results. It is perhaps the first paper promising ""out-of-the-box"" training of DGPs and this is a significant step forward - although given this strong claim it'd be great to also have the code at submission time, especially since it's < 200 lines.

It would be worth discussing or exploring some further properties of the method. For example, what does it imply to absorb the layer noise in the kernel? How does the model construction and sampling for inference affect the uncertainty calibration? 

I find the experiments extensive, well executed and convincing. However, I didn't understand what line 223 means (""we used the input dimension.."") and why can't we see a deeper AEPDGP (does that choice make it fair for one model but unfair for the other?). Since the method is relatively fast (Table 3) and performance improves with the number of layers, this begs the question of ""how deep can you go?"". It'd be great to see a deeper architecture in the final version (in [8] the authors make a similar observation about the number of layers).

Other comments:
- including an algorithm showing the steps of the sampling-based inference would greatly improve clarity
- line 176: Shouldn't f_i^l be f_i^{l-1} before ""(recall...""?
- line 211: It's not clear to me what ""whiten inputs and outputs"" means and why this is done
- line 228: What does ""our DGP falls back to the single-layer GP"" mean? Learning the identity mapping between layers?
- line 264: Reference missing.
- line 311: Does ""even when the quality of the approximation..."" refer to having more inducing points per layer, or is it a hint about the within-the-layer approximation?
- which kernel is used for classification?","The paper addresses the problem of approximate inference in deep GP models. The authors propose to use a Gaussian variational approximation to the posterior distribution over the latent function values at some inducing inputs. This approximation is then adjusted by minimizing the variational loss. The key difference of the proposed work with respect to other previous approaches is that the authors perform sampling to approximation the expected log-likelihood by Monte Carlo. For each data point in a minibatch, the authors sample the corresponding values of the latent functions at the first layer evaluated at that input and then repeat the process in subsequent layers. The advantage of this is that they avoid Gaussian approximations used by previous approaches and they avoid having to assume independence between the different values of the latent functions at different layers for a particular input. The experiments performed illustrate the superior performance of the proposed approach with respect to existing techniques.

Quality

The paper seems to be sound. My only critique is that the authors use models with a very large number of latent functions at each layer of the deep GP network. They use min(30, D) where D is the dimensionality of the original data. In the problems from figure 1 this means that typical values for the number of latent functions are 13, 8, 8, 8, 26, 4, 9 and 22. These are many more functions that necessary and can produce significant overfitting problems in most cases. This can be noticed when one compares the numbers for test log-likelihood reported in Figure 1 with those reported by Bui et al. 2016, which are higher in almost all the analyzed data sets. I would suggest the authors to repeat the experiments using a smaller number of latent functions in the hidden layers of the deep GP network. For example, Bui et al. 2016 use 2 or 3 functions in the hidden layers. Why did the authors not use a smaller number of hidden functions?

Clarity

The paper is clearly written and easy to read.

Originality

The proposed method is original and different from previous work. It is also expected to work better in practice by eliminating the biases of previous approaches.

Significance

The proposed method seems to be the best existing technique for approximate inference in deep Gaussian process models and is therefore very significant.

Minor comments:

- clarify what dimension is in line 169.
- y_n and f_n^L should have subscript i instead of n in equation 16.
","The paper proposes a new variational approximation for deep Gaussian processes (DGPs) that captures some dependence between the latent GP functions across layers. This is based on augmenting each layer-specific GP with inducing variables (as done in the standard sparse variational approximation) and re-using each GP conditional prior (that has as inputs the function values from the previous layer) in the variational distribution. This leads to a quite simple lower bound that involves intractable expectations over the log likelihood terms that can be dealt with during optimization based on the reparametrization trick. The paper is clearly written. 

The variational approximation used in this paper is better than the one from the original DGP paper and at the same time is somehow simpler, in the sense that it leads to a simpler expression for the lower bound. A nice feature of the approach is that the layer-specific noisy corruptions are not necessary for the approximation to work and also arbitrary kernel functions can be easily accommodated.

The experimental comparison is quite comprehensive and it shows that the method is promising. 
Something I didn't see in the experiments is a comparison with the original variational approximation of Damianou and Lawrence. Where are the results of this earlier approximation?     "
From Parity to Preference-based Notions of Fairness in Classification,"Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, Adrian Weller",https://proceedings.neurips.cc/paper/2017/hash/82161242827b703e6acf9c726942a1e4-Abstract.html,"This paper introduces a new notion of fairness in decision-making. While there are already many notions of fairness proposed in past work, I found the proposed definition unique in the sense that it takes an game-theoretic approach to fair decisions. Specifically, the authors propose an envy-free criterion: once the decisions are made, no group of people with a protected attribute would prefer to be (counterfactually) in another group. Both demographic parity and equality of opportunity metrics can be weakened to yield their envy-free equivalents. 

Once defined, the authors define a straightforward way to design a learning algorithm with constraints. The results show that envy-free leads to a lower reduction in accuracy than other stricter forms of fairness. 

Overall, this is a good conceptual addition to the discussion on fairness, making the connections to past work in game theory and economics. ","ML models control many aspects of our life such as our bank decision on approving a loan or a medical centers that use ML to make treatment decision. Therefore, it is paramount that these models are fair, even in cases where the data contains biases. This study follows previous studies in the field but argues that the definition of fairness provided in previous studies might pose a too high bar which results in unnecessary cost in terms of accuracy. Instead they present an alternative fairness definition, presenting an algorithm to apply it for linear classifiers together with some empirical results.

I find this paper very interesting and relevant but lacking in several aspects. I think that it will benefit from some additional depth which will make it a significant contribution. Here are some issues that I find with this paper:
1.	I am not sure that the treatment of the bias is comprehensive enough. For example, if z is a gender feature. A classifier may ignore this feature altogether but still introduce bias by inferring gender from other features such that height, weight, facial image, or even the existence of male/female reproducing organs. I could not figure out from the text how this issue is treated. This is important that in the current definition, if a model never treats a person with male reproducing organs for cancer, this model may be gender-fair. The paragraph at lines 94-100 gives a short discussion that relates to this issue but it should be made more explicit and clear.
2.	There is a sense of over-selling in this paper. The abstract suggests that there is an algorithmic solution but only in section 3 an assumption is added that the classification model is linear. This is a non-trivial constraint but there is no discussion on scenarios in which linear models are to be used.

Some additional comments that the authors may wish to consider:
3.	The introduction can benefit from concrete examples. For example, you can take the case of treating cancer: while male and female should receive equally good treatment, it does not mean that female should be treated for prostate cancer or male should be treated for breast cancer in the same rates.
4.	Lines 66-74 are repetition of previous paragraphs
5.	Line 79: “may” should be “many”
6.	When making the approximations (lines 171-175) you should show that these are good approximation in a sense that a solution to the approximated problem will be a good solution to the original problem. It may be easier to state the original problem as (9) and avoid the approximation discussion.
","This work addresses fairness in machine learning by adopting notions like envy freeness from the fair division literature.  The basic idea is to create classifiers such that no group prefers (""envies"") its treatment under a classifer to some other group, under some sane conditions to ensure the classifier is meaningful.  The paper looks at fairness in two contexts, treatment (roughly speaking, how the decision is made) and impact (roughly speaking, the impact of a decision w.r.t. a specific group).  As with any economic problem, there will be a tradeoff between efficiency and fairness; in the context of this paper, that is the tradeoff between the ""utility"" achieved by the classifier and the ""group benefit"" received by the different groups individually.  The paper directly translates notions like E-F into this language, discusses ways to train classifiers with added E-F constraints, and provides some nice experimental results exploring what unfair, parity-based fair, and their proposed social choice-style fair classifiers do.

In terms of ""core CS,"" the contribution here is not huge.  Indeed, the paper takes a concept from an orthogonal literature (computational social choice) and applies it quite directly to general supervised learning as a hard constraint, and then discusses a reasonable tweak to help with the non-convexness of that new problem.  I absolutely don't see this as a problem, though; indeed, the paper is well-motivated, reads quite nicely, and presents a simple adaptation of a very grounded (from the social choice side) concept to fairness in ML, which is still a pretty brittle and poorly-understood area of ML.  While I don't think group-based preference methods like E-F would be appropriate for all ""fairness in ML"" style applications, I find the method compelling for many situations."
Nonparametric Online Regression while Learning the Metric,"Ilja Kuzborskij, Nicolò Cesa-Bianchi",https://proceedings.neurips.cc/paper/2017/hash/821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html,"      This paper describes a novel algorithm for online nonparametric regression problem. It employs Mahalanobis metric to obtain a better distance measurement in the traditional online nonparametric regression framework. In terms of theoretical analysis, the proposed algorithm improves the regret bound and achieves a competitive result to the state-of-the-art. The theoretical proof is well organized and correct to the reviewer.

However, the novelty of the proposed algorithm may be limited. The overall algorithm consists two separable components: online nonparametric regression with Mahalanobis metric, the estimation of gradient outer-product. 

The first component is a straightforward extension from [7], which proposed a general framework including all kinds of distance metric spaces. In Algorithm 1, it is obvious that the proposed work specializes the metric space as Mahalanobis distance and keep other steps unchanged. Therefore, the statement that the proposed algorithm is a generalized version of the algorithm from [7] in line 244 is improper. This extension is valuable only when the motivation of incorporating Mahalanobis distance is clearly introduced and convincing. Otherwise, this work mainly discusses a special case in the framework of [7]. On the other hand, the significance of the proposed method and the related regret bound should be discussed with more details. Because compared with the original method in [7], the improvement of regret bound depends on the specific problem.

The second component is an application from [14] to estimate the gradient outer-product. The authors should discuss the increased computational complexity brought by this phase.

Some minor issues:
1.	How does one compute the active center efficiently in line 8 of Algorithm 1
2.	$\tilde{\rho}$ in Theorem 2 is not defined. 
3.	What's the relationship between smoother f_0 and Mahalanobis metric?
4.	There is no experimental evaluation to demonstrate the improvement of the proposed method in comparison with the original framework in terms of accuracy or convergence rate.
","This paper presents a theoretical exploration on online regression with simultaneous metric learning. The framework is based on previous work, with modifications such as employing ellipsoid packing and Mahalanobis distance. The use of L_infty norms to cope with potential spectral leaking is also interesting. The authors were well aware of the limitation of their work, and pointed to a few meaningful directions for future work. 
It could be discussed how the modification to the regression algorithm in Algorithm 1 (with ellipsoids' radii shrinking over time) might affect the scalability of their findings. For instance, Line 12 uses a simple averaging step to calculate the prediction. Would it be possible or reasonable to use a weighted average, with the weighting decided by the distances? 
Also, is it possible to apply the scheme given at the end on *regression* problem as well, i.e., using accumulated prediction error within each ball to control the ball radius?  
These may not fall well within the scope of the current paper, but some relevant discussions may be helpful. 

A minor correction: Line 201, ""... is ran"" -> ""... is run""","This paper studies the online nonparametric regression problem. The authors first extend the algorithm of Hazan and Megiddo to accommodate arbitrary pre-specified Mahalanobis distances (i.e. metrics induced by positive definite matrices), and they then study the stochastic setting and propose a method to approximate the norm induced by the expected gradient outer product using an estimator proposed by Trivedi et al, 2014 by applying the previous algorithm over epochs of doubling length, and using the empirical outer product over the previous epoch (plus a regularization term) as the metric for the current epoch. The authors present a guarantee that generalizes Hazan and Megiddo in the static metric setting, and a guarantee in the estimation setting that reflects the approximation error.

The paper is well-written, clear, and presents both a natural and interesting extension of previous work in this setting. It is also a nice combination of previous work.  

Here are some questions and suggestions:
1) On lines 50-52, the authors discuss how the expected gradient outer product matrix is a natural choice for the metric in the stochastic setting. Is it possible to prove that this is actually the best matrix? 
2) On line 120, \eps-covering and \eps-packing numbers should probably be defined precisely.
3) From a readability standpoint, the paper refers to Lemma 4 on line 203 without ever introducing it. In fact, it's not included in the paper at all. It would be good to fix this.
4) The effective rank expression is somewhat technical. Can the authors present some intuitive examples demonstrating situations where the effective rank leads to much better guarantees?
5) On lines 317-318, why is it reasonable to assume that \gamma_n \leq n^{-\alpha} for n large enough for Theorem 2? Can the authors at least provide some compelling examples?
  
"
Working hard to know your neighbor's margins: Local descriptor learning loss,"Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas",https://proceedings.neurips.cc/paper/2017/hash/831caa1b600f852b7844499430ecac17-Abstract.html,"This paper proposes a novel triplet loss for learning local patch descriptors for wide-baseline matching, based on maximizing the distance difference between a matching pair and the closest non-matching patch to either of the two patches from the positive pair. In experimental evaluation, the proposed scheme outperforms other very recent descriptor training methods such as [23,24] and state-of-the-art hand-crafted descriptors such as SIFT and RootSIFT. 

I find the proposed objective function and top-line performance of the method to be promising. However, at present, the paper has two major shortcomings that make it not quite ready for NIPS:

- The presentation of the method in Section 3.1 is sloppy and overly confusing. There are typos, e.g., in the definition of distance in line 63 it should probably say a_i p_j instead of a_i p_i. It is not clear why the authors use an asymmetric notation (a and p, i* and j'). The intuition behind lines 65-66 and eq. 1 is not clearly explained. I find Figure 1 to be somewhat helpful but Figure 2 to be completely mysterious. A more geometric illustration of the quadruplets (a_i, p_i, p_{j'}, a_{i*}) and resulting triplets -- or a clearer explanation with a more streamlined notation -- would be better. 

- There is no proper ablation study comparing the effect of the proposed triplet sampling scheme with other baselines, while keeping all the other elements of the method the same. The experiments are extensive, but they only compare the proposed method with other published systems that may be different in multiple respects. In particular, while L2Net [24] apparently uses the same architecture, its objective function has a completely different form. I would like to see a comparison with the same architecture and basic form of objective function but more ""vanilla"" pair or triplet terms and sampling schemes. Without such an apples-to-apples comparison, it is impossible to say for sure whether it is really the proposed triplet definition or some other implementation details that account for the good performance of the method. If the authors provide such comparisons in the rebuttal, I may be willing to change my mind to favor acceptance.

Post-rebuttal comment:

I agree with R3 that the rebuttal is somewhat persuasive and am raising my rating accordingly. I will not object if this paper is accepted to NIPS, although I still can't help feeling that the authors should have explained their contribution more clearly and better distinguished their work from other methods that are on the surface very closely related. It is also unclear to me whether the proposed hard negative mining technique is only effective for the application of learning local descriptors for patch matching, which would make it of fairly limited interest to the NIPS audience, or whether it can be useful more generally. Once again, I agree with R3 in desiring to see major changes to the paper before it can be published (which, of course, is not possible in the NIPS review process).","The paper presents a variant of patch descriptor learning using neural networks and a triplet loss. While many similar approaches exist, the particular variant proposed here appears to have better results in a large number of benchmarks.

Still, I am really unsure about the technical contribution as the paper is not very clear on this point. The approach appears to be very similar to others such as Balntas 16 that also used triplet losses and deep nets to learn patch descriptors. It seems that the main difference is to consider in the loss hard negative examples. However, Balntas 16 *also* uses hard negatives in the triplets (not merely random samples as argued here on line 46). So what is the difference that changes the empirical results so much?

The paper generally needs far more polish. Experiments should carefully describe the difference between different approaches (e.g. descriptor dimensionally, neural network architecture, training set, and anything else that could make a difference). Then, the main reason for the observed empirical boost should be unequivocally identified through empirical assessment.  For example, if the claim is that the key is to pool hard negatives, a carefully ablation study comparing this to alternative tripled-formation strategies should be included. Part of such a study may be included in the HardTFeat vs TFeat experiment of Fig. 7, but I would liket to check with the authors that they ran both methods using the same code base and only changing the feature sampling strategy. If more than one thing changes at a time, is difficult to reach a conclusion.

If the authors could clearly identify an interesting reason explaining their empirical boost (e.g. they may conclude that the key is a new and better way of doing hard negative mining compared to what has been done so far), then the paper could be interesting enough for acceptance. If, however, the boost is due to other differences such as more tuning or a tweaked architecture, then there would be much less of a reason to accept this into NIPS.

Rebuttal: the authors provided an informative rebuttal.

One of my key question was about the difference with Blantas 16. The authors argue that Blantas 16 does *not* do hard negative mining. On re-reading Blantas 16, this is partially true: their point is that a simplified form of hard negative mining (called in-triplet hard negatives) is just as effective and in fact superior to hard negative mining, tested in e.g. ""Discriminative learning of deep convolutional feature point descriptors"".

Given the new experiments in the rebuttal, it seems that the main idea here is a new variant of negative mining, within each batch rather than in the whole datasets as done in ""Discriminative learning of deep convolutional feature point descriptors"", which seems to be similar to the HNM method described in the rebuttal.

Hence to me the main message of the paper is that, while HB and HNM are very simple and very similar approaches, HB is in practice far better than HNM.  The authors should modify the paper to include a careful experimental analysis of this point, extending and consolidating the new experiments in the rebuttal.

With this and other promised improvements, the paper would be good enough for acceptance in my opinion. However, the modifications required from the submitted version are fairly large.




","This paper proposes an approach to learn local feature descriptors with a CNN. In other word, the point is to defeat the seemingly invincible SIFT [19]. In particular, the goal is to learn a descriptor that is invariant enough while still being discriminative for accurate pairwise matching of interest points. 
The novelty in this paper lies in the usage of a novel loss, whose goal is to maximize the distance between the closest positive and closest negative. The principle is similar to the ""first-to-second nearest distance ratio"" rule used in SIFT to determine good descriptor matches from bad matches.

I find the paper really good. In a nutshell: the paper is well written; the problem is well defined; the idea is novel, simple and effective; experimental comparisons are plentiful, convincing and done on actual CV tasks (image retrieval, wide baseline stereo) in addition to the usual intermediary evaluation on patch matching/descriptor retrieval.

The quality of experiments is at the level of a CVPR paper. There are comparisons with many recent state-of-the-art approaches, on recent datasets, on most matching-based tasks. Results show the consistent superiority of the proposed approach compared to existing methods. What's more, the approach seems easy to replicate and doesn't require additional data for training.

Some minor problems: a bit of opacity in Section 3.1:
   - In eq (1), min(dij,dji) is not clearly defined. I understand that it corresponds to min(d(ai,pj'),d(pi,ai*)) mentioned just above, but the change of notations is confusing.
   - speaking of which, why use a different superscript (prime and star) for i* and j'? They correspond to similar quantities and are defined almost the same way.
   - it is not clear what Figure 2 is showing. After spending some time, I can finally understand, but it would benefit the paper it was better explained."
Hiding Images in Plain Sight: Deep Steganography,Shumeet Baluja,https://proceedings.neurips.cc/paper/2017/hash/838e8afb1ca34354ac209f53d90c3a43-Abstract.html,"The authors present a new steganography technique based on deep neural networks to simultaneously conduct hiding and revealing as a pair. The main idea is to combine two images of the same size together. The trained process aims to compress the information from the secret image into the least noticeable portions of the cover image and consists of three processes: a prep-Network for encoding features, the Hiding Network creates a container image, and a Reveal Network for decoding the transmitted container image. On the positive side, the proposed technique seems novel and clever, although it uses/modifies existing deep learning frameworks and therefore should be viewed as an application paper. The experiments are comprehensive and the results are convincing. 

The technique, however, resembles greatly the image decomposition problem, e.g., for separating mixtures of the intrinsic and reflection layers in previous literatures. I'd hope the authors to clarify how the problems are different and if the proposed technique can be used to resolve the layer separation problem. More important, I wonder if existing approaches on layer separations can be used to directly decode your encrypted results. 

I am also a bit concerned about the practicality of the proposed technique.  First, since the container image will be transmitted and potentially intercepted. It appears that one can tell directly that the container image contains hidden information (the image contains ringing type visual artifacts). if that is the case, the approach is likely to undermine the effort. Second, if the cover and secret images appear similar, the technique may fail to robustly separate them. So a more interesting question is how to pick a suitable cover image for a specific secret image. But such discussions seem largely missing. Third, the requirement the cover and the secrete images should have the same size seems to be a major limitation. One would have to resize the images to make them match, it may increase file sizes, etc. 

Overall, I think the proposed approach is interesting but I have concerns on the practicality and would like to see comparisons with state-of-the-art layer separation techniques. ","The authors propose a method of using deep neural networks to embed a secret image unobtrusively into a carrier image so that it can be transmitted without drawing attention to it. The proposed architecture shares some ideas with auto-encoders.  The network is trained with a joint objective so that the carrier image with the embedded image is similar to the original cover and the reconstructed secret image is similar to the original secret image.  This is a lossy encoding so it is not reconstructed perfectly which may be acceptable for images that won't be subject to forensics. One question that remained for me is the degree to which the reconstructed image is a domain based reconstruction ... is it filling the image from understanding of the domain characteristics? 

The authors anticipate many questions that occur to the reader such as how does this compare to least significant bit encoding, how is the embedding stored and how does it perform on images not in the training set. For instance, the authors show that typical LSB-based steganography detection methods fail on their encoded images. The authors also investigate detectability and possible exploits such as using the residual between the original cover image and the encoded image to try and gain information about the secret. They show that adding a penalty for correlation between this residual and the original secret reduces the informativeness of the residuals. The practical applicability, advances over state of the art in terms of both coding density and detectability combined with the insightful analysis about how the embedding is stored make this paper both practically significant and theoretically enlightening (though there are no formal theorems in the paper).  It is essentially an applied paper that uses existing techniques in a novel way.","This paper tries to hide one full size color image into another of the same size (steganography) using deep neural networks. The framework is relatively simple, and results look encouraging. I have the following major comment:

The author should compare the proposed steganography framework with existing steganography techniques. I understand that existing steganography techniques normally only hide a small message within the noisy regions of a larger image, but the authors should at least show some comparison on those tasks.

Other than that, the frameworks looks simple and clean, and I do think that it could be the new avenue for exploration on steganography."
Lookahead  Bayesian Optimization with Inequality Constraints,"Remi Lam, Karen Willcox",https://proceedings.neurips.cc/paper/2017/hash/83f97f4825290be4cb794ec6a234595f-Abstract.html,"The authors address the problem of Bayesian optimization (BO) with inequality constraints. All BO algorithms for constrained optimization using a myopic strategy in which they just optimize the expected utility of the next immediate evaluation. The authors proposed to consider instead multiple look ahead steps. In this case, computing the optimal next evaluation location requires to solve an intractable dynamic programming problem. To simplify the complexity of the approach, the authors consider a roll out strategy in which a specific policy is applied at each step to select the next evaluation location as a function of just the current state. The proposed policy optimizes the expected improvement with constraints obtained by the next immediate evaluation. Integration over the possible outcomes of each function evaluation is performed numerically using Gauss-Hermite weights. The experiments show that the proposed approach can produce small gains when considering a 1-step look ahead setting.

","This paper seems a continuation of last year: Bayesian optimization with a finite budget... where the authors have added new elements to deal with inequality constraints. The method uses a approximation of a lookahead strategy by dynamic programming. For the constrained case, the authors propose an heuristic that combines the EIc criterion for all the steps except for the last one were the mean function is used. The authors claim that the mean function has an exploitative behaviour, although it has been previously shown that it might be misleading [A].
      A considerably amount of the text, including Figure 1, can be mostly found in [16]. Although it is nice to have an self-contained paper as much as possible, that space could be used to explain better the selection of the acquisition heuristic and present alternatives. For example, the experiments should show the result a single-step posterior mean acquisition function, which will correspond to h=0 as a baseline for each of the functions/experiments.
      Concerning the experiments, there are some details that should be improved or addressed in the paper:
      - Why P2 does not include h=3?
      - How is it possible that EIc for P2 goes upwards around n=25?
      - The plots only shows median values without error bars. I can understand that for that, for such number of repetitions, the error bars might be small, but that should be adressed. Furthermore, a common problem of lookahead methods is that, while most of the time outperform greedy methods, they can also fail catastrophically for selecting the wrong path. This would results in some lookahead trials resulting in poor results. Using the average instead of the median would show also the robustness of the method.
      - The number of iterations is fairly small for the standard in Bayesian optimization. In fact, it can bee seen that most methods have not reach convergence at that point. This is specially problematic in P1 and P4 where the plots seems to cross exactly at the end of experiments,
      - It is unclear why the performance decreases after h=2. If possible, the authors should provide an intuition behind that results. Furthermore, the fact that for most of the experiments h=1 is the optimal strategy seems to indicate that the whole dynamic programing and heuristics might be excessive for CBO, or that more complex and higher-dimensional problems are need to illustrate the benefits of the strategy.
      
      [A] Jones, Donald R. ""A taxonomy of global optimization methods based on response surfaces."" Journal of global optimization 21.4 (2001): 345-383.","This work proposes a new method for Bayesian with restrictions in which the policy for collecting new evaluations is none myopic, in the sense that it takes into account the impact of the next evaluation in the future behavior of the policy. Both problems are important in Bayesian optimization and haven't been studied considered together, to the best of my knowledge.

The paper is very well written and structured. The problem is interesting and the adaptation of rollout for this scenario is well executed. 

My only comment is about the experimental section and how the proposed policy has been used. A 'pure' non-myopic policy should ideally consider at each step of the optimization as many look-aheads as the number of remaining evaluations that are available.  In their experiments, the authors only consider a maximum of 3 look-ahead steps which makes difficult to evaluate how the policy will perform in the pure non-myopic case. If the reason for this is the computational complexity of the policy, an analysis of  the limitations of this method should be described in the paper. If this is not the case, and the policy can be computed in cases with many steps ahead, the authors should include these results in the paper.

Apart form this point that I think would improve the quality of this work, I think that this is a good paper.


"
Online Learning with Transductive Regret,"Mehryar Mohri, Scott Yang",https://proceedings.neurips.cc/paper/2017/hash/8420d359404024567b5aefda1231af24-Abstract.html,"The paper considers the classic setting of prediction with expert advice and proposes a new notion of regret called ""transductive regret"", which measures the performance of the learner against the best from a class of finite-state transducers. Precisely, a transducer will read the history of past predictions of the learner and output a distribution over the next predictions; for forming the next prediction, the learner can take advantage of the class of transducers under consideration. The resulting notion of regret generalizes a number of known settings such as external and internal regret, swap regret and conditional swap regret. The main contribution is proposing an algorithm that achieves a regret of at most sqrt(E*T), where E is bounded by the number of transitions in the considered class of transducers. The authors also extend their algorithm to other settings involving time-selection functions and sleeping experts.

The paper is very technical and seems to target a rather specialized audience. The considered setting is very general and highly abstract---which is actually my main concern about the paper. Indeed, the authors fail to motivate their very general setting appropriately: the only justification given is that the new notion generalizes the most general related regret notion so far, that of conditional swap regret. There is one specific example in Section 4.2 that was not covered by previous results, but I'm not sure I understand why this particular regret notion would be useful. Specifically, I fail to see how exactly is the learner ""penalized"" by having to measure its regret against a restricted set of experts after having picked another expert too many times---if anything, measuring regret against a smaller class just makes the problem *easier* for the learner. Indeed, it is trivial to achieve bounded regret in this setting: just choose ""b"" n times and ""a"" in all further rounds, thus obtaining a regret of exactly n, independently of the number of rounds T. The paper would really benefit from providing a more convincing motivating example.

The paper is easy to read for the most part, with the critical exception of Section 4.1 that describes the concept of weighted finite-state transducers (WFSTs). This part was quite confusing for me to read, specifically because it is never explicitly revealed that in the specific online learning setting, labels will correspond to experts and the states will remain merely internal variables of WFSTs. Actually, the mechanism of how a WFST operates and produces output labels is never described either---while the reader can certainly figure this out after a bit of thought, I think a few explanatory sentences would be useful to remove ambiguity. The fact that the letters E and T are so heavily overloaded does not help readability either.

The analysis techniques appear to be rather standard, with most technical tools apparently borrowed from Blum and Mansour (2007). However, this is difficult to tell as the authors do not provide any discussion in the main text about the intuition underlying the analysis or the crucial difficulties that needed to be overcome to complete the proofs. The authors also provide some computational improvements over the standard reduction of Blum and Mansour (2007) and its previous improved variants, but these improvements are, in my view, minor. Actually, at the end of Section 5, the authors claim an exponential improvement over a naive implementation, but this seems to contradict the previous discussion at the beginning of the section about the work of Khot and Ponnuswami (2008).

I would also like to call the work of Maillard and Munos (ICML 2011) into the authors' attention: they also study a similar notion of regret relative to an even more general class of functions operating over histories of actions taken by the learner. This work is definitely relevant to the current paper and the similarities/differences beg to be discussed.

Overall, while I appreciate the generality of the considered setting and the amount of technical work invested into proving the main results, I am not yet convinced about their usefulness. Indeed, I can't really see the potential impact of this work in lack of one single example where the newly proposed regret notion is more useful than others. I hope to be convinced by the author response so that I can vote for acceptance---right now, I'm leaning a little bit towards suggesting rejection.

Detailed comments
=================
052: Notation is somewhat inconsistent, e.g., the loss function l_t is sometimes in plain typeface, sometimes bold...
055: Missing parens in the displayed equation (this is a recurring error throughout the paper).
093: Is it really important to have O(\sqrt{L_T}) regret bounds for the base learner? After inspecting the proofs, requesting O(\sqrt{T}) seems to be enough. In fact, this would allow setting alpha = 1/\sqrt{T} artificially for every base learner and would make the statement of all theorems a bit simpler.
099: ""t is both the number of rounds and the number of iterations""---I don't understand; I thought the number of iterations was \tau_t = poly(t)?


Comments after rebuttal
=======================

Thanks for the response. I now fully appreciate the improvement in the results of Section 5, and understand the algorithmic contribution a little better. I still wish that the paper would do a better job in describing the algorithmic and proof techniques, and also in motivating the study a little better. I'm still not entirely sold on the new motivating example provided in the response, specifically: how does such a benchmark ""penalize"" a learning algorithm and how is this example different from the one by Mohri and Yang (2014) used to motivate conditional swap regret? 

I raise my score as a sign of my appreciation, but note again that the authors have to work quite a bit more to improve the presentation in the final version.","There are multiple interesting contributions in this paper.  First, the algorithm FastSwap matches a swap regret bound with better per-iteration complexity. Second, the transductive regret setting is introduced along with multiple algorithms for the setting, composing with time-selection and sleeping regret settings.  The exposition is extremely intelligible, the execution thorough, and the results interesting.

I suspect the inequality on the first line after line 352 is incorrect, but then the inequality on the second line after line 352 looks correct.  Also, why switch between L1-norm notation (block after line 348) and an explicit sum over i (block after line 350)?  I feel like Harrison Bergeron. Thankfully the blocks after line 371 and line 375 both use L1-norm notation, but now there is an extra subscripting-i in the second term (?) which is apparently copy-pasted along.  Typos aside, I think the proof technique is sound: hitting with the stochastic row is at most the maximum, which is bounded by the best fixed choice plus the slave algorithm regret, and the latter is amenable to Jensen's inequality; furthermore the L1 distance is additive to the regret and can be bounded via RPM without incurring the full O(N^3).","The paper presents a new notion of regret called transductive regret that generalizes existing regrets. The key is that the authors allow an arbitrary set of weighted finite-state transducers, which allows flexible design of 'swaps' and generalizes external, internal, swap, and conditional swap regrets. The authors first present an efficient algorithm for swap regret, which makes use of a recent development in power iteration methods called RPM. The computational benefit comes with a mild assumption, but the authors successfully defend it by resorting their algorithm back to the standard one, which allows being 'never slower than prior work'. Next, this core technique is repeatedly applied to enjoy low regret in transductive regret, time-selection transductive regret, and sleeping transductive regret.

Overall, the paper is easy to read. The paper sets up a powerful generalization of existing regrets and present algorithms that are efficient, which I believe provides enough novelty.

On the other hand, the computational benefit is only 'fast when it can' and I am not sure how much benefit one gets in practice. I would love to see at least in a toy experiments what kind of alpha values one can expect.

Details:
* I hope to see some motivations for moving beyond the popular external regret. I know this topic has been studied for a while, but still not intuitive and needs some work to broaden the audience. At least, the authors could refer to a few papers that discuss the benefits and applications of these swap-based regrets.
* L126: what is \Sigma^*?
* L120: the acronym WFST is not defined. could just put the definition and save reader's time.
* L256: ""arbitrary losses"" -> in what sense? beyond linear loss function?"
Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex,"Chaobing Song, Shaobo Cui, Yong Jiang, Shu-Tao Xia",https://proceedings.neurips.cc/paper/2017/hash/84b20b1f5a0d103f5710bb67a043cd78-Abstract.html,"Paper Summary:
The main idea is that Nesterov's acceleration method's and Stochastic Gradient Descent's (SGD) advantages are used to solve sparse and dense optimization problems with high-dimensions by using an improved GCD (Greedy Coordinate Descent) algorithm. First, by using a greedy rule, an $l_1$-square-regularized approximate optimization problem (find a solution close to $x^*$ within a neighborhood $\epsilon$) can be reformulated as a convex but non-trivial to solve problem. Then, the same problem is solved as an exact problem by using the SOTOPO algorithm. Finally, the solution is improved by using both the convergence rate advantage of Nesterov's method and the ""reduced-by-one-sample"" complexity of SGD. The resulted algorithm is an improved GCD (ASGCD=Accelerated Stochastic Greedy Coordinate Descent) with a convergence rate of $O(\sqrt{1/\epsilon})$ and complexity reduced-by-one-sample compared to the vanilla GCD.

Originality of the paper:

The SOTOPO algorithm proposed, takes advantage of the l1 regularization term to investigate the potential values of the sub-gradient directions and sorts them to find the optimal direction without having to calculate the full gradient beforehand. The combination of Nesterov's advantage with SGC advantage and the GCD advantage is less impressive. Bonus for making an efficient and rigorous algorithm despite the many pieces that had to be put together

Contribution:
-Reduces complexity and increases convergence rate for large-scale, dense, convex optimization problems with sparse solutions (+),
-Uses existing results known to improve performance and combines them to generate a more efficient algorithm (+),
-Proposes a criterion to reduce the complexity by identifying the non-zero directions of descent and sorting them to find the optimal direction faster (+),
-Full computation of the gradient beforehand is not necessary in the proposed algorithm (+),
-There is no theoretical way proposed for the choice of the regularization parameter $\lambda$ as a function of the batch size. The choice of $\lambda$ seems to affect the performance of the ASGCD in both batch choice cases (-).

Technical Soundness: 
-All proofs to Lemmas, Corollaries, Theorems and Propositions used are provided in the supplementary material (+),
-Derivations are rigorous enough and solid. In some derivations further reference to basic optimization theorems or Lemmas could be more en-lighting to non-optimization related researchers (-).

Implementation of Idea: 
The algorithm is complicated to implement (especially the SOTOPO part).


Clarity of presentation: 
-Overall presentation of the paper is detailed but the reader is not helped to keep in mind the bigger picture (might be lost in the details). Perhaps reminders of the goal/purpose of each step throughout the paper would help the reader understand why each step is necessary(-),
-Regarding the order of application of different known algorithms or parts of them to the problem: it is explained but could be more clear with a diagram or pseudo-code (-),
-Notation:  in equation 3, $g$ is not clearly explained and in Algorithm 1 there are two typos in referencing equations (-),
-For the difficulty of writing such a mathematically incremental paper, the clarity is at descent (+).

Theoretical basis: 
-All Lemmas and transformations are proved thoroughly in the supplementary material (+),
-Some literature results related to convergence rate or complexity of known algorithms are not referenced (lines 24,25,60,143 and 73 was not explained until equation 16 which brings some confusion initially). Remark 1 could have been referenced/justified so that it does not look completely arbitrary (-),
-A comparison of the theoretical solution accuracy with the other pre-existing methods would be interesting to the readers (-),
-In the supplementary material in line 344, a $d \theta_t$ is missing from one of the integrals (-).

Empirical/Experimental basis: 
-The experimental results verify the performance of the proposed algorithm with respect to the ones chosen for comparison. Consistency in the data sets used between the different algorithms, supports a valid experimental analysis (+),
-A choice of better smoothing constant $T_1$ is provided in line 208 (+) but please make it more clear to the reader why this is a better option in the case of $b=n$ batch size (-),
-The proposed method is under-performing (when the batch size is 1) compared to Katyusha for small regularization $10^{-6}$ and for the test case Mnist while for Gisette it is comparable to Katyusha. There might be room for improvement in these cases or if not it would be interesting to show which regularization value is the threshold and why. The latter means that the algorithm proposed is more efficient for large-scale problems with potentially a threshold in sparsity (minimum regularization parameter) that the authors have not theoretically explored. Moreover, there seems to be a connection between the batch size (1 or n, in other words stochastic or deterministic case) and the choice of regularization value that makes the ASGCD outperform other methods which is not discussed (-).

Interest to NIPS audience [YES]: This paper compares the proposed algorithm with well-established algorithms or performance improvement schemes and therefore would be interesting to the NIPS audience. Interesting discussion might arise related to whether or not the algorithm can be simplified without compromising it's performance.
","This paper aims to solve L1 regularized ERM problem. The developments in this paper seem to be motivated by the desire to combine several successful techniques into a single algorithm: greedy coordinate descent, SVRG and acceleration. Accelerated SVRG is known as Katyusha and hence the main task is to combine Katyusha (which would randomize over a minibatch of examples n) with greedy coordinate descent (which would update a subset of the d feature vectors in a “greedy” fashion). The way this is done in this paper is interesting as the solution is surprisingly simple and effective. 

The authors observe (by citing older literature) that without L1 regularization, and in the batch setting, greedy coordinate descent can be expressed as a gradient descent step if instead of the standard L2 norm in the upper bound one used the L1 norm (one also needs to change the scaling/Lipschitz constant). By adding the L1 regularizer, this property is lost, and a subset of variables might be updated. However, the resulting method could still be interpreted as a variant of greedy coordinate descent. This strategy is then combined with Katyusha and the result is an accelerated (via Katyusha momentum), stochastic (over n) and greedy (over d) method. The authors show that in some settings, the resulting complexity can beat that of Katyusha itself.

It is important the the authors are able to devise a fast method for solving the subproblems involved. The key subproblem, (3), is solved via a novel, nontrivial and efficient method called SOTOPO. The starting point here is a variational reformulation of the squared L1 norm as a convex minimization problem over the unit simplex. The authors then write the problem as a min-min problem in the original and auxiliary variables. Switching the order of taking the min ultimately leads to an efficient method for solving (3). This seems of independent interest, which is good.

I like the paper. It is well written. It presents some interesting novel ideas, leads to an efficient method, works in practice in several regimes (interestingly, for both n > > d and n < < d regimes, although this should be investigated in more detail), and also leads to improved complexity for the L1 regularized ERM problem in certain regimes.


Some comments:

1)	Line 31: There are some earlier contributions to accelerated randomized coordinate descent than [11, 20]. The first is due to tNesterov [2012], but suffered from expensive iterations. This was remedied by Lee and Sidford [arXiv: 1305.1922] and Fercoq & Richtarik [arXiv: 1312.5799]. Further improvements were made with the introduction of nonuniform probabilities into accelerated randomized coordinate descent. This was done by Qu and Richtarik [arXiv: 1412.8060] and then extended to strongly convex setting by Allen-Zhu, Qu, Richtarik and Yuan [arXiv: 1512.09103], and later independently by Nesterov and Stich [2016].

2)	The setup of this paper reminds me of a similar synthesis of two methods: SVRG and randomized coordinate descent. This was done in Konecny, Qu & Richtarik [arXiv:1412.6293]. The difference here is that their method is not accelerated (Katyusha did not exist then), and instead of greedy, they use randomized coordinate descent. I am wondering what the connections are.

3)	Regarding experiments: the APPROX method of Fercoq and Richtarik [arXiv:1312.5799] should be included. This is a batch method in n, but capable to randomize over subsets of the d variables, and capable of utilizing sparsity in the data. This method should do very well in the high d and low n regime.

4)	Explain the effect of \eta on the sparsity level in subproblem (3) – even with lambda = 0. Clearly, if eta is small enough, the bound used upper bounds the standard quadratic approximation. In such a case, SOTOPO is not needed; and Katyusha applies directly. There is a thin line here: it will be useful to comment on L1 vs L2 and so on in light of this.


Some minor comments:

1)	What are  (???) in Step 1 of Alg 1?
2)	19 and 20: are referred to -> refer to
3)	19: that uses some algebra trick to -> using an elusive algebra trick to
4)	20: samples -> sample
5)	27: resulted -> resulting  {this appears in many places in the paper, such as lines 51, 56, …}
6)	29: reduces -> reduce
7)	38: preferable than -> preferable to
8)	40: GSD has much -> GCD to have much
9)	50: entries -> entry
10)	55: sqoximation -> approximation
11)	88: is -> are
12)	99: While -> Since
13)	121: of the -> of

=== post rebuttal feedback ===

I am keeping my decision. ","This paper proposes a new greedy coordinate descent type algorithm. The algorithm uses a novel Gauss-Southwell coordinate selection rule, where a convex minimization problem involving the l_1-norm squared is used to select the coordinates to be updated. This coordinate selection strategy is interesting, and it has been developed with practicality in mind, which is important. The authors have also incorporated an acceleration strategy, and presented the ASGCD algorithm (Accelerated Stochastic Greedy Coordinate Descent). 
Greedy coordinate selection schemes are very popular at the moment, and I think there will be many authors interested in this strategy. However, there seem to be quite a few typos in the paper, and the standard of English should be improved. I suggest that the authors thoroughly proofread the papers to correct these errors. Also, the numerical experiments seem to support the proposed algorithm but the problems being tested are quite small, and it would have been good for the authors to have evaluated the algorithm on some large-scale problems.

"
Reinforcement Learning under Model Mismatch,"Aurko Roy, Huan Xu, Sebastian Pokutta",https://proceedings.neurips.cc/paper/2017/hash/84c6494d30851c63a55cdb8cb047fadd-Abstract.html,"The paper tackles the robust MDP setting, where the problem being solved lies within some set of possibilities, and the goal is to obtain a policy that does well in the worst case.  In particular, the paper starts from the (known) robust Bellman equation and derives a number of model-free algorithms (analogs to Q-learning, SARSA, TD-learning, LSTD, GTD, and more, many with convergence guarantees in the robust MDP setting. The paper itself contains the results of a single experiment with robust Q-learning, with more in the supplemental materials.

---Quality---

This paper presents a *lot* of theoretical results -- the supplemental material is a full version of the paper and is 24 pages long! I cannot say that I have evaluated it all in full detail. That said, it does seem to me that the principle ideas that underly the new derivations are sensible and the conclusions seem reasonable.

I think the empirical evaluation falls a little short. In the main paper, only one experiment with one algorithm is presented, and the results are barely discussed. I understand that space is tight, and the paper is packing a lot in. Still, I encourage the authors to try to fit a little more analysis of the experimental results in. The reader needs a little help to pull out the high points from those plots. Even in the appendix, there are more experiments, but only using Q-learning and again, there is very little in the way of interpretation of the results. All that said, I do think the results are enough to provide evidence that robust Q-learning is doing something interesting and maybe that's all that can fit in this paper.

---Clarity---

This paper is very dense, as it is essentially a survey of all of model-free reinforcement learning, but adding the robustness wrinkle at every step. Given that, I think the authors have done a good job of organizing and presenting the main ideas. I appreciated the concise intuitive descriptions of the role of each result in the broader theoretical framework. 

Minor points:

Am I missing something or does the symbol used for the discount factor change partway through the paper? It's not a big deal, but consistency would be better.

Labels on the figures are *far* too small to read.

---Originality---

As far as I am aware this paper's exploration of model-free robust RL is novel. Of course the analysis of these algorithms relies quite a bit on existing results, but I felt the paper did a good job of contextualizing its contributions.

---Significance---

I think these results are interesting and of potential practical importance. The motivating problem of training an agent on a simulator that is suspected not to be a perfect match with the environment in which the agent will be evaluated is completely plausible. In some ways I find these results more compelling than the model-based study of robust-RL, where it is often assumed that the agent knows quite a bit about the uncertainty over the model, an assumption I often find implausible. Here the agent is simply assuming that the training environment is incorrect and imposes *it's own* assumption about the uncertainty over the dynamics. Obviously its later success depends on whether that model of uncertainty was adequate. This makes a lot of sense to me!","Overview:

The authors propose an extension of Robust RL methods to model-free applications. Common algorithms (SARSA, Q-learning, TD(lambda)) are extended to their robust, model-free version.

The central point of the paper is replacing the true and unknown transition probabilities  P_i^a with a known confidence region U_i^a centered on an unknown probability p_i^a. Transitions according to p_i^a are sampled via a simulator. The robustness of the algorithms derive then from an additional optimization step in U_i^a. The authors clarify that, since the addition of term U to p must not violate the probability simplex. However, since p_i^a is unknown, the additional optimization is actually performed on a term U_hat that ignores the above restriction. This mismatch between U and U_hat, represented by a term beta, determines whether the algorithms converge towards to an epsilon-optimal policy. The value of epsilon is also function of beta. 

The authors propose also an extension of RL with function approximation architectures, both linear and nonlinear. In the linear case, the authors show that a specific operator T is a contraction mapping under an opportune weighted Euclidean norm. By adopting the steady state distribution of the exploration policy as weights, and by adopting an assumption from previous literature concerning a constant alfa < 1, the authors prove that, under conditions on alfa and beta, the robust operator T_hat is a contraction, and that the solution converges close to the true solution v_pi, where the closeness depends again on alfa and beta. 

Furthermore, the authors extend the results of Ref.6 to the robust case for smooth nonlinear approximation architectures. An additional assumption here is to assume a constant confidence bound U_i^a = U for all state-action pairs for the sake of runtime. The authors extend on the work of Ref.6 by introducing the robust extension of the mean squared projected bellman equation, the MSRPBE, and of the GTD2 and TDC update algorithms, which converge to a local optima given assumptions on step lengths and on confidence region U_hat. 

In conclusion, the authors apply both nominal and robust Q-learning, SARSA and TD-learning to several OpenAI gym experiments. In these experiments, a small probability of transitioning to a random state is added to the original models during training. The results show that the best robust Q-learning outperforms nominal Q-learning. However, it is shown that the performance of the robust algorithms is not monotone with p: after increasing for small values of p, the advantage of the robust implementation decreases as U and U_hat increase in discrepancy.

Evaluation:

- Quality: the paper appears to be mathematically sound and assumptions are clearly stated. However, the experimental results are briefly commented. Figures are partial: of the three methods compared, Q-learning, SARSA and TD-learning, only results from Q-learning are briefly presented. The algorithms of section 4 (with function approximation) are not accompanied by experimental results, but due to the theoretical analysis preceding them this is of relative importance. 

- Clarity: the paper is easy to read, with the exception of a few symbolic mismatches that are easy to identify and rectify. The structure of the paper is acceptable. However, the overall clarity of the paper would benefit from extending section 4.2 and 5, possibly by reducing some portions of section 3. 

-  Originality: the model-free extensions proposed are an original contribution with respect to existing literature and to NIPS in particular.

- Significance: the motivation of the paper is clear, as is the contribution to the existing theoretical research. Less clear is the kind of application this work is intended for. In particular, the paper assumes the existence of a simulator with unknown transition probabilities, but known confidence bounds to the actual model being simulated, to obtain samples from. While this assumption is not by itself wrong, it is not clearly mentioned in the abstraction or in the introduction and takes the risk of appearing as artificial. 

Other comments:
- Line 194: the definition of beta given in this line is the one Lemma 4.2 rather than the one of Lemma 3.2;
- Line 207: here and in the following, nu is used instead of theta to indicate the discount factor. Please make sure the symbology is consistent;
- Line 268: the sentence ""the state are sampled with from an"" might contain a typo;
- Eq. 7-8: there appears to be an additional minus signs in both equations.  

","Summary:
This paper claims to extend the theory of robust MDPs to the model-free reinforcement learning setting. However that claim ignores relevant prior work (see originality comments below). The paper also presents robust versions of Q-learning, SARSA, and TD-learning.

Quality:
As far as I can tell the main paper is technically sound, but I did not carefully read through the appendix. However the claim that this paper extends the theory of robust MDPs to the model-free reinforcement learning setting is incorrect.

Clarity:
On the one hand this paper is well written and reasonably easy to understand. On the other hand, the organization of the supplementary material is quite confusing. Rather than an appendix, the supplementary material is a longer version of the original paper. What is particularly confusing is that the numbering of lemmas and theorems, and even their statement differs between the main paper and supplementary material.

Originality:
The theory of robust MDPs has already been extended to the model-free reinforcement learning setting, see for example [1]. In effect, the main contribution of this paper is not original. On the other hand, as far as I know this is the first time anyone has derived robust versions of Q-learning, SARSA, and TD-learning.

Significance:
I expect robust versions of Q-learning, SARSA, and TD-learning to be useful for future research. It would be nice to see a comparison between the methods proposed in this paper and existing robust MDP methods for model-free reinforcement learning.

Minor comments:
The fonts in figure 1 are way too small to be readable, and it's very hard to differentiate between the nominal and robust graphs when printed on black and white.

[1] RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning, Marek Petrik, Dharmashankar Subramanian, Conference on Neural Information Processing Systems 2014
"
Concrete Dropout,"Yarin Gal, Jiri Hron, Alex Kendall",https://proceedings.neurips.cc/paper/2017/hash/84ddfb34126fc3a48ee38d7044e87276-Abstract.html,"The author formulates dropout as a variational bayes approach (as is also done in a few recent papers). This provides a theoretically grounded way of optimizing the amount of noise per layers. Previous work uses gaussian noise to be able to use reparametrization trick. In this work, a smooth relaxation of the categorical distribution, known as concrete variable or gumbel softmax is used to reduce variance of the gradient. The final algorithm is easy to implement and can be added to any existing architecture at almost no cost. 

The experimental results shed insights on the amount of noise required with respect to 1) the number of samples 2) the size of each layers 3) the position of the layer in the network. 

Overall, the paper is well written except for section 3, the most important part. 

1) The prior is not specified in the paper and referred to the appendix. In the appendix, there is no explicit equation of the prior.

2) Please use a less compact formulation for the posterior (instead of diag and Bernoulli^{K_l}). It could be rewritten slightly differently and be more obvious to many reader how it relates to conventional dropout.

3) in equation 3, $l$ is undefined. I'm assuming it has nothing to do with the index of the layer. Also, since the prior is not really defined, it's hard to verify this equation. Please specify the prior explicitly and write more steps on how to obtain this equation.

4) Equation 5: I thought that gumbel noise was -log(-log(u)), not +log(u). I might have misunderstood something somewhere. If this is indeed the wrong equation for gumbel noise, please rerun experiments (it should not change much the end-result).
 
5) The lack of comparison with [23] is disappointing and the justification for doing so is not convincing.

6) I would have loved to see experimentations with a per-weight noise parameter as is done in [23]. Can the author explain why this experiment is not in the paper?

==== post discussion ===

Overall the paper is good. Please spend the time to make the paper more clear about the issues discussed by the reviewers. ","This paper attempts to incorporate the dropout rate of Bayesian neural network dropout into the standard gradient based optimization routine. The focus is in relating the dropout rate, p, with a Bernoulli likelihood. To achieve the optimization of p, this is relaxed into the continuous domain through the Concrete distribution, which in this case has a simple sigmoidal form. Experiments test the method in terms of uncertainty modeling and boost in predictive accuracy.

In general this is a nicely written paper, but the presentation can be improved further. I did enjoy the intuitive parts, but perhaps they should be a little reduced to make space for some more technical details. For example, the prior p(\omega) in Section 3 and expanding on the form and derivation of eq. (5). I also think it would be better to only refer to ""epistemic"", ""aleatoric"" uncertainty in Section 2 and onwards (eg. Fig 1) switch to referring to the proper quantity measured.

Technically, this seems like a well motivated work, although I it seems that relaxing the discrete p into (5) moves the approach closer to something like [23]. I was hoping to see a comparison and I do not understand why it has been omitted, even if it is known to underperform. As far as novelty is concerned, this work builds heavily on work by Gal et al. and (inference-wise) moves towards the direction of [23], to some extent. Having said that, there are a few important novel elements, in particular the ability to optimize p as well as some intuitive justifications.

The experiments are quite extensive and the code is given at submission time, which is great. In general, the new method doesn't seem to offer a significant boost, but by being robust means that it can speed up experiment time. It would be nice to see some more quantitative proof that the uncertainty is better calibrated, e.g. in active learning (the RL experiment was nice). Finally, the paper would be very much improved if the authors could compare the overall approach to that of tuning the dropout rate with Bayesian Optimization (which is a very common strategy).

Some smaller comments:
- in line 107- ..., this is implying that w and p have to co-adapt during optimization?
- eq. (3): please state clearly what is K
- Plots 1b, 1c actually look very similar to me, as opposed to what is stated that one has a trend and the other is flat.
- figure 1d is very interesting.
- Fig 2: what happens when the error-bar collapses? Why does that happen? Also, how is the standard ""Dropout"" method tuning p?
- lines 250-...: From this argumentation I can still not see why the probabilities collapse to zero.

EDIT after rebuttal: ====
I have read the reply by the authors. I believe this will be an interesting read, especially once the comments raised by the reviewers (concerning clarity etc) are addressed.","This paper proposes an approach for automatic tuning of dropout
probabilities under the variational interpretation of dropout as a Bayesian approximation. Since the dropout probability characterizes the overall
posterior uncertainty, such tuning is necessary for calibrated inferences, 
but has thus far required expensive manual tuning, e.g., via offline grid 
search. The proposed method replaces the fixed Bernoulli variational distribution 
at training time with a concrete/gumbel-softmax distribution, allowing use of the
reparameterization trick to compute low-variance stochastic gradients
of the ELBO wrt dropout probabilities. This is evaluated on a range of tasks
ranging from toy synthetic datasets to capturing uncertainty in semantic 
segmentation and model-based RL.

The motivation seems clear and the proposed solution is simple and
natural, combining dropout Bayesian inference with the concrete
distribution in the obvious way. The technical contributions of the
paper seem relatively minimal, but this is not necessarily a problem;
it's great when a simple, straightforward method turns out to
give impressive results. 

That said, given the straightforwardness of the approach, the
significance of this paper rests heavily on the evaluation, and here I
am not really convinced. Although there are a lot of experiments, none
seems to show a compelling advantage and collectively they fail to
answer many natural questions about the approach. For example:
 - what is the training time overhead versus standard dropout? (line
   281 says 'negligible' but this is never quantified, naively I'd
   expect differentiating through the gumbel-softmax to add
   significant overhead)
 - do the learned dropout probabilities match the (presumably optimal)
   probabilities obtained via grid search?
 - how to set the temperature of the concrete distribution (and does
   this matter)? The code in the appendix seems to used a fixed t=0.1
   but this is never mentioned or discussed in the paper.
 - how does this method compare to other reasonable baselines, eg
   explicit mean-field BNNs, or Gaussian dropout with tuned alpha? The
   latter seems like a highly relevant comparison, since the Gaussian
   approach seems simpler both theoretically and in implementation (is
   directly reparameterizable without the concrete approximation), and
   to dismiss it by reference to a brief workshop paper [28] claiming
   the approach 'underperforms' (I don't see such a conclusion in
   that paper even for the tasks it considers, let alone in any
   more general sense) is lazy.
 - there is a fair amount of sloppiness in the writing, including
   multiple typos and cases where text and figures don't agree (some
   detailed below). This does not build confidence in the
   meticulousness of the experiments.
More specific comments: 
Synthetic data (sec 4.1): I am not sure what
to conclude from this experiment. It reads as simply presenting the
behavior of the proposed approach, without any critical evaluation of
whether that behavior is good. Eg, Fig 1b describes aleatoric
uncertainty as following an 'increasing trend' (a bit of a stretch
given that the majority of steps are decreasing), and 1c describes
predictive uncertainty as 'mostly constant' (though the overall shape
actually looks very similar to 1b) -- but why is it obvious that
either of these behaviors is correct or desirable? Even if this
behavior qualitatively matches what you'd get from a more principled
Bayesian approach (I was expecting the paper to argue this but it
doesn't), a simple synthetic task should be an opportunity for a real
quantitative comparison: how closely do the dropout uncertainties
match what you'd get from, e.g., HMC, or even exact
Bayesian linear regression (feasible here since the true function is
linear -- so concrete dropout on a linear model should be a fair
comparison).

UCI (sec 4.2): the experimental setup is not clear to me -- is the
'dropout' baseline using some fixed probability (in which case, what
is it?) or is it tuned by grid search (how?). More fundamentally,
given that the goal of CDropout is to produce calibrated uncertainties,
why are its NLL scores (fig 2, a reasonable measure of uncertainty)
*worse* than standard dropout in the majority of cases?

MNIST (sec 4.3): again no real comparison to other approaches, no
evaluation of uncertainty quality. The accuracy 'matches' hand-tuned
dropout -- does it find the same probabilities? If so this is worth
saying. If not, seems strange to refer to the converged probabilities
as 'optimal' and analyze them for insights.

Computer vision (sec 4.4): this is to me the most promising
experiment, showing that concrete dropout matches or slightly beats
hand-tuned dropout on a real task. It's especially nice to see that
the learned probabilities are robust to initialization since this is a
natural worry in non-convex optimization. Some things that seem
confusing here:
 - what is the metric ""IoU""? presumably Intersection over Union but
   this is never spelled out or explained.
 - the differences in calibration between methods are
   relatively small compared to the overall miscalibration of all the
   methods - why is this? 
 - text says ""middle layers are largely deterministic"" which seems
   inconsistent with Fig 8c showing convergence to around p=0.2?
(nitpick: ""Table"" 2 is actually a figure)

Model-based RL (sec 4.5): It's nice that the model matches our
intuition that dynamics uncertainty should decrease with
experience. But it sounds like this tuning doesn't actually improve
performance (only matches the cumulative reward of a fixed 0.1
dropout), so what's the point? Presumably there should be some
advantages to better-calibrated dynamics uncertainty; why do they not
show up in this task?

The method proposed in this paper seems promising and potentially
quite useful. If so it should be possible to put together a careful
and clear evaluation showing compelling advantages (and minor costs)
over relevant baselines. I'd like to read that paper! But as currently
submitted I don't think this meets the NIPS bar. 

UPDATE: increasing my rating slightly post rebuttal. I still think this paper is marginal, but the clarifications promised in the rebuttal (describing the tuning of dropout baselines, and the problems with Gaussian dropout, among others) will make it stronger. It's worth noting that recent work (Molchanov et al., ICML 2017 https://arxiv.org/abs/1701.05369) claims to fix the variance and degeneracy issues with Gaussian dropout; this would be an interesting comparison for future work."
Multiresolution Kernel Approximation for Gaussian Process Regression,"Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler",https://proceedings.neurips.cc/paper/2017/hash/850af92f8d9903e7a4e0559a98ecc857-Abstract.html,"The authors consider the problem of large-scale GP regression; they propose a multiresolution approximation method for the Gram matrix K. In the literature, most approximation approaches assume either (1) a low rank representation for K, which may not be supported by the data, or (2) a block-diagonal form for K, the structure of which has to be identified by clustering methods, which is not trivial for high-dimensional data. The current paper proposes MKA, a novel approximation approach that uses captures local and global properties for K. The Gram matrix K is approximated as a Kronecker sum of low-rank and diagonal matrices, a fact that significantly reduces the computational complexity of the linear algebra calculations required in the context of GP regression.
      
The paper initiates a very interesting discussion on the nature of local and global kernel approximations, but I feel that certain aspects ofthe methodology proposed are not sufficiently clear. Below, I list some considerations that I had while reading the paper.
      
What is the effect of the clustering mentioned in step 1 of the methodology? Is the method less sensitive to the result of clustering than the local-based methods in the literature?

Does the approximate matrix \tilde{K} converge to true matrix K as the d_{core} parameter is increased? By looking at the experiments of Figure 2, it appears that MKA is rather insensitive to the d_{core} value.
      
The effect of approximating K in many stages as described in Section 3 is not obvious or trivial. The authors attribute the flexibility of MKA to the reclustering of K_l before every stage. However, I would suspect that a slight variation of the output at any stage would have dramatic consequences in the stages that follow.
      
It is not clear which compression method was used in the experiments. I would think that the cost of SPCA is prohibitive, given the objective of the current paper. It could still be worth mentioning SPCA if there was some experimental comparison with the use of MMF in the context of MKA.
      
I think that the experimental section would be stronger if there was also a demonstration of how well the MKA approximates the original GP. Although we get an idea of the predictive mean in Figure 1, there is no information of the predictive variance.
      
Minor comments:
The line for the Full GP method in Figure 2 is almost not visible. ","The paper introduces a new kernel approximation method enabling inversion of positive symmetric matrices with linear complexity. In contrast to previous methods, it is not based on a low rank approximation but instead uses local factorization on a so called c-core-diagonal form with a hierarchical factorization.

The idea is interesting and the results are very promising. Nevertheless, I am lacking some more intuitive discussion why this approach is superior to other methods in the comparison. As I understand it there are two novel components in this paper. 1. The hierarchical factorization and 2. the factorization into a c-core-diagonal form. The authors have not fully explained why these ingredients are important. Also, can these two strategies be used separately or only combined?

I am also lacking details such as: 
* Based on what criteria are the rows/columns in K_0 clustered?
* How are the number of clusters p_l decided?
* How do you decide the size of c and the corresponding indices CD(c)?

Also, the presentation of the algorithm could be clearer. This would be improved if you move the pseudo algorithm in the supplementary material to the main paper I think.

Minors:
* The notation K' appears after eq (2) but never introduced (I assume K'=K+sigma^2 I )
* Row 248: ""than than"" -> ""than""","The paper applies a fast hierarchical matrix decomposition to the task for inverting and finding the determinant of the kernel matrix of kernel based methods. The overall complexity of the algorithm is equivalent to or lower then calculating the values in K, O(n^2).

In contrast to inducing point based methods, here the kernel is decomposed in such a way as to capture both the long range (dubbed ""PCA-like"") and short-range (dubbed ""k-nearest neighbour type"") interactions in data. 

The paper is clearly written and gives good intuitions as why the approach works, a good description of the algorithm and compelling results on real data."
Learned D-AMP: Principled Neural Network based Compressive Image Recovery,"Chris Metzler, Ali Mousavi, Richard Baraniuk",https://proceedings.neurips.cc/paper/2017/hash/8597a6cfa74defcbde3047c891d78f90-Abstract.html,"This is a very interesting work that shows how CNN transform many field BEYOND machine learning, since they are applied here to a reconstruction problem of the compressed sensing/denoising type. The author shows how, using CNN and a trick called ""unrolling"", they can improve significantly on the Denoising-AMP algorithm. 

The presented approach outperforms the previous state-of-the-art  in terms of both accuracy and runtime. What is interesting is that this further application of the ""unrolling trick"" illustrate the recent  trend towards using training data to improve the performance of iterative algorithms. 

I do beleive this would be of interest to the NIPS crowd to see the application of these ideas in different dirrection than mainstream machine learning.

Minor comments: 
* I find this sentence ambiguous ""First, we demonstrate the state-evolution (S.E.) heuristic, a series of equations that predict the performance of D-AMP, holds for LDAMP as well."" It is heuristic or rigorous or demonstrated?  The whole ""theoretical"" section should be a bit more clear about what is computed with heuristic method but is beleived to be exact, what is rigorous, and what is maybe approximative.
* As far as I know, AMP has an long history going beyond [5] in statistical mechanics under the name ""Thouless-Anderson-Palmer"" who introduced the Onsager correction.

","### Summary

In this paper, it is raised a challenge problem in compressive sensing recovery, which is to combine the hand-crafted priors with neural network priors. An unrolled optimization technique is used. The major contribution of this work is to apply the unrolling technique to the D-IT and D-AMP algorithm to form the LDIT and LDAMP algorithm. The improvement in terms of PSNR and time is demonstrated in the Experiments section.

Overall, this paper proposes an interesting idea with applications in compressive sensing recovery. However, this paper should benefit from better organization and more clarification of the motivations.

- How to demonstrate the deep prior is better, or at least to some extent, than sparse priors?
- What is the motivation of using unrolled optimization with deep priors?
- Are there any special reasons for the theoretical analysis? Mostly the analysis directly follows from the properties of D-AMP and SGD. A description of how the findings improve the previous results should help the clarification.

","Summary of the work:

The authors present a theoretically motivated NN architecture for solving inverse problems arising in compressed sensing. The network architecture arises from ‘unrolling’ the denoiser-based approximate message passing (D-AMP) algorithm with convolutional denoiser networks inside. The work is nicely presented, and to my knowledge, original. The authors present competitive results on known benchmark data.

Overall evaluation:

I enjoyed reading this paper, well presented, theoretically motivated work. I was pleased to see that the method achieves very competitive performance in the end both in terms of accuracy and in terms of speed.

Comments and questions:

1. Why is the Onsager correction so important?: The Onsager correction is needed because the error distribution differs from the white Gaussian noise assumption typically used to train/derive denoising methods. However, if the whole D-IT unrolled network is trained (or only fine-tuned after initial training) end-to-end, then surely, subsequent denoisers can adopt to the non-Gaussian noise distribution and learn to compensate for the bias themselves. This would mean that the denoisers would no longer be good denoisers when taken out of context, but as part of the network they could co-adapt to compensate for each other’s biases. If this is not the case, why?
2. Validation experiments for layer-wise-training: My understanding is that all experiments use the layer-wise training procedure, and there is no explicit comparison between layer-wise or end-to-end training. Is this correct?
3. Connections to the theory denoising autoencoders: This paper is presented from the perspective of compressive sensing, with solid theory, and unsurprisingly most of the references are to work from this community/domain. However, denoisers are applied more generally in unsupervised learning, and there is a relevant body that the NIPS community will be familiar with which might be worth discussing in this paper. Specifically, (Alain and Bengio, 2012) https://arxiv.org/abs/1211.4246 show that neural networks trained for denoising learn approximate the gradients of the log data generating density. Therefore, one could implement iterative denoising as “taking a gradient step” along the image prior. This is exploited in e.g. (Sonderby et al, 2017) https://arxiv.org/abs/1610.04490 where the connection to AMP is noted - although not discussed in detail.
4. Lipschitz continuity remark: the authors say ConvNets are clearly Lipschitz, otherwise their gradients would explode during training. While this is true, and I would assume doesn’t cause any problems in practice, technically, the Lipschitz constant can change with training, and is unbounded. A similar Lipschitz-continuity requirement arises in - for example - the Wasserstein GAN’s discriminator network, where Lipschitz continuity with a certain constant is ensured by weight clipping. Can the authors comment on the effect of the Lipschitz constant’s magnitude on the findings?

Minor comments:

Table 1. would benefit from highlighting the fastest/best methods (and those that come within one confidence interval from the winner)"
Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks,,https://proceedings.neurips.cc/paper/2017/hash/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Abstract.html,"The paper presents a method for survival analysis using deep Gaussian processes for applications with competing risks (i.e. comorbidities which need to be considered in determining treatment) using a multi-task approach where the (right-censored) survival times for each risk are viewed as different task, benefiting from a shared intermediate representation.  

The paper is very clear and well-written.  The originality of the paper is good.  While this is a somewhat of a niche application area of machine learning, I suspect the paper will be of high interest to those working in that area, as well as being a useful practical application of (multi-task) deep Gaussian processes.  I am not expert in deep Gaussian processes, but as far as I can see the paper is technically sound.  The experimental validation is reasonably compelling (hopefully a longer journal paper with more experimental details will be forthcoming?).

I like this paper very much; it is always good to see recent advances in machine learning being applied to important real world problems, especially non-standard applications such as survival analysis.

Line 130 should be ""it depends on"" (omit ""is"")?

Line 140 should be ""These limitations seem to have..."" (rather than ""seems"")?

Line 261 the use of ""are diagnosed"" suggests a rate with the time period missing, perhaps ""have been diagnosed"" if this refers to the total number of US citizens with an existing CVD and cancer diagnoses?
","The paper presents a 2-layer Gaussian process model to fit survival time with competing risks, extending the MGP model as in reference 19 by removing the exponential link and adding one more layer of Gaussian process. The nonparametric model estimates patients' specific survival curves. But compare to other parametric models, it cannot tell the effects of covariates. A variational Bayes algorithm is given so that the inference is scalable. The novel model introduced, as well as many relevant existing concepts and models, are well reviewed. Numerical examples are provided to illustrate and support the methodological contributions. Empirically, the model outperforms all benchmarks and explanations are convincing.

My only concern is about prediction. As stated in Line 75 to 83, people often care about hazard functions, survival functions or cumulative incidence functions. For the model in this paper, are all these functions in a closed form, or to be estimated by Monte Carlo method? The paper can be further improved if the authors give a comparison of these functions among different models.","The paper applies the deep multi-task GP for competing risk survival analysis. Experimental results on synthetic and real data comparing with four other methods are convincing. I have the following questions/comments
      1. In line 176, it is not clear to me what ""direct Monte Carlo sampling"" entails here, since this involves the posterior (conditioned on data).
      2. In line 195, what does ""free-form"" distribution mean -- perhaps a mixture of deltas?
      3. I suggest to tabluate results for give in Figures 4 to 6, so that we can clearly see the variance of the results.
      4. The conclusion from lines 289 to line 292 seems rather too strong without further investigation into the the learnt hyperparametsers and posterior. For example, can you explicitly show in the paper the ""shared representation"" that is mentioned?
      5. The authors should include comparisons the Bayesian WGP [L\'{a}zaro-Gredilla, 2012].
      "
Unsupervised Transformation Learning via Convex Relaxations,"Tatsunori B. Hashimoto, Percy S. Liang, John C. Duchi",t,"The paper proposes an algorithm for unsupervised learning of transformations based on modeling nearest neighbor pairs as linear combination of transforms. The technique only models the transformations, and not the full data distribution and so can (in principle) be applied to other data sets (for eg, learning from MNIST, but applying to characters from other languages). While this problem/objective function is non-convex, they provide a convex relation by approximating the true transformation matrix as a linear combination of rank-1 matrices derived from sampling the data (and they show that this is a good approximation to the true transform matrix).  They show that the technique recovers known transforms such as stroke thickness/rotation in addition to new transforms (blur, loop size). 

Overall, I found the ideas in the paper interesting, and the paper well written."
Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations,"Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, Luc V. Gool",https://proceedings.neurips.cc/paper/2017/hash/86b122d4358357d834a87ce618a55de0-Abstract.html,"The paper aims to produce compressible representations.
The paper introduces two new tools:
1) Differentiable vector quantization. The hard assignment to the nearest center vector is approximated by soft softmax weights.
2) Entropy estimation for the marginal code distribution. The entropy is estimated by counting the occurrences of code symbols. The counting is done over a subset of the training data.

The vector quantization works better than scalar quantization.

Some minor cons:
- The entropy estimation would be expensive to do exactly. So the computation is approximated only on a subset of the training data. 
- The method becomes less simple, if considering the autoencoder pretraining, the minimization of the cluster energy and the softmax temperature annealing.
- It is not clear how to prevent the network to invert the soft assignments.


Suggestions for clarity improvements:
- When reading section 3, I wondered how `p` was computed. The estimation was explained only in the experimental Section 4.
- The SHA abbreviation collides with the existing Secure Hash Algorithms (SHA).
SHAE may be a better name (Soft-to-Hard AutoEncoder).

Questions about evaluation:
- How is the bitrate computed? Is the P(E(Z)) prior frozen after training?
And is the number of bits computed by -logP(E(z_{testImage}))?
I expect that the test images may have higher coding cost than the training images.

Update:
I have read the rebuttal.","The paper proposes an end-to-end approach for compressing neural networks. Instead of learning the network and then reducing it, they learn a network
such that the parameters are compressible. They obtain results competitive with state-of-the-art while having a data-dependent and end-to-end approach, which is a very nice result.

I particularly enjoyed reading this paper, I found it very clear and well motivated. The comparison to the current literature in that area is very thorough and the mathematical
motivations are well formulated. The softmax with temperature sigma is a simple trick for soft-to-hard encoding that elegantly solves the non-differentiability problem in quantization and allows to control the smoothness of the assignment. I found the soft assignments and entropy estimation to be elegant solutions to the well-defined problem the authors described in section 3.1.

Though I am not an expert in that field, I found this work very nice and it seems to be a significant step towards having end-to-end approaches for compressing neural networks, which look promising compared to previous approaches. This can have a large impact considering the increasing use of neural networks on mobile or embedded devices with limited memory.","This paper proposed a unified end-to-end framework for training neural networks to get compressible representations or weights.
The proposed method encourages compressibility by minimizing the entropy of representation or weight.  Because the original entropy-based objective cannot be minimized directly, the authors instead relax the objective and use discretized approximations.
The proposed method also use an annealing schedule to start from a soft discretization to gradually transform to a hard discretization. 


My main concerns with this paper are on experiments and comparisons. The authors performed two sets of experiments, one on compressing representations of image and the other on compressing classifier weights.

For the image compression experiment, the proposed method offers visually appealing results but it is only compared to BPG and JPEG. I suggest the authors also include comparisons against other neural network based approaches [1].

In the weight compression experiment, the proposed method does not show advantage over previous state-of-the-art. The authors also noted this but argued that the proposed method is trained end-to-end while the previous methods have many hand designed steps. However, I would expect integrated end-to-end training to improve performance instead of hindering it. I suggest the authors investigate more and improve their method to fully realize the benefit of end-to-end training.

[1] Full Resolution Image Compression with Recurrent Neural Networks "
Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM,"Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, Steven Z. Wu",https://proceedings.neurips.cc/paper/2017/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html,"Pros:
1. Natural and important question: in many practical scenarios achieving a certain level of accuracy is crucial while privacy is a secondary concern.
2. Simple and easy to implement meta-approach based on gradual release of private information technique (from Koufogiannis et al 2015) combined with AboveThreshold technique.
3. Examples of applications together with an experimental evaluation 
Cons:
1. Using gradually decreasing levels of privacy with AboveThreshold is a rather obvious approach to the problem so should be considered the baseline in this case. The addition 
 of the gradual release technique is nice but limits strongly the kind of approaches one can use with it off the shelf. Most importantly, as pointed out by the authors, for now its unknown how to use it with the more powerful approximate differential privacy. I suspect that the improvement from this part is relatively small (the smallest value of epsilon is still likely to dominate) so it is not clear if it's useful.
2. A comparison of the above mentioned baseline used with stronger algorithms for the tasks described could would have been very interesting but unfortunately is not included.

Overall the work addresses an important question and describes some interesting ideas and a bit of experimental evaluation. The main drawback is that perhaps the more novel part of this paper seems of limited use for now but I think the work would still be of interest to those working on learning with privacy. 

Update:
I agree with the authors that the definition and application of ex-post privacy are interesting enough to merit acceptance.","The authors present a framework for carrying out privacy-preserving optimization achieving target accuracy level rather than achieving target privacy level as usually studied in the literature. The authors show that for perturbation(input or output) based methods, one can apply the gradual release algorithm in [14] to achieve better privacy bound compared to the commonly used ""doubling trick"". The authors apply the framework to the ridge and logisitc regression problems and presented theoretical bounds in each case. Experiments are carried to validate the approach.

This paper addresses the important problem of privacy-preserving learning. It takes a new angle on considering the accuracy first. This can be useful in the case when the application requires some minimal model quality.  The approach in the paper is a clever application of the method developed in [14].  The tool developed in the paper can be useful in many similar applications.

However, the framework in the paper is limited to the pure privacy and to the perturbation based methods.  It would be useful for the authors to discuss these limitations. In addition, it would be good to compare to the method of using AboveThreshold with differentially private model evaluation (which can usually be done with very high accuracy.) ","Summary:

This paper considers a different perspective on differentially private algorithms where the goal is to give the strongest privacy guarantee possible, subject to the a constraint on the acceptable accuracy. This is in contrast to the more common setting where the we wish to achieve the highest possible accuracy for a given minimum level of privacy.

The paper introduces the idea of ex-post differential privacy which provides similar guarantees to differential privacy, but for which the bound on privacy loss is allowed to depend on the output of the algorithm. They then present a general method for private accuracy constrained empirical risk minimization which combines ideas similar to the noise reduction techniques of Koufogiannis et al. and the Above Threshold algorithm. At a high level, their algorithm computes ERM models for increasing privacy parameters (using the noise reduction techniques, rather than independent instantiations of a private learning algorithm) and then uses the above threshold algorithm to find the strongest privacy parameter satisfying the accuracy constraint. The noise reduction techniques allow them to pay only the privacy cost for the least private model they release (rather than the usual sum of privacy costs for the first k models, which would be required under the usual additive composition laws) and the modified above threshold algorithm has privacy cost scaling only logarithmically with the number of models tested.

They apply this algorithm to logistic and ridge regression and demonstrate that it is able to provide significantly better ex-post privacy guarantees than two natural baselines: inverting the privacy privacy guarantees to determine the minimum value of epsilon that guarantees sufficiently high accuracy, and a doubling trick that repeatedly doubles the privacy parameter until one is found that satisfies the accuracy constraint.

Comments:

Providing algorithms with strong utility guarantees that also give privacy when possible is an important task and may be more likely to be adopted by companies/organizations that would like to preserve privacy but are unwilling to compromise on utility. The paper is clearly written and each step is well motivated, including discussion of why simpler baselines either do not work or would give worse guarantees. "
Triple Generative Adversarial Nets,"Chongxuan LI, Taufik Xu, Jun Zhu, Bo Zhang",https://proceedings.neurips.cc/paper/2017/hash/86e78499eeb33fb9cac16b7555b50767-Abstract.html,"In this paper, the authors propose a new formulation of adversarial networks for image generation, that incorporates three networks instead of the usual generator G and discriminator D. In addition, they include a classifier C, which cooperates with G to learn a compatible joint distribution (X,Y) over images and labels. The authors show how this formulation overcomes pitfalls of previous class-conditional GANs; namely that class-conditional generator and discriminator networks have competing objectives that may prevent them from learning the true distribution and preventing G from accurately generating class-conditional samples.

The authors identify the following deficiency in class-conditional GAN setups:
“The competition between G and D essentially arises from their two-player formulation, where a single discriminator network has to play two incompatible  roles—identifying fake samples and predicting labels”. 
The argument goes that if G were perfect, then a class-conditional D has an equal incentive to output 0 since the sample comes from G, and to output 1 since the image matches the label. This might force D to systematically underperform as a classifier, and therefore prevent G from learning to produce accurate class-conditional samples.

A question about this:
- What if a classifier network and a real/fake network are used instead of combining both decisions into a single net? Does the problem still arise?
- In practice, does G ever produce realistic enough samples that the hypothetical situation described above actually arises? In my experience training GANs, the situation is always that D can easily recognize samples as fake, and generally needs to be heavily constrained in order to allow learning to happen at all. Since this argument is very important to motivating the paper, it would be nice to have a clear demonstration of this failure mode on real data, even if it is toy data.

The proposed model builds two conditional models - generator P(x | y) and classifier P(y | x). By drawing samples from the marginals P(y) and P(x), they can draw samples from the joint distributions (x,y) parametrized by each of these conditional models, to be fed into the discriminator D.  The model achieves state-of-the-art semi-supervised learning results most notably on CIFAR-10. Qualitatively, the model achieves clear disentangling of class and style on challenging datasets including SVHN and Cifar-10, and compelling class-conditional samples of automobiles and horses. Overall, I think the proposed model is very interesting and the experimental results are strong, but the paper would benefit from a clearer motivation and illustration of the particular failure modes that it is trying to overcome.","Summary:
The paper presents a GAN-like architecture called Triple-GAN that, given partially labeled data, is designed to achieve simultaneously the following two goals: (1) Get a good generator that generates realistically-looking samples conditioned on class labels; (2) Get a good classifier, with smallest possible prediction error. The paper shows that other similar GAN-based approaches always implicitly privileged either (1) or (2), and or needed much more labeled data to train the classifier. By separating the discrimination task between true and fake data from the classification task, the paper outperforms the state-of-the-art, both in (1) and (2). In particular, the classifier achieves high accuracy with only very few labeled dataset, while the generator produces state-of-the-art images, even when conditioned on y labels.

Quality & Clarity:
The paper indeed identifies plausible reasons of failure/inefficiency of the other similar GAN-type methods (such as Improved-GAN).  The underlying idea behind Triple-GAN is very elegant and comes with both nice theoretical justifications and impressive experimental results.
However the overall clarity of exposition, the text-flow and the syntax can still be much improved. Concerning syntax: too many typos and too many grammatically incorrect sentences. Concerning the exposition: there are too many vague or even unclear sentences/paragraphs (see below); even the statements of some Corollaries are vague (Corollary 3.2.1 & 3.3.1). Some proofs are dismissed as evident or refer to results without precise citation. There are some long and unnecessary repetitions (f.ex. l.58-73 and l.120-129 which are essentially the same), while other points that would need more explanations are only mentioned with a few unclear sentences (l.215-220). This is in striking contrast with the content's quality of this paper. So before publication, there will be  some serious work to do on the overall presentation, exposition, and syntax!

Detailed comments:
l.13-14: the expression 'concentrating to a data distribution', which is used in several places in this paper, does not exist. Please use f.ex. something like 'converges to'. 
l. 37-38: formulation of points (1) and (2) is too vague and not understandable at the first read. Please reformulate.
l. 46-49: good point, but formulation is cumbersome. Reformulate it more neatly. You could f.ex. use a 'on the one hand ... on the other ...' type of construction.
l. 52-56: Unclear. ('the marginal distribution of data' , 'the generator cannot leverage the missing labels inferred by the discriminator': what do you mean?). Please reformulate.
l.58-73: too many unnecessary overlap with l.120-129. (And text is clearer in l.120-129, I find...). Please mind repetitions. One only needs repetitions if one messed up the explanation in the first place...
l. 140&157: please format your equations more nicely.
l .171: proof of Lemma 3.1: last sentence needs an explanation or a reference.
l.174: Corollary 3.2.1: I find it clearer to say p(x) = p_g(x) = p_d(x) and p(y) = p_g(y) = p_d(y).
l.185: Corollary 3.3.1: What is a 'regularization on the distances btw ...' ? Define it. Without this definition, I can't say whether the proof is obvious or not. And in any way, please say one or two sentences why the statement is true, instead of dismissing it as obvious.
l.215-220: I don't see what the REINFORCE alrogrithm, which is used in reinforcement learning, has to do with your algorithm. And the whole paragraph is quite obscure to me...Please reformulate. (Maybe it needs to be a bit longer?). 
Esperiments:
- please state clearly when you are comparing to numbers reported in other papers, and when you really re-run the algorithms yourself.
l.228: '10 000 samples' should be '10 000 test samples'
l.232: '(Results are averaged over 10 runs)' : don't put it in parenthesis: it's important! Also, remind this in the caption of Table 1.
l.254-263: how many labeled data do you use for each of the experiments reported here? Please specify!
Also, you did not mention that triple-GAN simply also has more parameters than an improved GAN, so it already has an advantage. Please mention it somewhere.
table 1: what are the numbers in parenthesis? 1 std? Calculated using the 10 runs? 
l.271: 'the baseline repeats to generate strange samples' : reformulate.
Generally: there are quite a few articles ('the') and other small words missing in the text. A careful proof reading will be necessary before publication.

Answer to rebuttal:
We thank the reviewers for their answers and hope that they will incorporate all necessary clarifications asked by the reviewers. And please be precise: in your rebuttal, you speak of a distance over joint probability distributions, and then cite the KL-divergence as an example. But mathematically speaking, the KL-divergence is not a distance! So please, be precise in your mathematical statements!","This paper proposes a three-player adversarial game to overcome the fact that a discriminator in a semi-supervised setting has two incompatible roles, namely to classify and separate real data from fake data.

The paper is well-written and the authors display a good knowledge of the GAN literature. I think it proposes a solution to a relevant problem, and the empirical evidence presented to back up the claims being made is convincing.

The major criticism I have to voice is on the way in which the core concepts behind Triple-GAN are presented. The ideas start off clearly, but eventually become less and less principled as heuristic regularizers are added to the loss function.

The notion that there exists three players also seems a little arbitrary: the same distinction could be applied to ALI/BiGAN, where the generator encompasses both conditional distributions, but in that case the ALI/BiGAN authors chose not to make the distinction.

I feel like the joint distributions would be a much better place to make that distinction. There are three different joint distributions: the “labeled” joint p(x, y), the “unlabeled” joint p(x)p(y | x), and the “generative” joint p(y)p(x | y), and all three need to match.

Instead, Triple-GAN folds the “unlabeled” and “generative” joint distributions into a “mixture” distribution with mixture weights alpha and (1 - alpha). In analogy with GAN and ALI/BiGAN, equilibrium is attained when the “labeled” and “mixture” joints match. The equality between the “unlabeled” and “generative” joint is guaranteed if one of the two is equal to the “labeled” joint. This is achieved by explicitly enforcing that the “unlabeled” joint matches the “labeled” joint through maximum likelihood of the “unlabeled” joint on “labeled” joint samples.

It feels like these constraints could be alleviated by considering three discriminator classes instead of two, each corresponding to one of the three joint distributions.

Additional note: the semi-supervised results presented in Table 1 appear to be inconsistent in the way SOTA is displayed: sometimes Triple-GAN is presented as better than other methods, even though the error bars overlap (e.g., Improved-GAN on MNIST n=100). How are the uncertainties computed? Also, the ALI numbers are out-of-date: for instance, ALI achieves an error rate of 17.99 (±1.62) on CIFAR10, and therefore its error bar overlaps with Triple-GAN."
Deep Learning with Topological Signatures,"Christoph Hofer, Roland Kwitt, Marc Niethammer, Andreas Uhl",https://proceedings.neurips.cc/paper/2017/hash/883e881bb4d22a7add958f2d6b052c9f-Abstract.html,"This paper proposes a deep neural network model to learn from persistence diagrams extracted from data. A persistence diagram is a 2D point sets describing the topological information of a given data in the view of a chosen scalar function. While a diagram describes useful global information of the data, existing learning methods [25,18] only use it in a kernel-based setting. The key contribution of this paper is to construct an input layer for persistence diagrams. This is non-trivial as persistence diagrams behave very differently from traditional vectorized features (non-Euclidean metric). The key idea is to learn a set of single-point diagrams, as well as parametrized distances from these diagrams to any training/testing diagram. This way, any input persistence diagram can be transformed into a set of distances from these single-point diagrams. And be passed to the next layer of the deep neural network. The locations of the single point (\mu) and the parameters for distance (\sigma) can be learned through the training accordingly. Two real world datasets are used to demonstrate the proposed method. On a 2D shape dataset, the proposed method performs on par with state-of-the-arts. On the network dataset, it outperforms state-of-the-arts significantly.

I like the idea. The construction of the input layer from persistence diagrams is novel and well-thought. Experimental results are convincing. I did not check the proofs carefully. But they are believable to me.

The presentation has a lot of room to improve. Notably the notations and explanations in page 4 could have been simplified and supplemented with more pictorial illustrations.

First, the rotation of the diagrams by \pi/4 does not seem necessary to me. I am guessing the goal is to make x_1 the persistence so the learning result is more interpretable. But this was not discussed in the rest of the paper. 

Second, the presentation can be simplified by separating the diagram and the essential dots (so the first line of Eq. (3), and the notation R_\star can be removed). In fact, in experiment 1, essential dots are not used (as the shapes are generally trivial in topology). In experiment 2, they are separated from the main diagrams in the model. A unified presentation can be moved to the supplemental material completely to improve the readability of the paper. 

Third, R_\Delta, R_\Star, etc, should be replaced by other symbols. They look like extended Euclidean planes rather than point sets.

Forth, the presentation could benefit from more figures and explanations. (The right half of Fig 1 is doing so, but with insufficient explanations.)

- It would be much helpful to see some illustrations of the learning results on the real datasets. E.g., what are the single-point diagrams learned in these data. 

- I understand [25] does not work due to speed issues. How about simply vectorize the diagrams and use them as features? This will strengthen the result but demonstrating the necessity of deep neural networks? 
","The authors propose a “input layer” for a deep network that takes topological persistence diagrams as input and transforms them to representations that can be optimized by the network through back-propagation. The proposed transformations are proved to be Lipschitz continuous and differentiable. The merits of the proposed architecture are demonstrated using 2D shape classification, and graph classification experiments.

Comments:
1. The authors have done a very good job in introducing persistent homology and persistence diagrams. The presentation of the paper is mostly smooth and easy to read.

2. The representation is well-thought, and is essentially a sum of likelihoods of the points in the PD with respect to 2D Gaussians whose mean and covariances are learned.

3. It is clear that (3) is continuous, but the remark below definition (3) should be clarified. For example, the expressions with the limits ignore the exponential term.

4. In order to prove Lemma 2, we need to show that the partial derivatives are bounded on the entire domain. However the proof only shows the bounds at the extreme values. Although it is easy to see, it may be worth mentioning how the derivatives are bounded in the interior of the domain.

5. What is the significance of Lemma 1? Does it connect to the stability of the representations with respect to the perturbations of function f?

6. The results of the 2D shape classification experiment are not very convincing although the methodology seems solid. The authors should explore more suitable applications such as 3D shape recognition (see [20]) where PDs have been shown to be a good representation. There are many applications where PD features are a good choice. Section 5.2’s results are good. Experiments seem like an area of the paper that can be improved (given that the paper proposes a deep learning system).

Update:
--------
The authors have answered most of my comments satisfactorily.","SUMMARY 
* This paper proposes to map topological signatures (eg multisets of intervals from persistent homology) to task dependent representations.  
* Whereas previous approaches have used predefined maps to extract features from the topological signatures, here it is proposed to learn this map. Training the representation appears to be a natural and important step. 
* The paper includes experiments demonstrating that the proposed approach can help making TDA methods more useful in classification problems. 

CLARITY 
* In think that the paper would benefit from a more intuitive and self contained background section. Maybe consider expanding the example to explain more of the abstract notions discussed in this section. 

RELEVANCE 
* Mapping topological signatures to useful representations is an important problem in TDA. This paper proposes an approach to using trainable features, in contrast to fixed features that have been considered in other works. 
* The new theoretical results are limited to proving a certain stability wrt the Wasserstein metric, a result which seems to be quite direct from the definitions. 
* One of the conclusions is that, while using topological signatures does not always reach state of the art in classification tasks, the proposed approach is better than previous approaches using topological signatures. 

MINOR 
* Fig 1 left, should birth not come before death? 
* In Def 1, what is the multiplicity function and why is \Delta defined to have an infinite value? 
* What is \mathcal{D}_i and n_max in line 104? 
* Explain or replace braket notation in line 122. 
* What is the role of \nu in (3)? 

CONCLUSION 
* In general I think that this is a valuable contribution towards increasing the usefulness of traditional TDA methods in the context of machine learning. "
Revenue Optimization with Approximate Bid Predictions,"Andres Munoz, Sergei Vassilvitskii",https://proceedings.neurips.cc/paper/2017/hash/884d79963bd8bc0ae9b13a1aa71add73-Abstract.html,"Myerson's reserve price auction gives a way to maximize revenue when the value distributions of bidders are known. However, in practice, we only have access to samples from bidders' valuations. Finding the optimal reserve price is often a hard problem, more so, when the valuations of a buyer depend on multiple features.

In this paper, the authors give a scalable, practical and efficient algorithm for computing reserve price that also ties the revenue one can achieve to a standard prediction task. They also refine a result by Goldberg et al. (SODA'01) and characterize distributions that are amenable to revenue optimization via reserve price, formalizing the intuition that reserve prices are better suited for low-variance distributions. The technical novelty lies in relating the reserve price computation to a problem similar to k-means clustering.

I have read only the theoretical part of this paper. This is not a clustering paper and I am not qualified enough to correctly judge its standing in the algorithmic game theory and mechanism design area. This is why I am giving an accept with low confidence.
    ","This paper studies revenue optimization from samples for one bidder. It is motivated by ad auction design where there are trillions of different items being sold. For many items, there is often little or no information available about the bidder’s value for that precise good. As a result, previous techniques in sample-based auction design do not apply because the auction designer has no samples from the bidder’s value distribution for many of the items. Meanwhile, ads (for example) are often easily parameterized by feature vectors, so the authors make the assumption that items with similar features have similar bid distributions. Under this assumption, the authors show how to set prices and bound the revenue loss.

At a high level, the algorithm uses the sample and the bid predictor h(x) to cluster the feature space X into sets C1, …, Ck. The algorithm finds a good price ri per cluster Ci. On a freshly sampled feature vector x, if the feature vector falls belongs to the cluster Ci, then the reserve price r(x) = ri. The clustering C1, …, Ck is defined to minimize the predicted bid variance of each group Cj. Specifically, the objective is to minimize sum_{t = 1}^k \sqrt{\sum_{i,j:xi,xj \in Ct} (h(xi) – h(xj))^2}. The authors show that this can be done efficiently using dynamic programming. The best price per cluster can also be found in polynomial time. For each cluster Ct, let St be the set of samples (xi, bi) such that xi falls in Ct. The algorithm sets the price rt to be the bid bi such that (xi,bi) falls in St and bi maximizes empirical revenue over all samples in St.
 
The authors bound the difference between the expected revenue achieved by their learning algorithm and an upper bound on optimal revenue. This upper bound is simply E_{(x,b)}[b]. They show that the bound depends on the average bid over the sample and the error of the bid predictor h. The bound is also inversely proportional to the number of clusters k and the number of samples. They perform experiments on synthetic data and show that their algorithm performs best on “clusterable” synthetic valuation distributions (e.g., bimodal). It also seems to perform well on real ad auction data.
 
Overall, I think the motivation of this work is compelling and it’s an interesting new model for the sample-based mechanism design literature. I’d like to see more motivation for the assumption that the learner knows the bid prediction function h. Is it realistic that an accurate prediction function can be learned? It seems to me as though the learner would probably have to have a pretty robust knowledge about the underlying bid function in order for this assumption to hold. The paper could potentially be strengthened by some discussion about this assumption.
 
In the experimental section, it would be interesting to see what the theoretical results guarantee for the synthetic data and how that compares to the experimental results.
 
I’m confused about equation (12) from the proof of lemma 1. We know that r(x) >= h(x) – eta^{2/3}. Therefore, Pr[b < r(x)] >= P[b < h(x) – eta^{2/3}] = P[h(x) – b > eta^{2/3}]. But the proof seems to require that Pr[b < r(x)] <= P[b < h(x) – eta^{2/3}] = P[h(x) – b > eta^{2/3}]. Am I confusing something?

=========After feedback =========
Thanks, I see what you mean in Lemma 1. Maybe this could be made a bit clearer since in line 125, you state that r(x) := max(h(x) - eta^{2/3}, 0).","The paper addresses the problem of setting reserve prices in ad auctions. Some of the issues here (which have been raised by previous papers too) are: 
	1. We don't have distributions, only past data. 
	2. Each ad opportunity is different and so we need some way to generalize. 
The setting here is to assume that the ad opportunities have feature vectors that allow us to generalize. Moreover, they assume that they have a regressor h(x) that predicts the bid as a function of the feature vector x. (There is a joint distribution over bids and features but we are only allowed to use the features to set the reserve price.) The naïve approach is to simply use h(x) as the reserve but this can be improved. The intuition is that you want to avoid the reserve price being higher than the bid, so it's a good idea to shade it down a bit. 

The main contribution is an algorithm to set the reserve prices as a function of h, and also the past data to do some clustering. They give theoretical and empirical evaluation of this algorithm, which shows improved performance. 

Related work: 
The work of Devanur et al., 2016 is actually more closely related than the authors realize. They say 
"" Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information. However, the authors consider only cases where this side information is given by
\sigma \in  [0, 1], thus limiting the applicability of these results—online advertising auctions are normally parameterized by a large set of features."" 

Their model can be used in the setting of the current paper by letting \sigma = h(x). The first order stochastic dominance (FOSD) requirement in that paper is somewhat different than h being a regressor for the bid, but I suspect many reasonable regressors will actually satisfy it, since it is quite a weak requirement. Their algorithm is in fact very similar to the clustering algorithm in this paper (RIC). The main difference is that they do a dynamic clustering instead of a static clustering as in RIC. 

I would recommend including their algorithm in the set of experiments. 
"
Mapping distinct timescales of functional interactions among brain networks,"Mali Sundaresan, Arshed Nabeel, Devarajan Sridharan",https://proceedings.neurips.cc/paper/2017/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html,"# Main paper

Overall an interesting read but the novelty is not so clear to me.

## Section 2.1

It is not specified how the MVAR model order (p in S1 eqs 1-3) is selected, while this is an important hyperparameter. In particular for cross-validation experiments it needs to be trained in-fold. Because the paper explicitly  resamples fMRI time series and aims for interpretability, it would be useful to mention how the max lag changes with resampling so that it can be made explicit that the time scale of the lag is in range with plausible causality in brain circuits and what we already know about long-memory processes.

Figure 1 is very nice but for panels A and B the stripe plot (bottom right of each panel) is quite redundant given that there are already two other representation of the networks in the same panel.

## Section 3.1

Around line 171: please explicitly mention your feature space dimensions here, easier to understand if we don't have to look for this later.

Here and in section 3.2 the feature spaces generated have different dimensions (91, 182, 273) and more importantly probably different correlation structures and sparsity patterns. There is no mention here of the type of regularization that is used, nor of how that hyperparameter is tuned.

## Section 3.2

For rest vs task, the accuracy reported (single-subject case) actually seems quite low (maybe the atlas choice?) - but it's hard to say without comparison. If you want to make an argument that iGC and dGC are good representations of the data because they are sensitive to task differences, then it would be good to compare with cruder techniques like classical correlation and partial correlation (e.g. with Whittaker-style inversion).

lines 198-199 : how exactly are matrices 'averaged across several folds of S' ? Unclear to me how that sits in the cross-validation scheme.

lines 206 - 'across as few as 10 subjects' - that in fact seems to me like a lot - you are losing individual predictions, so if that's what it takes to stabilise the estimator it's in fact not very suitable for single-subject prediction I would argue. Again, hard to say without comparison to other connectivity estimators.

# Supplementary

## Section S1

Covariance matrices in equation 4 should be described explicitly - e.g. `""$\Sigma_1$ is the reduced covariance matrix, and $\Sigma_2$ is the full covariance matrix""` - by line 7 the matrices with subscript 2 have not been defined.

## Section S2

line 13 - 'off-diagonal correlation value' should be 'off-diagonal covariance value'.","The paper deals with a controversial problem related with the use of Granger-Geweke causality (GC) to fMRI data.

This paper proposes a classification task as an indirect validation of the discriminant power of functional connectivity features based on GC. In particular, they define a set of features used in a machine learning approach (SVM classifier + Recursive feature elimination (RFE) for feature selection) and analyze the classification performance and the features. These can result in a characterization of the brain functional networks.

The work is well motivated, including a broad introduction about the issue in the first section and a proper presentation of the different concepts related with fMRI (which I am not confident to completely understand).

The machine learning approach has sense for me and serve to show that the features are significant to distinguish task versus rest conditions.

I have only one concern about the RFE method application which is the fact that the number of samples (500 subjects) is not much larger than dimensions of the feature vector (~182). It could be interesting to validate the obtained result by using permutation testing.

As a minor comment, the last section of the paper should be conclusions.
","The authors apply instantaneous and lag-based Granger-Geweke causality (iGC and dGC) to infer brain network connections at distinct timescales from functional MRI data. With simulated fMRI data and through analysis of real fMRI recordings, they presented the following novel insights. First, iGC and dGC provide robust measures of brain functional connectivities, resolving over long-term controversy in the field. Second, iGC and dGC can identify complementary functional connections, in particular, the latter can find excretory-inhibitory connections. Third, downsampling the time series to different extents provides an efficient method for recovering connections at different timescales.

Clarity:
The paper is well organised and clearly written with nice figures. The result of real data analysis may be difficult to understand without background knowledge in neuroscience.

Significance:
I think that the insights written in this paper have significant impact on researchers working on this topic. Recently analyses of resting-state fMRI data have led to many interesting results in neuroscience. However, most publications employed correlation matrices for measuring functional connectivities. So, if iGC and dGC would become alternative tools, we could extract/grasp multi timescale processing in the brain. In order to convince more researchers of practical usefulness of iGC and dGC, it would be necessary to compare with existing techniques further and to work on more real fMRI examples.

On the other hand, the scope of this paper is rather narrow and impacts on other research topics in the NIPS community are rather limited.

Correctness:
Since the authors applied existing connectivity measures, I don't expect any technical errors.

Originality:
I feel that a  certain amount of researchers are sceptical about applications of GC measures to fMRI data. I have not seen the messages of this paper which may resolve over long-term controversy in the field.

To sum up, the novel insights derived from analysis of simulated and real fMRI data with GC measures are interesting and could have potential to advance the research field. Since further real-world examples would be preferable to convince other researchers in the field, I would say that this paper is weak accept.

There are a few minor comments.

- p.3 Eq.(2): I know a MVAR model with variable time lag for each connection which is determined by DTI fibre length. The simulation model in Section 2 assumes variable timescale at each node. I wonder whether we can set timescale of each connections separately at a predetermined value in this formulation or we have to further extend the model in Section 2 to construct more realistic model incorporating knowledge from structural MRI data.

- Section 3: I guess that the language task could contain slow brain processing and might be a good example to show multi timescale network structures by fMRI downsampling. I wonder whether there exist other tasks in HCP which we can see similar results."
Improved Training of Wasserstein GANs,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron C. Courville",https://proceedings.neurips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html,"+ good results
+ pretty pictures

- exposition could be improved
- claims are not backed up
- minor technical contribution
- insufficient evaluation

Let me first say that the visual results of this paper are great. The proposed algorithm produces pretty pictures of bedrooms and tiny cifar images.

Unfortunately the rest of the paper needs to be significantly improved for acceptance at NIPS. The exposition and structure of the paper is below NIPS standards. For example
L3: ""..., but can still generate low-quality samples or fail to converge in some settings."" I'm assuming the authors mean ""can only...""?
l6: ""pathological behavior"". Pathological seems the wrong word here...
Sections 3 and 4 should be flipped. This way the exposition doesn't need to refer to a method that was not introduced yet.

Secondly, the paper is full of claims that are not backed up. The reviewer recommends the authors to either remove these claims or experimentally or theoretically back them up. Here are a few:
 L101: Proof that only simple functions have gradient norm almost everywhere, note that almost everywhere is important here. There can be exponentially many parts of the landscape that are disconnected by gradient norm!=1 parts.
If a proof is not possible, please remove this claim.
 L 144: Two sided penalty works better. Please show this in experiments.

The actual technical contribution seems like a small modification of WGAN. Box constraint is relaxed to a l2 norm on gradients.

Finally the evaluation is insufficient. The paper shows some pretty generation results for various hyper-parameter setting, but doesn't show any concrete numeric evaluation, apart from the easy to cheat and fairly meaningless inception score. The authors should perform a through user study to validate their approach, especially in light of a slim technical contribution.

Minor:
How is (3) efficiently evaluated? Did the authors have to modify specific deep learning packages to compute the gradient of the gradient? How is this second derivative defined for ReLUs?

Post rebuttal:
The authors answered some of my concerns in the rebuttal. I'm glad the authors promised to improve the writing and remove the misleading claims. However the rebuttal didn't fully convince me, see below.

I'm still a bit hesitant to accept the argument that prettier pictures are better, the authors rebuttal didn't change much in that regard. I'm sure some image filter, or denoising CNN could equally improve the image quality. I would strongly suggest the authors to find a way to quantitatively evaluate their method (other than the inception score).

I'm also fairly certain that the argument about the second derivative of ReLU's is wrong. The second derivative of a relu is not defined, as far as I know. The authors mention it's all zero, but that would make it a linear function (and a relu is not linear)... The reviewers had an extensive discussion on this and we agree that for ReLU networks the objective is not continuous, hence the derivative of the objective does not point down hill at all times. There are a few ways the authors should address this:
 1. Try twice differentiable non-linearities (elu, tanh or log(1+exp(x)).
 2. Show that treating the ReLU as a linear function (second derivative of zero) does give a good enough gradient estimate, for example through finite differences, or by verifying how often the ""gradient"" points down hill.","The authors suggest a simple alternative to the weight clipping of Wasserstein GAN. Although presentation could be greatly improved, it is a decent contribution with practical use. Please implement the following changes for reviewers to better evaluate the soundness of the claims.

The result given in Section 2.3 is not sufficiently explained. Please elaborate on how this result helps explain the problem with weight clipping. Also the claim in Line 85 is not implied by the paragraph above. It may help to formalize the paragraph statement in a Proposition and write Linen 85 as a corollary. Line 99 then can refer to this corollary. The same claim is also used in Line 130. Hence it is very important to clarify this claim.

It would be useful to see the effect of batch normalization on the Swiss roll data in Figure 1b. Is the result as good as gradient penalty? Please consider adding this to the figure.

AFTER AUTHOR FEEDBACK AND REBUTTAL:
The authors have made the mistake of claiming (in their feedback) that the second derivative is nonzero because they do not take the second derivative of ReLU wrt a variable but first wrt input variables and then wrt weights. Unfortunately this is not exactly correct. Thanks to the careful observation of R3, in a simple 1 layer ReLU network, the second derivative of ReLU wrt its input comes up as an additive term even then. This term will be zero in almost all local neighborhoods, but more importantly, it will not be defined simply because second derivative of ReLU wrt its input does not exist since the first derivative is not continuous. It is not clear what will happen with more layers and I think this analysis would be a nice future direction. 

I suggest acceptance of the paper. But I ask reviewers to implement the following change: Present your results using tanh as the nonlinearity, which lets you define first and second order derivatives without any mathematical mistake. Later, explain the audience that the same result holds for ReLU networks in practice even though second derivative is not defined and try to justify why (for example local derivatives should work unless you land on finitely many ""bad"" points in practice). Adding the results showing nonzero ReLU gradient which the authors referred to in their feedback could help. "
Adaptive stimulus selection for optimizing neural population responses,"Benjamin Cowley, Ryan Williamson, Katerina Clemens, Matthew Smith, Byron M. Yu",https://proceedings.neurips.cc/paper/2017/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html,"
The authors present an algorithm for selecting stimuli in a closed-loop experiment to maximize the variance in firing across a population of neurons (Adept). This is an important step in closed-loop stimulus selection, which have focused on learning the response properties of single neurons. The algorithm uses pre-computed features of the stimuli given by a neural network to predict firing rates to new stimuli. The methods were presented clearly and the stimulus selection is efficient enough to be useful to real experiments.

There are a few limiting points of the Adept method (which may be addressed in future work). The applicability of this algorithm is limited, as mentioned by the authors, in that it depends on having a good set of features to describe the stimulus space. Such features may not always be available outside select areas such as V4. It would be valuable to explore the Adept algorithm's behavior when the features chosen do not provide a good basis for prediction spike rates (perhaps for only a portion of the population - would the algorithm learn less for those cells than random stimulus selection?).
Additionally, the algorithm assumes a limited discrete set of stimuli. Finally, it is not clear how the stimulus selection may be affected by different types of noise (in particular, correlated noise across cells or time) in the responses.","An interesting method to speed up experiemntal design to reveal neural coding. The idea of using CNN to quantify stimulus similarity is interesting, but it also raises a paradox issue: if a CNN is able accurately predict the statistics of neuronal responses (such as in V4), then we do not have to do the experiment anymore, since analyzing the CNN data can tell us everything; on the other hand, if the CNN only partially or mistakenly measures the stimulus similarity that should be observed in the brain area recorded, then this mismatch will affect the experimental result. It would be interesting to see how this issue can be solved. But overall, this is a good attemp and is acceptable by NIPS.","This paper developed a simple and practical method for active learning (the Adept algorithm) a visual stimuli that maximizes a population cost function. The approach is elegant and intuitive. The technique was primarily discussed through the importance of choosing experimental conditions for experiments in which the experimenter must choose which kind of stimuli to present given a particular area of neural recording, and that Adept can adaptively find images to show next. Pretrained convolutional neural networks were used to estimate the response of unknown stimuli using kernel methods with a set of underlying features using Nadaraya-Watson regression. The first experiment used synthetic data from a separate CNN. The second, more interesting example performed a feedback experiment by showing stimuli to macaque’s while recording in V4, and was able to see an average 20% higher mean response from neurons with chosen stimuli.

- The approach doesn't account for variability in the response. It would be interesting to see comparison under different noise models.
"
Matrix Norm Estimation from a Few Entries,"Ashish Khetan, Sewoong Oh",https://proceedings.neurips.cc/paper/2017/hash/89d4402dc03d3b7318bbac10203034ab-Abstract.html,"The paper introduces a method for estimating Schatten k-norms of a matrix from very sparse samples of matrix entries. The paper focuses on the sampling density regime, where existing matrix completion techniques fail. The basic idea of this paper is to estimate the trace of M^k from samples of M. The trace of M^k is given by the sum of weights of k-cycles, where the weight of each k-cycle is given by the product of edge weights. The estimator for a sampled matrix is then modified to take the probability of a particular cycle being sampled. This probability, which depends on the type of the cycle, is given explicitly for all types of cycles when k <= 7. The paper provides stochastic bounds on the accuracy of the estimate. The method is effective on synthetic datasets. The overall quality of the paper is good (particularly the proof of Theorem 1). 

Below are a few comments regarding the current submission:
1) When d is significantly bigger than k, is it possible to directly obtain M^k and compute its trace? Matrix multiplication for sparse matrices is highly efficiently. Would Trace(M_0^k) provide a good estimate for Trace(M^{k}) when M_0 is sampled from M? If so, would this be more efficient than $O(d^{2.373}$?
2) Alternatively, when k is even, trace(M^k) is the same as the squared sum of the entries of M^{k/2}. Is it possible to estimate that from M_0^{k/2}?
3) The bound in theorem 1 seems to be not very tight. rho^2 is bigger than $(\kappa\mu)^{2\kappa}g(k)$, where $\kappa$ is the condition number, and \mu is the incoherence number. It seems to be \rho is fairly bigger. If \rho is bigger then, r^{1-k/2} has to be significantly smaller than $d\cdot p$, in order for the current bound to be useful. I would like to see comments on this. It is good to given a simplified bound on some simple matrices, e.g., the matrix whose entries are 1, or something similar. 
4) $d$ is usually reserved for vertex degree, and it is recommended to use $n$ and $m$ for matrix dimensions. 
5) Are there any real examples? 

I would like to see how to address these issues in the rebuttal. 



","Summary of paper:
The paper proposes an estimator for the Schatten norm of a matrix when only few entries of the matrix is observed. The estimator is based on the relation between the Schatten k-norm of a matrix and the total weight of all closed k-length walks on the corresponding weighted graph. While the paper restricts the discussion to symmetric positive semi-definite matrices, and guarantees given for uniform sampling, the generic principle is applicable for any matrix and any sampling strategy. The results hold under standard assumptions, and corollaries related to spectrum and rank estimation are also provided in the appendix.

Clarity:
The paper is very well written and organised, including some of the material in the supplementary. The motivation for the work, and the discussion that builds up to the estimator and well presented. The numerical simulations are also perfectly positioned.

Quality:
The technical quality of the paper is high. The main structure of the proofs are correct, but I didn’t verify the computations in equations (26)-(36).
The most pleasing component of the theoretical analysis is the level at which it is presented. The authors restrict the discussion to symmetric psd matrices for ease of presentation, but the sampling distribution is kept general until the final guarantees in Section 3.1. The authors also correctly note estimating the Schatten norm itself is never the end goal, and hence, they provide corollaries related to spectrum and rank estimation in the appendix. The numerical simulations do not involve a real application, but clearly show the advantage of the proposed estimator over other techniques when sampling rate is very small. That being said, I feel that further numerical studies based on this approach is important.

Significance:
The paper considers a classical problem that lies at the heart of several matrix related problems. Hence, the paper is very relevant for the machine learning community. The generality at which the problem is solved, or its possible extensions, has several potential applications.
To this end, I feel that it will be great if the authors can follow up this work with a longer paper that also includes the more complicated case of rectangular matrices, analyses other sampling strategies (like column sampling / sub matrix sampling), and more thorough experimental studies.

Originality:
Since the underlying problem is classical, one should consider that many ideas have been tried out in the literature. I feel that the individual components of the discussion and analysis are fairly standard techniques (except perhaps the computational simplification for k<=7). But I feel that the neat way in which the paper combines everything is very original. For instance, the use of polynomial concentration in the present context is intuitive, but the careful computation of the variation bound is definitely novel.

Minor comments:
1. Line 28-29: Can we hope to estimate the spectrum of a matrix from a submatrix? I am not very optimistic about this.
2. Appendix B: It would be nice if a description of this method is added. In particular, what is the time complexity of the method?
3. Line 238,262,281,292: In standard complexity terminology, I think d^2p = O(something) should actually be d^2p = \Omega(something). Big-O usually means at most upto constant, whereas here I think you would like to say that the number of observed entries is at least something (Big-Omega).
4. Line 263: How do you get the condition on r? The third term in (8) is less than 1 if r is less than d^(1-1/k). ","This paper presents a new estimator which is effective to predict the matrix schatten norm. The main technique is constructing a k-cyclic pseudograph to fit the statistics of the matrix. Though the quantity ""k"" is restricted to be smaller than 8, it seems not a big hurt to the contribution since many applications involve only ""k = 1 or k = 2"".

I would like to see more discussion on how to extend the results to general non-symmetric matrices. Authors claimed that the algorithm can naturally be generalized, but I wonder whether the theoretical results still hold.

It is also worth mentioning the major difference from [2] and [12]."
On the Power of Truncated SVD for General High-rank Matrix Estimation Problems,"Simon S. Du, Yining Wang, Aarti Singh",https://proceedings.neurips.cc/paper/2017/hash/89f0fd5c927d466d6ec9a21b9ac34ffa-Abstract.html,"This paper shows that given an estimate that is close to the target high-rank PSD matrix A in spectral norm a simple truncated SVD gives an estimate of A that is also close to A in Frobenius norm as well. This leads to a number of applications: high-rank matrix completion, high-rank matrix denoising and low-dimensional estimation of high-dimensional covariance. The paper also gives a bound that depends on the sigma_k - sigma_{k + 1} gap (Theorem 2.2).

Overall, I think this is a pretty strong paper that gives a nice set of results. Specific discussion of some of the strengths and weaknesses is below.

Strengths:
-- Clean set of results, good presentation and clear comparison with the previous work.
-- The algorithm is very simple and is very practical.

Weaknesses:
-- The constant in multiplicative error depends on k and delta
-- Results for high-rank matrix completion are conditional on the matrix satisfying a mu_0-spikiness condition which I didn’t find very natural
-- No experimental results are given

Technical comments:
-- In theorem 2.1 you need a bound on delta in terms of \epsilon and \sigma_{k + 1}. How do you ensure such a bound without knowing sigma_{k + 1}?
","The paper gives an aftermath analysis on the quality of matrix approximation under the Frobenius norm given a bound on the spectral norm. I have capacity to plowing through dense papers so long as they are well put, the results are interesting, and the techniques are novel. I am afraid to say that after 4 pages I felt a strong urge to cease reading the paper. I slogged through the end... The paper does improve the approximation result AM'07. However, the writing is so dense and unfriendly that I wasn't able to distill the gist of it. I will be happy to stand corrected but the paper seems a much better match for ALT / COLT.","This paper shows that given an estimate that is close to a general high-rank PSD matrix in spectral norm, the simple SVD of A produces a multiplicative approximation of A in Frobenius norm (Theorem 2.1). 

This result is discussed with three problems: High-rank matrix completion, High-rank matrix denoising and Low-dimensional approximation of high-dimensional covariance. 

This paper is well presented, but I would expect more discussions on the results and applications (not only refer to another work). More discussions on related work would be even better. "
TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning,"Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li",https://proceedings.neurips.cc/paper/2017/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html,"The paper proposes a mechanism to reduce the traffic of sending gradients from the workers to the server, by heavily quantizing them. Unlike previous work, they quantize the gradients into the three levels as the name of the paper implies. They show that this allows DNN training from scratch and provides convergence guarantees. Thus, this is a significant improvement over the 1-bit quantization case. 

Suggestion: The authors should elaborate the choices made in layer-wise ternarizing in the experiments reported. An analysis related to Line 168-169 would also be useful to understand the trade-offs.
","Basic idea is to speed-up distributed training by quantizing gradients to 3 values.
Parameter Server syncs gradients, works keep local copy of parameters now and scalar
is shared among workers.

Paper is well written and easy to follow. Experimental results are strong and convincing.
Good paper, overall.

Line 106: You compare this to 32bit floats, but for many setups, quantizing to 16bit
is reasonably safe and training converges to same accuracy than when using floats for
the gradient. If you use then 4 bit for your gradient, you get a factor of 4
communication reduction when sending to the gradient server.
","The authors tried to reduce communication cost in distributed deep learning. They proposed a new method called Ternary Gradients to reduce the overhead of gradient synchronization. The technique is correct and the authors made some experiments to evaluate their method. Overall, the method is novel and interesting and may have significant impact on large scale deep learning. Some comments below for the authors to improve their paper.

1. Some notations and words should be made clearer.  For example, s is defined as the max value of abs(g), but the domain of the max operation fails to show up in the equation. 

2. The author mentioned that they tried to narrow the gap between the two bounds in the paper for better convergence, but not mentioned for better convergence rate or better convergence probability. And it’s a pity that there’s no theoretical analysis of the convergence rate.
3.  One big drawback is that the author made too strong assumption, i.e. the cost function has only one single minimum, which means the objective function is convex. It’s rare in practice. To my best knowledge, there’re many efficient algorithms for convex optimization in distributed systems. Therefore, the proposed method will not have big impacts.
4.  The author should use more metrics in the experiments instead of just accuracy.
"
GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter",https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html,"Generative adversarial networks (GANs) are turning out to be a very important advance in machine learning. Algorithms for training GANs have difficulties with convergence. The paper proposes a two time-scale update rule (TTUR) which is shown (proven) to converge under certain assumptions. Specifically, it shows that GAN Adam updates with TTUR can be expressed as ordinary differential equations, and therefore can be proved to converge using a similar approach as in Borkar' 1997 work. The recommendation is to use two different update rules for generator and discriminator, with the latter being faster, in order to have convergence guarantees.

***Concerns:
      
1-Assumptions A1 to A6 and the following Theorem 1 are symmetric wrt h and g, thus one can assume it makes no difference which one of the networks (discriminator or generator) has the faster updates. A discussion on why this symmetry does not hold in practice, as pointed out by the authors in introduction, is necessary.
      
2-The assumptions used in the proof should be investigated in each of the provided experiments to make sure the proof is in fact useful, for example one of the key assumptions that a = o(b) is clearly not true in experiments (they use a = O(a) instead).
      
3-The less variation pointed out in Figure 2 are probably due to lower learning rate of the generator in TTUR compared to SGD, however since the learning rate values are not reported for Figure 2, we can not be sure.
      
4-In Figure 6, variance and/or confidence intervals should be reported.
      
5-The reported learning rate for TTUR appears to be only linearly faster for the discriminator (3 times faster in Figure 6, 1.25 times faster in Figure 3). This is not consistent with the assumption A2. Moreover, it is well known that a much faster discriminator for regular GANs undermines the generator training. However, according to Theorem 1, the system should converge. This point needs to be addressed.
      
6-There is insufficient previous/related work discussion, specifically about the extensive use of two time updates in dynamic systems, as well as the research on GAN convergence.
      
7-In general, theorems, specifically Theorem 2, are not well explained and elaborated on. The exposited ideas can become more clear and the paper more readable with a line or two of explanation in each case.

      
In summary, while the proofs for convergence are noteworthy, a more detailed investigation and explanation of the effect of assumptions in each of the experiments, and how deviating from them can hurt the convergence in practice, will help clarify the limitations of the proof.
","The authors of the submission propose a two time-scale update rule for GAN training, which is essentially about using two different learning rates (or learning rate schedules) for generator and discriminator.

I see the two main contributions of the submission as follows: 1) a formal proof of GAN convergence under TTUR assumptions, and 2) introduction of FID (Frechet Inception Distance) score for more meaningful measurement of performance. Contribution 1) alone is providing a significant step toward the theoretical understanding of GAN training. Contribution 2) provides a useful tool for future evaluation of generative model performance, and its motivation is clear from Section 4.

Some minor comments:
- I'm not quite sure what I'm supposed to see in Figures 4 vs. 5, and they take up a lot of relative space in the submission.
- How were the final learning rates chosen? I feel like I'm missing the description of a recipe, in case this goes beyond brute-force search.","The paper proposes a two time-scale update rule (TTUR) for GAN training, which is to use different learning rates for the discriminator and the generator. Under certain assumptions, TTUR allows the proof of convergence of GANs, and the results carry to Adam. To evaluate the effectiveness of TTUR, the paper additionally proposed Frechet Inception Distance (FID), i.e., the approximate Frechet distance in the code space defined by the Inception model. Empirically, by tuning the two learning rates carefully, TTUR is able to achieve faster and more stable training as measured by FID, with comparable sample quality given the same FID score. 

The practical technique introduced by the paper is relatively simple and the contribution lies in the theoretical analysis of the convergence under TTUR. The assumptions and theorem in Section 2 mostly follow [1], and a similar analysis has been seen in proving the convergence of actor-critic algorithms using two time-scale update in reinforcement learning [2]. The result for Adam includes some careful treatment for the second-moment statistics. The intuitive explanation from line 76 to line 81 is helpful for understanding but too concise. More elaboration may help readers to appreciate the difference between TTSU and equal time scale update, and why the TTSU makes the difference here.

As for experiments, it’s nice to see that a single update for the critic in TTUR improves upon the 10 updates for the critic in the improved WGAN in terms of wall time. But it would be good if the progress in terms of the number of outer iterations were also provided since it gives a sense of the algorithmic difference between one update with a larger learning rate vs. more updates with a smaller learning rate.

Overall, despite the technical similarity to some existing work, the paper gives a proof for the convergence of GAN training under TTUR including Adam, which is a good contribution to the theory of GAN and should be included in the literature. 

[1] V. S. Borkar. Stochastic approximation with two time scales. 
[2] Konda, Vijay R., and John N. Tsitsiklis. ""Actor-critic algorithms."""
A Unified Approach to Interpreting Model Predictions,"Scott M. Lundberg, Su-In Lee",https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html,"The authors show that several methods in the literature used for explaining individual model predictions fall into the category of ""additive feature attribution"" methods. They proposes a new kind of additive feature attribution method based on the concept of Shapely values and call the resulting explanations the SHAP values. The authors also suggest a new kernel called the shapely kernel which can be used to compute SHAP values via linear regression (a method they call kernel SHAP). They discuss how other methods, such as DeepLIFT, can be improved by better approximating the Shapely values.

Summary of review:
Positives:
(1) Novel and sound theoretical framework for approaching the question of model explanations, which has been very lacking in the field (most other methods were developed ad-hoc).
(2) Early versions of this paper have already been cited to justify improvements in other methods (specifically New DeepLIFT).
(3) Kernel SHAP is a significantly superior way of approximating Shapely values compared to classical Shapely sampling - much lower variance vs. number of model evaluations (Figure 3).
Negatives:
(1) Algorithm for Max SHAP is incorrect
(2) Proof for Kernel SHAP included in supplement is incomplete and hastily written
(3) Paper has significant typos: main text says runtime of Max SHAP is O(M^3) but supplement says O(M^2), equation for L under Theorem 2 has a missing close parenthesis for f()
(4) Argument that SHAP values are a superior form of model explanation is contentious. Case studies in Figure 4 have been chosen to favor SHAP. Paper doesn't have a discussion of the runtime for kernel SHAP or the recommended number of function evaluations needed for good performance. LIME was not included in the Figure 5 comparison.

Detailed comments:

The fact that Shapely values can be adapted to the task of model explanations is an excellent insight, and early versions of this paper have already been used by other authors to justify improvements to their methods (specifically New DeepLIFT). Kernel SHAP seems like a very significant contribution because, based on Figure 3, it has far lower variance compared to Shapely sampling when estimating the true Shapely values. It is also appealing because, unlike the popular method LIME, the choice of kernel here is motivated by a sound theoretical foundation.

However, there are a few problems with the paper. The algorithm for Max SHAP assumes that when i1 is greater than i2 and i2 is the largest input seen so far, then including i1 contributes (i1 - max(reference_of_i1, i2)) to the output. However, this is only true if none of the inputs which haven't been included yet have a reference value that exceeds max(reference_of_i1, i2). To give a concrete example, imagine there are two inputs, a and b, where a=10 and b=6, and the reference values are ref_a = 9 and ref_b = 0. The reference value of max(a,b) is 9, and the difference-from-reference is 10. The correct SHAP values are 1 for a and 0 for b, because b is so far below the reference of a that it never influences the output. However, the line phi[ind] += max(ref,0)/M will assign a SHAP value of 3 to b (ref = xsorted[i] - r[ind], which is 6-0 for b, and M=2). In the tests, it appears that the authors only checked cases where all inputs had the same reference. While this is often true of maxpooling neurons, it need not be true of maxout neurons. Also, the algorithm seems difficult to implement on a GPU in a way that would not significantly slow down backpropagation, particularly in the case of maxpooling layers. That said, this algorithm is not central to the thesis of the paper and was clearly written in a rush (the main text says the runtime is O(M^3) but the supplement says the runtime is O(M^2)).

Additionally, when I looked up the ""Shapely kernel proof"" in the supplement, the code provided for the computational proof was incomplete and the explanation was terse and contained grammatical errors. After several re-reads, my understanding of the proof is that:
(1) f_x(S) represents the output of the function when all inputs except those in the subset S are masked
(2) Shapely values are a linear function of the vector of values f_x(S) generated by enumerating all possible subsets S
(3) kernel SHAP (which is doing weighted linear regression) also produces an answer that is linear in the vector of all output values f_x(S)
(4) therefore, if the linear function in both cases is the same, then kernel SHAP is computing the Shapely values.
Based on this understanding, I expected the computational proof (which was performed for functions of up to 10 inputs) to check whether the final coefficients applied to the vector of output values when analytically solving kernel SHAP were the same as the coefficients used in the classic Shapely value computation. In other words, to use the standard formula for weighted linear regression, the final coefficients for kernel SHAP would be the matrix (X^T W X)^{-1} X^T W where X is a binary matrix in which the rows enumerate all possible subsets S and W are the weights computed by the shapely kernel; I expected the ith row of this matrix to match the coefficients for each S in the equation at the top of the page.
However, it appears that the code for the computational proof is not comparing any coefficients; rather, for a *specific* model generated by the method single_point_model, it compares the final Shapely values produced by kernel SHAP to the values produced by the classic Shapely value computation. Specifically, the code checks that there is negligible difference between kernel_vals and classic_vals, but classic_vals is returned by the method classic_shapely which accepts as one of its arguments a specific model f and returns the shapely values. The code also calls several methods for which the source has not been provided (namely single_point_model, rawShapely, and kernel_shapely).

The statement that SHAP values are a superior form of model explanation is contentious. The authors provide examples in Figure 4 where the SHAP values accord perfectly with human intuition for assigning feature importance, but it is also possible to devise examples where this might not be the case - for example, consider a filter in a convolutional neuron network which has a ReLU activation and a negative bias. This filter produces a positive output when it sees a sufficiently good match to some pattern and an output of zero otherwise. When the neuron produces an output of zero, a human might assign zero importance to all the pixels in the input because the input as a whole did not match the desired pattern. However, SHAP values may assign positive importance to those pixels which resemble the desire pattern and negative importance to the rest - in other words, it might assign nonzero importance to the input pixels even when the input is white noise.

In a similar vein, the argument to favor kernel SHAP over LIME would be more compelling if a comparison with LIME were included in Figure 5. The authors state that didn't do the comparison because the standard implementation of LIME uses superpixel segmentation which is not appropriate here. However, I understand that kernel SHAP and LIME are identical except for the choice of weighting kernel, so I am confused why the authors were not able to adapt their implementation of kernel SHAP to replicate LIME. Some discussion of runtime or the recommended number of function evaluations would have been desirable; the primary reason LIME uses superpixel segmentation for images is to reduce computational cost, and a key advantage of DeepLIFT-style backpropagation is computational efficiency.

The authors show a connection between DeepLIFT and the SHAP values which has already been used by the authors of DeepLIFT to justify improvements to their method. However, the section on Deep SHAP is problematic. The authors state that ""Since the SHAP values for the simple components of the network can be analytically solved efficiently, this composition rule enables a fast approximation of values for the whole model"", but they explicitly discuss only 2 analytical solutions: Linear SHAP and Max SHAP. Linear SHAP results in idential backpropagation rules for linear components as original DeepLIFT. As explained earlier, the algorithm for Max SHAP proposed in the supplement appears to be incorrect. The authors mention Low-order SHAP and state that it can be solved efficiently, but Low-order SHAP is not appropriate for many neural networks as individual neurons can have thousands of inputs, and even in the case where a neuron has very few inputs it is unclear if the weighted linear regression can be implemented on a GPU in a way that would not significantly slow down the backpropagation (the primary reason to use DeepLIFT-style backpropagation is computational efficiency).

Despite these problems, I feel that the adaptation of Shapely values to the task of model interpretation, as well as the development of kernel SHAP, are substantive enough to accept this paper.

REVISED REVIEW: The authors have addressed my concerns and corrected the issues.","The paper introduces a united framework for explaining model predictions on a single input. Under the class of additive feature importance, there is a only one solution that satisfies a set of desired properties. Efficient approximation methods to SHAP values are proposed.

The authors are tackling a very important problem that draws lots of attention in recent years. The paper is clearly written and it is pleasure to read. The properties proposed for desirable methods are intuitive, though the local accuracy criterion seems too restrictive. It is very novel to use the Sharpley value and give its associated theoretical properties. One sanity check is that when the original model is already interpretable, i.e. the features are 0/1, the method can return the same model. However it seems from Corollary 1 this is not the case since E[x_j] is nonzero. Can the authors provide more explanation?  Also, more details about the computation cost(time) of DeepSHAP can be provided to help people have a clear expectation of the method when applying that in practice.","Summary:
There are many recent efforts in providing interpretability to complex models, such as LIME, DeepLift and Layer-wise relevance propagation. In complex models, change in input features contributes to the output change nonlinearly. Many previous works tried to distribute the feature contributions fairly. In this work, the author pointed that Shapley value -- marginal contribution of each feature -- from game theory is the unique solution that satisfies desired properties. The author provides a unified framework for many of the interpretable methods by introducing the concept of additive feature attribution methods and SHAP as a measurement for feature importance. A SHAP value estimation method is provided. The author supported the method with user studies and comparisons to related works on benchmark datasets.

Quality:
This paper is technically sound with detailed proofs and supportive experiments. I think there could be more discussion on the shapley value estimation accuracy vs computational efficiency.

Clarity:
The paper organized the materials well. The author provides enough background and motivations. However, in the implementation of model specific approximations on page 6, section 4.2, it could be more helpful if the author can provide an algorithm flow (for example, how do we change the algorithm in DeepLift to satisfy SHAP values).
There are some other minor issues about the clarity:

In page 4, equation (10), \bar{S} is not defined, should it be \bar{S} \subseteq S, and the last symbol in the row should be x_{\bar{S}} instead of x?

In page 6, line 207, the citation style seems to be inconsistent.

Originality:

I think this paper nicely pointed out that the proper distribution of the feature contribution is Shapley value from game theory. And the author linked the Shapley value to many existing method to identify the proper way of constructing the methods by proofs. It is interesting that the author explains the enhanced performance in original vs current DeepLift from better approximation in shapley value.

I am wondering if the author could also link the shapley value to the gradient-based approaches [1,2], since there are still a lot of interpretations based on these approaches. 

[1]: Simonyan et al. (2013): ""Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"", http://arxiv.org/abs/1312.6034
[2]: Springenberg et al. (2015): ""Striving for Simplicity - The All Convolutional Net"", http://arxiv.org/abs/1412.6806

Significance:
This work provides a theoretical framework for feature importance methods. The better defined metric SHAP could become a guidance of feature importance evaluation in the research field of interpretable methods. 
"
Nonbacktracking Bounds on the Influence in Independent Cascade Models,"Emmanuel Abbe, Sanjeev Kulkarni, Eun Jee Lee",https://proceedings.neurips.cc/paper/2017/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html,"Nonbacktracking Bounds on the Influence in Independent Cascade Models develops bounds on the expected number of nodes that eventually would be ""infected"" from a seminal set. The bounds computations depend on a recursive computation that is of lower order than a Monte Carlo computation to the same end, and obtains an estimate of variance at no additional computational cost.  The propagation ends when all nodes have been ""touched"", with the probability diminishing of becoming infected as the length of the path from the source nodes increases. The recursion takes care not to propagate back to the previous node from which the infection came, hence the non-backtracking property.  A more exact method would exclude not only immediate backtracking, but any loop formed by the propagation path, as the authors recognize. 

Not being specifically versed in this literature, the paper appears correct and novel.  It is apparently written for a specific audience who is familiar with current approaches to this problem. ","The paper develops upper and lower bounds, using correlation inequalities, on the value of the influence function (expected influenced set size) under the Independent Cascade model. The work is a clear improvement on the upper bounds of Lemonnier et al., NIPS 2014. The theoretical results are a solid contribution towards a bound-based analysis of influence computation and maximization, though the evaluation could be improved to better evaluate and communicate the impact of the results. 

1) As future work building on this line of bounds, It would be interesting to see work connecting them to actual influence maximization, e.g. running a greedy IM procedure, and comparing the seed sets under greedy IM vs. seed sets returned by greedy maximization over the UB or LB measures. It would have been nice to see some dialogue with maximization as part of the work, but it would have been difficult to fit into the short format.

2) In the Experimental Results (Section 4), it would be preferable to see the evaluation decomposed over the two main sources of randomness being studied, the graph and the seed node {s]. It would be interesting to dig into properties of the seed nodes for which the bounds are tighter or looser (degree? Centrality? etc.) It would also be preferable to see an evaluation of larger seed sets, as part of the difficulty with computing influence is interactions between seed nodes.

3) An obvious omission is the study of real world graphs, which have higher degrees and contain many more triangles than any of the simulated graphs considered in the paper. The higher degrees make backtracking steps in random walks less frequent. The triangles and other cycles generally impede message passing algorithms. It’d be worth evaluating and commenting on these considerations. 

4) The state of the art for fast IM computation with approximation guarantees is, I believe, (Tang-Xiao-Shi, SIGMOD 14). Worth including in the citations to IM papers. Furthermore, the use of the word “heuristic” on Line 20 is usually reserved for algorithms without formal guarantees. The methods in the citation block [10, 17, 3, 7, 20] almost all feature formal guarantees for IM in terms of worst-case approximation ratios (some in probabilistic senses). To call all these papers “heuristic” feels overly dismissive. As a related comment on the literature review, on Line 32 ""A few exceptions include” makes it sound like there is going to be more than one exception, but only one is given. Fine to rewrite to only expect on exception, better to give more cites.","Finding influence in the IC model is a #P hard problem, and as such one cannot expect to solve it exactly and this motivates the search for approximating influence efficiently. Monte-Carlo methods may be used, but they may be computationally expensive based on the graph characteristics. This paper looks to obtain lower and upper bounds on the influence, which (if tight) serve as a proxy for the actual influence, and otherwise serve as a way to bound the variance of the influence which may then be useful for a Monte-Carlo algorithm. The paper presents both upper and lower bounds, with the main idea being that influence is given by the counting the probabilities of all simple paths and summing them up, so an upper bound would ""over-count"" and a lower bound would ""under-count"" in some suitable way. For the upper bound, the paper proposes using non-backtracking paths and gives an efficient algorithm that counts the path weights for all such paths. For the lower bound, a directed acyclic subgraph is constructed and influence for this subgraph is used to bound the influence of the actual graph. 
The paper is technically correct, and reasonably well-written. The upper bound and the algorithm to efficiently compute it is novel. One negative aspect is that is no theoretical study of tightness of these bounds. It will be good to understand for what kinds of graphs is the upper bound tight. (Presumably the lower bound is tight for DAGs.) "
Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls,"Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, Yuanzhi Li",https://proceedings.neurips.cc/paper/2017/hash/8b8388180314a337c9aa3c5aa8e2f37a-Abstract.html,"The result considers a very relevant and realistic setting when the true solution has low rank, and the algorithm is very simple. The paper is creative, overall well written and most times clearly distinguishes the presented results from existing ones. The additional practical impact compared to [6] might not be huge but still valuable, and the paper gives several interesting insights on the side.

A unfortunate point at the moment is that the algorithm is not set into context with projected gradient onto a rank-k constraint (with bounded nuclear norm, and  importantly in the otherwise unconstrained case also). Your Equation (3.1) is less closer to FW but much more to projected gradient, for which a rich literature already exists.
This is especially blatant for the practical rank-k iterates version (Section 5), where the new iterate simply is W_t (as in PG), and no FW step update is even performed.

Could your result be extended to k-restricted strong convexity (only strongly convex over the rank-k directions). Plenty of papers have studied analogous results for the vector case, but are not cited here.

Equation (1.2) and following: You don't clearly explain the dependence on T for your method. The discussion of eps-dependence on the inner loop complexity of [6] is appreciated, but you don't explain why lazySVD can't be used within [6] as well. At least explain how your Theorem 4.3 with k=1 serves the rank-1 case.
Also, the paper would benefit from a more clear discussion of the approximation quality to which the internal (1,k)-SVD need to be solved (which is well known for the FW case). It is discussed nicely for the new method, but not for the rank-1 ones (FW and [6]).

Instead of some of the more technical lemmas and details, I'd prefer to have more on Section 5 (keeping the iterates rank k) in the paper, as this is more crucial for potential users. Maybe add the word 'iterates' in the title for clarity.

- minimum non-zero singular value of X^*, so it satisfies \sigma_min(X^*) <= 1/k:
""so that it satisfies"" should be rephrased: If it is satisfied, your comparison holds, if not, it is not outperformed, right?


Minor comments:
 - line 29: Recently, there were
 - second footnote on page 2: ""with a meaningful theoretical support"", ""reasonable"": what is meant here? better reformulate
 - line 218: Turing completeness: for how many layers?
 
== update after rebuttal ==
Please clarify the nomenculature and equivalence with PGD with projection onto the non-convex set of the rank-constraint, as this is crucial for the context of the paper. Otherwise, the authors have clarified most questions. 
For restricted strong convexity, indeed please discuss at least the direction and concept. See e.g. https://arxiv.org/pdf/1703.02721 for the matrix case, and for the easier vector case e.g. http://papers.nips.cc/paper/4412-greedy-algorithms-for-structurally-constrained-high-dimensional-problems.pdf","
---- After the rebuttal ----
I cannot help but notice some similarities of this method and the CLASH method in the vector case:
https://arxiv.org/pdf/1203.2936.pdf

The CLASH authors in effect make a projection onto the rank-k ball (corresponding to Combinatorial part in the CLASH method), followed by the projection onto the rank constraint (Least Absolute SHrinkage part). The latter is now a vector problem thanks to the affine invariance of the Frobenius norm in this paper. The authors in the CLASH work on the vector case directly; however, as opposed to performing the simple shrinkage update as in this paper, the CLASH authors actually solve the ""fully corrective problem."" Perhaps, this would be of interest to the authors.
-----
The main idea of this paper is based on a simple observation: injecting a rank-k constraint into the subproblem in PGD does not affect 
the convergence rate, as soon as the solution rank is less than or equal to k. This simple observation however leads to an algorithm with linear convergence rate that possess the benefits of FW method: low per iteration computational cost and low-rank updates. Note however, from a purely mathematical standpoint, I would argue calling this method as a Frank-Wolfe variant, as the rigorous description of FW is composed of the computation of a search direction and the application of a linear minimization oracle, followed with an averaging step. This algorithm however is based on a nonconvex quadratic subproblem, but this subproblem can be solved with theoretical guarantees efficiently.
                                        
That would be interesting to see the practical implications of the results section 5 that bounds the rank of the decision variable. Did you try this variant with replacement step in some numerical experiments? I would suggest addition of these results into the appendix. Note that there is a particular recent research direction about the storage efficiency of FW variants, e.g. [1].
 
In the current version of the manuscript, theoretical guarantees hold only if the solution rank is less than or equal to the input rank k. In practice, however, the solution is not exactly low rank due to the presence of the noise and imperfections in tuning the constraint radius. It is approximately low rank with a fast decay in the spectrum. As far as I can see the proof still holds when solution X* is replaced with the best rank-k approximation of the solution [X*]_k (see e.g. the guarantees provided in [1]). This would impose theoretical guarantees when f(X*) – f([X*]_k) is small.
                                                                                                        
The numerical experiments fall short of satisfaction for the nips venue. All the experiments with real data are solved using the auto selection of k heuristic. It is not clear from the text if this heuristic still possesses the theoretical guarantees of the original variant where k is fixed in advance. Plots with #1-SVD computations at x axis are very useful to compare the performance in terms of the overall computational cost. However, I would still suggest adding some information about the k chosen at each iteration while solving the real data problems. It would be also interesting to see some practical evidence of what happens when the input rank k is chosen slightly less than the solution rank, or when the solution is not low rank but it has a fast decay in the singular value spectrum.
 
As noted in the paper, algorithm outperforms other convex competitors in the convergence rate when the constraint radius is small, but there is no significant advantage when the radius is large. This is an expected result as the smaller radius tempts a lower rank solution. However, it is not possible to deduct which values of the radius are practically reasonable in the experiments presented in the paper (i.e. which values of theta results in lower test error).

 [1] Yurtsever et.al. Sketchy decisions: Convex low-rank matrix optimization with optimal storage. AISTATS, 2017.


 ","The paper modified the Frank-Wolfe algorithm for strongly convex optimization over the trace-norm ball, by changing 1-svd with k-svd. It is proven that the convergence rate was improved from sublinear to superlinear. The theoretical results are solid and thorough. Experimental results show advantages of the proposed algorithm. I have the following minor comments:
1. It is understandable that if the rank is underestimated, then the optimal solution will not be approached. However, it is unclear whether if the rank is overestimated, then the optimal solution can still be achieved. Such an issue should be analyzed or evaluaed.
2. I don't know why the minimum nonzero singular value of X^* should be <= 1/k in line 51.
3. Although the theorems assumed strong convexity, the examplar models in experiments are not strongly convex. Can the authors replace them with other practical problems that fit for the assumptions?
4. In polynomial neural network, I believe that the rank cannot be small for good classification performance.

Finally, the authors revealed their identity by writing ""our immediate prior work [6]"" in line 58.

=== review update ==
I don't object accepting the paper if the identity-revealing issue is ignored. Hope what the authors said ""none of the author(s) of the current paper is an author of [6]."" is true."
Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach,"Roel Dobbe, David Fridovich-Keil, Claire Tomlin",https://proceedings.neurips.cc/paper/2017/hash/8bb88f80d334b1869781beb89f7b73be-Abstract.html,"This paper proposes the use of rate distortion theory to develop (i) decentralized policies and (ii) distributed policies with restricted number of communication for approximating an optimal centralized controller.  The problem is very relevant in light of more complex and larger decision problems today, and I find the general idea very interesting.  However, some key aspects did not become clear to me and need to be addressed; in particular: is the considered decision problem static or dynamic?; missing discussion on stability; how to solve the centralized problem?; and a discussion on related work on decentralized control.


Comments:

1) From the general problem formulation, it did not become clear to me whether the authors consider a static or dynamic decision problem, i.e. is the optimization problem solved once, or is a sequential decision problem considered over a time horizon.  The problem (1) is a static problem (u is introduced as the collection of the inputs of all agents, but not over several time steps).  However, later the authors typically talk about policies, which alludes to some decision making problem over time.  This must be clarified in order to appreciate the result and compare it to others, e.g. those mentioned in related work.  I suggest to possibly revisit the problem formulation, and in any case state whether a static or dynamic problem is considered.  Also for the example in Section 5, it is not clear if this problem needs to be treated as a dynamic problem, or whether the decision is simply considered for each time individually (and thus can be seen as a static problem).

2) In the introduction, the authors mention relevant work on distributed systems from optimization, game theory, and POMDPs.  There is also a large body of work on distributed and decentralized methods from systems and control theory, which should be mentioned in this context (see e.g. book by Siljak “Decentralized control of complex systems,” or Lunze “Feedback control of large-scale systems” on some classical work).  Furthermore, in the discussion of Related work in Section 2, the authors point out as a main novelty that they seek decentralized policies with no communication (rather than distributed with some communication).  The problem of decentralized control is a classic one in control theory (see e.g. above books), so considering multi-agent policies with no communication is by itself not new at all.  The authors should mention this work and balance their statements on novelty.

3) The centralized problem (1) may by itself represent a difficult optimization problem.  However, the authors do assume availability of optimal control data {u^*,x} in their later approximations.  I think that the authors should discuss, how they expect the centralized problem can be solved and/or clearly state the availability of a solution as an assumption.

4) Unless the authors limit their attention of this work to static decision problems, the aspect of dynamic stability should be mentioned (even if not a contribution of this work).  In particular, I would expect that stability can potentially be lost when moving from the centralized policy to the decentralized one because there seems to be no particular objective in the decentralized learning/approximation that would ensure or incentivize a stable controller.  This should be discussed in order to make the reader aware of this (possibly critical) aspect of the controller.

5) The authors present the main technical results for a centralized controller whose state and input distribution is jointly Gaussian (sec. 3.3 + 4.1).  Is this a reasonable assumption?  What decision problems are of this kind (maybe linear problem with Gaussian uncertainty??)?

6) In Section 5, why is the restriction to linear power flow equations without loss of generality?

7) I do not understand figure 4(a).  What is the grey versus the white region?  What quantity do the graphs represent?
","In the context of learning cooperative policies for multi-agent systems, it often requires some sort of communications among agents. This paper considers the case where no communication is available. The authors provided an information theoretical framework based on rate distortion theory to solve multi-agent system problem in a near-optimal way with decentralized policies that are learned with data from historical measurements. The authors also provided a natural extension to address which nodes a node should communication with to improve the performance of its individual policy. The proposed framework was theoretically proven and also illustrated by solving an OPT problem in power system operation. The paper is very well written and the results are technically correct. The problem is also huge interesting to the community. Thus, I strong recommend its acceptance. 
I only has some minor comments regarding the application about DER coordination: 
There are other recent works which considers DER optimal actions. For example, 
1) https://arxiv.org/abs/1610.06631
2) http://ieeexplore.ieee.org/document/7961194/
If possible, could you please compare your work in this regard with the existing ones?

","This paper considers reinforcement learning in decentralized, partially observable environments. It presents an ""information theoretic framework based on rate distortion theory which facilitates analysis of how well fully decentralized policies are able to reconstruct the optimal
solution"" and presents an extension ""that addresses which nodes an agent should communicate with to improve the performance of its individual policy."" The paper presents some proof of concept results on an optimal power flow domain.

On the positive side, the idea of applying rate distortion to this problem is interesting and as far as I can tell novel. I rather like it and think it is a promising idea.

A major problem, however, is that even though the paper is motivated from the perspective of sequential problems, the formalization is (1) is a static problem. (The authors should describe in how far this corresponds to a DCOP problem). In fact, I believe that the approach is only approximating the right objective for such 1-shot problems: the objective is to approximate the optimal *centralized* policy p(u* | x). However, such a centralized policy will not take into account the affect of its actions on the distribution of information over the agents in the future, and as such much be sub-optimal from in a *decentralized* regime. Concretely: the optimal joint policy for a Dec-POMDP might sacrifice exploiting some information if it can help maintaining predictability and therefore the future ability to successfully coordinate.

Another weakness of the proposed approach is in fact this dependence on the optimal centralized policy. Even though computing the solution for a centralized problem is easier from a complexity point of view (e.g., MDPs are in P), this in practices is meaningless since---save very special cases---the state and action spaces of these distributed problems are exponential in the number of agents. As such, it is typically not clear how to obtain this centralized policy.
  		
The paper gives a highly inadequate characterization of the related work. 

For instance just the statement ""In practice however, POMDPs are rarely employed due to computational limitations [Nair et al., 2005]."" is wrong in so many ways:
-the authors mentions 'POMDP' but they cite a paper that introduces the 'networked distributed POMDP (ND-POMDP)' which is not a POMDP at all (but rather a special case of decentralized POMDP (Dec-POMDPs)).
-the ND-POMDP makes very strong assumptions on the interactions of the agents ('transition and observation dependence'): essentially the agents cannot influence each other. Clearly this is not the use case that the paper aims at (e.g., in power flow the actions of the agents do affect each other), so really the paper should talk about (perhaps factored) Dec-POMDPs.
-Finally, this statement seems a way to try and dismiss ""using a Dec-POMDP"" on this ground. However, this is all just syntax: as long as the goal is to solve a decentralized decision making problem which is not a very special sub-class** one can refuse to call it a Dec-POMDP, but the complexity will not go away.

**e.g., see: Goldman, C.V. and Zilberstein, S., 2004. Decentralized control of cooperative systems: Categorization and complexity analysis.

Also the claim ""this method is the first of its kind to provide a data-driven way to learn local cooperative policies for multi-agent systems"" strikes me as rather outrageous. I am not quite certain what ""its kind"" means exactly, but there are methods for RL in Dec-POMDPs that are completely decentralized, e.g.:

Peshkin L, Kim KE, Meuleau N, Kaelbling LP. Learning to cooperate via policy search. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence 2000 Jun 30 (pp. 489-496). Morgan Kaufmann Publishers Inc.

An overview of Dec-POMDP RL methods is given in:

Oliehoek, F.A. and Amato, C., 2016. A concise introduction to decentralized POMDPs. Springer International Publishing.


All in all, I think there is an interesting idea in this submission that could be shaped into good paper. However, there are a number of serious concerns that I think preclude publication at this point."
Neural system identification for large populations separating “what” and “where”,"David Klindt, Alexander S. Ecker, Thomas Euler, Matthias Bethge",https://proceedings.neurips.cc/paper/2017/hash/8c249675aea6c3cbd91661bbae767ff1-Abstract.html,"Summary:

This paper presents a CNN architecture suitable for fitting neural data from visual cortices where the same features are repeated at many locations in the visual field. The method replaces the standard fully connected layer at the end of deep CNNs with a sparse factorized layer that explains each neuron’s response in terms of a location and a ‘cell type’. A progression of more complex experiments on simulated data show that the system can correctly recover neural RFs with fewer samples. Finally, the approach is applied to calcium imaging data from mouse primary visual cortex, where it improves on the state of the art by 10%.

Major comments:

This paper presents a promising approach to neural system identification. The main architectural innovation, the factorization of the densely connected layer into sparse what and where components, is well-motivated neurally and could see widespread use in the future. 

The experiments are done to an extremely high standard, and convincingly show that the proposed approach can recover correct response mappings on challenging datasets. The visual cortex results, for instance, contain a true test set which the model will be evaluated on only after review. This is a model of rigorous methodology.

The paper also reports a variety of small tricks that may also be valuable to practitioners, such as their weight initialization scheme based on spike triggered averages.
","The main idea describes in this paper is to exploit a key property of the organization of neurons in our visual cortex to improve the fitting of CCNs to neural data. The main idea is to apply a mask over space and features on the output of feature maps is a good one.o This allows to exploit the convolutional component of the architecture and lower the sample complexity of the fitting procedure. In general, the experiments are carried out well and the paper is technically sound. My main criticism of this paper is that the overall contribution of the paper remains relatively limited. It is likely that the approach will constitute an important technical contribution as part of a larger study but by itself, the significance of the approach remains too limited to justify publication.

Minor point: spike-triggered covariance is a method, not a model (see pp. 1)
","I am no expert in this field, so right now I can not fully judge on the potential impact of the paper at hand in this field of science.

So first  I want to ask  a few clarifying questions to the authors to give me a clearer picture. I will also give a preliminary judgement at the end.

Introuction
34-47:  I think it should be mentioned that using more expressive models such like CNNs will come at a cost. Simple models such as GLMs are very interpretable, it also easier to argue for biological plausibility than with neural network trained by backpropagation. 

I do agree with your point about:""Thus, we should be able to learn much more complex nonlinear functions by combining data from many neurons and learning a common feature space from which we can linearly predict the activity of each neuron.""

Related Work:
The approaches of McIntosh et al.[17] and Batty et al. [18] seem to be the most similar to yours. What were the practical and theoretical reasons for not comparing to them in your experiments?

3 Learning a common feature space
This section describes a well-known insight from transfer learning in ML. Maybe establishing a link here is useful as you use a ML technique for a neuroscience application.

5 Learning non-linear feature spaces
Is this comparison to GLMs fair? Your proposed architecture seems to share a lot of properties with the ground truth (such as convolutional layers). 

5.2 Application to data from primary visual cortex 
It seems you needed to make a few modifications to your architecture. Can you respond to a possible çritique that says your approach only works better because you have more degrees of freedom to adjust hyperparameters to a given task?  Without these adjustments, were you still able to improve over previous state of the art?
--

The paper is well written and my initial impression is that this is a good contribution. The intuition to combining data from many neurons and learning a common feature space is intuitive and  appears straightforward. I can't fully judge if this is novel or incremental, compared to the works of  McIntosh et al.[17] and Batty et al. and I have some open questions with regards to the experiments.
 
"
Learning Active Learning from Data,"Ksenia Konyushkova, Raphael Sznitman, Pascal Fua",https://proceedings.neurips.cc/paper/2017/hash/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Abstract.html,"Key insights/novelty: 
* Trains a regression model that predicts improvement in generalisation error
* To the best of my knowledge this is an original approach to this problem (although similar, but sufficiently different, methods do exist)

Questions:
* In the experimental section, I recommend that the authors also include experiments on rotated checkerboards - the discriminating features of checkerboards are aligned to the axes, and using random forest classifiers may yield an overly optimistic performance curve. Rotating at 45 degrees may yield overly pessimistic curve, but it will explain the utility of the proposed methods better. 
* Baseline methods are not satisfactory since none of the proposed baselines can learn as much as the topology of the feature space as the proposed method (eg Zhu, Ghahramani, Lafferty ICML2003). At the very least this method should be compared in the discussions. 
* I'm not convinced by the results since they don't seem to be significantly better than baselines. Particularly with uncertainty sampling on the real data. This point relates to my suggestion of rotating the checkerboard so that the discriminating features will not be axis-aligned. 
* This method will require a relatively substantial proportion of data to be labelled to begin with. While you say that only a couple of hours were spent in labeling for the offline experiments, this time is domain-specific. Are there any techniques that can be used to improve this? 
* The conclusions of this paper were very short and underwhelming. 

General Statements: 
* Very well written, clear motivation. Easy to follow story. 

Small typos/recommended corrections: 
* L79. Unannotated data? 
* Showing the relative accuracy only shows one side of the picture. It would be insightful to show the accuracies of both too. Practitioners will be interested in knowing whether it is normally with learning strategies that have a good initial generalisation that disimprove moving forward. 
* Algorithm 1: unclear which datasets the classifiers are trained on. Recommend clarifying. And in line 14, please clarify. Do you mean $\{\delta_m: 1 \leq m \leq M\}$? 
* Should a selection function be passed into algorithm 1 to clarify line 7? ","This paper proposes a novel active learning method, which trains a regression model to predict the expected error reduction for a candidate example in a particular learning state.

The studied problem is very interesting and novel. Compared to existing active learning approaches with hand-crafted heuristics, the proposed approach can learn the selection strategies based on the information of the data example and model status.

The experiments show that this method works well both in synthetic data and real data.

At the early state of active learning, there is very few labeled data. That implies there is very few training data for the regressor, and the training data itself is not reliable. So the prediction of the regressor could be inaccurate, i.e., the predicted expected error reduction may be inaccurate, resulting less effective sample selection. 

The learning process has Q different initializations, T various labeled subset sizes and M data points in each iteration, so the matrix of observations is QMT*(K+R). If these three numbers are large, the learning process could be time consuming.

There are many state-of-the-art active learning methods. It would be more convincing if more methods compared in the experiments.
","This paper proposes a data-driven method for active learning, using a regressor to select the best strategy. 

The paper is well-organized, however, there are several concerns about the paper:
1. The contribution of this paper is not significant. The proposed method has limitation, it requires previous datasets and there is no guarantee about the performance of the warm start. Without filling the gap between the optimal regressor and the one created by Monte-Carlo approach, the experiment results are not convincing.
2. The proposed method is too heuristic, there is no theoretical foundation.
3. What is the representative dataset? It needs to be explained clearly.
4. Why did the authors not use the UCI datasets in [11]? The authors should provide some explanations about the choice of the datasets.
"
Controllable Invariance through Adversarial Feature Learning,"Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig",https://proceedings.neurips.cc/paper/2017/hash/8cb22bdd0b7ba1ab13d742e22eed8da2-Abstract.html,"Summary:
This paper is about learning representations designed with specific invariances. The authors propose a formulation that builds on Adversarial Training (Goodfellow et al 2014) by adding a new component to that handles invariances. The new model they proposed includes, an encoder, a discriminator and a predictor since their approach focuses on the classification results. They evaluate their approach on three benchmarks and show results that compete with the state of the art.

Positive Points:
-The paper is well written and easy to follow
-The formulation is interesting and the claims are sound, including the invariance learning in the hidden representation and formulating it using conditional entropies is novel.
-The evaluation outperforms the state of the art in some cases.

Negative Points:
-The results on the Fair Classification as well as the Multi-lingual Machine Translation are relatively close to the state-of-the-art which makes me question the benefit of using their approach versus a VAE.
-The authors should explain why the VAE outperformed there approach in the case of ""Health"" data in Figure1(a). More comments on the results would be useful.

I would encourage the authors to evaluate their approach on CELEB-A dataset and show visualization comparing to GANs and VAEs since CELEB-A includes many invariances that could be learned and tested on.","This paper tackles the problem of learning fair representations, that is, deducing the representation invariant to ""nuisance"" variables, and proposes adversarial training between encoder, predictor, and discriminator such that, given representation from the encoder, discriminator tries to predict nuisance variables and the predictor aims to predict the label. 

The idea is interesting, and the presented theoretical analysis is nice. Also demonstration with three tasks looks to show its effectiveness as it outperforms baselines. But I concern that, as in the multi-lingual machine translation task, using invariant representation would improve the performance of original tasks. In the 1st Fair Classifications task, it did not improve the performance with original input x, and in the 3rd Image Classification task, logistic regression with x seems a too weak baseline.

Writing is mostly clear and easy to follow.

Detailed comments:

- In the equation below the line 100, it was not clear to me how to deduce 3rd term from 2nd term.  
- In Fair Classifications task, single hidden layer NN seems too shallow for encoder and decoder, and have you tried deeper NNs for them?","The paper proposes to learn invariant features using adversarial training. Given s a nuisance factor (s attribute of x), a discriminator tries to predict the nuisance factors s  (an attribute of the input) given a encoder representation h=E(x,s)  , and an encode rE  tries to minimize  the prediction of nuisance factor and and to predict the desired output. The encoder is function of x and s.

Novelty:

The paper draws some  similarity with Ganian et al on unsupervised domain adaptation and their JMLR version. The applications to  Multilingual  machine translation and fairness applications  are to the best of the knowledge of the reviewer new in this context and are  interesting. 

Comments:

- The application of the method to invariant features learning in object recognition is not realistic. In machine translation s being the one hot of the language at hand or in fairness s being an attribute of the input , is realistic. But if one wants invariance to rotation or lightning one can not supply the angle or the lightening to the encoder. E(x,s) is not realistic for geometric invariance i.e when s is indeed hidden and not given at test time. Have you tried in this case for instance E being just E(x)?

See for instance this work  https://arxiv.org/pdf/1612.01928.pdf  that uses actually a very similar training with just E(x)

- I think the paper should be more focused on the multiple sources / single target domain adaptation aspect. Geometric invariances are confusing in this setting, since the nuisance factor can not be assumed known at test time.

- How would you extend this to deal with continuous nuisances factors s?

  "
Visual Interaction Networks: Learning a Physics Simulator from Video,"Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, Andrea Tacchetti",https://proceedings.neurips.cc/paper/2017/hash/8cbd005a556ccd4211ce43f309bc0eac-Abstract.html,"Visual Interaction Networks.

This paper presents a general-purpose model based on convolutional networks and recurrent neural networks with an interaction network to predict future states from raw visual observations.

The use case is compelling and the models demonstrate excellent performance in predicting future states and also demonstrating good performance with noisy backgrounds and springs with invisibility and springs and billiards with variable mass.

MAJOR
- in the section training procedure 3.3, please clarify the sequence of 8 unseen future states. I read from 3.1 that the training sequence is 14 frames. Are these next 8 unseen future state frames the next 8 frames after the training sequence of 14 frames or are these randomly selected frames from future time?
- apologies but the actual setup of the tests is unclear to me from the descriptions. the model is trained on the training simulations. then each test simulation is fed into the model and the inverse normalized loss is calculated for the vision with ground truth dynamics and the test of loss of the model? 
- I found it curious that the authors didn't compare to some other techniques in the related works section. The authors clearly show the merits of their model in the comparisons that they did by altering aspects of the network architecture. It may be correct to say that there is no prior work to compare to and would be an excellent statement to make if that is true. I apologize that I don't know enough about this area of research to know what other approaches would be reasonable to compare (if any).
- Is the loss calculated in the inverse normalized loss the same or different than the loss in section 3.3? 


MINOR comment
- I would close the related work section with a paragraph summing up the differences of this work from previous work. 
- are frames from each simulation the same as timesteps as discussed in table 1?

Comments in categories:
Quality
The paper seems technically sounds. The claims are well-supported by experimental results. The work is complete.
There is limited discussion of the strengths and weaknesses of the approach especially as there is no comparison to prior work. It is possible that this is completely novel and I would recommend the authors state that if it is true. It is very compelling to say so.  

Clarity
The paper is clearly written for the most part and well-organized. There are some points of clarification that commented above.

Originality
I am not qualified to put this work into context. The authors do reference prior work but no results from these experiments are compared as baselines. The difficulty of the problem and the need for the additional architecture in the model is warranted through the comparison to the visual RNN in figure 3.

Significance
The results seem important if not at least performing close to the ground truth. I think other practitioners would build upon these ideas. Prior work is not demonstrated in their experiments so it is impossible to assess their work in context though this work may be completely novel and I would like to see a statement along these lines. Closing the related work with a comment of how their work fits in would be greatly warranted to the uninitiated in this area.

In general, the authors present compelling evidence that their model predicts future state from raw visual observations. ","The paper presents an approach with predicts the next few frames given a sequence of input image frames. It introduces a Visual Interaction Neural net model to perform the prediction task.   The model includes 3 different components, CNN, a dynamic predictor and state decoder. The proposed approach is able to outperform the proposed baseline algorithms with respect to loss and Euclidean prediction error.

The problem is very interesting but the paper is not self-explanatory as it relies on supplementary document significantly which makes it almost impossible to understand based on the content in the paper.  As a person who actively work with deep learning techniques, it was really hard for me to understand the dynamic predictor structure based on the paper. The interaction net is very new to me, and I assume to most of NIPS crew too, and hence I expect to hear more about it in the paper. Perhaps, you can buy some spaces by removing some rows from Table 1 and explain the details of your model more, especially interaction network. Another problem is elaborating the loss function in the paper, how do you calculate it? It is not clear to me. 
Looks like the interaction networks understand the underlying physics between the objects very well, is this correct? And if yes, how could it be extended to other scenarios with more complexity (such as video games). Have you ever tried it? 
How does the proposed normalized weighted sum(line1791) differs from the general loss calculation(same weight for all frame) in your experiments? Intuitively, it makes sense to use the normalized one but I am interested to know the impact on the result. 1-2 line explanation could be enough. 
 
","Summary
---

Interaction Networks (INs) [2] and the Neural Physics Engine [7] are two recent and successful instantiations of a physics engine. In each case a hand programed 2d physics engine was used to simulate dynamics of 2d balls that might bounce off each other like billiards, orbit around a made up gravitational well, or a number of other settings. Given ball states (positions and velocities), these neural approaches predict states at subsequent time steps and thus simulate simple entity interaction. This paper continues in the same vein of research but rather than taking states as input, the proposed architecture takes images directly. As such, the proposed Visual Interaction Networks (VINs) must learn to embed images into a space which captures state information and learn how to simulate 2d physics in that embedding space.

The VIN comes in three parts.
1) The visual encoder is a CNN which takes a sequence of frames which show the three most recent states of a simulation and embeds them into a different embedding for each object (the number of objects is fixed). Some tricks help the CNN focus on pairs of adjacent frames and do position computations more easily.
2) The dynamics predictor does the same job as an IN but takes state embeddings as input (from the visual encoder) and predicts state embeddings. It's implemented as a triplet of INs which operate at different time scales and are combined via an MLP.
3) The state decoder is a simple MLP which decodes state embeddings into hand coded state vectors (positions and velocities).
Hand coded states are be extracted from the ground truth 2d simulation and a loss is computed which forces decoded state embeddings to align to ground truth states during training, thus allowing the dynamics predictor to learn an appropriate 2d physics model.

This model is tested across a range of dynamics similar to that used in INs. Performance is reported both in terms of MSE (how close are state predictions to ground truth states) and relative to a baseline which only needs to extract states from images but not predict dynamics. Across different number of objects and differing dynamics VINs outperform all baselines except in one setting. In the drift setting, where objects do not interact, VINs are equivalent to an ablated version that removes the relational part from each IN used in the dynamics predictor. When compared to the simulation rolled out by the ground truth simulator using MSE all models show increased errors longer from the initial state (as expected), but VIN is smallest at every time step. A particularly interesting finding is that VINs even outperform INs which are given ground truth states and need not decode them from images. This is hypothesized to be due to noise from the visual component of VINs. Finally, videos of VIN rollouts are provided and appear qualitatively to convincingly simulate each intended 2d dynamics.


Strengths
---

This is a well written paper which presents the next step in the line of work on neural physics simulators, evaluates the proposed method somewhat thoroughly, and produces clean and convincing results. The presentation also appropriately orients the work relative to old and new neural physics simulators.


Weaknesses
---

This paper is very clean, so I mainly have nits to pick and suggestions for material that would be interesting to see. In roughly decreasing order of importance:

1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not
ablated. How important is the added complexity? Will one IN do?

2. Section 4.2: To what extent should long term rollouts be predictable? After a certain amount of time it seems MSE becomes meaningless because too many small errors have accumulated.  This is a subtle point that could mislead readers who see relatively large MSEs in figure 4, so perhaps a discussion should be added in section 4.2.

3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder.
While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated.
Why is this particular dimension of difficulty interesting?

4. line 232: This hypothesis could be specified a bit more clearly. How do noisy rollouts contribute to lower rollout error?

5. Are the learned object state embeddings interpretable in any way before decoding?

6. It may be beneficial to spend more time discussing model limitations and other dimensions of generalization. Some suggestions:
    * The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).
    * How many different kinds of physical interaction can be in one simulation?
    * How sensitive is the visual encoder to shorter/longer sequence lengths? Does the model deal well with different frame rates?


Preliminary Evaluation
---

Clear accept. The only thing which I feel is really missing is the first point in the weaknesses section, but its lack would not merit rejection.

"
Repeated Inverse Reinforcement Learning,"Kareem Amin, Nan Jiang, Satinder Singh",https://proceedings.neurips.cc/paper/2017/hash/8ce6790cc6a94e65f17f908f462fae85-Abstract.html,"
		  The authors present a learning framework for inverse reinforcement learning wherein an agent provides policies for a variety of related tasks and a human determines whether or not the produced policies are acceptable or not. They present algorithms for learning a human's latent reward function over the tasks, and they give upper and lower bounds on the performance of the algorithms. They also address the setting where an agent's is ""corrected"" as it executes trajectories.
		  This is a comprehensive theoretical treatment of a new conceptualization of IRL that I think is valuable. I have broad clarification/scoping questions and a few minor points.
		  
		  The main criterion for suboptimality given in (2) is relative to the initial state distribution, and a given \gamma. In Section 7, l294, suboptimality is given in terms of Q^\star (which is not explicitly defined but I take to mean the Q function for the policy of the human.) The authors state that ""...whenever a suboptimal action is spotted in state s, it indeed implies that the agent's policy is suboptimal for s as the initial state. Hence, we can run Algorithm 1 and Theorem 2 immediately applies."" What confuses me is it seems that this could happen for a state that has small (or zero) mass in the initial state distribution, and that might take a while to get to depending on P, and hence seeing this mistake along a trajectory may have little impact on (2) and may not indicate that the policy is suboptimal in that sense. Can the authors explain if they are being more stringent in this section, or what the connection is to the first formulation?
		  
		  I think it would help to have a concrete example -- and I am not asking for actual experiments here -- but a crisper example that the reader could latch onto as the different algorithms are discussed. The examples in the introduction from lines 18 through 28 are good motivation, but in my opinion they are too broad to help the reader really understand how these algorithms might work in practice. (In my opinion they are a little too ""strong AI"" to be helpful.) If the authors are going to argue in lines 143--149 about what requirements on human supervision are ""reasonable,"" I think that would be better supported with a concrete example.
		  
		  Minor comments:
		  
		  l.4 - ""surprised the agent"" - surprised, the agent
		  l.23 - goal but - goal, but
		  l.38 - I know what you mean, but I don't know if \Delta(S) is standard enough to use w/o definition
		  l.96 - I think ""behaviourally equivalent in *all* MDP tasks"" or similar might help clarify this statement for the reader
		  l.301 - ""using optimal policy"" - ""using the optimal policy"" (or an optimal policy)
      ","## Summary

This is a theoretical work in which the authors present a new problem domain called repeated inverse reinforcement learning (RIRL). This domain describes interactions between a human (expert) and the RIRL agent, whereby the agent repeatedly presents solutions to tasks (in the form of a discounted state occupancy) and the human responds with a demonstration if surprised by the (suboptimality) of the agent's choice. At each iteration the agent also  potentially makes some choice about the domain, which can include the state transition structure, and/or the task specific part of the reward function. The goal is to determine the task independent reward function, which in real terms might refer to such things as norms, preferences and safety considerations. Towards the end of the paper, the authors relax the requirement for a full state occupancy to be presented, instead allowing trajectories to be presented. The solution is presented either as an optimal policy for any desired task, or by identifying the task independent reward function up to an equivalence class.

The main contributions of the paper are two algorithms and a series of convergence bounds proven for each setting. The algorithms making use of a 'volume reduction in ellipsoid' algorithm from the optimization literature, which reduces the space of possible reward functions to a minimal-volume enclosing ellipsoid. The paper is quite dense in theoretical results and pushes a number of proofs to appendices. Also, because of space considerations there are some very compact descriptions with high level intuition only. However, these results appear to be sound (to the best of my ability to judge) and to have general applicability in the newly defined problem domain. The authors could do a little more to provide both a concrete example to motivate the domain and some intuition to guide the reader (please see my comments below). Also, the tractability of this approach for 'interesting' domains is difficult to judge at this early theoretical stage.


## More detailed comments

# p2
# Authors use \Delta to represent a probability distribution over various sets, but don't define this notation anywhere, or scope its meaning (some authors forbid continuous sets, other authors forbid discrete distributions with zero components when using this notation).
# Or the identity function used for the state occupancy vector.

# p4, line 158
  Each task is denoted as a pair (X, R). 
# The authors do not describe the meaning of R in the general bandit description. From later text it becomes apparent, but it should be stated here explicitly.

# p5, lines 171-177
# Is this really a generalisation? It seems more like a constraint to me. It certainly implies a constraint on the relationship between rewards for two different states which share features. Extension/modification might be a better word than generalisation. 
# It is also implied, but not stated that the policy now maps from features to actions, rather than from states to actions. 

# p5 line 183, language
    ...for normalization purpose,
# for normalization purposes,

# p5, line 186, meaning
  ...also contains the formal protocol of the process.
# It is unclear what the authors mean by this.

# p5 line 197, clarity
    Therefore, the update rule on Line 7...
# on Line 7 of Algorithm 1

# p5 line 205, clarity
    ...in the worst case...
# I think 'at it's smallest' (or similar) would be clearer. As this 'worst case' would represent the most accurate approximation to theta* possible after . A good thing.

# p5 line 206, clarity
  To simplify calculation, we relax this l∞ ball to its inscribed l2 ball.
# This took me a little while to interpret. It would be clearer if the authors talked about the lower bound represented by the l∞ ball being relaxed, rather than the ball being relaxed.


# p6 line 208 and Equation below
# When the authors say 'The unit sphere', I guess they are now talking about the l2 norm ball. This should be explicit. As should the vol function representing the volume of an ellipsoid. Also some intuition could be given as to why the third part of the inequality is as it is. If I am right then C_d sqrt(d)^d is the volume of an l2 ball that contains \Theta0 and C_d (ε/4)^d is the volume of the aforementioned lower bound l2 ball.

# p7 meaning
    We argue that this is not a reasonable protocol for two reasons: (1) in
expectation, the reward collected by the human may be less than that by the agent, which is due to us conditioning on the event that an error is spotted
# Is there a way of making this statement clearer.

# p7 clarity
  demonstration were still given in state occupancy
# Is the meaning 'demonstration were still given in terms of a state occupancy vector'
# and '(hence μ t )' could be expanded to '(we call the resulting state occupancy μ t ). 

# p8 discussion for Algorithm 2, intuition.
# The authors could give some intuition behind the construction of the batch state vectors \bar{Z} and \bar{Z}^* in the Algorithm. The former appears to be an unnormalised state occupancy following \pi_t = \pi_{t-n} whose initial state distribution is uniformly sampled from the states with mistakes in between iterations t-n and t. Likewise, the \bar{Z}* vector is an unnormalised sum of discounted state visits from the n demonstrations initiated by the mistakes.  Is this correct? 

","This is a theoretic paper that focuses on the repeated Inverse Reinforcement Learning problem proposed by the authors. The problem is interesting itself. The authors describe the problem and the challenge of identifying reward functions, and propose an identification algorithm for agent choose the tasks when it observes human behavior. The authors also provide an upper bound of the number tasks to reach desired accuracy when agent chooses the tasks, and also a lower bound on the number of mistakes that are inevitable in the worst case when nature chooses the tasks.  The authors also propose a trajectory version of the ellipsoid algorithm for MDPs. The problem and theories described in this paper are novel and interesting. The paper is well written. It will be better there are some empirical results. "
Inference in Graphical Models via Semidefinite Programming Hierarchies,"Murat A. Erdogdu, Yash Deshpande, Andrea Montanari",https://proceedings.neurips.cc/paper/2017/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html,"The authors show how SDP methods based on the Sum-of-Squares (SOS) hierarchy may be used for MAP inference in discrete graphical models, focusing on binary pairwise models. Specifically, an approximate method for binary pairwise models is introduced to solve what is called PSOS(4), then the solution is rounded to an integer solution using a recursive scheme called CLAP (for Confidence Lift And Project). Preliminary empirical results are presented which appear encouraging.

This is an interesting direction but I was confused by several aspects. Please could the authors clarify/note the following:

1. l.60-67 Optimizing an LP over a relaxation in the Sherali-Adams hierarchy, and relation to Theorem 1.
For the first order relaxation, i.e. the standard pairwise local polytope, only edges in the original model need be considered. This is because, in this case, it is straightforward to observe that consistent marginals for the other edges (which do not exist in the original model) exist while remaining in the pairwise polytope for the complete graph. By asusmption their values do not affect the optimum.
For the next higher order relaxation, i.e. the triplet-consistent polytope TRI, in fact again it may be shown *for binary pairwise models whose graph is chordal* that one need only consider triangles in the original model, since there always exist consistent marginals for all other edges. This is not obvious and was shown recently by Rowland, Pacchiano and Weller ""Conditions beyond treewidth..."" AISTATS 2017, Sec 3.3.
This theme is clearly relevant to your approach, as described in l.81-89. Further, properties of TRI (equivalent for binary pairwise models to the cycle polytope, see Sontag PhD thesis 2010) may be highly relevant for this paper since I believe that the proof for Theorem 1 relies on showing essentially that the SOS(4) relaxation lies within TRI - please could you comment? If that is true, is it true for any binary pairwise model or only subject to certain restrictions?

2. l.72 Here you helpfully suggest that SDP relaxations do not account well for correlations between neighboring vertices. It would be very valuable if you could please elaborate on this, and other possible negatives of your approach, to provide a balanced picture. Given this observation, why then does your method appear to work well in the empirical examples you show later?

3. I appreciate space constraints are very tight but if possible to elaborate (even in the Appendix) on the approach, it would be very helpful. In particular: what exactly is happening when r is varied for a given model; how should we think about the directed i->j constraints in l.124-5?

4. Alg 1. When will this algorithm perform well/badly? Can any guarantees be provided, e.g. will it give an upper bound on the solution? 

5. CLAP Alg 2. Quick check: is this guaranteed to terminate? [maybe for binary pairwise models, is < \sigma_0, \sigma_s > always \geq 1/2 or some other bound?] 
Could you briefly discuss its strengths and weaknesses. Are other methods such as Barak, Kelner, Steuer 2014 ""Rounding sum-of-squares relaxations"" relevant? 

6. Sec 4 Experiments. 
When you run BP-SP, you obtain marginals. How do you then compute your approximate MAP solution? Do you use the same CLAP rounding approach or something else? This may be important since in your experiments, BP-SP performs very well.
Since you use triangles as regions for PSOS(4), could you try the same for GBP to make the comparison more similar? Particularly since it appears somewhat odd that the current GBP with 4-sets is not doing better than BP-SP.
Times should be reported for all methods to allow more meaningful comparisons [I recognize this can be tricky with non-optimized code but the pattern as larger models are examined would still be helpful].
If possible, it would be instructive to add experiments for larger planar models with no singleton potentials, where it is feasible to compute the exact MAP score. 

Minor points:

In a few places, claims are perhaps stronger than justified - e.g. in the Abstract, ""significantly outperforms BP and GBP"" ; l. 101 perhaps remove ""extensive""; l. 243 - surely the exact max was obtained only for the small experiments; you don't know for the larger models?

A few capitalizations are missing in the References, e.g. Ising, Burer-Monteiro, SDP

=======================

I have read the rebuttal and thank the authors for addressing some of my concerns.","Summary:
The authors observe that SDP is not used for MAP inference due to its inability to model expressive local interactions between variables and due to the poor scalability of the problem formulation. They address the first limitation by introducing a partial sum-of-squares relaxation which allows regional interactions between variables, with regions formulated as a vertex cover. To address the second limitation, the authors introduce an approximate optimizer using an augmented Lagrangian for constraints, and then project the approximate relaxed solution into a feasible space. Experiments show acceptable scalability and competitive performance.

Pros:
 - Clearly written and organized paper 
 - Increases the applicability of SDP to a broader set of problems and domains
Cons:
 - Seems half-finished, with a minimal theoretical analysis at the end that ends abruptly
 - Does not discuss scalability-expressivity tradeoffs in PSOS assumptions, nor the approximation quality of the optimization procedure

Detailed Comments:
This paper is clearly written and makes a case for a set of limitations of current work and a set of innovations that address these limitations. I have difficulty gauging the depth of these contributions, and the paper would benefit from a more nuanced discussion of the tradeoffs involved (beyond performance plateauing after r=10, for example). I was surprised by the abrupt ending of the paper, and felt that a longer treatment of the theoretical analysis would be more coherent, particularly given the puzzling performance of PSOS.","In general I like the paper and recommend acceptance.  However this is not a clear cut win. There are a number of caveats in the paper.  Fixing these may help the readers to appreciate contribution of the paper better.

Here are  my comments on what would be good to correct/improve

a) Only binary model is discussed in the paper and one discovers it only on ln.108.

b) The resulting algorithm is comapred with sum-product version of BP (message-passing), in spite of the fact that the suggested algorithm solves the optimizatiom (maximum likelihood) optimization (not inference) problem.  It would be more appropriate to compare the algorithm with min-sum message-passing algorithm."
Gauging Variational Inference,"Sung-Soo Ahn, Michael Chertkov, Jinwoo Shin",https://proceedings.neurips.cc/paper/2017/hash/8d420fa35754d1f1c19969c88780314d-Abstract.html,"The paper presents two new approaches to variational inference for graphical models with binary variables - an important fundamental problem. By adding a search over gauge transformations (which preserve the partition function) to the standard variational search over (pseudo-)marginals, the authors present methods called gauged mean field (G-MF, Alg 1) and gauged belief propagation (G-BP, Alg 2). Very helpfully, both are guaranteed to yield a lower bound on the true partition function - this may be important in practice and enables empirical testing for improved performance even for large, intractable models (since higher lower bounds are better). Initial experiments demonstrate the benefits of the new methods.

The authors demonstrate technical skill and good background knowledge. The new approaches seem powerful and potentially very helpful in practice, and appear very sensible and natural when introduced in Sec 3.1 given the earlier background. The discussion and extensions in Sec 3.2 are interesting. Theorem 1 is a strong theoretical result - does it hold in general for a graph with at most one cycle?

However, I have some important questions/concerns. If these were fixed, then this could be a strong paper:

1. Algorithm details and runtime.

Both algorithms require a sequence of decreasing barrier terms \delta_1 > \delta_2... but I don't see any comments on how these should be set? What should one do in practice and how much impact could this have on runtime and accuracy?

There appear to be no comments at all on runtime. Both algorithms will likely take much longer than the usual time for MF or BP. 
Are we guaranteed an improvement at each iteration? Even if not, because we always have a lower bound, we could use the max so far to yield an improving anytime algorithm.

Without any runtime analysis, the empirical accuracy comparisons are not really meaningful. There are existing methods which will provide improved accuracy for more computation, and comparison would be very helpful (e.g. AIS or clamping approaches such as Weller and Jebara NIPS 2014; or Weller and Domke AISTATS 2016, which shows monotonic improvement for MF and TRW when each additional variable is clamped).

Note that with clamping methods: if a model has at most one cycle, then by clamping any one variable on the cycle and running BP, we obtain the exact solution (compare to Theorem 1).
More generally, if sufficient variables are clamped st the remainder is acyclic then we obtain the exact solution. Of course, for a graph with many cycles this might take a long time - but this shows why considering accuracy vs runtime is important.

What is the quality of the marginals returned by the new approaches? Does a better partition function estimate lead to better marginals?

2. Explanation of the gauge transformation.

Given the importance here of the gauge transformation, which may be unfamiliar to many readers, it would help significantly if the explanation could be expanded and at least one example provided. I recognize that this is challenging given space constraints - even if it were in the Appendix it would be a big help.

l. 177-179 please elaborate on why (8) yields a lower bound on the partition function.


Minor details:

1.238-240 Interesting that G-BP performs similarly to BP for log-supermodular models. Any further comment on this?

If/when runtime is added, we need more details on which method is used for regular BP.

A few typos -
l. 53 alternative -> alternating (as just defined in the previous sentence). I suggest that 'frustrated cycle' may be a better name than 'alternating cycle' since a balanced cycle of even length could have edge signs which alternate.
l. 92 two most -> two of the most
l. 134-5 on the expense of making underlying -> at the expense of making the underlying
In Refs, [11] ising -> Ising; [29] and [30] bethe -> Bethe","This paper introduces a variant of variational mean field and belief propagation using Gauge transformation. The Gauge transformation  that is used was introduced in refs [19,20] (as far as I can tell). The G-MF and G-BP (with variants) algorithms are interesting and provide a better bound on the partition function that the canonical MF. Presented numerical results show considerable improvement in precision for some examples of graphical models. 

What is not very clear from the paper is whether these approaches are scalable to really large graphical models. The big advantage of both variational mean field and BP is that they can be used in a distributed way on really large datasets. The authors only mention more efficient implementation of the method in the conclusion as a part of future work. 

The choice of  the graphical models used for the numerical experiments could be better motivated. While this work seems to have good theoretical interest, it would be nice to have examples of more practical applications. This is also only mentioned as a subject of future work. 

In conclusion, the paper is rather interesting theoretically offering a new variant of generic variational algorithm, it is less clear if it offers a perspective for practical applicability due to the computational cost.

I've read the author's feedback and took it into account in my score. ","Summary:

The paper proposes two novel variational methods for approximating the partition function of discrete graphical models. Starting from a known connection between gauge transformations and existing variational inference methods, belief propagation (BP) and mean-field (MF), the paper proposes a more general optimization problem with additional degrees of freedom (with respect to existing methods). This allows them to obtain provable lower bounds, and tighter approximations. In particular, they prove that the method is exact for certain (simple) graph structures where BP fails. The method is evaluated experimentally on synthetic models, showing improvements over simple baselines.

Comments:

The writing is excellent, and the ideas are presented nicely. A few examples for the not-so-common ""dual"" graph represetation would have been nice, but it's fine given the space limitations.

The theoretical analysis is solid and represents a nice contribution to the field. I like how the approach seems to be orthogonal to many other exisitng approaches for improving the accuracy (e.g., structured mean field, etc), and therefore potentially complementary. It's likely that there will be follow up work on this line of work.

My only criticism is on the experimental evaluation, which is below standard. The algorithm is compared to simple baselines on synthetic models only, but there is no indication of runtime. There are ways of improving the approximation obtained with plain MF and BP methods. For example, multiple random initializations or clamping variables to estimate partition functions of simpler models (and then sum up). The key question of whether the proposed approach pays off in terms of computational cost is left unanswered.


Minor:

- It is said that Algorithm 1 (line 160) converges to a local optimum of (6). It would be good to justify this better, given that (block) coordinate descent is not always guaranteed to converge
- Notation in the bottom part of page 6 is very confusing
- clarify the (0,0,...) notation in (5)"
Teaching Machines to Describe Images with Natural Language Feedback,"huan ling, Sanja Fidler",https://proceedings.neurips.cc/paper/2017/hash/8e68c3c7bf14ad0bcaba52babfa470bd-Abstract.html,"The paper presents a learning framework of incorporating human feedback in RL and applies the method to image captioning. specifically, the authors collect human feedback on machine’s caption output, including scoring (from perfect to major error) and correction (if not perfect or acceptable). Then authors train a feedback network to simulate human’s feedback assignment on the correctness of a phrase. Later, the authors use reinforcement learning to train the captioning model, with rewards computed from both auto-metrics (like weighted BLEU) and simulated human feedback based on the feedback network. The authors conducted experiments on COCO to evaluate the effectiveness of the model. 
Though a interesting task, the paper has room to improve, e.g.,
1.	The novelty of the paper is not very clear. The setting of including human in the loop for RL training is interesting. However, the actual implementation is not an online human-in-the-loop setting, instead it is a two-stage batch mode of human labeling. Therefore, the authors need to design a simulator (e.g., the feedback network) to estimate human feedback in RL training, which is quite common in many RL settings. BTW, as presented in the related work, using RL to optimize non-differentiable metric is not new. 
2.	The authors report major results on COCO in table 6. However, the performance of the baselines are far from state-of-the-art (e.g., in BLEU-4 score, usually the state of the art results are around 30%, while in table 6 the baseline result is at 20%), and the improvement from using feedback over baseline is less than 0.5% in BLEU-4, which usually is not statistically significant. 
3.	There is lack of detailed analysis and examples showing how feedback helps in the RL framework. e.g., given RLB(RL baseline) and RLF (w/ feedback) give similar scores, are they making similar error? Or w/feedback, the model predict very differently?
","The paper presents an approach for automatically captioning images where the model also incorporates natural language feedback from humans along with ground truth captions during training. The proposed approach uses reinforcement learning to train a phrase based captioning model where the model is first trained using maximum likelihood training (supervised learning) and then further finetuned using reinforcement learning where the reward is weighted sum of BLEU scores w.r.t to the ground truth and the feedback sentences provided by humans. The reward also consists of phrase level rewards obtained by using the human feedback.

The proposed model is trained and evaluated on MSCOCO image caption data. The proposed model is compared with a pure supervised learning (SL) model, a model trained using reinforcement learning (RL) without any feedback. The proposed model outperforms the pure SL model by a large margin and the RL model by a small margin.

Strengths:

1.	The paper is well motivated with the idea of using human in the loop for training image captioning models.

2.	The baselines (SL and RL) are reasonable and the additional experiment of using 1 GT vs. 1 feedback caption is insightful and interesting.

3.	The work can be great significance especially if the improvements are significantly large over the RL without any feedback baseline.

Weaknesses:

1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information – which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.

2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.

3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?

4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by “as an example”?  

5.	L216-217: What is the rationale behind using cross entropy for first (P – floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?

6.	L222: Why is the official test set of MSCOCO not used for reporting results?

7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?

8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?

9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?

10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, “We use either … feedback collection”: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop “by”

Post-rebuttal comments:

I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.

So, I would like to change my rating to marginally below acceptance threshold.","Summary:
The authors propose to incorporate human feedback in natural language for the task of image captioning. Specifically, they take a snapshot of a phrase-based caption model, predict captions for a subset of images, collect annotations to identify the mistakes made by the model if any, train a feedback network and ‘teach’ the model to improve over it’s earlier snapshot with this feedback using reinforcement learning. Experiments show that such feedback helps improve the performance using automatic evaluation metrics for captioning.

Strengths:
(a) The paper addresses a well-motivated aspect for learning - human feedback. Their results suggest that human feedback on the model’s performance is more valuable than additional caption annotations, which is intuitive and a positive outcome.
(b) The authors have done a good job on collecting human feedback in a least ambiguous way to finetune the model later. Their selection of a phrase-based captioning model aids this collection procedure.

(c) Experiments are comprehensive with different ablations to prove the effectiveness of the human feedback.

Weaknesses:
(a) Human evaluation on the quality of captions would have given a better sense of performance. Even though on a smaller scale, such evaluations throw more light than automatic correlations which have been shown to correlate poorly with respect to human judgement.

Comments:
(a) L69 - [3] does not use reinforcement learning. [34] seems to be doing reinforcement learning on visual dialog.

(b) Table 6 -Rouge - L second row - Is the value incorrect ? Does not seem to be in same ballpark as other models.

(c) Figure 2 appears incomplete. Without properly labeling (c_t, h_t, h_{t-1}, etc), it is very unclear as to how takes in the image and outputs caption phrases in the diagram.

(d) Sec 3.3: The overall setup of RL finetuning using feedback has not been discussed until that point. However, this section seems to describe feedback network assuming the setup thereby raising confusion. For example, after reading L162, one might assume that feedback network is trained via reinforcement learning. L164 - ‘Newly sampled caption’ has no context. I recommend adjusting the text accordingly to setup the overall training procedure.

(e) L245 - What does this sentence mean ?

(f) How is decoding done at evaluation -- beam search / sampling / greedy policy ?

(g)  L167 - inconsistency in superscript.
 
(h) L222 - Why such a small testing dataset, which is almost 1/20th of the training dataset ?

Typos:
(a) L28-29 - reinforcement learning (RL)
(b) L64 - it exploits
(c) L74 Policy Gradients -> policy gradients 
(d) L200 - typo in the equation specifying range for \lambda?

References:
[34] Das, Abhishek, et al. ""Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning."" arXiv preprint arXiv:1703.06585 (2017)."
Associative Embedding: End-to-End Learning for Joint Detection and Grouping,"Alejandro Newell, Zhiao Huang, Jia Deng",https://proceedings.neurips.cc/paper/2017/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html,"Strengths:
- The idea is very novel. Unlike typical approaches to pose estimation that first detect the people and estimate the keypoints, this approach detects keypoints bottom-up, and then groups together keypoints that belong to the same person. The idea of tags to do this grouping is very intuitive, straightforward but effective.
- To the best of my knowledge, the final performance is state-of-the-art on all the important benchmarks.
- The exposition is clear and understandable.

Weaknesses
- The model seems to really require the final refinement step to achieve state-of-the-art performance.
- How does the size of the model (in terms of depth or number of parameters) compare to competing approaches? The authors mention that the model consists of 4 hourglass modules, but do not say how big each hourglass module is.
- There are some implementation details that are curious and will benefit from some intuition: for example, lines 158-160: why not just impose a pairwise relationship across all pairs of keypoints? the concept of anchor joints seems needlessly complex.","Paper summary: The paper addresses the problem of detecting multiple people and their corresponding keypoints/joints in images. The paper is an extension of [31], where the neural network predicts an ID for each keypoint as well.

Paper strengths:
- The proposed method achieves state-of-the-art performance on MPII and COCO (person category) datasets.

Paper weaknesses:
- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].

- ""Embedding"" is an overloaded word for a scalar value that represents object ID.

- The model of [31] is used in a post-processing stage to refine the detection. Ideally, the proposed model should be end-to-end without any post-processing.

- Keypoint detection results should be included in the experiments section.

- Sometimes the predicted tag value might be in the range of tag values for two or more nearby people, how is it determined to which person the keypoint belongs?

- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck.

Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.
","The paper proposes a method for the task of multi-person pose estimation. It is based on the stacked hour glasses architecture for single person pose estimation which is combined with an associative embedding. Body joint heatmaps are associated with tag heatmaps indicating which individual the joint belongs to. A new grouping loss is defined to assess how well predicted tags agree with ground truth joint associations. The approach is evaluated on the MPII multi-person and on MS-COCO, achieving state of the art performance for both of them.

Strengths:
- State-of-the-art performance is achieved for the multi-person pose estimation problem on MPII Multi-person and MS-COCO.

Weakness:
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
- Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4.

Comments:
The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained.  

"
"Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications","Linus Hamilton, Frederic Koehler, Ankur Moitra",https://proceedings.neurips.cc/paper/2017/hash/8fb5f8be2aa9d6c64a04e3ab9f63feee-Abstract.html,"This paper is concerned with learning Markov random fields (MRF).  It is a theoretical paper, which is ultimately focused with proving a particular statement: given a variable X in an MRF, and given some of its Markov blanket variables B, there exists another variable Y that is conditionally dependent on X given the subset of B.

In general this statement is not true; so the goal here is to identify some conditions where this is true.  Most of this paper is centered around this, from which the ability to learn an MRF follows.

The paper is mostly technical; my main complaint is that I do not think it is very intuitive.  It appears that central to the results is the assumption on non-degeneracy, which I believe should be explained in higher level terms.

Intuitively, there are a number of situations where the above statement would not hold.  For example, if a potential is allowed to have determinism, which would lead to the MRF having inputs with zero probability, then one could construct a potential that makes some variable functionally dependent on other variables.  This would allow one to construct an MRF where conditioning on some members of a Markov blanket would fix the values of other members, hence separating a variable from the rest of the network.  However, this appears to be ruled out by the non-degeneracy conditions.

Another simple condition is if a potential were uniform.  In this case, some edges on the MRF could be vacuous and could be removed.  This also appears to be ruled out by the non-degeneracy conditions.

A more general condition is if a potential over multiple variables actually represented (for example) the product of unary potentials.  Again, this would involve vacuous MRF edges, although it is not immediately clear to me if this case is ruled out as well.

The point however is that it would be helpful for me to know what these non-degeneracy conditions imply, at a more abstract level.  For example, do they rule out the above cases?  Are there other cases that need to be ruled out?  This would help in absorbing the technicalities.  Another place where this would help is in justifying the machinery needed to prove the desired result.  As the authors state, the result is ""simultaneously surprising and not surprising.""  Going further, is there a set of simple and intuitive conditions (but possibly stronger conditions) that would lead to analogous results on learning MRFs?  This would also be helpful in making the paper clearer.
","The authors consider the problem of automatically learning the structure of a general Markov Random Field model. 

Their main contribution is a lower bound on the mutual information between a variable u and a variable v conditioned on any set of variables which does not contain the full Markov blanket of u. The bound depends on the maximum degree of the MRF graph, the size of the variable state spaces, the order of the interactions and some broad conditions on the value of the clique potentials. From this result, the authors derive an algorithm which can recover the structure of an MRF with high probability in ""polynomial"" time (exponential in the order of the potentials), and with bounded queries.

This paper is a bit of an outlier for NIPS, which isn't necessarily a bad thing, but should come with some form of disclaimer. Both the bound and algorithms have theoretical significance. However, it should be clearly stated that the proposed algorithms are given as a proof of existence, rather than as a method to follow: the form of the constants make any actual guaranteed application intractable, to say nothing of the sample complexity to have sufficiently accurate estimates of the mutual information values. If the authors believe that the algorithms could still yield reasonable results, then this should be demonstrated at the very least on artificial data.

","	The setup in Eq. 1 is unclear on a first reading. What is the
	exact meaning of r here? This is a critical constant in the
	major result proven, and not carefully defined anywhere in the
	paper. I *think* that r is the maximum clique size. This is
	particularly ambiguous because of the statement that
	theta^{i1,i2,...,il}(a1,a2,...,an) ""is assumed to be zero on
	non-cliques"". I've assumed the meaning of this is that, more
	precisely, that function ignores aj unless there is some value
	t such that aj=it. Then, this makes sense as a definition of a
	Markov random field with maximum clique size r. (I encourage
	clarification of this in the paper if correct, and a rebuttal
	if I have misunderstood this.)

	In any case, this is an unusual definition of an MRF in the
	NIPS community. I think the paper would have more impact if a
	more standard definition of an MRF were used in the
	introduction, and then the equivalence/conversion were just
	done in the technical part. The main result only references
	the constant r, so no need to introduce all this just to state
	the result.

	On a related note, it's worth giving a couple sentences on the
	relationship to Bresler's result. (For an Ising model, r=2,
	etc.) In particular, one part of this relationship is a bit
	unclear to me. As far as I know, Bresler's result applies to
	arbitrary Ising models, irrespective of graph structure. So,
	take an Ising model with a dense structure so that there are
	large cliques. In this case, it seems to me that the current
	paper's result will use (r-1) nodes while Bresler's will still
	use a single node-- thus in that particular case the current
	paper's result is weaker. Is this correct? (It's not a huge
	weakness even if true but worth discussing in detail since it
	could guide future work.) [EDIT: I see now that this is *not* the case, see response below]

	I also feel that the discussion of Chow-Liu is missing a very
	important aspect. Chow-Liu doesn't just correctly recover the
	true structure when run on data generated from a tree. Rather,
	Chow-Lui finds the *maximum-likelihood* tree for data from an
	*arbitrary* distribution. This is a property that almost all
	follow-up work does not satisfy (Srebro showed bounds). The
	discussion in this paper is all true, but doesn't mention that
	""maximum likelihood"" or ""model robustness"" issue at all, which
	is hugely important in practice.

	For reference: The basic result is that given a single node $u$,
	and a hypothesized set of ""separators"" $S$ (neighbors of $u$) then
	there will be some set of nodes $I$ with size at most $r-1$ such
	that $u$ and $I$ have positive conditional mutual information. 
	
	The proof of the central result proceeds by setting up a
	""game"", which works as follows:

	1) We pick a node $X_u$ to look at.
	2) Alice draws two joint samples $X$ and $X'$.
	3) Alice draws a random value $R$ (uniformly from the space of
	possible values of $X_u$
	4) Alice picks a random set of neighbors of $X_u$, call them $X_I$.
	5) Alice tells Bob the values of $X_I$
	6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
	wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
	happens if both or neither are true.

	Here I first felt like I *must* be missing something, since
	this is just establishing that $X_u$ has mutual information
	with it's neighbors. (There is no reference to the ""separator""
	set S in the main result.) However, it later appears that this
	is just a warmup (regular mutual information) and can be
	extended to the conditional setting.

	Actually, couldn't the conditional setting itself be phrased
	as a game, something like

	1) We pick a node $X_u$ and a set of hypothesized ""separators""
	$X_S$ to look at.
	2) Alice draws two joint samples $X$ and $X'$ Both are
	conditioned on the some random value for $X_S$.
	3) Alice draws a random value $R$ (uniformly from the space of
	possible values of $X_u$
	4) Alice picks a random set of nodes (not including $X_u$ or $X_S$, call them $X_I$.
	5) Alice tells Bob the values of $X_I$
	6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
	wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
	happens if both or neither are true.

	I don't think this adds anything to the final result, but is
	an intuition for something closer to the final goal.

	After all this, the paper discusses an algorithm for greedily
	learning an MRF graph (in sort of the obvious way, by
	exploiting the above result) There is some analysis of how
	often you might go wrong estimating mutual information from
	samples, which I appreciate.
	
	Overall, as far as I can see, the result appears to be
	true. However, I'm not sure that the purely theoretical result
	is sufficiently interesting (at NIPS) to be published with no
	experiments. As I mentioned above, Chow-Liu has the major
	advantage of finding the  maximum likelihood solution, which the current method
	does not appear to have. (It would violate hardness results
	due to Srebro.) Further, note that the bound given in Theorem
	5.1, despite the high order, is only for correctly recovering
	the structure of a single node, so there would need to be
	another lever of applying the union bound to this result with
	lower delta to get a correct full model.

EDIT AFTER REBUTTAL:

Thanks for the rebuttal. I see now that I should understand $r$ not as the maximum clique size, but rather as the maximum order of interactions. (E.g. if one has a fully-connected Ising model, you would have r=2 but the maximum clique size would be n). This answers the question I had about this being a generalization of Bressler's result. (That is, this paper's is a strict generalization.) This does slightly improve my estimation of this paper, though I thought this was a relatively small concern in any case. My more serious concerns are that a pure theoretical result is appropriate for NIPS."
Subset Selection and Summarization in Sequential Data,"Ehsan Elhamifar, M. Clara De Paolis Kaluza",https://proceedings.neurips.cc/paper/2017/hash/8fecb20817b3847419bb3de39a609afe-Abstract.html,"This paper presents a framework for sequential data summarization. The solution is based on the maximization of a potential function with three components: the first one enforces the selection of the best set, the second one promotes the sparsity of the selected set, and the third one enforces the selected set to obey a first-order Markovian model. The effect of the last two components are controlled by parameters. The resulting optimization problem is not convex and the authors propose a message passing algorithm that seems to work well in practice using a memory trick for slowing down the convergence. The proposed method is an alternative to the sequential extensions of the Determinantal Point Process (DPP) and compares favorably with it.

Quality
The technical formulation of the method seems sound and correct. The method is compared favorably using synthetic and real data with sequential and non-sequential summarization methods. 

Clarity
The paper is well written and easy to understand. More details about the values of the parameters in the real experiments and an explanation about the low value of the parameter that enforces the Markovian model would be welcome.

Originality
As noted previously, the method is an original alternative to the sequential extensions of DPP.

Significance
The method represents a nice tool for the summarization of video and behavioral data.
","The paper consider a new problem of subset selection from set X that represents objects from sequence Y with additional constraint that the selected subset should comply with underlaying dynamic model of set X. The problem seems interesting to me and worth investigation.

The novel problem boils down to maximization problem (11). The authors suggest a new message-passing scheme for the problem. 

The problem can be seen as a multilabel energy minimization problem (T nodes, M labels, chain structure with additional global potential -log(\Phi_{card}(x_{r_1}, \ldots, x_{r_T}))) written in overcomplete representation form [23]. The problem is well known in energy minimization community as energy minimization with label costs [2]. It is shown that global potential -log(\Phi_{card}(x_{r_1}, \ldots, x_{r_T})) can be represented as a set of pairwise submodular potentials with auxiliary variables [2, 3]. Therefore, the problem (11) can be seen as multilabel energy minimization with pairwise potentials only. There are tons of methods including message passing based to solve the problem approximately (TRW-S, LBP, alpha-beta swap, etc). While they may work worse than the proposed message passing, the thoughtful comparison should be performed.

I believe that the paper in its current form cannot be accepted. Although the considered problem is interesting, the related work section should include [1,2,3]. Also, different baselines for experimental evaluation should be used (other optimization schemes for (11)), as it is clear from the experiments that DPP based methods do not suit for the problem. I think that the improved version may have a great impact.

typos:
- (8) \beta was missed
- L229 more more -> more
- L236 jumping, jumping -> jumping

[1] Wainwright, M.J., Jordan, M.I.: Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 2008
[2] Delong, Andrew, et al. ""Fast approximate energy minimization with label costs."" International journal of computer vision 96.1 (2012): 1-27.
[3]  M. A. Fischler and R. C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981.","This paper proposes a subset selection model for sequential data.  Compared to prior work, this model takes into account  the underlying dynamics in the sequential data by incorporating a dynamic term in the modelling of the potential function.  Maximising the dynamic potential term pushes the selected sequence of representatives to follow (match) the dynamics in the target set.  
The paper is well motivated  while the idea is straightforward. The experimental results shows the significant improvement over the baseline models, however, the datasets used are not hard enough to be convincing. 

Pros:
1. This paper proposes a feasible way to model the dynamics in the sequential data, which hasn't been pay much attention to in prior work. 
2. Although the optimization of the model is non-convex, the paper provides a message-passing based algorithm to tackle it. 

Cons:
1. The dynamic potential only considers the first-order Markov Model, which limits the modelling power of dynamics. 
2. Since the three potential terms are relatively independent to each other, hence the performance is sensitive to the  tuning of  the hyper-parameters (\Beta and \gamma).
3. The experiments show the significantly better performance over the other baseline models, nevertheless,  more quantitative experiments on harder dataset are expected. 

"
Z-Forcing: Training Stochastic Recurrent Networks,"Anirudh Goyal ALIAS PARTH GOYAL, Alessandro Sordoni, Marc-Alexandre Côté, Nan Rosemary Ke, Yoshua Bengio",https://proceedings.neurips.cc/paper/2017/hash/900c563bfd2c48c16701acca83ad858a-Abstract.html,"The paper introduces a training technique that encourages autoregressive models based on RNNs to utilize latent variables. More specifically, the training loss of a recurrent VAE with a bidirectional inference network is augmented with an auxiliary loss.

The paper is well structured and written, and it has an adequate review of previous work. 

The technique introduced is heuristic and justified intuitively. The heuristic is backed by empirical results that are good but not ground-breaking. 

The results for speech modelling are very good, and for sequential MNIST good. The auxiliary loss brings no quantitative improvement to text modelling , but an interesting use for the latent variables is shown in section 5.4.

Some concerns:

Line 157 seems to introduce a second auxiliary loss that is not shown in L_{aux} (line 153). Is this another term in L_{aux}? Was the backward RNN pretrained on this objective? More details should be included about this. Otherwise, this paper should be rejected as there is plenty of space left to include the details. 

Line 186: How was the \beta term introduced? What scheduled? Did detail this prove essential? No answers to these questions are provided

Line 191: What type of features were used? Raw signal? Spectral? Mel-spectral? Cepstral? Please specify.

Line 209: Why the change from LSTM to GRU? Was this necessary for obtaining good results? Were experiments run with LSTM?

Line 163: Gradient disconnection here and on paragraph starting on line 127. Can they be shown graphically on Figure 1? Maybe an x on the relevant lines?

Figure 2 has very low quality on a print out. Should be improved.

Line 212: ""architecturaly flat"" do the refer to number of layers? Do any of the baseline models include information about the 2D structure of the MNIST examples? specify if any does as this would be advantageous for those models.

The section that refers to IWAE: Were the models trained with IWAE or just evaluated? I think it was just evaluated, but it should be more clearly specified. Would it be possible to train using IWAE?





","The authors of the paper propose a method to improve the training of stochastic recurrent models. More explicitly, they let the approximate posterior for the latent state at time step t depend on the observations of the future–which the authors claim to be novel, but which has been done before. Further, a heuristic in the form of an auxiliary regulariser is presented.

## Major
- The idea to let the approximate posterior be based on future time steps goes back to at least [1] who used a bidirectional recognition network
- Mostly aesthetically point: The authors wish to encourage the hidden state to carry information about the future; they identify the expressiveness of the likelihood as the problem. One way out would be to then chose a graphical model that reflects this; i.e. through the common Markov assumptions. This is what the authors of [2, 3] do. The paper does not mention this approach.
- I feel unsatisfied with proposed auxiliary cost. Some questions are
	- Do we have a probabilistic interpretation of it? What does adding such a model mean? Maybe it is a proxy to a sequential prior?
	- Adding the auxiliary costs leads to improved generalisation. But does it also reduce training loss? What about training perplexity?


## References
[1] Bayer, Justin, and Christian Osendorfer. ""Learning stochastic recurrent networks."" arXiv preprint arXiv:1411.7610 (2014).
[2] Krishnan, Rahul G., Uri Shalit, and David Sontag. ""Deep kalman filters."" arXiv preprint arXiv:1511.05121 (2015).
[3] Karl, Maximilian, et al. ""Deep variational Bayes filters: Unsupervised learning of state space models from raw data."" arXiv preprint arXiv:1605.06432 (2016).","This paper presents a generative model for sequences and an associated training algorithm based on amortized variational inference. The generative model is equivalent to a nonlinear state-space model and the inference model is based on a RNN running backwards along the sequence. In addition to the variational inference cost function, the authors propose the use of two extra penalty terms in the cost function to encourage the latent variables to reconstruct the state of the RNN of the inference model.

The description of the generative model, inference model and auxiliary costs is clear.

Comprehensive experiments are performed on speech, image and text sequences. The authors report good performance of their model against many recent models. Since the value of this article is based on this empirical performance, I consider it essential that the authors release the code as they promise in Section 5.

The article is well written and tackles the very relevant problem of learning generative models of sequences."
Regret Minimization in MDPs with Options without Prior Knowledge,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, Emma Brunskill",https://proceedings.neurips.cc/paper/2017/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html,"The paper proposes variants of SMDP-UCRL (SUCRL), FSUCRLv1 and v2, and analyzes the regret behavior of the two variants. The crucial difference is that, while SUCRL requires knowledge of certain parameters related to the option behavior, the proposed variants estimate them from the data. The two variants differ in whether the parameters and their confidence intervals are explicitly estimated, or if a ""nested"" procedure is used to obtain the parameters implicitly. The regret bounds given almost match that of SUCRL, with the exception of some cost for the parameter estimation. Some empirical validation is also presented. 

The paper is clearly written for most parts. Given the nature of the contribution, it is to be expected that the paper will be dense at some points. The crucial insight is the use of the irreducible Markov chain representation for an option. Having said that, since the primary contribution of the work is Theorem 1, it would be nice to seem intuition for the proof in the main body of the paper. Currently all of the proof is in the appendix. ","Overview:

The authors attempt to improve current regret estimation for HRL methods using options. In particular they attempt to do so in the absence of a distribution of cumulative rewards and of option durations, which is a requirement of previous methods (UCRL and SUCRL). 

After assuming that options are well defined, the authors proceed to transform the inner MDP of options, represented by a transition matrix Po, into an equivalent irreducible Markov chain with matrix P'o. This is done by merging the terminal states to the initial state. 

By doing so, and assuming that any state with a termination probability lower than one can be reached, the stationary distribution mu_o of the chain is obtainable; which in turn is utilized to estimate the optimistic reward gain. With respect to previous methods, this formulation is more robust to ill-defined estimates of the parameters, and better accounts for the correlation between cumulative reward and duration. This method is coined as FSUCRL. 

The algorithm is complemented by estimating confidence intervals for the reward r, for the SMDP transition probabilities, and for the inner Markov Chain P'o. Two versions of this same algorithm, FUSCRL Lvl1 and Lvl2, are proposed. The first version requires directly computing an approximated distribution mu_o from the estimate of P'o. The second version nests two empirical value iterations to obtain the optimal bias. 

The paper concludes with a theoretical analysis and a numerical application of FSUCLR. On a theoretical ground, FSUCRL Lvl2 is compared to SUCRL. The authors argue that the goodness of the bound on the regret predicted by FSUCRL Lvl2 compared to that of SUCRL depends various factors, including the length of the options and the accessibility of states within the option itself, and provide conditions where FSUCRL Lvl2 is likely to perform better than SUCRL. 

As an indication, 4 algorithms, UCRL, SUCRL, FSUCRL Lvl1 and FSUCRL Lvl 2 are tested on a gridworld taken from ref. 9, where the maximum duration of the options is assumed known. Result confirm the theoretical expectations previously discussed: FSUCRL Lvl2 performs better of both SUCRL and FSUCRL Lvl1, partially due to the fact that the options' actions overlap.  

Evaluation:

- Quality: The paper appears to be theoretically sound and the problem discussion is complete. The authors discuss in depth strength and weaknesses of their approach with respect to the previous SUCRL. The provided numerical simulation is not conclusive but supports the above considerations;
- Clarity: the paper could be clearer but is sufficiently clear. The authors provide an example and a theoretical discussion which help understanding the mathematical framework;
-  Originality: the work seems to be sufficiently original with respect to its predecessor (SUCRL) and with respect to other published works in NIPS;
- Significance: the motivation of the paper is clear and relevant since it addresses a significant limitation of previous methods;

Other comments:
- Line 140: here the first column of Qo is replaced by vo to form P'o, so that the first state is not reachable anymore but from a terminating state. I assume that either Ass.1 (finite length of an option) or Ass. 2 (the starting state is a terminal state) clarify this choice. In the event this is the case, the authors should mention the connection between the two;
- Line 283: ""four"" -> ""for"";
- Line 284: ""where"" s-> ""were"";
","This paper provides an extension to Fruit & Lazaric, 2017 for the problem
of exploration and exploitation with temporal abstraction in the framework of options. It weakens the previous assumptions on the structure of the given set of options: it does not require inclusion of primitive options or upper bounds on the distributions of the sojourn time or cumulative return.

A novelty of the proposed analysis is that it leverages the MDP structure ""within"" options rather than only the SMDP-level. This perspective is in the spirit
of the original options paper ""between mdps and smdps [...]"" which advocated for
a framework capable of interleaving various time scales (mdp or smdp).

The analysis is constructed on the observation that the internal structure
of each option is amenable to an MDP with a random horizon specified by the
termination function. The authors show how an optimistic policy can be written
in terms of the ""internal"" stationary distributions induced by each each option.
Confidence bounds are then obtained over these stationary distributions rather
than the cumulative return and sojourn times as in Fruit & Lazaric 2017.

The theoretical guarantees obtained under this setting capture some interesting properties which have been observed empirically, eg.: long options are harder to learn, options whose transition models have sparser support facilitates learning.

Despite its technicality, the main paper remains readable and the main results on 
the regret are accessible. It should be useful to readers of different backgrounds.

----

- The average reward criterion is adopted throughout the paper. I would like to
know if your analysis could also accommodate the discounted setting (which is also more commonly used in practice).

- While the appendix motivates the assumption on the initiation sets (only one state per option) it does not address the question of compositionality in Assumption 1. How can the compositionality be satisfied in general ?

Line 251 : ""[...] somewhat worst-case w.r.t. to the correlation"".

This statement is unclear. However, the question of correlation between options
is interesting and should be developed in the text."
Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity,"Asish Ghoshal, Jean Honorio",https://proceedings.neurips.cc/paper/2017/hash/907edb0aa6986220dbffb79a788596ee-Abstract.html,"[After-rebuttal edit begins]

I read the authors' response, and thank them for clarifying many of the issues I raised. However, the authors dismissed my concerns that the quantities in the characterization of the sample complexity are independent of the dimensions (p and k) of the problem.

Basically, the authors are saying this. Let's look at an asymptotic regime where we have a sequence of graphs with growing p and k, but where: (1) alpha (restricted faithfulness), (2) the largest effective influence, (3) the least eigenvalue of the Markov-blanket covariance matrix, (4) the largest diagonal entry of the covariance matrix, and the (5) largest entry of the precision matrix, are all constant (or more generally upper/lower bounded depending on whether they're in the numerator/denominator), in this asymptotic regime, the sample complexity is order of k^4 log(p).

My concern was, are such limits even possible? If so, are they meaningful? So I took a little bit of a closer look. Based on Lemma 5, to enforce (2)-(5) the weights have to remain roughly constant, and at least bounded from above. Then, based on the proof of Proposition 1, I looked at what kind of alpha we would get when fixing w. And I found that unless the number of children remains bounded (as k grows, mind you!), then alpha will tend to zero with k. The good news is that it still seems polynomial, but this certainly goes against the advertised claim. I don't think this is acceptable.

This is not a new issue that I'm raising, I gave the authors a chance to respond to my concern in this rebuttal. In this light, I am forced to lower my recommendation for this paper, despite my general appreciation of results of this nature (constraints that lead to polynomial sample/time learning).

[After-rebuttal edit ends]


Summary

This paper considers the problem of recovering the structure and parameters of Gaussian Bayesian Networks. In particular, it establishes that as long as noises are homoscedastic, then under a milder minimality/faithfulness assumptions it is possible to efficiently recover the GBN.

Clarity

The paper is heavy on notation, but everything is explained and organized clearly.

Some suggestions. Line 43, move the assumption before the sample complexity statement on Line 40, so that \alpha has meaning. Line 130, elaborate on how effective influence governs the Markov blanket property. In Definition 2, Line 137, say that the key is the reverse direction, in turn relaxed in Definition 3. Use Line 157 to introduce the term ""homoscedastic"" before it's use later. In Assumption 2, would it be better to index and write \kappa_i(\alpha)? In Line 196 the precision matrix is defined without calling it inverse covariance matrix, but then we only call it inverse covariance matrix. Line 199, say We have that (if)... (then). Line 204, say is defined as instead of the def symbol. 

Significance

Recovering parsimonious probabilistic models of data will always be a problem of importance. Finding among such models those for which we can have provable guarantees is necessary when we want to have confidence in the models that we produce. In this vein, this paper succeeds in giving weaker conditions and an algorithm, thus stronger results, to learn a rather rich subset of probabilistic models that are generally notoriously difficult to learn. 


Flaws

I have not found major flaws, but there are many details that need to be clarified in the main body of the paper. Many of these are about the dependence between the meta-parameters of the problem.

- Is the only role of k in bounding the size of non-zero elements of columns of the precision matrix? Does it influence anything else?
- Proposition 1 doesn't make it clear what the relationship is betwen the structural properties of G, W, and \sigma^2 on one hand, and the resulting \alpha on the other. Would \alpha end up depending on p and k, for example?
- Methods that rely on conditional independence tests may require strong faithfulness to analyze, but do we know that they will fail without it? (Line 181)
- Is it evident that p does not influence the induced norm of the precision matrix? (Remark 2)
- Also, couldn't the infinity norm of the regressors and the least eigenvalue of the Markov-blanket covariance, depend on p and k? (Lemma 3)
- I see that Theorem 1 is particularly careful in keeing all the terms I'm asking about explicit. But elsewhere in the paper the operator norm is capped by k only, and no mention is made about the regressor, etc. A clarification is needed.
- I would relate the terminal node selection procedure by the max/min thresholding used in the algorithm, back to the property stated in Lemma 1 (Lines 270-271). Because whereas for terminal nodes we know the behavior, it has not been characterized for non-terminal nodes, and the implicit assumption being the ratio would be larger for those.
- Is the actual dependence on k conjectured to be k^2 (re: Fig 2)?




","This paper concerns learning sparse Gaussian Bayesian networks with equal noise variance. The paper shows that, under certain assumptions, a polynomial-time algorithm can recover such network in high probability using a polynomial number of samples.

Most learning problems in Bayesian networks are NP-hard and thus it is a pleasure to see variants that are learnable in polynomial time. The paper was quite technical and I was not able to check correctness of the results.


Theorem 1. The theorem says that the guarantees hold for some choices of delta and alpha. Does this mean that it is not enough to choose arbitrary delta and alpha and then collect so many samples that the inequality is satisfied?

Line 63: Note that the size of the search space alone does not imply that the search is prohibitive. Indeed, the search space of the presented algorithm is also super-exponential. ","Summary:
    The paper considers a Gaussian Bayesian Network which is directed acyclic and is governed by structural equations which are linear with additive gaussian noise of identical variance. This class of bayesian networks have been shown to be identifiable in prior work. There has been no polynomial time algorithm known even for this case.
The authors propose a polynomial time algorithm which involves inverse covariance estimation, ordinary least squares and a simple identification rule for terminal vertices (leaf vertices in the Directed Acyclic Graph) for this problem. Under the assumption of restricted strong adjacency faithfulness (RSAF) (which essentially says that for all induced sub graphs, variables in the Markov Blanket exhibit high enough influence on a variable) the algorithm identifies the correct graph with O(k^4 log p) samples where k is the maximum size of the Markov Blanket.

Strengths:
   a) As far as I am aware this is the first fully polynomial time algorithm for the problem under this setting. 
   b) The paper hinges on few important observations: 
     1) Learning undirected gaussian graphical models is very very efficient due to large number of papers on inverse covariance estimation. 
     2) But usually inverse covariance estimation can only recover the Markov Blanket of every variable (considering the moralized graph) and cannot in general reveal directions.
     3) However, if a topological ordering of the vertices respecting the partial order in the DAG is known, then Ordinary least squares can be repeatedly used to figure out parents of a variable i by considering an induced graph with vertices in the ordering upto i and the inverse covariance structure at every subsequent stage.
     4) The authors exploit the following key fact construct a topological ordering by discovering terminal vertices (lead vertices) recursively: When a terminal vertex i is regressed on the rest of the nodes, then ratio of regression coefficient of vertex j in the graph to (i,j)th entry in the inverse covariance matrix equal to the noise variance in the problem. In fact, for non terminal vertices, the ratio is strictly larger under RSAF condition. This involves only inverse covariance estimation which is efficient.
  c)) Because the authors define effective influence characterizing the markov blanket in terms of the weights in the structural equations, authors can handle cases even when the model is not causally faithful (this is a standard assumption for most causal inference algorithms).
  d) In all its a really good paper. The authors have also compared it to some state of the art algorithms like PC, MMHC and GES and shown that for synthetic gaussian models, they do better in terms of recovery.
 
 However, I only have some semi-mild to some mild comments. Comment d is the most serious. It would be great if authors can address it.

 Weaknesses:
      a) In page 3 at the end, Markov Blanket of vertex i is defined using effective influence and the information theoretic definition that a set of nodes such that conditioned on them, the rest of the variables are independent to i. Under casual faithfulness, markov blanket is simply - parents, children and co-parents. I think defining it in terms of effective influence lets them deal with pathological cases not covered by causal faithfulness. This lets them work on examples in page 5. I think this must be made clear. Because a lot of readers of BN literature may be used to a graph theoretic notion of Markov Blanket. 
     b) In the example in page 5 below Proposition 1, it appears the for i,j which are neighbors if one chooses a pathological set of weights, only then effective influence becomes zero which can still be handled by the algorithm of the authors. I find it to be a corner case although I agree technically its a super set. The reason for this comment is most other algorithms like PC, MMHC are completely non parametric as far as at least skeleton + v-structure discovery is concerned. But this algorithm has very tight parametric assumptions (so it may not be that fair a comparison).

    c) Page 2, before Section 2 ""MMHC works well in practice but inherently a heuristic algorithm"" - I partially agree with this. MMHC has two parts - one is skeleton recovery (which is provably correct under infinite samples) and the other part used score based optimization heuristic to orient. So it may not be totally correct to say it is heuristic. Another minor point - Like 74 , complexity of PC is said to be p^k. k in the paper is Markov Blanket size. However the PC's complexity exponent is the max degree in the graph. Markov blanket is much larger.
   
   d) Slightly serious concern:
       Page 7 , Algorithm 1, Line 24: Now the algorithm after identifying the order is considering induced subgraphs of first i vertices in the ordering and using regression of i on the remaining is finding the parents of i. First, to find the markov blanket of i i the induced sub graph , \hat{\Omega}_{z_i,j} is used - It should be z_i,z_j in the subscript (I think). But the major concern is which inverse covariance is it ? Is it the induced subgraph's inverse covariance upto vertex i in the ordering ? or is it the inverse covariance of the entire graph estimate at 6? Please clarify. This seems important because this step of identifying node i's parents from just from the ordering and regression is something crucial. In fact, the explanation I understood is:  the global invariance covariance matrix's markov blanket of i is intersected with the the induced sub graph of vertices till i, now the node i is regressed on its non zero entries. And it will turn out that the parent set of i is correctly identified. This is because in the induced sub graph, markov blanket of i will not include the co-parents. This has been explained in the last but one paragraph in page 14 in Appendix A for proof of theorem 1. But I think this reasoning should be included in the main paper and Line 24 in Algorithm 2 must be clarified even better.

e) In Appendix B.5 authors talk about some ref 31 which can split the graph into connected components. Does it mean that the graph in that table has connected components of size at most 500. So has the algorithm applied after the connected components found out ?? If this is the case 500 is not a large number for MMHC algorithm. In fact in the 2006 paper they claim it can actually process 1000 node graphs. Why did the authors not compares to that in that case ?

f)Line 100, (i,j) seems to be directed edge from j to i. From my experience, this seems like an odd choice. It is usually i->j. But this is just my personal quip.

g) Page 11 . Appendix, Proof of Lemma 1, First line. ""forward direction follows from (5)"". Should it not be Lemma 5 ?? 

"
Learning Neural Representations of Human Cognition across Many fMRI Studies,"Arthur Mensch, Julien Mairal, Danilo Bzdok, Bertrand Thirion, Gael Varoquaux",https://proceedings.neurips.cc/paper/2017/hash/908c9a564a86426585b29f5335b619bc-Abstract.html,"This paper proposes a new model architecture dedicated to multi-dataset brain decoding classification. Multi-dataset classification is a tricky problem in machine learning, especially when the number of samples is particularly small. In order to solve this problem, the author(s) employed the ideas of knowledge aggregation and transfer learning. The main idea of this paper is interesting but my main concerts are on the limited novelty compared to the previous work. Furthermore, I do not find any references or discussions in order to present the limitation of the proposed methods. Some reconstructive comments are listed as follows:
1. The main problem of this paper is the empirical studies. Author(s) must discuss in details how they registered and matched the different size of fMRI images to a common space in order to employ them for learning! Further, they must explain how different cognitive states are extracted from task-based fMRI datasets in order to combine them with rest-mode fMRI features! 
2. Another problem in the empirical studies is that this paper did not compare the performance of the proposed method with state-of-the-arts methods applied to each of employed datasets! It could be interesting to figure out what new embedded information can be discovered by combining these datasets.
3. Another problem is the generality of the proposed methods. In order to apply the proposed method to other existed datasets, Author(s) must present more technical issues and general problems that could be happened for combining these datasets, e.g. the challenge of removing noise and sparsity, or even matching the same cognitive tasks from different datasets, etc.
4. In Figure 4, what is the maximum and minimum of correlation? It is better to show the fluctuation of values in this figure by using a bar (from dark red to dark blue).
5. As another problem in Figure 4, why all regions of the human brain are activated for a singular task?
6. The error bars must be reported in Figure 2 and Figure 3.    
","This paper introduces a framework to analyze heterogeneous fMRI datasets combining multi-task supervised learning with multi-scale dictionary learning. The result shows that classification of task conditions in a target dataset can be improved by integrating the large-scale database of humman connectome project (HCP) using their method.

The proposed entire framework looks fairly reasonable to me. However, the method contains many different technical ideas and each of them is not necessarily well validated. The technical ideas include at least the following: 1) use of resting state data instead of task data for training the first stage; 2) use of multi-scale spatial dictionary combining multiple runs of sparse dictionary learning; 3) using Dropout for regularization; and 4) adding HCP data to improve classification on another target dataset. In my view, current results only support point 4. The following comments are related to the other points.

In the experiment (Sec2.2), the comparison between single and multi-scale dictionaries (methods 2 and 3) seems to be made for different dictionary sizes (256 and 336). However, I think they should be balanced for a fair comarison (for instance, learn 336 atoms at once for the non-multi-scale dictionary), since currently the effects of having different dimensionalities and different spatial scales cannot be dissociated. I would also suggest comparing with simply using PCA to reduce to the same dimensionality, which may clarify the advantage of using localized features.

Moreover, the methods 1-3 use L2-regularization but 4-5 uses Dropout, which may also not be a fair comparison. What happens if you use L2-regualarization for methods 4-5, or otherwise Dropout for 1-3?

I also wondered what happens if the spatial dictionary is trained based on the task data rather than resting-state data. The use of rest data for training sounds reasonable for the goal of ""uncovering atomic bases"" (Introduction) but it would be necessary to compare the two cases in terms of both test accuracy and interpretability. In particular, can similar spatial maps in Figs4,5 be obtained even without resting-state decomposition?


Other minor comments:
- Eq1,3: what does the objective actually mean, when taking expectation with respect to M? Is it correct that the objective is minimized rather than maximized? - Eq3: ""log"" seems to be dropped.
- Fig2: what are the chance levels for each dataset? No result by ""full multinomial"" for LA5c.
- Line235: what does ""base condition"" mean?","This work presents a multi-task learning approach to meta-analysis in functional neuroimaging (fmri). 

The implementation and evaluation steps are very careful, evidence for benefits of transfer learning are presented. Results reveal that tricks from deep learning (stochastic optimization and stochastic regularization / drop-out) are useful - also for these medium scale, linear estimation problems. 

A main weakness is the starting point of using z-score maps as representation of the data. While it may be a productive reduction of complexity, it would have been nice to see a critical discussion of this step.

A few other opportunities a missed: 
There is limited formal quantification of uncertainty (modeling/Bayes, resampling) 
There is no attempt of integrating the label structures across studies (i.e. borrowing power  from similar labels wrt cognitive structure/ontology)
There is limited attempt to place the work in a greater context of neuroimage meta-analysis. The literature review misses relevant work on meta-analysis e.g. based on coordinate based reconstructed activity maps (such as Turgeltaub's ALE or Nielsen's Brede methods). 

Very nice to see the learning curves in Fig 3! However, they lead me to the opposite conclusion, namely that the benefits of transfer learning are surprisingly small...
Again, this may be related to the specific input chosen or the lack of modeling of output structure

In conclusion, this is a solid piece of work- presenting real progress in the field 

"
Conic Scan-and-Cover algorithms for nonparametric topic modeling,"Mikhail Yurochkin, Aritra Guha, XuanLong Nguyen",https://proceedings.neurips.cc/paper/2017/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html,"This paper presents a geometric algorithm for parameter estimation in LDA when the number of topics is unknown.  The method obtains impressive results—its predictive performance on synthetic data is comparable to a Gibbs sampler where the true number of topics is fixed.  Moreover the algorithm appears to scale very well with the number of documents (like all spectral methods).  Unfortunately, the meat of the paper is dense and difficult to parse.  The paper attempts to provide some geometric intuition — but the lack of a clean and well-described picture makes the scattered references to the geometric story (i.e., ""conic scans"") hard to follow.  The caption for figure 1 does not say enough to clearly explain what is being depicted.  I don’t have much familiarity with the relevant background literature on geometric algorithms for LDA inference and will defer to other reviewers with stronger background.  But I think the general NIPS reader will have too much difficulty reading this paper.  ","The paper proposes a nonparametric topic model based on geometric properties of topics. The convex geometric perspective of topic models gave rise to some interesting questions. This paper tackles the problem of finding an appropriate number of topics by topic simplex covering algorithm called conic scan coverage approach. 

Overall, this paper is clearly written, and easy to follow. Theoretical justifications of CSC support the main claim of the paper as well. However, unlike the CSC algorithm, the document CSC algorithm contains some arbitrary steps such as the mean shifting and spherical k-means. The justification of these steps may bring some other questions.
For example, is the mean shifting result always around the initial direction? if not, is the initializing direction in step 4 of algorithm2 necessary? A random selection of an initial direction would be enough since it would also cover the topic simplex, eventually.
A similar question arises from the spherical k-means; the resulting topics are no longer a vertex of topic convex. What if the variance of the original topic vertex before these steps is relatively low than the others? (may be a long document could not be the topic vertex?) Do we still need the mean-shifting and k-means in this case? or is it better to keep these vertices? It would be more interesting if there are some (empirical?) discussions on the variances of the point estimate and the proposed steps.

","* Summary

This paper introduces a novel algorithm, Conic Scan Coverage, that is
based on convex geometry ideas and can perform non-parametric topic
modeling. The algorithm is intuitively based on the idea of covering
the topic simplex with cones. The papers presents the results of an
experimental evaluation and the supplementary contains detailed proofs
and example topics inferred by the algorithm.

* Evaluation

I have very mixed feelings about this paper.

On the one hand, I must say I had a lot of fun reading it. I am really
fond of the convex geometric approach to topic modeling, it is such an
original and potentially promising approach. Indeed, the results are
very good, and the topics presented in the supplementary material look
great.

On the other hand, I have a lot of uncertainty about this
paper. First, I must confess I do not understand the paper as well as
I would like. Of course, it could mean that I should have tried
harder, or maybe that the authors should explain more. Probably a bit
of both. Second, the experiments are weak. However, the paper is so
original that I don't think it should be rejected just because of
this.

To summarize:

+ Impressive results: excellent runtime and statistical performance, good looking topics
- Weak experimental evaluation

+ Very original approach to topic modeling
- Algorithm 1 seems very ad-hoc, and justification seems insufficient to me

Could you please show me a few example of inferred topic distribution?

* Discussion

- The experimental evaluation could be much better.

  The python implementation of Gibbs-LDA is pretty bad, it makes for a
  very poor baseline. You really need to say somewhere how many
  iterations were run.

  When comparing against algorithms like LDA Gibbs or HDP Gibbs, as in
  table 1, you can't report a single number. You should plot the
  evolution of perplexity by iteration. For large datasets, Gibbs LDA
  can reach a good perplexity and coherence in surprisingly few
  iterations. Also, there should be some error bars, a number in
  itself is not very convincing.

  You should also share information about hyperparameter settings. For
  instance, HDP and LDA can exhibit very different perplexity for
  different values of their alpha and beta hyperparameters.

  Finally, you should also vary the vocabulary size. I did note that V
  is very small, and it is worrysome. There are been several
  algorithms proposed in the past that seemed very effective for
  unrealistic small vocabulary sizes and didn't scale well to this
  parameter.

- The algorithm seems very ad-hoc

  I am surprised that you choose a topic as the furthest document and
  then remove all the documents within some cosine distance. Why makes
  such a farthest document a good topic representative? Also, if we
  remove the documents from the active sets based solely on one topic,
  are we really trying to explain the documents as mixture of topics?
  I would be really curious to see a few documents and their inferred
  topic distributions to see if it is interpretable.

- What good is Theorem 4?

  Theorem 4 is very interesting and definitely provides some good
  justification for the algorithm. However, it assumes that the number
  of topics K is known and doesn't say much about why such a procedure
  should find a good number of topics. Indeed, in the end, I don't
  have much intuition about what exactly drives the choice of number
  of topics.
"
Online Learning for Multivariate Hawkes Processes,"Yingxiang Yang, Jalal Etesami, Niao He, Negar Kiyavash",https://proceedings.neurips.cc/paper/2017/hash/92a0e7a415d64ebafcb16a8ca817cde4-Abstract.html,"This paper describes an algorithm for optizimization of Hawkes process parameters in on-line settings, where non-parametric form of a kernel is learnt. The paper reports a gradient approach to optimization, with theoretical analysis thereof. In particular, the authors provide: a regret bound, justification for simplification steps (discretization of time and truncation of time over which previous posts influence a new post), an approach to a tractable projection of the solution (a step in the algorithm), time complexity analysis.

The paper is very well written, which is very helpful given it is mathematically involved. I found it tackling an important problem (on-line learning is important for large scale datasets, and non-parametricity is a very reasonable setting when it is hard to specify a reasonable kernel form a priori). The authors provide theoretical grounding for their approach. I enjoyed this paper very much and I think it is going to be influencial for practical application of Hawkes processes.

Some comments:
- Can you comment on the influence of hyperparameter values (regularization coefficients, potentially also discretization step)? In experiments you set them a-priori to some fixed values, and it is not uncommon in the experiments that values of regularization hyperparameters have a huge influence on the experiments.
- Your experiments are quite limited, especially the synthetic experiments where many values are fixed and a single matrix of influence forms is considered. I think it would be much more compelling if some simulation was considered, where values would be repeatedly drawn. Also, why do you think this specific setting is interesting/realistic?
- I suggest making the font in Figures larger (especially for Figure 2 it is too small), it doesn't look good now. This is especially annoying in the supplementary (Figure 4).

","The paper deals with an interesting and important problem -- how to learn the triggering functions of multivariate Hawkes processes in an online fashion. The proposed method gets rid of the parametric assumptions of the triggering functions and assumes that the triggering functions come from the reproducing kernel Hilbert space and is a nonparametric method. It is proven that the proposed algorithm can achieve a sublinear regret bound. And the proposed nonparametric method is numerically evaluated on synthetic and real datasets. 

Comments:

1.	The paper is overall well written before section 3. The current algorithm section and theoretical analysis section are not easy to follow. It seems that the authors have skipped some necessary details which makes it a bit difficult for readers to fully understand their algorithm and their theoretical analyses.    
2.	The notations in section 3 are a bit heavy. There are quite a few tuning parameters in the proposed algorithm. From the current descriptions it is not clear how to set these parameters in practice. For example, how to determine the regularization coefficients and how to choose the step size in your algorithm? Will the values of these parameter influence the final output? It is not clear in section 3. In proposition 2, only polynomial kernels are considered. How about other types of reproducing kernel? Need more discussions here. 
3.	For the theoretical section, is your current loss function convex? I think this is an important issue and more clarifications are needed. 
4.	The numerical evaluation of the algorithm can be improved. For example, a bigger size of problem can be considered. 
5.	This submission is using a template for NIPS 2016.
","The authors propose an online algorithm for inferring the triggering functions among different dimensions of an MHP. Due to the vast application of Hawkes processes in modeling real-world problems with streaming data and the importance of revealing the interrelation among different dimensions in these scenarios, the problem of the paper is interesting and important. However, the authors didn’t motivate the problem well and their experiments are not satisfying enough. 

I think the paper took a principled approach to estimate the log-likelihood of MHPs as a function of only triggering kernels by discretizing the time axis and using an online kernel learning method. It has a strong theoretical basis. However, I have the following concerns:

1.	Although the authors provide good theoretical results about the efficiency of the proposed algorithm, there are no experiments showing the scalability of the model in real-world scenarios.
2.	The experimental results need further improvements. For example, there should be a theoretical or experimental analysis of the sensitivity of the proposed algorithm performance with respect to f^(0), \mu^(0), step size and other parameters. 
3.	The performance of MLE-SGLP and MLE seems more or less the same as the proposed method. A quantitative measure would clarify the differences.
4.	The analysis of the performance of the algorithm with respect to the number of training data is missing. 
In summary, the paper has a strong theoretical basis, but lacking in some theoretical/experiential analysis that mentioned above, which doubt the superiority of the proposed algorithm with respect to the other methods like MLE-SGLP and MLE.  
"
An Empirical Study on The Properties of Random Bases for Kernel Methods,"Maximilian Alber, Pieter-Jan Kindermans, Kristof Schütt, Klaus-Robert Müller, Fei Sha",https://proceedings.neurips.cc/paper/2017/hash/92af93f73faf3cefc129b6bc55a748a9-Abstract.html,"he paper presents an experimental evaluation of different kernel methods using random bases applied to classification tasks. 

Despite the focus of the paper on empirical evaluation, the paper contributes new insights that relate kernel approximation methods to neural networks. In particular the paper shows that a kernel classifier using random Fourier features (RFF) for kernel approximation can be cast as a shallow neural network.

The experimental results are interesting and show that allowing the learning of the RFF's random Gaussian matrix W_B produce it is possible to obtain high accuracy with several orders of magnitude less features. 

This said, I think that the contribution of the paper is still short of what is expected for the conference. The introduction creates the false expectation that the paper will provide insights on the differences/similarities between kernel methods and deep learning architectures (from the introduction ""Understanding these differences in efficiency between approximated kernel methods and deep architecture is crucial to use them optimally in practice""). However, the experiments just compare an approximate kernel method against adaptive version of the same method, which are equivalent to a shallow neural network. The extension of the results and experimentation to deep learning architectures would make a very good contribution. ","The authors conduct an interesting empirical study on the use of random bases in kernel methods and its connection and interpretation to neural networks.

Up to my knowledge the ArcCos kernel in connection to the ReLU neural networks interpretation in eq (5) is new and an interesting result. My suggestion is to stress more this novel contribution, also in the abstract. The title suggests that the paper is only focusing on an empirical study, but this eq (5) looks novel and interesting to me.

In section 3.1 the authors could somewhat better specify which parts are new versus existing. The connection between neural networks and kernel methods in eqs (2)(6) has been discussed in previous work: in the book  
J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, J. Vandewalle, Least Squares Support Vector Machines, World Scientific, Singapore, 2002
neural networks interpretations were given both to the primal representation (2) and the dual representation with the kernel function, together with approximations to the feature map, by Nystrom approximation on a subset.
The paper
Suykens J.A.K., Vandewalle J., Training multilayer perceptron classifiers based on a modified support vector method, IEEE Transactions on Neural Networks, vol.10, no.4, Jul. 1999, pp.907-911.
explicity took the feature map equal to the hidden layer of an MLP neural network. 

The experiments contain an interesting comparison between RB, UAB, SAB, DAB, illustrating the effect of the number of features. However, shouldn't the number of features be considered as a tuning parameter? It would be good to pay more attention to the definition of the tuning parameters in the different methods, and how they are selected. 

In Table 1 only the best accuracy is given. Also to mean accuracy (and standard deviation) would be interesting to know here. 

 

","Summary:

The authors provided an empirical study contrasting neural networks and kernel methods, with a focus on how random and adaptive schemes would make efficient use of features in order to improve quality of learning, at four levels of abstraction: data-agnostic random basis (baseline kernel machines with traditional random features), unsupervised data-adaptive basis for better approximation of kernel function, supervised data-label-adaptive basis by kernel target alignment, discriminatively adaptive basis (neural nets). The paper concluded with several suggestions and caveats for efficient use of random features in practice.

Comments:

- 1 -
Line 123, especially for sake of comparing UAB case where the underlying assumption is that using the true kernel function k in prediction yields the ""best"" performance so that UAB tries to approximate it, I would suggest testing in experiments a baseline model that utilizes the true kernel function k in prediction. Also this would suggest, for example in Fig. 1 at which point of the KAE curve the accuracy is sufficiently good (despite many theoretical results available).

- 2 -
The four datasets chosen in the paper certainly demonstrate proof for conclusions finally drawn. However, in order to support those conclusions to a definitively convincing extent, more datasets should be needed. For example, the performance scores in Tab. 1 do not seem to be too significantly different marginally for each task. And reasons for inconsistent behaviors across different tasks (CoverType vs others in Fig. 1 for instance, Line 222) are not well explained through empirical exploration.

- 3 -
It is not clear how big the difference can be between the four adaptive schemes in terms of training time, which can be crucial in practice. In addition, as the number of (approximate) features D increases, how does the training time increase accordingly in general for each scheme empirically? It would be interesting to also report this."
"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions","Aryeh Kontorovich, Sivan Sabato, Roi Weiss",https://proceedings.neurips.cc/paper/2017/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html,"The paper shows that Algo 1 is strongly Bayes-consistent in finite doubling-dimensional metric spaces. This is the second consistent nearest-neighbor variant (after the margin-regularised version in [23]). Interestingly here the Bayes consistency is obtained via a sample-compression scheme. Bayes consistency is then also proved in infinite dimensional setting. This seems to imply that Algo 1 is preferable not only to nearest neighbor but also to kNN.

Pros:
- Thorough theoretical analysis of a novel version of multi-class nearest neighbor learning (Algo 1), which builds on a number of recent results, but goes much deeper than existing work.
- Algo 1 was derived by minimising a generalisation bound, so it is very nice to see that it actually has good properties. It shows an example where theory is useful in practice. 
- Well written - albeit necessarily terse, it is complete with definitions and background so it can be followed.

Cons:
- More a question than a con: Bayes-consistency is nice to have, but can anything be said about the finite sample behavior of Algo 1 ? (For instance it is known that the Tsybakov margin condition gives fast rates for the original kNN.)

","This work develops a compression-based algorithm for multiclass learning; the authors claim the method is both efficient and strongly Bayes-consistent in spaces of finite doubling dimension. They also provide one example of a space of infinite doubling dimension with a particular measure for which their method is weakly Bayes consistent, whereas the same construction leads to inconsistency of k-NN rules. Overall, I think this paper is technically strong and seems to develop interesting results, but I have a few concerns about the significance of this paper which I will discuss below. If the authors can address these concerns, I would support this paper for acceptance.

Detailed comments:

I did not check the proofs in the appendix in detail but the main ideas appear to be correct. My main comments are on the efficiency of the method (which I expect the authors can easily address) and on the significant of the results in infinite dimensions.

(I) I have a question about the efficiency of the algorithm:
It is claimed that the complexity of Algorithm 1 is $O(n^4)$. I can see that the for loop traverses a set $\Gamma$ of cardinality at most $O(n^2)$, and in line 5 of the algorithm we pay at most $O(n)$ (which globally yields $O(n^3)$). What is the cost of constructing a $\gamma$-net? Is this $O(n^2)$, and if so, what is the algorithm for doing this? I do not yet believe the efficiency claim by the authors until this part is explained.

It is absolutely necessary that Algorithm 1 is efficient, as otherwise one can just turn to standard $k$-NN methods (not based on compression) in order to obtain the same results, although through different methods.

(II) Infinite dimensions:

It is not clear to me why the results/discussion related to finite dimensions is significant.

If I understand correctly, the results of Cérou and Guyader state that, for a particular (realizable) distribution, no $k$-NN learner (where $k$ grows with $n$ in the appropriate way) is Bayes-consistent. Thus, for a particular distribution, $k$-NN fails to be Bayes-consistent. But perhaps it succeeds for other realizable distributions satisfying equation (3). I can see that in the line after Theorem 3, the authors *seem to* claim that in fact the paper of Cérou and Guyader implies that *any* (realizable) distribution satisfying equation (3) will cause $k$-NN to be Bayes inconsistent; am I correct in this interpretation of Line 221? If so, is this easy to see from Cérou and Guyader's paper?

On the other hand, the authors show that for a particular metric space of infinite doubling dimension and for a particular realizable distribution, KSU is Bayes-consistent. This is interesting provided that $k$-NN fails to be Bayes consistent for all constructions satisfying equation (3), but it is not interesting if the point is simply that, for the Preiss's construction, $k$-NN is not Bayes consistent while KSU is.

It is therefore critical if the authors clarify the extent to which $k$-NN fails to be Bayes consistent (is it for all constructions satisfying equation (3) or just for Preiss's construction?).

Also, the authors should mention Biau, Bunea, Wegkamp (2005), ``On the kernel rule for function classification'', because the paper of Cérou and Guyader (near the top of page 13 therein, or page 352 from internal numbering) claims that Biau et al. also show a nearest neighbor-type method that is weakly Bayes consistent in a space of infinite dimension. Mentioning this result (slightly) weakens the force of the authors' results in infinite dimensions.

Minor comments:

Lines 78-80: How do the authors know about the key motivation behind [15, 23, 16]?

Line 277: I think $n(\tilde{\gamma})$ should be replaced by $n(\gamma)$.

In the 3-point proof sketch of Theorem 2, from lines 262-281:
The paper would benefit from a more explicit/longer explanation part 3.


UPDATE AFTER REBUTTAL

I am satisfied with the authors' responses and recommend acceptance. Also, regarding the confusion about the citations, I'm happy the authors were able to straighten out which paper was actually intended, while also pointing out the disadvantages of the approach from the older paper.","The authors consider a compressed 1NN algorithm that was introduced in a former article, and propose a model selection scheme for the compression level based on an upper bound of the generalization error of the classifier. The authors prove several theoretical results for the algorithm, both in the finite and infinite dimension setting. 

The article is well written and the sketches of proofs that are presented put the emphasis on the relevant steps of the proof without going too deep on technicalities. Overall the paper is interesting, my only concern is about the significance of the contribution for a wide audience. The computational cost of the procedure that is presented (O(n^4)) does not seem quite attractive, and after a quick look at the cited bibliography there does not seem to be many real applications of the strategy. Being not an expert of the field I would like the authors to motivate their procedure a little bit more. Is it only theoretical ? Can it be thought as a theoretical surrogate for any compression algorithm that could be routinely used in practice ? This would first widen a little bit the audience for the public, but that would also be consistent with the fact that in many places the authors mention that their procedure is attractive regarding its running time. Do this refer to the classification step once the compressed classifier is built ?  "
Causal Effect Inference with Deep Latent-Variable Models,"Christos Louizos, Uri Shalit, Joris M. Mooij, David Sontag, Richard Zemel, Max Welling",https://proceedings.neurips.cc/paper/2017/hash/94b5bde6de888ddf9cde6748ad2523d1-Abstract.html,"This paper tackles the problem of estimating individual treatment effects using proxy variables. Proxy variables partially capture the effects of the unobserved confounders, but not all effects. Therefore, simply conditioning on proxy variables does not work; one needs a better way of controlling for unobserved confounding.

The authors present a method based on variational auto-encoders that models the joint probability of observed and unobserved variables. They are able to model a multi-dimensional representation for the confounders (z). Their identification still depends on some parametric assumptions about z (e.g., normally distributed), but it is still an improvement over past work because they are able to model z as high-dimensional, while also allowing high dimensional proxy variables. 

I like how the authors tested their method on multiple real and simulated datasets. While any causal inference method is unlikely to be optimal for all contexts, the results do show encouraging results. From simulations, it seems that this result is able to perform well even as the noise in proxy variables is increased, although more testing is needed to understand the properties of the method. This is one of the weaknesses of the proposed approach; hard to characterize any properties of the identification procedure because we do not have a good theory for variational autoencoders.

Still, the proposed method is flexible enough that it can be practically useful to control for non-trivial forms of confounding in individual causal effects.","-	The authors utilized Variational Autoencoders (VAE) in order to estimate the individual treatment effect (ITE) and the average treatment effect (ATE) for the causal model in Figure 1. However, five other causal diagrams can also be considered with confounder proxies (see Figure 1 in [Miao et al., 2016]). It would be interesting to check whether the proposed method can be applied to all these causal diagrams or there are some limitations.
-	Please give some justifications why parametrizing the probability distributions in the form of Equations (2-4) would be a good choice for estimating ITE and ATE. For instance, why covariates of x_i are independent given z_i? Is it restricting the expressive power of this method?
-	In your method, do we need to know the dimension of latent variable z? For instance, in Experimental Results section, it is assumed that z is a 20-dimensional variable. Does the performance degrade if D_z is not adjusted correctly?
-	In Table 1, is there any good reason that BNN has a better performance than CEVAE?
-	When z is categorical, it seems that we can just estimate p(x_i|z_i) by your method and then estimate ITE and ATE from the results in [Kuroki and Pearl, 2011]. It would be interesting to compare the performance of this method with CAVAE?
-	Figure 3 shows that model misspecification may increase the sample complexity. It would be interesting to characterize the sample complexity overhead from model misspecification.
-	Under some conditions, can we have a theoretical guarantee that CEVAE gives a good approximation of ITE or ATE? 
-	Some typos:
o	Line 126: p(Z,Xt,y)->p(Z,X,t,y)
o	In equation (2), please define D_x and D_z.
","This paper addresses the problem of predicting the individual treatment effects of a policy intervention on some variable, given a set of proxy variables used to attempt to capture the unknown latent variable space. The approach is based on variational autoencoders (VAE) to leverage big data for causal inference. VAEs have the advantage of making few assumptions about the latent structure and strong empirical performance, but the disadvantage of lacking a guarantee to converge to the true minimum. Empirical results indicate the proposed method compares favorably to some benchmarks.                                             

In general the paper is well written and the results appear sound and consistent with the existing literature.

However, there doesn't seem to be a lot of novel content in the paper: 1) On the causality side, the contribution seems to be theorem 1, but this is basically a restatement of a do-calculus result from (Pearl, 2009). 2) On the deep learning side,  the VAE approach seems to be mostly based on TARnet from (Shalit et al., 2016) [more below]. The paper seems to basically be a combination of these two methods.

In section 3, it's not clear what the main differences are between the proposed CEVAE method and the TARnet method from (Shalit et al., 2016) - because of this, it's difficult to determine whether there is any significantly novelty here and whether this might be relevant to the potential impact of the paper.

Unless I have misunderstood section 3, there does not seem to be enough novelty to warrant acceptance for this paper."
Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach,"Emmanouil Platanios, Hoifung Poon, Tom M. Mitchell, Eric J. Horvitz",https://proceedings.neurips.cc/paper/2017/hash/95f8d9901ca8878e291552f001f67692-Abstract.html,"This paper builds on the work of Platanios et al. (2014, 2016) on estimating the accuracy of a set of classifiers for a given task using only unlabeled data, based on the agreement behavior of the classifiers. The current work uses a probabilistic soft logic (PSL) model to infer the error rates of the classifiers. The paper also extends this approach to the case where we have multiple related classification tasks: for instance, classifying noun phrases with regard to their membership in multiple categories, some of which subsume others and some of which are mutually exclusive. The paper shows how a PSL model can take into account these constraints among the categories, yielding better error rate estimates and higher joint classification accuracy.

[UPDATED] I see this paper as just above the acceptance threshold. It is well written and the methodology seems sound. It provides a compelling example of improving prediction accuracy by applying a non-trivial inference algorithm to multiple variables, not just running feed-forward classification.

The paper is less strong in terms of novelty. It's not surprising that a system can achieve better accuracy by using cross-category constraints. The paper does this with an existing PSL system (from Bach et al. 2015). The paper discusses a customized grounding algorithm in Section 3.3.2 (p. 6), but it seems to boil down to iterating over the given instances and classifier predictions and enumerating rule groundings that are not trivially satisfied. The only other algorithmic novelty seems to be the sampling scheme in the second paragraph of Section 3.3.3 (p. 7). [UPDATE: The authors say in their response that this sampling scheme was critical to speeding up inference so it could run at the necessary scale. If this is where a significant amount of the work's novelty lies, it deserves more space and a more thorough exposition.]

The paper could also benefit from deeper and more precise analysis. A central choice in the paper is the use of probabilistic soft logic, where variables can take values in [0, 1], rather than a framework such as Markov Logic Networks (MLNs) that deals with discrete variables. The paper says that this choice was a matter of scalability, and ""one could just as easily perform inference for our model using MLNs"" (lines 213-214, p. 5). I don't think this is true. The proposed model applies logical operators to continuous-valued variables (for the error rates, and the predictions of probabilistic classifiers), which is meaningful in PSL but not MLNs. If one were to model this problem with an MLN, I think one would replace the error rate variables with learnable weights on rules connecting the classifier outputs to the true category values. Such modeling decisions would change the scalability story: there would be no edges in the MLN between variables corresponding to different input instances (e.g., different noun phrases). So one could do exact inference on the sub-networks for the various instances in parallel, and accumulate statistics to use in an EM algorithm to learn the weights. This might be roughly as efficient as the optimization described in Section 3.3.3. [UPDATE: In the MLN I'm envisioning here, with one connected component per noun phrase, information is still shared across instances (noun phrases) during parameter estimation: the weights are shared across all the instances. But when the parameters are fixed, as in the E-step of each EM iteration, the inference algorithm can operate on each noun phrase independently.]

The experimental results could also use more analysis. It's not surprising that the proposed LEE algorithm is the best by all metrics in the first row of Table 1 (p. 8), where it takes cross-category constraints into account. I have a number of questions about the other two rows:

[UPDATE: Thanks for the answers to these questions.]
* What are the ""10%"" data sets? I didn't find those mentioned in the text.
* Does simple majority voting really yield an AUC of 0.999 on the uNELL-All data set? If that's the case, why are the AUCs for majority voting so much lower on NELL-7 and NELL-11? Which setting is most representative of the full problem faced by NELL?
* Why does the proposed method yield an AUC of only 0.965 on uNELL-All, performing worse than majority voting and several other methods? [UPDATE: The authors note that the classifier weights from the proposed method end up performing worse than equal weighting on this data set. This could use more analysis. Is there a shortcoming in the method for turning error rates into voting weights?]
* On the other three data sets in these rows, the proposed approach does better than the algorithms of Platanios et al. 2014 and 2016, which also use reasonable models. Why does the new approach do better? Is this because the classifiers are probabilistic, and the new approach uses those probabilities while the earlier approaches binarize the predictions? [UPDATE: This still seems to be an open question. The paper would be more satisfying if included some additional analysis on this issue.]
* For a given probabilistic classifier on a given labeled data set, the two methods for computing an ""error rate"" -- one thresholding the prediction at 0.5 and counting errors; the other averaging the probabilities given to incorrect classifications -- will yield different numbers in general. Which way was the ""true"" error rate computed for these evaluations? Is this fair to all the techniques being evaluated? [UPDATE: The authors clarified that they thresholded the predictions at 0.5 and counted errors, which I agree is fair to both probabilistic and non-probabilistic classifiers.]

Smaller questions and comments:

* p. 1, Figure 1: If there is a shortage of space, I think this figure could be removed. I skipped over it on my first reading of the paper and didn't feel like I was missing something.
* p. 2, line 51: ""we define the error rate..."" Wouldn't it be helpful to consider separate false positive and false negative rates?
* p. 2, line 55: The probabilistic equation for the error rate here doesn't seem syntactically valid. The error rate is not parameterized by a particular instance X, but the equation includes \hat{f}^d_j(X) and (1 - \hat{f}^d_j(X)) outside the P_D terms. Which X is used there? I think what's intended is the average over the data set of the probability that the classifier assigns to the incorrect classification.
* p .4, line 148: Were the techniques described in this Identifiability section used in the experiments? If so, how was the prior weight set?
* p. 5, Equation 3: Why does the model use \hat{f}^{d_1}_j here, rather than using the unobserved predicate f^{d_1} (which would be symmetrical with f^{d_2}) and relying on constraint propagation?
* p. 5, line 219: This line starts using the symbol f for a probability density, which is confusing given the extensive use of f symbols for classifiers and target functions.
* p. 6, line 223: What is p_j? Is it always 1 in the proposed model?
* p. 6, line 225: How were the lambda parameters set in the experiments?
* p. 6, line 257: The use of N^{d_1} in these counting expressions seems imprecise. The expressions include factors of D^2 or D, implying that they're summing over all the target functions in D. So why is the N specific to d_1?
* p. 7, line 300: ""response every input NP"". Missing ""for"" here.
* p. 8, Table 1: The use of different scaling factors (10^-2 and 10^-1) in different sections of the table is confusing. I'd recommend just presenting the numbers without scaling.","This paper proposes a method for estimating accuracies of a set of classifiers from unlabelled data using probabilistic logic rules. The method can take constraints into account, in particular stating that labels are mutually exclusive or have a subsumption relationship. It is demonstrated to outperform competing methods on four datasets. 

The paper is carefully written but dense. There are no theoretical results in the paper, and the experimental results have been crammed into 1.5 page. As a result the paper tries to do too many things and doesn't achieve any of them very well: I couldn't reconstruct the method from the paper, and the experiments seem to show that it works well in some carefully engineered settings with special-purpose metrics, but doesn't do a full strengths and weaknesses analysis (as could have been achieved with artificially generated data). 

Small point: in Table 1 I'm not sure why the units are different in each of the three main rows (x10^-2, x10^-1). ","# Summary
This work proposes an approach to infer the error rate of a set of classifiers using (only) unlabeled data by leveraging possible logical constraints given by the relationships between the classes. This is achieved by translating these constraints into a probabilistic logic model and inferring a state of the unknown variables (the unknown true classes and error rates of the classifier) that has a high probability. The approach is tested on a noun phrase classification task as well as a FMRI classification task both when the constraints are taken into account and when they are not. In both cases the approach compares favourably with existing approaches both in terms of the approximation of the error rate and the estimation of the target classifier.

# Quality
Overal, I found the work to be described well and the paper to form a coherent story. After reading the work, however, I was left with an understanding of the method, yet little in the way of understanding the result. Most importantly, I do not think I understand why the method would give useful, (and surprisingly good!) results if the constraints between classes are not present (so in the uNELL and uBRAIN settings in the experiments). The text suggests the authors have a better intuition for this, yet, perhaps due to space constraints they were not able to convey these insights in a way that made me better understand these results. Perhaps they can do so in a revised version or the rebuttal. So, in general, my main concern here is that the paper does an excellent job explaining the ""how"" of the method, perhaps leaving too little space to explore the ""why"".

# Clarity
The paper is very clearly written and already seemed to be thoroughly proofread, for which I want to thank the authors. My only typographic comment has to do with the bolded items in Table 1, which at first confused me because they are not only bold but also red. Also, I can not find what the 10\% in the table refers to.

Having said this there were a number of claims in the paper that were not directly clear to me, which I hope the authors can clarify in a revised version. In line 63, it was unclear to me what independence (between which two quantities) you are referring to specifically. In line 119 it would be nice if the reference to the related work is made more specific. For the claim starting on line 139 it is not directly clear to me how this follows from the explanation that precedes it. In equation (1), I suppose it was not very clear to me why the error term in these equations does not depend on the specific instance (X). I understand this makes sense later when in the final model the error rate is inferred to a value that works across all instances, but at this point in the explanation this was not clear to me.

# Originality & Significance
My second main concern is the scope of the work, since it seems to be tightly integrated in an existing research programme. This is clear both from the experiments, which are done on two datasets, and the references, making it hard to judge the efficacy of the method beyond the specific setting studied here. Perhaps the authors can comment on their thoughts on the general applicability of their methods in other classification problems and when they would expect them to work well, especially when the logical constraints are not present as in the uNELL en uBRAIN examples in the paper."
Ranking Data with Continuous Labels through Oriented Recursive Partitions,"Stéphan Clémençon, Mastane Achab",https://proceedings.neurips.cc/paper/2017/hash/97416ac0f58056947e2eb5d5d253d4f2-Abstract.html,"The ms tries to solve a problem called continuous ranking, which is an extension of bipartite ranking. The idea of continuous ranking is to find a score function that increase or decrease with output y with highest probability.

In bipartite ranking, each data point x is given a binary label y \in {+1, -1}. The goal is to find a score function s(x) such that the difference of p(s(x)>=t|y=+1) and p(s(x) < t|y=-1) is maximal for all threshold t. This can be achieved by maximizing AUC of the score function.

The ms extends the bipartite ranking idea for continuous output Y, by introducing a threshold y to separate the data into two ordered parts. By recursively divide the two ordered parts into further ordered two parts, we constructed an ordered binary tree, which provides a continuous ranking on the leaf nodes.

The ms requires the user to provide score function s(x) and distribution of output Y. If distribution of Y is not known, then it is approximated by empirical data distribution of the training data. Then we can calculate the IAUC for the given score function and output distribution. The ms proves that the IAUC could be used to evaluate the score function s(x) in the sense of continuous ranking.

The major contribution of the ms is from theoretical side. However, my limited knowledge could not help me to fully understand the theoretical novelty. I will try to comment more from the practical side.

The ms emphasized the differences between continuous ranking and regression (Figure 2). In continuous ranking, we optimize IAUC, in regression we minimize mean square error. It is a bit confusing to understand ""regression function"" since on page 4, section 3.2, proposition 1, point 3, the regression function is an optimal scoring rule.

The numerical experiment uses specially designed data. The generating function z²(z+1)(z+1.5)(z+2) is uncommon in real datasets and there is no noise. The oscillation that causes problems in Figure 3b looks so small that I worry it will be easily dominated by noise. It will be nice to see some results for real applications. 

","This paper generalizes bi/multi-partite ranking problem and uses the pair (x, y), where x is the feature vector and y is a continuous real-valued label not the discrete label, to find optimal scoring function s(x). The existence of the optimal scoring rules for continuous ranking is given by Proposition 1. A dedicated functional criterion, called the IROC curve here or the maximization of the Kendall \tau related to the pair (s(x), y) are used as the performance measures. A recursive statistical learning algorithm which tailored to empirical IROC curve optimization are presented. An oriented binary tree can be used as piecewise constant scoring function for continuous ranking.

My majority concern about this paper is the motivation of the continuous ranking is not well described. The authors didn’t point out the disadvantage of bi/multi-partite ranking and why the generalization of continuous ranking is meaningful. As the discrete binary label is used as the measurement is hard to obtain and only the relevance or not is indicated by y, the continuous real-valued label can be used for ranking itself. The author summarize the potential applications as quality control and chemistry but these scenario are also suitable to bi/multi-partite ranking. This paper need a good example to display the difference between the continuous ranking and the bi/multi-partite ranking. 

The author should point out the proof hardness of Proposition 1 & 2 and Theorem 1. The proofs looks like a trivial variant of the corresponding part of bi/multi-partite ranking.

I think the authors should cite the bi/multi-partite ranking papers and AUC criterion paper.
[1] Stéphan Clémençon, Marine Depecker, Nicolas Vayatis: Ranking forests. Journal of Machine Learning Research 14(1): 39-73 (2013).
[2] Aditya Krishna Menon, Robert C. Williamson: Bipartite Ranking: a Risk-Theoretic Perspective Journal of Machine Learning Research 17(195):1−102 (2016).
[3] Corinna Cortes, Mehryar Mohri: AUC Optimization vs. Error Rate Minimization. NIPS: 313-320 (2003).","
The paper studied the problem of continuous ranking, which is a generalization the bi-partite/multi-partite ranking problem in that the ranking label Y is a continuous real-valued random variable. Mathematical formulation of the problem is given. The problem of continuous ranking considered as multiple sub-problems, each corresponds to a bipartite ranking problem. Necessary conditions for the existence of optimal solutions are given. The IROC (and IAUC) measures are proposed for evaluating the performances of scoring functions. Finally, a continuous ranking algorithm called Crank is proposed. Experimental results on toy data showed the effectiveness of the proposed Crank algorithm. 

The problem investigated in the paper is very interesting and important for learning to rank and related areas. The paper is well written. The proposed framework, measures, and algorithms are clearly presented. The empirical evaluation of the paper is weak, as it is based on toy data and weak baselines. 

Pros. 
1.	Generalizes the bipartite/multi-partite ranking problems to the continuous ranking problem. Rigorous formulation of the problem and strong theoretical results. 
2.	Extending the conventional ROC to IROC for measuring the continuous ranking functions. 
3.	Proposing Crank algorithm for conducting continuous ranking. 

Cons. 
1.	The authors claim that “theoretical analysis of this algorithm and its connection with approximation of IROC* are beyond the scope of this paper and will be the subject of a future work”. I am not convinced that “its connections with approximation of IROC* are beyond the scope of this paper”. As to my understanding of the paper, the goal of proposing IROC is to guide the proposal of new continuous ranking algorithms. Thus, it is very important to build the connections. Otherwise, the performances of the Crank algorithm cannot reflect the effectiveness of the proposed theoretical framework. 

2.	The experiments in the paper are really weak. The proposed Crank algorithm is tested based on a toy data. The authors conclude in the last section “… finds many applications (e.g., quality control, chemistry)…”. It is necessary to test the proposed framework and solutions on real problems. The baselines are CART and Kendall, which are not designed for the ranking problem. It is better to compare the algorithm with state-of-the-art bipartite and multi-partite ranking models. The generation of the toy examples, the setting of parameters are not given, which makes it is hard to reproduce the results. 

3.	The literature survey of the paper is not enough. Since the continuous ranking problem is a generalization of the bipartite ranking and multi-partite ranking problems, it is better if the authors could use a section to analyze the advantages and disadvantages of existing ranking models, and their real applications. Currently, only a few references are listed in Section 2.2. 

4.	Minor issue: Line 52: Kendall tau  Kendall $\tau$; Line 66: F^{-1}(u) = inf{s\in R: F(s}>= t), the last t should be u; 

"
Scalable Log Determinants for Gaussian Process Kernel Learning,"Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew G. Wilson",https://proceedings.neurips.cc/paper/2017/hash/976abf49974d4686f87192efa0513ae0-Abstract.html,"Summary of the paper:

	This paper describes two techniques that can be used to estimate (stochastically) the log determinant of a positive definite matrix and its gradients. The aim is to speed up the inference process several probabilistic models that involve such computations. The example given is a Gaussian Process in which the marginal likelihood has to be maximized to find good hyper-parameters. The methods proposed are based on using the trace of the logarithm of a matrix to approximate the log determinant. Two techniques are proposed. The first one based on Chevyshev polynomials and the second one based on the Lanczos algorithm. The first one seems to be restricted to the case in which the eigenvalues lie in the interval [-1,1]. The proposed method are compared with sparse GP approximations based on the FITC approximation and with related methods based on scaled eigenvalues on several experiments involving GPs.

Detailed comments:

Clarity:

The paper is very well written and all details seem to be clear with a few exceptions. For example, the reader may not be familiar with the logarithm of a matrix and similar advanced algebra operations. There are few typos in the paper also.  In the Eq. above (2) it is not clear what beta_m or e_m are. They have not being defined. It is also known that the Lanczos algorithm is unstable and extra computations have to be done. The authors may comment on these too.

Quality: 

I think the quality of the paper is high. In particular, the authors analyze the method described in several experiments, comparing with other important and related methods. These experiments seem to indicate that the proposed method gives competitive results at a small computational cost.

Originality:

The paper a strong related work section and the methods described seem to be original. 

Significance:

My main concern with this paper is its significance. While it is true that the proposed method allow for a faster evaluation of the log determinant of the covariance matrix and its gradients. One still has to compute such a matrix with squared cost in the number of samples (or squared cost in the number of inducing points, if sparse approximations are used). This limits a bit the practical applicability of the proposed method. Furthermore, it seems that the experiments carried out by the authors involve a single repetition. Thus, there are no error bars in the results and one can not say whether or not the results are statistical significant. The differences with respect to the scaled eigenvalues problems (ref. [32]) in the paper seem very small.
","This paper extends the Chebyshev method [9] and Lanczos method [28] for computing log-determinants, to hyperparameter learning that involves derivatives in gradient based optimization. The methods are not completely novel, but the experimental evaluations are solid and they reveal the superiority of Lanczos over Chebyshev and other compared methods, including scaled eigenvalues and FITC. I find that the paper is overall well presented. The authors may want to elaborate on the currently very brief section 3.4, which discusses the surrogate method. For example, ""interpolation with a cubic basis function and a linear tail"" is less than clear, and the method to ""approximate derivatives of the log determinant using the surrogate"" is also too obscure.","This is a well written paper that introduces a new method for approximating log determinants for GP hyperparameter estimation or model comparison when the number of observation points is large. It is based on Stochastic trace estimators (Monte Carlo) and Gauss quadratures. Furthermore, one asset of the method is to provide an estimate of the log determinants gradients for a reasonable extra cost. The effectiveness of the method is illustrated on several diverse examples where it is compared to the scaled eigenvalue method and exact computations when available.

The question of speeding up the kernel parameter inference is of great importance to my opinion and it is sometimes overlooked when dealing with massive datasets. The proposed approach seems to be a sensible alternative to stochastic gradient methods. However, a comparison (or at least a list of pros and cons) between these two methods is kind of missing in the paper.

The method is clearly presented and the illustrations on the examples are convincing. 

My only worry is about the method is the first step that consists in approximating the kernel by one allowing Matrix Vector Multiplication. This introduces a rather strong limitation to when the proposed method can be applied. Methods based on variational approximations of the posterior have become increasingly popular over the past years, would the proposed method still be applicable in this case?  "
Fair Clustering Through Fairlets,"Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Sergei Vassilvitskii",https://proceedings.neurips.cc/paper/2017/hash/978fce5bcc4eccc88ad48ce3914124a2-Abstract.html,"While there is a growing body of work on fair supervised learning, here the authors present a first exploration of fair unsupervised learning by considering fair clustering. Each data point is labeled either red or blue, then an optimal k-clustering is sought which respects a given fairness criterion specified by a minimum threshold level of balance in each cluster. Balance lies in [0,1] with higher values corresponding to more equal numbers of red and blue points in every cluster. This is an interesting and natural problem formulation - though a more explicit example of exactly how this might be useful in practice would be helpful.

For both k-center and k-median versions of the problem, it is neatly shown that fair clustering may be reduced to first finding a good 'fairlet' decomposition and then solving the usual clustering problem on the centers of each fairlet. While this is NP-hard, efficient approximation algorithms are provided based on combining earlier methods with an elegant min cost flow construction.

On 3 example datasets, it is shown that standard clustering yields low balance (perhaps not surprising). A preliminary investigation of the cost of fairness is provided where balance of at least 0.5 is required and the number of clusters is varied. It would be interesting to see also the sensitivity to balance: keep the number of clusters fixed, and vary the required balance.

Minor points:
l. 16 machine learning are -> machine learning is
In a few places, e.g. l. 19, \citep{} would improve the readability, so Kleinberg et al. (2017a) -> (Kleinberg et al. 2017a).
l. 53 define -> defines. In fact, I think you just look at the costs of regular clusterings compared to fair so don't really use this ratio?
Fig 1 caption, would give -> might give. x is assigned to a -> x is assigned to the same cluster as a
l. 102 other measure -> other measures, please could you elaborate?
l. 132, 136 Y -> Y_j as in l. 161
Near Definition 5, an image showing an example of how fairlets together make up the fair clustering would be helpful, though I appreciate it's hard given space constraints - in the Supplement would still be good.","The paper studies the problem of performing fair clustering under the disparate impact doctrine as definition of fairness for binary protected classes, e.g., the gender. The paper introduces a measure of fairness in clustering, which is named as balance; and introduce the concept of fairlets as the minimal sets satisfying fairness while preserving the clustering objectives. With these two notions in the hand, the authors state the problem of learning fair clustering as a two step problem, where the fist stage aims to find the fairlets and the second cluster (as in standard clustering algorithms) these fairlets into clusters. However, the authors prove that finding fairlets is an NP-hard problem  under both k-center and k-median clustering objectives that, however, this problem can be approximated in polynomial time.  

The technical contributions of the paper are significant, and the paper in general is readable (although mathematically heavy at times).  In general, I believe that the paper constitutes one of the  first attempts to deal with algorithmic fairness in the context of unsupervised learning, which is an open and very interesting problem from the community. It also shows that there is still a lot of future work to do in this area: First, balance as fairness measure is only applicable to balanced datasets in terms of sensitive attributes values. Second, the proposed approaches are only valid for one binary protected classes like the gender. Third, the solution of the algorithm is still approximate and the bounds proved in the paper still need to be tighten. 

The main room for improvement in the paper is the experimental section, where the authors made little effort to validate and analyze the results of the proposed algorithms. In particular, an increase in the size of Figure 3 (it is hardly readable) as well as further explanation and analysis of its results would improve the quality of the paper.","In this paper the authors address the generation of 'fair' clusters. The authors acknowledge there is no clear or unique definition of fairness, they decided to use the notion of disparate impact. Based on this, they derived versions of well-known clustering algorithms, also providing some theoretical guarantees and an algorithmic properties. 

The paper is well-written and organised. It is in general easy to follow. The related work section could be improved though. It is not always clear what the connection is between this work and others (especially with Kemel 2013). Also, a couple more papers could be relevant for the discussion. 

https://ai2-s2-pdfs.s3.amazonaws.com/08a4/fa5caead14285131f6863b6cd692540ea59a.pdf
https://arxiv.org/pdf/1511.00830.pdf

They also address fair clustering algorithms, so I find the statement '... we initiate the study of fair clustering algorithms' too strong.

The notion of fairlets I find interesting and appealing. The theoretical analysis seems sound. The experimental section is the weakest part of the paper. As it is, it doesn't add much to the story. A comparison with at least a realistic baseline is needed. 

"
A Linear-Time Kernel Goodness-of-Fit Test,"Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur Gretton",https://proceedings.neurips.cc/paper/2017/hash/979d472a84804b9f647bc185a877a8b5-Abstract.html,"%%% UPDATE: Thank you for your response, which has been read %%%

This is a nice paper, which builds on work from ICML 2016. The main contribution is a more efficient (liner time) version of a previously explored approach to goodness-of-fit testing based on Stein's method. There are some small gaps, or areas where it could be improved, but the overall standard seems high enough for NIPS.

Some minor comments:

- The effect of the choice of J is not explored in any theoretical detail. I am still left unsure whether J=1 would be sufficient if p and q differ in several parts of the domain.

- The power of the test is only asymptotically \alpha, so that it might not be fair to compare different methods in terms of rejection rate alone at finite sample size. For instance, it could be that FSSD-opt just rejects almost everything (i.e. the power is incorrect). For the RBM example it was claimed that all tests had power \alpha \approx 0.05, but (a) this is almost impossible to see in the figure (for \sigma_per = 0 the rejection rates in Fig 2b are too small to see visually from the figure if they are close to 0.05 or not), and (b) this was not checked for the Gauss-Laplace problem.

- How was \grad \log p(x) computed in the RBM example?

- More details of the gradient descent over {v_i} could be provided. For instance, when p and q are well-separated, is it numerically hard to locate the optimal v_i?

- The comparison of FSSD-rand to FSSD-opt is fine, but would not a FSSD-QMC (quasi Monte Carlo) alternative also be possible (and more competitive than FSSD-rand)?

- Is there any theoretical guidance on the choice of the test/train split?

- In theorem 9 there is a typo: ""exists a functions""","The authors have utilized the kernel Stein discrepancy in order to construct a new one-sample hypothesis test [i.e. test the null of whether a sample of points were iid from a distribution P]. Previous work that had accomplished this was expensive for large datasets: for a dataset of n datapoints, it was O(n^2) to compute the statistic. Other previous work had constructed a test statistic that was O(n) in computation time, but as it only used a small amount of the total information available, the authors show it did not have a high power as a test statistic for local alternatives. The authors of this paper have constructed a test statistic that is also linear in n to compute, but it substantially more powerful for local alternatives. They demonstrate this empirically and also theoretically by studying the Bahadur efficiency of the statistic.

This novelty of this paper is that it finds a clever way to construct a linear [in computation time] test statistic that angles at approximating the kernel Stein discrepancy ||E_Q{T_P g*}||_{H^d} [here g* is the optimizing g, H^d is the direct sum of d RKHSes, and T_p is the Langevin operator] with instead something like sup_x ||g*(x)||_2. These ideas could be used outside of the hypothesis testing regime, i.e., one could studying whether the topologies induced on the space of probability measures are the same for the approximated kernel Stein discrepancy and the original one.

Detailed Comments:
L191: [nit] You've used zeta for T_P k already.
L205: The formulations of hat{FSSD} in (2) do not make it obvious that the computation required is O(d^2*J*n). Perhaps you can cite/use the formulation from L122 so this is more obvious?
L261: What exactly does the ""for some v"" in Theorem 7 mean? E.g., can this be for all v? From v drawn from nu?
L295: In their experiments, Chwialkowski et al. (https://arxiv.org/abs/1602.02964) showed that the KSD with a Gaussian kernel experienced a substantial loss in power in higher dimensions (as seen also in your Fig. 2a), but in work by Gorham and Mackey (https://arxiv.org/abs/1703.01717), they showed that this power loss could be avoided by using an inverse multiquadric kernel due to its less rapid decay.  Since the Gaussian and inverse multiquadric KSD running times are the same and the power of the Gaussian KSD is known to degrade rapidly in higher dimensions, the inverse multiquadric KSD may be a more appropriate ""quadratic-time"" benchmark.  How does the power of FSSD compare with the inverse multiquadric KSD in your experiment?","The kernel Stein discrepancy goodness-of-fit test is based on a kernelized Stein operator T_p such that under the distribution p (and only under the distribution p), (T_p f) has mean 0 for all test functions f in an RKHS unit ball. The maximum mean of (T_p f) over the class of test functions under a distribution q thus provides a measure of the discrepancy between q and p. This quantity is known as the kernel Stein discrepancy and can also be expressed as the RKHS norm of a witness function g.

In this paper, instead of estimating the RKHS norm of g using a full-sample U-statistic (which requires quadratic time to compute) or an incomplete U-statistic (which takes linear time but suffers from low power), the authors compute the empirical norm of g at a finite set of locations, which are either generated from a multivariate normal distribution fitted to the data or are chosen to approximately maximize test power. This produces a linear-time test statistic whose power is comparable to that of the quadratic-time kernel Stein discrepancy in simulations. The authors also analyze the test's approximate Bahadur efficiency.

This paper is well-written and well-organized, and the proposed test dramatically improves upon the computational burden of other tests, such as the maximum mean discrepancy and kernel Stein discrepancy tests, without sacrificing too much power in simulations. I certainly believe this paper is worthy of inclusion in NIPS.

Although this is not often a salient concern in machine learning, in more traditional statistical applications of hypothesis testing, randomized tests engender quite a bit of nervousness; we don't want a rejection decision in a costly clinical trial or program evaluation to hinge on having gotten a lucky RNG seed. For permutation tests or bootstrapped test statistics, we know that the rejection decision is stable as long as enough permutations or bootstrap iterations are performed. For the authors' proposed test, on the other hand, randomization is involved in choosing the test locations v_1 through v_J (or in choosing the initial locations for gradient ascent), and the simulations use a very small value of J (namely, J=5). Furthermore, in order to estimate parameters of the null distribution of the test statistic, a train-test split is needed. Which raises the question: are the test statistic and rejection threshold stable (for fixed data, across different randomizations)? It would be helpful to have additional simulations regarding the stability of the rejection decision for a fixed set of data, and guidelines as to how many train-test splits are required / how large J needs to be to achieve stability.

A minor comment: line 70, switch the order of ""if and only if"" and ""for all f""."
Rotting Bandits,"Nir Levine, Koby Crammer, Shie Mannor",https://proceedings.neurips.cc/paper/2017/hash/97d98119037c5b8a9663cb21fb8ebf47-Abstract.html,"
	This paper studies a kind of non-stationary stochastic bandits in which the expected reward of each arm decays as a function of the number of choosing it. They consider three cases: the reward function is (1) non-parametric, (2) parametric and decaying to 0, and (3) parametric and decaying to a positive constant that depends on each arm.
	For each case, they propose an algorithm and analyze its regret upper bound. They also conducted simulations which show superiority of their proposed algorithms compared with existing algorithms.

	Nicely written paper.
	The assumption of the rested rotting bandits seems natural in the applications of advertising and recommendation. The value of the proposed methods would increase by demonstrating effectiveness for real datasets
	using rotting models that are suited to the applications.
      ","

      The paper introduces a novel variant of the multi-armed problem, motivated by practice.
      
      The approach is technically sound and well grounded. There are a few typos and minor concerns (listed 
      below), despite which I believe this paper can be accepted.      
      
      Decaying bandits - e.g., Komiyama and Qin 2014, Heidari, Kearns and Roth 2016 - are mentioned, but Deteriorating bandits - e.g., Gittins 1979, Mandelbaum 1987, Kaspi and Mandelbaum 1998, etc. - are not
      
      Can't you use algorithms from non-stationary MABs, which you say are ""most related to our problem"", for benchmarking? It's quite unfair to use only UCB variants.
      
      Line 15: ""expanse"" -> ""expense""
      Line 19: you have skipped a number of key formulations and break-throughs between 1933 and 1985
      Line 31-32: probably a word is missing in the sentence
      Line 63: ""not including"" what?
      Line 68: ""equal objective"" -> ""equivalent objective""
      Line 68: this is an unusual definition of regret, in fact this expression is usually called ""suboptimality"", as you only compare to the optimal policy which is based on past allocations and observations. Regret is usually defined as distance from the objective achievable if one knew the unknown parameters. 
      Line 105: in the non-vanishing case, do you allow \mu_i^c = 0 or negative?      
      Line 106-118: some accompanying text describing the intuition behind these expressions would be helpful
      Line 119,144: headings AV and ANV would be more reasonable than names of the heuristics      
      Line 160: ""w.h.p"" -> ""w.h.p.""
      Table 1: ""CTO"" is ambiguous, please use the full algorithm acronyms  
      Figure 1: it would be desirable to explain the apparent non-monotonicity
      Line 256: It would be helpful for the reader if this sentence appeared in the Introduction. Btw, the term ""rested bandits"" is not very common; instead ""classic bandits"" is a much more popular term
      
      
      ","This paper studies a variation of the classical multi-armed bandits problem, where the expectation of the rewards for each arm decays with respect to the number of pulls. The authors present natural settings motivating this variation, such as displaying ads to a user. The first authors cover the non-parametric case, and describe an algorithm using sliding window average only (no UCB). They prove a regret in O(T^2/3 log T). The author then cover the parametric case, where the expected rewards are parametrized by a known finite set of models. When the rewards decay to zero, they give a ""plug-in"" algorithm that obtain o(1) regret under certain assumptions. When the rewards decay to positive constant, they give a ""UCB"" algorithm obtaining O(log T) regret under certain assumptions. The author finally show results on empirical experiments.

The core article is well written and the concepts are easy to follow. Since this setting is novel the article does not often use high level known results in their proofs, which make the theoretical analysis fairly technical.
An important limitation is the necessary assumptions in the parametric case (Assumptions 4.2 and 4.3). First, these assumptions are hard to verify even for simple parametric models. (In the example analysed by the author, one has to wait for n~10^800 to observe bal(n) < n.) Second, the authors do not comment the generality of these assumptions. It seems that they are necessary for the current theoretical approach only, and modifying the proof would requires different assumptions."
Scalable Planning with Tensorflow for Hybrid Nonlinear Domains,"Ga Wu, Buser Say, Scott Sanner",https://proceedings.neurips.cc/paper/2017/hash/98b17f068d5d9b7668e19fb8ae470841-Abstract.html,"This paper documents an approach to planning in domains with hybrid state and action spaces, using efficient stochastic gradient descent methods, in particular, in this case, as implemented in TensorFlow.

Certainly, the idea of optimizing plans or trajectories using gradient methods is not new (lots of literature on shooting methods, using fmincon, etc. exists).  And, people have long understood that random restarts for gradient methods in non-linear problems are a way to mitigate problems with local optimal.  What this paper brings  is (1) the idea of running those random restarts in parallel and (b) using an existing very efficient implementation of SGD.

I'm somewhat torn, because it seems like a good idea, but also not very surprising.  So, I'm not exactly sure how novel it is.

My major concern about the paper, though, is how it deals with discrete dimensions of the state and action and, more generally, a lack of differentiability in the transition or reward model.   All of the example domains seem to have dynamics that are continuous (though not differentiable).  It seems reasonably clear that a method such as this would not work well on a completely discrete domain.  It would be important, if this paper gets published to spend more time exploring exactly how much non-continuity affects the ability of the method to solve problems.

The next biggest concern has to do with reward:   although the paper discusses ways of mitigating the vanishing gradient problem, all of the domains presented here have a reward function that is non-zero at every stage and that almost surely provides a powerful gradient for the optimization algorithm.  It would be important to discuss the effectiveness of this methods on domains in which there is only a terminal reward, for example.

I found the comparison to RNNs to be somewhat distracting.  Either I misunderstood, or the optimization algorithm is applied over a finite fixed horizon 

I""m sort of surprised by the statement that transition dynamics are typically non-stationary;  I would have thought that in most planning problems they are, in fact, stationary.  But I guess it doesn't matter too much.  I was also somewhat confused by the assertion that the dynamics would typically have an additive-update form, where the additive component depends on the state and action:  that doesn't actually constitute any form of restriction, does it?    The comment about linearity in the update and vanishing gradients also does not make sense to me if delta is allowed to depend on s.

Ultimately, I would say that the planning instances you do address are impressively large and the performance of the algorithm seems good.  But it would be better to state more clearly what types of planning domains your method  could really be expected to perform well on.

----------------------

After rebuttal:  I appreciate the clarifications and think the paper would be stronger if you were somewhat clearer up front about exactly what subset of hybrid planning problems your method is appropriate for.  For instance, the idea that such systems are ""usually"" non-stationary, or additive isn't really right;  you can say that previous methods have focused on those, and that they constitute an important subclass, etc., etc. (which I believe), but ""usually"" just sounds wrong to me.   Similarly, a clear discussion about what assumptions you're making about the reward function and what would happen if they were violated would also be important.","In this paper the authors proposed a method that is based on symbolic gradient optimization tools such as Tensorflow to perform effective planning with hybrid nonlinear domains and high dimensional states and actions. They argued that the hybrid planning tool based on Tensorflow and RMSprop is competitive with MILP based optimization, and it is highly scalable. In general, I found the idea of solving hybrid planning problem with symbolic gradient optimization tools interesting. However, while this paper focuses on empirical studies using the RNN reformulation of the long term planning problem, I found the contribution of this paper insufficient to meet the standard of a NIPS paper. 

Pros:
1) The idea of formulating a planning problem by reversing the training parameters of the network given fixed inputs to optimizing the inputs (i.e., actions) subject to fixed parameters (effectively the transition and reward parameterization assumed a priori known in planning) is very neat. In this case, the proposed method embedded the reward and state transition of each stage of the planning problem into a RNN cell, and the cumulative reward of the fixed horizon problem is reconstructed using the MSE objective function of the Tensorflow loss function.

2) Through imposing the specific form of the transition function in page 4, the authors argued that the vanishing gradient issue in the cascade RNN structure is avoided.

3) To handle constrained actions, the author proposed using projected stochastic gradient descent method to optimize for the actions, and to handle discrete actions, the author proposed using a one-hot encoding method to the inputs. These are reasonable and common strategies to handle constraints and discrete inputs.

4) Since computation in symbolic optimization packages such as Tensorflow is highly scalable with the recent advance in GPU computations, the proposed method is effective compared to MILP based methods. 

Cons:
1) Since the method aimed to optimize the inputs (which are the point-based actions), it is not solving a closed-loop optimal control problem where the solution is a “policy” (which is a functional mapping from state-action history to actions). I can see that this Tensorflow reformulation solves the open-loop control problem, which is a very restricted form of planning.

2) While the experiment section contains three domains, the transition probability and the reward functions are rather specific. Especially the transition of the second and the third experiments follow the linear assumption that avoids the vanishing gradient issue in the cascade RNN structure. This leave me unconvinced that the RNN reformulation trick work for a general open-loop planning problem.

3) While the RNN reformulation of the planning problem is parallelizable, explicit performance analysis (in terms of speed and sub-optimality trade-offs) and its comparisons with MILP based method are not provided.  

Conclusion:
While I very much appreciate the extended numerical experiments performed by the authors. These experiments are too simplified and specific. In my opinion it is insufficient to convince readers that the proposed method outperforms MILP based solution algorithms in a general setting. Furthermore, numerical complexities and performance bounds in terms of sub-optimality (or asymptotic optimality) are omitted. Unless a more detailed analysis on the algorithms is given, I unfortunately do not think this paper has sufficient materials to pass the acceptance threshold.
","This paper describes a gradient ascent approach to action selection for planning domains with both continuous and discrete actions. The paper shows that backpropagation can be used to optimize the actions while avoiding long horizon problems such as exploding or vanishing gradients. Experimental results are given for 2D planar navigation, reservoir control and HVAC control and the proposed approach is shown to outperform existing hybrid optimization techniques.

Planning in hybrid domains is an important problem and existing methods do not scale to real world domains. This paper proposes a scalable solution to these problems and demonstrates its usefulness on several domains including two real world domains. As such, I believe it makes an important contribution to addressing real world planning problems. The paper is well written except for some minor comments (see below).

Evidence that the proposed solution works comes in the form of several experiments that demonstrate scalability and solution quality.
I have two concerns about the experiments that I would like to see the author's comments on:

1. The paper lacks any statistical significance results, error bars, or other statistics of variation.

2. Are the experimental baselines the state-of-the-art of hybrid domain planning problems? The introduction mentions other methods but justification is not given for not making an empirical comparison.

This paper needs a distinct related work section. The current treatment of related work is limited and is just in the introduction.

My last major question is whether or not Tensorflow is a critical part of the proposed method? It seems that what is important is the use of backpropagation and Tensorflow is just an implementation detail. I think the impact of the work might be more if the method was presented more generally. Or is there something I'm missing that makes Tensorflow essential?

Overall, the work presents a useful method for planning in hybrid domain problems. Its main limitations are the handling of related literature and statistical significance. Clarifying these issues and addressing the minor comments given below would help improve the impact of the work.

I have read the author's response and other reviews.

Minor comments:

Line 35: I.e., -> i.e.,

Line 35: Should cite Watkins for Q-learning

Line 106: Do you mean optimizing V directly?

Line 117: What do you mean by transfer function? Did you mean activation function?

Line 134: This statement seems too strong? Why are there always constraints on the actions? Is it more appropriate to say, ""planning problems frequently have constraints on the actions that can be taken""?

Line 164 - 167: The top line (d_t = ||s_t - s||) is too far over.

Line 170: It is not clear what the dynamics are for linear and bilinear. It would be useful to write them explicitly as done for the nonlinear dynamics.

Figure 3: This figure would be more clear if the y-axis was in the correct direction. It would also be useful to have a statement such as ""lower is better"" so the reader can quickly compare TF to the other baselines.

Figure 3: Since the MILP solution is optimal it would be more useful to see relative error values for TF and Heuristic. Absolute reward isn't as informative since its sensitive to reward scaling.

All figures: It would be nice to see error bars on all applicable plots


Line 235: ""to for"" -> for

Figure 6: space before (b)

All figures: check for black and white printing, especially figure 6

Line 262: RMSProb -> RMSProp"
Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models,"Chris Oates, Steven Niederer, Angela Lee, François-Xavier Briol, Mark Girolami",https://proceedings.neurips.cc/paper/2017/hash/98dce83da57b0395e163467c9dae521b-Abstract.html,"This paper develops DPMBQ, that casts the computation of integrals p(f) as an estimation problem using a nonparametric Bayesian method, namely Dirichlet process mixtures (DPM). The authors also establish theoretical properties of the proposed method. Considering the numerical approximation of integral, DPMBQ is able to accurately estimate it when both the functional of the output from a computer model and the distribution of the inputs x of the model are unknown. Overall, the paper is clearly written and delivered. There are some concerns below. 

1. Novelty. This paper develops DPMBQ by considering a DPM prior on the inputs x while assuming the Gaussian process prior on p(f). This setup allows an closed form on mu when choosing the conjugate prior. The idea is similar to the recent JASA paper ""Bayesian Nonparametric Longitudinal Data Analysis"" by Quintana, Johnson,  Waetjen, and Gold, who proposes a Gaussian process with DPM prior on the parameters in GP. Therefore, the key idea in this paper is not new to Bayesian nonparametric community. However, I am not an expert in computer model. Can the authors explain more on the novelty of the proposed DPMBQ? 

2. The combination of GP and DPM obviously make the computational burden very heavy, when n is large. Some discussion on how to incorporate some sparse GP methods into DMPBQ for computational efficiency will be useful. 

3. About the theoretical analysis. The results are straightforward to derive from the consistency results of the GP due to the nice property of the DPM. About condition (4), is it easy to generalize to unbounded sigma? 

4. The good results in the simulation study and real data analysis are no surprise since GP and DPM are two powerful nonparametric models that essentially approximates any continuous distributions. Therefore, of course DPMBQ will outperform the t-distribution assumption. It will be interesting to see the comparison between DPMBQ and some other methods that are not too naive. ","The paper is well written and motivated. The problem is that of computing integrals involving functionals of outputs from expensive models where large number of samples cannot be readily obtained. The assessment of numerical error is therefore of great importance in some potential applications. The authors in particular address an extension of Bayesian Quadrature to address a restriction in the classic formulation in which a closed form expression is available for the distribution over inputs. p(dx) The authors extend the framework to the case where this distribution can only be sampled from. They model the unknown distribution using a Dirichlet Process Mixture and the formulation seems convincing although it must be accepted that correlated x's must be ignored in this setup. The overall methodology is sound. 

As neither probabilistic numerics or inference with expensive computational models are my areas of expertise, I cannot assess the positioning of this work relative to other related works in the area. However, as a general reader, the simulated numerical experiments and application to cardiac modelling were interesting and in line with the justification of purpose given in the Introduction. It would be a paper/poster that would interest myself at NIPS. However, I could equally understand that the problem is too domain-specific.


","Summary

The paper presents a method for assessing the uncertainty in the evaluation of an expectation over the output of a complex simulation model given uncertainty in the model parameters. Such simulation models take a long time to solve, given a set of parameters, so the task of averaging over the outputs of the simulation given uncertainty in the parameters is challenging. One cannot simply run the model so many times that error in the estimate of the integral is controlled. The authors approach the problem as an inference task. Given samples from the parameter posterior one must infer the posterior over the integral of interest. The parameter posterior can be modelled as a DP mixture and my matching the base measure of the DP mixture with the kernel used in the Gaussian process based Bayesian Quadrature (BQ) procedure for solving the integral. The method is benchmarked using synthetic and a real-world cardiac model against simply using the mean and standard error of simulations from the model for parameter samples. It is shown to be more conservative - the simple standard error approach is over-confident in the case of small sample size.

Comments: 

This is an interesting topic and extends recent developments in probabilistic numerics to the problem of estimating averages over simulations from a model with uncertain parameters. I'm not aware of other principled approaches to solving this problem and I think it is of interest to people at NIPS. 

The paper is well written and easy to follow. A lot of technical detail is in the supplementary so really this is a pretty substantial paper if one considers it all together. Putting technical details in the supplement improves the readability. 

There is a proof of consistency of the approach in a restricted setting which gives some rigorous foundations to the approach. 

The matching of the mixture model components and kernel into conjugate pairs (e.g. Gaussian mixture with squared exponential kernel) seems like a neat trick to make things tractable. 

It is presented as an advantage that the method is conservative, e.g. the broad posterior over the quantity of interest is more likely to contain the ground truth. However, I was not sure what was the reason for the conservative behaviour and how general that behaviour is - is it generally the case that the method will be conservative or could there be cases where the procedure also underestimates the uncertainties of interest? Is this simply an empirical observation based on a restricted set of examples? I guess this is difficult to address theoretically as it is not an asymptotic issue. 

"
Bandits Dueling on Partially Ordered Sets,"Julien Audiffren, Liva Ralaivola",https://proceedings.neurips.cc/paper/2017/hash/995665640dc319973d3173a74a03860c-Abstract.html,"
        This paper deals with dueling bandits on a partially ordered set in the framework of the best arm identification. The goal of the dealing problem is identification of the set of maximal elements by doing as small number of stochastic pair comparisons as possible.　They propose a high-probability epsilon-approximate algorithm and a high-probability exact algorithm, and analyze their sample complexities. They also conduct numerical simulations to demonstrate effectiveness of their algorithms.
	
	The readability of this paper should be improved.
	For example, some subtitles such as `Contributions' and `Problem statement' are not appropriate for the descriptions following the subtitles. Some notion names such as a social poset and a chain in it are inappropriate because a social poset may not be a poset and a chain in it can contain a circle. Algorithm 1,2,3,4 should be pseudo-codes but do not look like pseudo-codes. Considering the relations among Algorithm 1,2 and 3, Algorithm 2 should be first (Algorithm 1) and Algorithm 1 should be the last (Algorithm 3).

	I cannot understand why what they call peeling is necessary.
	Why is it a problem to run UBSRoutine only once with epsilon_N?
	The authors should explain what merits are there using so called peeling.

	I think UNchainedBandits can fail to output the Pareto front
	in the case that a social poset contains an isolated cycle.
	In that case, UBSRoutine always outputs a set that includes at least one of cycle elements.

By reading the author’s feedback, now I understood the effect of peeling.

As for social posets with cycles, I also understood but I am still feeling something strange.
Their problem setting looks nice for regular posets, but not appropriate for social posets.
For example, consider  a cycle. Then the Pareto front is an empty set, but its epsilon-approximation
can be a (2/K)-sized set for any small epsilon. In fact, UnchainedBandits may output (2/K-1)-sized set.
In this case, a lot of subsets are epsilon-approximations of the Pareto front, and I don’t feel happy
even if one of them can be obtained. ","The paper addresses a variant of the dueling bandit problem where certain comparisons are not resolvable by the algorithm. In other words, whereas dueling bandit algorithms deal with tournaments (i.e. fully connected directed graphs), the algorithms in this paper deal with more general directed graphs.

The authors offer only vague conceptual reasons for the significance of this new problem setting, however from a practical point of view, this is an extremely important problem that the existing literature does not address at all. More specifically, ties between arms are major stumbling blocks for most dueling bandit algorithms; indeed, many papers in the literature explicitly exclude problems with ties from their analyses. On the other hand, in practice one often has to deal with ""virtual ties"": these are pairs of arms that could not be distinguished from each other given the number of samples available to the algorithm.

A good example to keep in mind is a product manager at Bing or Google who has to pick the best ranker from a pool of a hundred proposed rankers using pairwise interleaved comparisons, but only has access to say %1 of the traffic for a week: if it happens to be that there are certain pairs of rankers in the pool that are so similar to each other that they couldn't be distinguished from each other even using the entirety of the available samples, then for all practical purposes the are ties in the resulting dueling bandit problem. So, our product manager would want an algorithm that would not waste the whole traffic on such virtual ties.

The authors propose two algorithms that deal with this problem under two different levels of generality:
1- an algorithm to find an \eps-approximation of the pareto front in the more general social poset setting, which is an extension of the Condorcet assumption in the full tournament case;
2- as algorithm to find the exact pareto front in the much more restrictive poset setting, which is an analogue of the total ordering assumption for regular dueling bandits.

I personally find the former much more interesting and important because there is plenty of experimental evidence that transitivity is very unlikely to hold in practice, although the second algorithm could lead to very interesting future work.

The authors use a peeling method that uses epochs with exponentially increasing lengths to gradually eliminate arms with smaller and smaller gaps. In the analysis, this has the effect of rendering the regret incurred from most edges in the comparison graph negligible, hence the linear dependence on the gaps in regret bounds. The price that one pays for this simpler proof technique is an extra factor of K, which in the worst case makes the bound of the form O(K^2 log T / \Delta).

As a general comment, I think it would be a good idea for the authors to devote more space in the paper to situating the results presented here with respect to the existing dueling bandit literature: I think that would be much more helpful to the reader than the lengthy discussion on decoys, for instance. In particular, there is a very precise lower bound proven for Condorcet dueling bandits in (Komiyama et al, COLT 2015), and I think it's important for the authors to specify the discrepancy between the result in Theorem 1 and this lower bound in the full tournament setting when \eps is smaller than the smallest gap in the preference matrix.

Furthermore, given the numerous open questions that arise out of this work, I highly recommend having a lengthier discussion of future work. For instance, there is some recent work on using Thompson Sampling for dueling bandits in (Wu and Liu, NIPS 2016) and it is an interesting question whether or not their methods could be extended to this setting. Moreover, it would be interesting to find out if more refined results for the von Neumann problem, such as those presented in (Balsubramani et al, COLT 2016) can be transported here.

Another criticism that I have of the write-up is that in the experiments it's not at all clear how IF2 and RUCB were modified, so please elaborate on the specifics of that.","The paper studies an interesting variant of dueling bandit problem, on posets, where pairs-of-arms may or may not be comparable to each other.  For this setting, a natural problem is to recover the set of optimal arms, ones which are not dominated by any other arm. That is it is needed to recover the best element from all the chains. The authors introduce a very natural algorithm which is reminiscent of successive elimination. Regret guarantees and total number of comparisons are provided. 
The second part of the paper deals with introduction of decoys to  handle the problem of recovery of the entire pareto frontier. Decoys are essentially bad elements of the set that are dominated by every element in the chain.  The algorithms introduced are pretty intuitive and the formulations and problem statement novel. I have just one main complaint.

It is not clear to me for what algorithm are Theorem 3 and Theorem 4 applicable. Are they applicable for  UBS routine instantiated with Decoy comparison?  I found it confusing because the paragraph in lines 240-245 seems to hint at a hybrid approach where one runs Unchained bandits with direct comparison upto a certain time before running with decoy comparison thereafter. It would be great if the authors can clarify this."
Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search,"Mohammad Ali Bashiri, Xinhua Zhang",https://proceedings.neurips.cc/paper/2017/hash/99adff456950dd9629a5260c4de21858-Abstract.html,"The main result of the paper is an analysis of the away-step Frank-Wolfe showing that with line search, and over a polyhedral domain, the method has linear convergence. This is somewhat restrictive, but more general than several recent related works, and while they analyze some delicate fixed step-size methods, their main point seems to be that they can analyze the line search case which is important since no one uses the fixed step-size methods in practice (since they depend on some unknown parameters).

Some parts of the paper are written well, and it appears the authors are highly competent. A good amount of proofs are in the main paper, and the appendix supports the paper without being overly long.

However, for someone like myself who is aware of Frank-Wolfe but not familiar with the pairwise or away-step FW and their analysis, nor with the simplex-like polytopes (SLP), the paper is quite hard to follow. Section 3.1 and 3.2 discuss a metric to quantify the difficulty of working over a given polytope, and this is central to the paper and would benefit from a more relaxed description. Also, much of the paper is concerned with the SLP case so that they can compare with previous methods, and show that they are comparable, even if this case (with its special stepsize) is not of much practical interest, so this adds to the complexity. And overall, it is simply a highly technical paper, and (like many NIPS papers) feels rushed in places -- for example, equation (4) by itself is not any kind of valid definition of H_s, since we need something like defining H_s as the maximum possible number such as (4) is true for all points x_t, all optima x^*, and all possible FW and away-directions.

The numerical experiments are not convincing from a practitioners point-of-view, but for accompanying a theory paper, I think they are reasonable, since they are designed to illustrate key points in the analysis (such Fig 2 showing linear convergence) rather than demonstrate this is a state-of-the-art method in practice.

After reading the paper, and not being a FW expert, I come away somewhat confused about what we need to guarantee linear convergence for FW. We assume strong convexity and smoothness (Lipschitz continuity of the gradient), and a polyhedral constraint set. At this point, other papers need the optimal solution (unique, via strong convexity) to be away from the boundary or similar, but this paper doesn't seem to need that. Can you clarify this?

Overall, I'm impressed, and mildly enthusiastic, but I think some of the parts I didn't understand well are my fault and some could be improved by a better-expressed paper, and overall I'm somewhat reserving judgement for now.

Minor comments:

- Line 26, ""Besides,"" is not often used this way in written material.
- Line 36, lots of commas and the ""e.g."" make this sentence hard to follow
- Line 39, ""empirical competency"" is not a usual phrase
- Line 63, ""SMO"" is not defined, and I had to look at the titles of refs [19,20] to figure it out
- Eq (1), it's not clear if you require the polytope to be bounded (some authors use the terminology ""polytope"" to always mean bounded; maybe this is not so in the literature you cite, but it's unclear to a general reader). Basic FW requires a compact set, but since your objective is assumed to be strongly convex, it is also coercive, so has bounded level sets, so the FW step is still well-defined, and I know FW has been analyzed in this case. So it does not seem to be necessary that the polytope is bounded, and I was very unsure if it was or not (had I been able to follow every step of the proofs, this would probably have answered itself).
- Line 80, ""potytope"" is a typo
- Eq (4) is not a definition. This part is sloppy
- Eq (5) is vague. What does ""second_max"" mean? The only definition that makes sense is that if we define I to be the set of all indices that achieve the max, then the second max is the max over all remaining indices, though this could be an empty set, so then the value is not defined. Another definition that doesn't seem to work in your case is that we exclude only one optimal index. It was hard to follow the sentence after (5), especially given the vagueness of (5).
- Line 145 defines e_1, but this should be earlier in line 126.
- Section 3.2 would benefit from more description on what these examples are trying to show. Line 148 says ""n which is almost identical to n-1"", which by itself is meaningless. What are you trying to express here? I think you are trying to show the lower bound is n-1 and the upper bound is n?
- Line 160, is there a downside to having a slack variable?
- Line 297, ""not theoretical"" is a typo.
- Line 311, ""Besides,"" is not used this way.

== Update after reading authors' rebuttal ==
Thanks for the updates. I'm still overall positive about the paper.","The authors consider a frank wolfe method with away steps, and derive linear convergence bounds for arbitrary polytopes, for smooth and strongly convex loss functions. They introduce a novel notion of the geometrical complexity of the constraint set, and show that this complexity measure can be computed relatively easily (or bounded). They also derive statistical rates for kernel SVM, and show improved performance when compared to the SMO method. 

Detailed comments below:
- In general, is there some relationship  between H_s and Pw? FOr example, in Table 1 last row, it is hard to tell what is better since there is no way of comparing 1/Pw with n H_s

- line 122: what is ext(P) ?

- Section 3.2 essentially implies that the bounds are tight up to constant factors. Please mention this clearly. 

- Theorem 1 and Lemma 2: I don't get the ""must be feasible"" part. Don't Frank Wolfe methods always ensure that the iterates are feasible (via the convex combination of the new atom and old iterate)?

- I think the paper will benefit from providing wall clock time for the experiments as well. Ideally for large scale applications, the time the methods take to reach an optimum is more important. 

- On the same note as above, could you also compare to other methods that solve the SVM dual (libSVM, divide and conquer methods)? Again, these schemes have shown to be very scalable. Or you need to make clear why SMO is a valid choice to compare against. ","This paper presents extensions of existing variants of Frank-Wolfe or conditional gradient methods; in particular, it extends the performance (convergence rate)  of the Pairwise-FW variant of Garber and Meshi 2016 to handle more general polytope constraints, by using an away-step update. For the analysis of this algorithm variant, a new condition number of the feasible set is introduced that better captures the sparsity of the solution.  

Introducing a different notion of condition number Hs is described as one of the contributions. Can the authors present some more intuition and geometric interpretations, and more examples (other than simplex and its convex hull) where Hs is easy to bound nontrivially and reveals important conditioning information? 

Other comments: 
1. Mention in the introduction what AFW stands for
2. Can you give intuition for constants in eps (10) and (11)?
3. It’ll be helpful to have a list of parameters that appear throughout (e.g., \alpha, \beta, D, H_s, …) mentioning if they are problem parameters or algorithm parameters, and what they depend on.

It is helpful to clarify the significance of the results in this paper to decide whether they meet the high bar at NIPS. As is, I would place the paper as a (weak) accept."
Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces,"Daniel Milstein, Jason Pacheco, Leigh Hochberg, John D. Simeral, Beata Jarosiewicz, Erik Sudderth",https://proceedings.neurips.cc/paper/2017/hash/99c5e07b4d5de9d18c350cdf64c5aa3d-Abstract.html,"This paper presents a dynamic Bayesian network to improve the error made by iBCI decoders. This method allows the system to use longer signals history and reduce errors.

It seems that the state of the art is the use of Kalman filters to decode the neural signals to find the angle of motion. If that is the case, the authors show improvement upon this method by using a better tuning curve and a modified Kalman Filter.

It would be interesting to know the minimum recording time necessary to initialize the tuning curve and the decoder.

The authors state: ""While the Kalman filter state solely encodes the vector of desired cursor movement, we expand
 our decoder’s state to also include the two-dimensional final location (or goal position) on the screen to which the participant wants to move the cursor"" Does this mean that the goal needs to be known beforehand? Is this a supervised approach? In that case, this limitation should be clearly pointed out.




 ","The paper describes a novel brain-computer-interface algorithm for controlling movement of a cursor to random locations on a screen using neuronal activity (power in the ""spike-spectrum"" of intra-cortically implanted selected electrodes).

The algorithm uses a dynamic Bayesian network model that encodes possible target location (from a set of possible positions on a 40x40 grid, layed out on the screed). Target changes can only occur once a countdown timer reaches zero (time intervals are drawn at random) at which time the target has a chance of switching location. This forces slow target location dynamics.
Observations (power in spike spectrum) are assumed to be drawn from a multi modal distribution (mixture of von Mises functions) as multiple neurons may affect the power recording on a single electrode and are dependent on the current movement direction. The position is simply the integration over time of the movement direction variable (with a bit of decay).

The novel method is compared with several variations of a Kalman filter (using same movement state space but missing the latent target location) and an HMM. Experiments are performed off line (using data from closed loop BCI session using either a Kalman filter or an HMM) and also in 2 online closed-loop sessions (alternating between Kalman filter and the new method). Experiments performed an a tetraplegic human subject.

Overall the paper (sans supplementary material with is essential to understanding the experiment) is well written. The new model vastly outperforms the standard Kalman Filter, as is clear from the multiple figures and supplementary movies (!).

Some comments:

- I gather that the von Mieses distribution was used both for modelling the observations given the intended movement direction and for modelling the next goal position. I found the mathematical description of the selection of the goal position unclear. Is there a distribution over possible goal positions or is a single goal position selected at each time point during inference? Can you clarify?

- Once a timer is set, can the goal be changed before the timer expires? My understanding is that it cannot, no matter what the observations show. I find this to be a strange constraint. There should be more straightforward ways of inducing slow switch rates that would not force the estimate of the target position to stay fixed as new evidence accumulates. e.g. forced transition rates or state cascades. 

- My understanding is that the model has no momentum. I.e., the cursor can ""turn on a dime"". most objects we control can not (e.g hand movements). Would it not make more sense to use a model that preserves some of the velocity and controls some of the accelleration? It is known that some neurons in motor cortex are direction sensitive and other are force sensitive. Would it not be more natural?

- After an initial model learning period, the model is fixed for the rest of the session. It would be interesting to see how the level of performance changes as time passes. Is the level of performance maintained? Is there some degredation as the network / recordings drift? Is there some improvement as the subject masters control of the system? Would an adaptive system perform better? These questions are not addressed. 

- The paper is geared at an engineering solution to the BCI problem. There is no attempt at providing insights into what the brain encodes most naturally or how the subject (and neural system) adapt / learn to make the most out of the provided BCI. 

- citations are in the wrong format should be numerical [1] , [1-4] etc... not Author et al (2015).

 ","Summary:
The authors proposed a new method for cursor control in the intracortical brain-computer interfaces. To improve the performance of standard Kaman filter based decoder, they proposed a dynamic Bayesian network that includes the goal position as a latent variable, and also proposed a flexible likelihood model for neural decoding. With the proposed likelihood model and Bayesian network, they used semi-Markov model to analyze the dynamics. With a latent counter variable for both the goal position and cursor angle, their method is capable to integrate information over a longer history. They used junction tree algorithm to obtain the inference. Simulations for both offline and online experiments are used to show that their method can predict motion directions with neural activity, and has improved performance compared with Kaman filter baselines. 

Qualitative assessment:
The paper shows that the proposed method leads to slightly improved performance compared with traditional Kaman filter decoder in the brain-computer interface system. According to the offline experiment result in Figure 5, the flexible tuning likelihood and the semi-Markov method both make a contribution to improving the result. The topic is not novel, but the work is a reasonable advance on previous work. The paper is well organized and generally clearly written.

Comments:
1) In the dynamic Bayesian network proposed by the author, the goal position is incorporated as a latent state. The motivation is unclear. It seems that the system needs to infer the not only the intent of the movement, but also the final goal of the subject that uses the brain-computer interfaces, which is radically harder problem. This should be discussed more thoroughly.
2) In the offline experiment, the advantage of the proposed method can be observed compared to kaman filter decoder with various configuration. However, in the online experiment, the improvement is not obvious to me. In the left subfigure of figure 7, I can hardly conclude which gives better performance.
3) Check for typos in the derivation of the inference algorithm in the supplementary materials."
Fast Black-box Variational Inference through Stochastic Trust-Region Optimization,"Jeffrey Regier, Michael I. Jordan, Jon McAuliffe",https://proceedings.neurips.cc/paper/2017/hash/9a1756fd0c741126d7bbd4b692ccbd91-Abstract.html,"Summary of the paper:

This paper describes the use of a technique known as stochastic trust-region optimization in the context of variational inference (VI). In VI an objective needs to be maximized with respect to the parameters of an approximate distribution. This optimization task enforces that the approximate distribution q looks similar to the exact posterior. In complex probabilistic graphical models it is not possible to evaluate in closed form the objective. An alternative is to work with an stochastic estimate obtained by Monte Carlo. This has popularized the technique Black-box VI, in which one obtains the gradients by automatic differentiation. A limitation is however that one typically uses first order stochastic optimization methods with black-box VI. In this paper the authors propose an alternative that is able to consider second order information, and hence, converges in a smaller number of iterations. The proposed method has been implemented in STAN (a probabilistic programming framework), and compared with standard black-box VI. The results obtained show that the proposed method leads to a faster convergence.

Detailed comments:

Clarity:

The paper is very clearly written. The related work analysis is strong and important methods are also introduced in the paper. The paper also a convergence proof and additional proofs are fond in the supplementary material. 

Quality: 

I think that the quality of the paper is high. It addresses an interesting problem that is relevant for the community working on approximate inference and variational inference. The section on experiments is also strong and describes 182 statistical models on which the proposed approach has been compared with black-box automatic differentiation. The paper only shows results for 5 runs of each method on 5 dataset and 5 models. I would suggest to include additional results in the supplementary material.

Originality:

I think the proposed work is original. However, as the authors point out, there are other works that have considered second order information for the optimization process in VI. These include Ref. [7].

Significance:

The results obtained look significant. In particular, the proposed approach is able to converge much faster than ADVI in the figures shown. A problem is, however, that the authors do not compare results with related techniques. For example, the method described in Ref [7]. This questions a bit the significance of the results.

","SUMMARY OF THE PAPER:

The paper transfers concepts known in the optimization community and applies them to variational inference. It introduces a new method to optimize the stochastic objective function of black box variational inference. In contrast to the standard SGD, which optimizes the objective using only first order derivatives, the proposed methods takes estimates of the Hessian into account and approximates the objective function on a small trust region around the current iterate by a quadratic function. The algorithm automatically adapts the size of the trust region based on estimates of the quality of the quadratic model. The paper proves theoretically that the algorithm converges, and shows experimentally that convergence is typically faster than standard SGD.


GENERAL IMPRESSION / POSITIVE ASPECTS:

The paper is well written and the proposed algorithm is potentially highly relevant given its generality and the reported improvements in speed of convergence. The experimental part summarizes results from a very large number of experiments, albeit it is not clear to me whether this includes any experiments on large-scale datasets that require minibatch sampling.


WHAT COULD BE IMPROVED:

1. I appreciate the fact that the paper includes an unambiguous definition of the algorithm and a rigorous proof that the proposed algorithm converges. To make the method more accessible, it would however be helpful to also briefly explain the algorithm in more intuitive terms. For example, a geometric interpretation of \gamma, \lambda, \eta, and \delta_0 would help practitioners to choose good values for these hyperparameters.

2. An often cited advantage of VI is that it scales to very large models when minibatch sampling (stochastic VI) is used. Minibatch sampling introduces an additional source of stochasticity. The paper discusses only stochasticity due to the black box estimation of the expectation under q(z). How does the proposed algorithm scale to large models? Are the convergence guarantees still valid when minibatch sampling is used and do we still expect similar speedups?

3. It is not clear to me how $\sigma_k$, defined in Condition 4, can be obtained in practice.


MINOR REMARKS:

1. Algorithm 1: R^d should be R^D in the first line.

2. Lines 108-114 discuss a set of hyperparameters, whose motivation becomes clear to the reader only after reading the convergence proofs. It might be clearer to present the hyperparameters in a table with a row for each hyperparameter, and columns for the symbol, a short and descriptive name, and the interval of allowed values.

3. Eq. 7: which norm is used for the Hessian matrix here?

4. Line 154: It may be helpful to point out at this point that one draws *new* samples from p_0, i.e., one may not reuse samples that were used to generate g_k and H_k. Otherwise, $\ell'_{ki}$ are not i.i.d. The algorithm box states this clearly, but it was not immediately clear to me from the main text."
Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network,Lixin Fan,https://proceedings.neurips.cc/paper/2017/hash/9a3d458322d70046f63dfd8b0153ece4-Abstract.html,"The authors use a notion of generalized hamming distance, to shed light on the success of Batch normalization and ReLU units. 

After reading the paper, I am still very confused about its contribution. The authors claim that generalized hamming distance offers a better view of batch normalization and relus, and explain that in two paragraphs in pages 4,5. The explanation for batch normalization is essentially contained in the following phrase:

“It turns out BN is indeed attempting to compensate for deficiencies in neuron outputs with respect to GHD. This surprising observation indeed adheres to our conjecture that an optimized neuron should faithfully measure the GHD between inputs and weights.”

I do not understand how this is explaining the effects or performance of batch normalization.

The authors then propose a generalized hamming network, and suggest that ""it demystified and confirmed effectiveness of practical
techniques such as batch normalization and ReLU"".

Overall, this is a poorly written paper, with no major technical contribution, or novelty, and does not seem to provide any theoretical  insights on the effectiveness of BN or ReLUs. Going beyond the unclear novelty and technical contribution, the paper is riddled with typos, grammar and syntax mistakes (below is a list from just the abstract and intro). 

This is a clear rejection.

Typos and grammar/syntax mistakes:

—— abstract —— 
generalized hamming network (GNN)
-> generalized hamming network (GHN)

 GHN not only lends itself to rigiour analysis
->  GHN not only lends itself to rigorous analysis

“but also demonstrates superior performances”
-> but also demonstrates superior performance
—— 

—— intro —— 
“computational neutral networks”
-> computational neural networks

“has given birth”
-> have given birth

“to rectifying misunderstanding of neural computing”
-> not sure what the authors are trying to say


Once the appropriate rectification is applied ,
-> Once the appropriate rectification is applied,

the ill effects of internal covariate shift is automatically eradicated
-> the ill effects of internal covariate shift are automatically eradicated

The resulted learning process
-> The resulting learning process

lends itself to rigiour analysis
-> lends itself to rigorous analysis

the flexaible knowledge
-> the flexible knowledge

are equivalent and convertible with other
->  are equivalent and convertible with others, or other architectures?

successful applications of FNN
-> successful applications of FNNs



","This is a beautiful paper that interprets batch normalization and relu in terms of generalized hamming distance network and develops variations that improves. This connection is surprising (esp fig 2) and very intereting.

The correspondence between ghd and bn is interesting. However, it is also a bit non-obvious why this is true. It seems to me that the paper claims that in practice the estimated bias is equal to sum_w and sum_x in (3) and the whole wx+b equates the ghd. However, is it on avearge across all nodes in one layer? Also how does it vary across layers? Does the ghd mainly serve to ensure that there is no information loss going from x to w.? It is a little hard to imagine why we want layers of ghd stacked together. Any explanation in this direction could be helpful.

A minor thing: typo in the crucial box in line 80

Overall a great paper.

----------------------
After author rebuttal:

Many thanks for the rebuttal from the authors. My scores remain the same. Thanks for the beautiful paper!","This paper explores generalized hamming distance in the context of fuzzy neural networks. The paper shows that the neural computation at each neuron can be viewed as calculating the generalized hamming distance between the corresponding weight vector and input vector. This view point is useful as it automatically gives candidate bias parameter at the neuron and learning of additional parameters (using batch normalization)  is not required during the training. Furthermore, this view also allows the authors to understand the role of rectified linear units and propose a double-thresholding scheme to make the training process fast and stable. Based on these two observations, the paper proposes generalized hamming networks (GHN) where each neuron exactly computes the generalized hamming distance. These networks also utilize non-linear activations. 

The paper then presents simulation results showcasing the performance of the proposed GHN for image classification and auto-encoder design. The simulation results demonstrate that GHN have fast learning process and achieve very good performances on the underlying tasks. Interestingly, the simulation results show that the GHN do not necessarily require max-pooling. 

The paper is well written and explains the main ideas in a clear manner. The simulation results are also convincing. However, it appears that the plots are not complete, e.g., Fig. 4.2 does not show a curve for learning rate = 0.0001. Even though I would have preferred a more comprehensive (both theoretical and practical) evaluations, I over all find the idea of GHN interesting.

Minor comments:

1) In abstract, '(GNN)' should be '(GHN)'.
2) In line 55 of page 2,  'felxaible' should be 'flexible'."
Scalable Demand-Aware Recommendation,"Jinfeng Yi, Cho-Jui Hsieh, Kush R. Varshney, Lijun Zhang, Yao Li",https://proceedings.neurips.cc/paper/2017/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html,"This paper proposed an scalable and accurate demand-aware recommendation system which considers the form utility and time utility, i.e., recommending related item at the correct time to the user. The paper has written very well and technically sounds correct. The experimental section is complete as it compares to several approaches and applied it to a large scale problem. The optimization is explained very well and utilized the structure of the matrix X to boost the performance of the algorithm (linear in terms of number of non zero). ","Paper revolves over the observation that in e-commerce world customers rarely purchase two items that belong to the same category (e.g. Smartphone) in close temporal succession. Therefore, they claim that a robust recommendation system should incorporate both utility and time utility. An additional problem that is tackled in the paper is that many e-commerce systems have no explicit negative feedback to learn from (for example one can see only what items customer purchased - positives - and no explicit negatives in form of items user did not like).

I believe that the second problem they mention is not as big of a concern as advertised by authors. In absence of any explicit negative signal good replacements are long dwell-time clicks that did not end up in purchase, as well as cart additions that did not end up in the final purchase or returns. Many companies are also implementing swipe to dismiss that is useful for collecting explicit negative signal and can be applied to any e-commerce site easily.

The authors clearly define and describe the objective of modeling user intent as combined effect of form utility and time utility and identify limitations of minimizing the objective when formulated in the simplest form (as tensor nuclear norm) in terms of high computational cost. To tackle that the authors propose to relax the problem to a matrix optimization problem with a label-dependent loss and propose an efficient alternating minimization algorithm whose complexity is proportional to the number of purchase records in the matrix.

I see the biggest strength of this paper as reformulating the objective and adding relaxations that significantly reduce computational cost and still achieves good result.

Introducing time component adds another dimension and as you mentioned it results in a significant rise in computational cost O(mnl) where l is the number of time slots. What happens when you drop the time component in terms of accuracy? I saw that in your experiment you compare to many traditional collaborative filtering algorithms but you do not compare to your algorithm that doesn't take time in account or uses different definitions of what time slot is (e.g. just a s a two level definition it could be: purchased this month or not).

Related work is generally covered well. Experiments only cover offline evaluation. Therefore, we have no sense of how this algorithm would work in practice in a production system. For example I would be curious to compare against just showing popular items as recommendations (as done in this paper: E-commerce in your inbox: Product recommendations at scale, KDD 2015). Paper would be  much stronger with such results. Also, additional evaluation metrics should be considered, like traditional ranking metrics, precision at K, etc.

It would be also interesting to put this in context of ad retargeting. It is well known that users on the Web are re-targeted for the same product they bought (or same category) even after they bought the product. While some see this as a waste of ad dollars some claim this still brings in revenue as users often want to see items from same category in order to get reassurance that you bought the best product from the category. 

In practice, it also happens that they return the original product and purchase the recommended one from same category. In practice, recommender production systems take this signal in account as well.

","This is a strong paper, clearly written, well structured, and technically sound with elaborate mathematical derivations and sufficient supporting evidence gathered.

On a conceptual level the mathematics isn’t ground-breaking, but what it might lack in elegance it makes up for in efficiency and performance of the algorithm. That is to say, it’s an interesting and solid piece of work from an engineering perspective.

Some more emphasis might be placed on the weaknesses. For instance, in the two sentences on line 273-277, there's a rather brief remark on why the inter-purchase durations aren't perfectly modeled by the learned 'd' parameters. I think this is a missed opportunity at best, if not a slight omission. In particular, I believe that one of the interesting aspects of this work would that we can explicitly inspect the 'd' parameters (if they were meaningful). According to the author, the only thing to learn from them is which item category is durable/non-durable.

The main aspect that I believe is innovative is the performance of the algorithm on large datasets (as the title suggests). As I just mentioned, a by-product of this approach is it explicitly models the inter-purchase duration for different item categories, which may be useful by itself in a commercial setting.

The approach taken in this paper is quite pragmatic (as opposed to conceptual). This means that it's not overwhelmingly exciting (or reusable) on a conceptual level. Nevertheless, it does seem to push the envelop in terms of scalability of modeling (time-aware) repeat consumption. In terms of clarity, this paper is written in a way that makes implementation relatively painless. Moreover, a link to the author's implementation is provided in one of the 

Suggestions
In section 2.1, eq. (1), the predictions y_ijk depend on a threshold \tau. In the optimization problem, however, we set out to solve the non-thresholded version of the predictions. This is not necessarily wrong, but I believe it warrants some explanation.

In section 3.1, I would suggest to make a choice between either removing lines 163-169 or (better) include more details of the computation. I got lost there and needed the appendix in order to proceed. In other words, either defer those details to the appendix or explain more fully.

In section 3.2, I believe it would be nice to add a few words on why the proposed proximal X updates will converge to the original optimization problem. Of course, there is a citation [10], but it would be nice to be a little bit more self-contained. One or two sentences will do.

Minor typos
line 166: reposition some curly brackets in your latex for the 's' subscripts.
appendix line 51: replace U by Q

Some remarks
Have you considered model time directly as one leg in a type-3 tensor similar to your x_ijk (or at the level of category x_ick) and thereby removing the ""d - t"" term? Here, the index k could be given not as time, but as time-since-last-purchase (perhaps binned and indexed). This way, you also model ""d - t"" for instance x_{ic:} would give you a something that might be interpreted as time-utility defined over inter-purchase durations, given a user i and item category c.
"
Learning a Multi-View Stereo Machine,"Abhishek Kar, Christian Häne, Jitendra Malik",https://proceedings.neurips.cc/paper/2017/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html,"This submission proposes to learn 3D object reconstruction from a multi-view stereo setting end-to-end. Its main contribution is consistent feature projection and unprojection along viewing rays given camera poses and multiple images. Instead of mainly learning object shapes based on semantic image interpretation, this approach builds projective geometry into the approach to benefit from geometric cues, too. The paper proposes a way for differentiable unprojection that projects 2D image features to 3D space using camera projection equations and bilinear sampling. This is a simple yet effective way to implicitly build epipolar constraints into the system, because features from multiple views of the same world point unproject to the same voxel. While such combination of multiple views is of course not new, it is new as part of an end-to-end learnable approach.

I did not completely understand how recurrent grid fusion works, please explain in more detail. A dedicated figure might ease understanding, too. I would also appreciate a short note on GPU RAM requirements and run-time.   

The paper is very well written and illustrations make it enjoyable to read. The approach is experimentally evaluated by comparison to traditional multi-view stereo matching algorithms (e.g., plane sweeping) and it is compared to the recent 3D-R2N2 approach, that also uses deep learning for 3D object reconstruction. 

While all experiments are convincing and support claims of the paper, it would be good to compare to more deep learning-based 3D reconstruction approaches (several are cited in the paper, e.g. [27] + additional work like ""DeMoN: Depth and Motion Network for Learning Monocular Stereo"" from Thomas Brox's lab). Clear explanation of theoretical differences to [27] would further improve the paper. At present, it remains unclear whether this submission is just a multi-view extension of [27] without reading [27] itself. Better motivating the own contribution here seems necessary. 

I believe this is a great paper and deserves to be published at NIPS, but also must admit that I still have some questions.
","The paper presents an architecture for multi-view stereopsis. An image encoder (UNet) first  processes the images to produce 2D feature maps which are unprojected into 3D world occupancy grids. These grids are fused sequentially using a RNN. Finally the resulting fused grid is processed with a 3D UNet architecture to produce the final grid. 
Feature projection and unprojection are formulated in a differentiable manner that allows training the architecture end-to-end. The proposed system can reconstruct voxel occupancy grids or depth maps of an object from one or several views. The approach is evaluated on the synthetic ShapeNet dataset.

Strengths:
- 3D reconstruction from multiple views is a fundamental problem in computer vision.
- The paper is clear and reads well.
- The methods outperforms previous work when the camera pose is known and the object is segmented.

Weaknesses:
- The applicability of the methods to real world problems is rather limited as strong assumptions are made about the availability of camera parameters (extrinsics and intrinsics are known) and object segmentation.
- The numerical evaluation is not fully convincing as the method is only evaluated on synthetic data. The comparison with [5] is not completely fair as [5] is designed for a more complex problem, i.e., no knowledge of the camera pose parameters.
- Some explanations are a little vague. For example, the last paragraph of Section 3 (lines 207-210) on the single image case.

Questions/comments:
- In the Recurrent Grid Fusion, have you tried ordering the views sequentially with respect to the camera viewing sphere?
- The main weakness to me is the numerical evaluation. I understand that the hypothesis of clean segmentation of the object and known camera pose limit the evaluation to purely synthetic settings. However, it would be interesting to see how the architecture performs when the camera pose is not perfect and/or when the segmentation is noisy.  Per category results could also be useful.
- Many typos (e.g., lines 14, 102, 161, 239 ), please run a spell-check.","The paper presents a novel way of how to train a multi-view stereo system. I think this ieda is valuable, and the paper is well presented and thouroughly written. Some minor remarks:
- it is not clear from the beginning if the cameras are calibrated or if the relative camera poses are known for the system. 
- In the evaluation (table 1), visual hull is taken as baseline. I think this is rather unfair, because visual hull only uses silhouettes as input and not full images.
- Better indeed was to compare with classical existing stereo mehtods, such as plane sweeping. Why is this method only qualitatively compared to the proposed method (fig. 6)? Why is no quantitively comparison made?
- The resulting point clouds still show a large amount of noise voxels flowing around the generated 3D model. Can't you use a noise removal technique, just to create a visually more attractive result?
"
On Blackbox Backpropagation and Jacobian Sensing,"Krzysztof M. Choromanski, Vikas Sindhwani",https://proceedings.neurips.cc/paper/2017/hash/9c8661befae6dbcd08304dbf4dcaf0db-Abstract.html,"
The paper focuses on automatic differentiation of multiple-variable vector-valued functioned in the context of training model parameters from input data. A standard approach in a number of popular environments rely on a propagation of the errors in the evaluation of the model parameters so far using backpropagation of the estimation error as computed of the (local) gradient of the loss function. In this paper, the emphasis is on settings where some of the operators of the model are exogenous blackboxes for which the gradient cannot be computed explicitly and one resorts to finite differencing of the function of interest. Such approach can be prohibitively expensive if the Jacobian does not have some special structure that can be exploited. 
 
The strategy pursued in this paper consists in exploiting the relationship between graph colouring and Jacobian estimation. In a nutshell, the mapping encodes the outputs that jointly influence the model. The consequence of this is a known result stating that the chromatic number of the associated graph provides a bound on the number of calls to a function to compute its Jacobian, in the noiseless setting. The paper goes one step beyond this established fact and proposes an algorithms “rainbow” that defines the perturbation directions where the blackbox needs to be evaluated in order to compute the associated Jacobian . The crux of the algorithm is to encode the properties of the Jacobian (e.g. sparsity, symmetry) using a graph colouring procedure, arising from the aforementioned graph mapping, into a simple linear programme.
 
The authors theoretically evalute the performance of this algorithm in section 3.4.  Theorem 3.3. upperbounds the number of blackbox needed to evaluate its Jacobian in terms of the graph encoding input output dependencies as well as its intrinsic degrees of freedom among other things. In Section 4, the proposed algorithm is evaluated in following settings: two synthetic input-output dependency structures (random and power-law), a CNN, and a planning and control setting.
 
The paper is clearly written and properly spells out the relevant technical details of the proposed algorithm as well as its performance. 
The main weakness of the paper resides in section 3.3, focusing on the LP approach to define the perturbation direction needed to evaluate the Jacobian. There are also two minor points that would improve the legibility of the paper. First, the paragraph after theorem 3.3 (page 7 l.257-l .260) should provide further intuition about the bounds on the calls to the function needed by the algorithm to estimate its Jacobian. Then, the experiment section 4 would benefit from providing a rationale behind the choice of the scenarios explored and what is illustrated about the algorithm in each of them.
 
 
 
 
 
","This is an interesting paper that discusses the problem of how to recover the Jacobian. By observing an interesting relationship between the number of needed measurements and the chromatic number of the so-called intersection graph, the paper found that the number of measurements can be significantly reduced if the chromatic number is small. Recovery guarantee is provided in the paper. As far as the reviewer can tell, the paper is technically correct. This paper has some novelty and is well written, thus is recommended to be accepted.

I. The illustrated example is controversial. For such kind of systems, usually the system designer knows (from the physical model) that the Jacobians of some subsystems are zeros or identities, or of some specific structures. Thus, those parts (sub-matrices) of Jacobian should be determined directly and will not enter the measurement equations. In this case, for the current example, the remaining part of the Jacobian would not be sparse any more and would not seem to have any special structure. Can the authors show some real example in which the Jacobian is indeed sparse but the structures (special patterns) can not be physically determined?

II. Problem (2) is actually a feasibility problem and should be easily handled with all p\geq 1 (convexity).

III. More discussion on the bounds of k (chromatic number) is needed.

After rebuttal:

First of all, it seems the reviewer's questions are overlooked by the authors.

Furthermore, the reviewer thinks the authors have ``over claimed'' a bit in the response to another reviewer. It is questionable if the method could work under non-smooth step-up. It is even unclear if the algorithm is still numerically stable when the problem becomes stiff. Overall speaking, it is a good work but the method's scope of applicability needs more responsible discussion.","The authors propose a Jacobi estimation method that could be used as a tool in back-propagation of neural networks with black-box units. This is definitely an interesting problem with practical implications. The theoretical results presented in the paper seem to be technically right. But the novelty of the results, in the context of automatic differentiation, is not very clear. Unfortunately, the authors do not provide any information on the novelty or importance of their theoretical results compared to the rest of the results in the automatic differentiation literature. 
There are also some other unanswered questions that require clarification. First of all, the algorithm requires knowing some underlying relationship graph or a good approximation of it. The number of samples that are required heavily depends on such graph, It is not clear to me how such graph could be obtained in practice when dealing with practical black boxes. Moreover, the Jacobian is defined for smooth functions. And in many cases, the black boxes seem to be non-smooth mappings. For example, the ReLU unit, that is used in experiments, is non-smooth. Therefore, the Jacobian is not defined for it. Although, there are some other concepts like approximate Jacobians that could be defined for non-smooth mappings. But the authors do not provide any information on if their algorithm is still theoretically/practically valid when applied to non-smooth mappings and what does it recover. In general, another question that should be addressed about the algorithm is how to select the size of perturbations in practice. Although having smaller perturbations is better in terms of approximating the Jacobian, it might result in very noisy or information-less observations if the size of perturbations is too small. For example, the mechanical system might not be even sensitive to very small perturbations. Moreover, the size of the perturbation can depend on the curvature of the mapping as well.
"
Learning Disentangled Representations with Semi-Supervised Deep Generative Models,"Siddharth N, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, Philip Torr",https://proceedings.neurips.cc/paper/2017/hash/9cb9ed4f35cf7c2f295cc2bc6f732a84-Abstract.html,"This work extends [17], the original semi-supervised VAE proposed by Kingma et al. Instead of consider the label y’ as a partially observed variable, they also disentangle y from z, which is part of the latent variables. This yields a loss function that is different from the original paper. Since this variable is discrete, the author uses Gumbel-Softmax to optimize it.

To my understanding this loss is more interesting since every unlabled sample also contributes to the learning of p(y|x) instead of only the supervised ones.

On the other side, the experiments are very week
* MNIST: The original results of Kingma are better and there are lot’s of even stronger results in the litterature

* SVHN: There is a single point of comparison where the new approach seems to be significantly better than [17]

* Intrinsic Faces: A single point of comparison with the original paper [11] without mentioning any other work on this dataset.

* Multi-MNIST: The results are interesting but can’t be compared to other approaches since the dataset is specific to this work.

Overall, the idea is good, but author ignore any other work that has been done on semi-supervised VAE in the 3 years following [17]. And the experimental results are rather week and seems to be indicating that the new approach does not perform better the the original one.

","The authors develop a framework allowing VAE type computation on a broad class of probablistic model structures. This is motivated in particular by the idea that some lvs may have a straightforward meaning and have some labels available (e.g. which digit in MNIST/SVHN, what lighting direction in the face data), whereas others are more intangible (e.g. writing style in MNIST). They propose a slightly different approach to the semi-supervised VAE of Kingma et al., by considering the (semi)supervised variables y as LVs forced to specific values for the supervised data samples. This is straightforward in the setting where q(y|x) can be calculated directly, and can be handled by importance sampling if integration over z is required to calculate q(y|x). Experiments are presented on MNIST, SVHN and a faces image data with variation in lighting according 38 individuals. On all three dataset the authors show successful qualitative learning of the semisupervised LVs. Quantitatively the performance on MNIST doesn't quite match that of Kingma et al. While I'm not too worried about this it would be interesting to know the authors' guess as to the cause - a difference in architecture? the objective itself? On SVHN however they do somewhat better. On the faces dataset it's not clear that the baseline is more than a strawman, but presumably other methods haven't been evaluated here? 

The paper is very clearly written for the most part. There are a few places where more explanation/detail would have been beneficial (the page limit makes this challenging), in particular
- I was unclear about the faces dataset - is the lighting effectively an angle in [0,360]? What is ""n"" in the Equation used by the original publication? Possibly a surface tangent? 
- The description of the RNN used for counting is very terse. 

Minor comments
- Saliman & Knowles 2012 demonstrated representing q as a hierarchy of LVs so should probably be cited. 
- ""Composing graphical models with neural networks for structured representations and fast inference"" appears twice in the citations. 
- I don't know if I like ""latents"" instead of ""lvs"" but maybe I could get used to it. ","This paper investigates the use of models mixing ideas from 'classical' graphical models (directed graphical models with nodes with known meaning, and with known dependency structures) and deep generative models (learned inference network, learned factors implemented as neural networks). A particular focus is the use of semi-supervised data and structure to induce 'disentangling' of features.  They test on a variety of datasets and show that disentangled features are interpretable and can be used for downstream tasks.

In contrast with recent work on disentangling (""On the Emergence of Invariance and Disentangling in Deep Representations"", ""Early visual concept learning with unsupervised deep learning"", and others), here the disentangling is not really an emergent property of the model's inductive bias, and more related to the (semi)-supervision provided on semantic variables of the model.

My opinion on the paper is overall positive - the derivations are correct, the model and ideas contribute to the intersection of graphical models / probabilistic modeling and deep learning, and well-made/detailed experiments support the authors' claims.

Negative:
- I don't think the intersection of ideas in structured graphical models and deep generative model is particularly novel at this point (see many of the paper's own references). In particular I am not convinced by the claims of increased generality. 
Clearly there is a variety of deep generative models that fall in the PGM-DL spectrum described by the authors and performing inference with flexible set of observed variables mostly involves using different inference networks (with potential parameter reuse for efficiency). Regarding continuous domain latents: I don't see what would prevent the semi-supervised model from Kingma et al from handling continous latent variable: wouldn't simply involve changing q(y|x)  to a continuous distribution and reparametrize (just as they do for the never-supervised z)? Generally, methodologically the papers appears somewhat incremental (including the automated construction of the stochastic computation graph, which is similar to what something like Edward (Tran et al.) does).

- Some numerical experiments results are slightly disappointing (the semi-supervised MNIST numbers seem worse than Kingma et al.; the multi-MNIST reconstructions are somewhat blurry; what do samples look like?)

Positive:
- The paper is well written and tells a compelling story. 
- The auxiliary bound introduced in (4)-(5) is novel and and allows the posterior in the semi-supervised case to force learning over the classification network (in corresponding equation (6) of Kingma et al. the classification network does not appear). Do the authors know how the variance between (4) and the analogous expression from Kingma et al compare?
- The rest of the  experiments (figure 2, 5) paint a clear picture of why having semantics inform the structure of the generative model can lead to interesting way to probe the behavior of our generative models."
GP CaKe: Effective brain connectivity with causal kernels,"Luca Ambrogioni, Max Hinne, Marcel Van Gerven, Eric Maris",https://proceedings.neurips.cc/paper/2017/hash/9cf81d8026a9018052c429cc4e56739b-Abstract.html,"This paper addresses the problem of understanding brain connectivity (i.e. how activity in one region from the brain drives activity in other regions) from brain activity observations. More generally, perhaps, the paper attempts to uncover causal structure and uses neuroscience insights to specifically apply the model to brain connectivity. The model can be seen as an extension of (linear) dynamic causal models (DCMs) and assumes that the observations are a linear combination of latent activities, which have a GP prior, plus Gaussian noise (Eq 11).

Overall the paper is readable but more clarity in the details of how the posterior over the influence from i->j is actually computed (paragraph just below Eq 12).

I write this review as a machine learning researcher (i.e. non-expert in neuroscience) so I am not entirely sure about the significance of the contribution in terms of neuroscience. For the machine learning perspective, the paper does not really provide a significant contribution as it applies standard GP regression. Indeed, it seems that the claimed novelty arises from the specific kernel used in order to incorporate localization, causality, and smoothness. However, none of these components seem actually new as they use, respectively, (i) the squared exponential (SE) covariance function, (iii) a previously published kernel, and (iii) some additional smoothing also previously known. With this regards, it is unclear why this additionally smoothing is necessary as the SE kernel is infinitely differentiable. 

The method is evaluated on a toy problem and on a real dataset. There are a couple of major deficiencies in the approach from the machine learning (ML) perspective. Firstly, the mixing functions are assumed to be known. This is very surprising as these functions will obviously have an important effect on the predictions. Second, for the only real dataset used, most (if not all) the hyperparameters are fixed, which also goes against standard ML practice. These hyperparameters are very important, as shown in the paper for the toy dataset, and ultimately affect the connectivities discovered.

There also seems to be a significant amount of pre-processing to actually fit the problem to the method, for example, as explained in section 5, running ICA. It is unclear how generalizable such an approach would be in practice.
","This nicely written paper presents a Bayesian nonparametric model to infer effective connectivity from neuroimaging data represented as integro-differential equations. The main contribution of the paper is the introduction of causal kernels for Gaussian processes that (at least approximately) satisfy certain properties so that they can be used to claim effective (and not simply functional) connectivity. The model specification is conjugate so that the posterior over interaction kernels is given in closed form. The method is demonstrated on both synthetic data and on real MEG data.

Overall, I think this is a very good paper. However, there are a few parts of the paper where more discussion or more thorough descriptions could help make it more accessible and useful to a broader audience.

- First, a more thorough explanation as to why adding the imaginary Hilbert transform to Eq. (8) enforces causality would be welcome. 

- The authors mention time-shift invariance to justify Eq. (5), but it's not clear from what's written where this comes from.

- Why is the infinite differentiability of the functions a reasonable assumption? Why not OU or Matern or something that has some amount of smoothness?

- In the experiments, was the smoothness parameter learned? The inferred kernels look pretty rough, would a fully Bayesian treatment where the smoothness hyperparmeter is learned help with this overfitting?

- How were the hyperparameters chosen for the MEG experiment?

- The authors point out that a limitation of the model is that linear differential operators need to be used. What breaks down if we break this assumption? Is linearity just needed for analytic tractability? Or is there a statistical reason?

- How does the method scale with the number of signals?
","This paper proposed a statistical model for brain connectivity analysis, based on stochastic differential equations and Gaussian processes. The assumptions and constraints are well described and justified, and the model is carefully designed based on conditions. Experimental results are provided on synthetic and real data, which justified the modeling power and interpretability of the model.

1. Is it true that the convolution causal kernel a sufficient characterization for connectivity? The triggering signal from i to j can simply be a spiking event/signal, while the intensity or accumulated signal/amplitude is irrelevant. 

2. As much of the model formulation and analysis is done in frequency domain, it is helpful and more straightforward for the readers if some experiment results can also be provided in frequency domain.

3. Have you tried the model on signals including multiple frequency bands, like alpha and gamma bands? As different sources often appear in different frequency bands, it would be useful to model the connectivity involving those.

4. How are the hyper parameters are chosen in real-world data? For example, the temporal smoothing, localization and noise levels.

5. In a quite relevant work [1], different brain states are modeled using GPs with spectral kernels, like resting states and different sleeping states. Spatial-temporal correlation among brain regions and states are analyzed, which should be helpful to the authors.

[1] K. Ulrich, D. E. Carlson, W. Lian, J. S. Borg, K. Dzirasa, and L. Carin. Analysis of brain states from multi-region LFP time-series. NIPS, 2014."
Certified Defenses for Data Poisoning Attacks,"Jacob Steinhardt, Pang Wei W. Koh, Percy S. Liang",https://proceedings.neurips.cc/paper/2017/hash/9d7311ba459f9e45ed746755a32dcd11-Abstract.html,"The paper studies an interesting topic and is overall nice to read. However, I have doubts about the correctness of the main theorem (Proposition 1), which makes two statements: one that bounds the max-min loss M from below and above, and a second that further bounds the upper bound. While the second claim is proven in the appendix; the first is not. The upper bound follows from the convexity of U and Equation 5. (It might be helpful for the reader to explicitly state this finding and argue why D_p reduces to a set of \epsilon n identical points.) However, I can't see why the lower bound holds. Computing M requires the solution of a saddle point problem. If one knew one part of this saddle point solution, one could easily bound M from above and/or below. However, Algorithm 1 only generates interim solutions that eventually converge to the saddle point. Concretely, in (6), D_p is not (necessarily) part of the saddle point solution; and \hat theta is neither the minimizer of L w.r.t. D_c \cup D_p nor part of the saddle point solution. So why is this necessarily a lower bound?

It's hard to judge the quality of candidate set D_p. One could argue that it would be better to actually run the algorithm till convergence and take the final data pair (x, y) \epsilon n times. Alternatively, one could take the last \epsilon n data points before the algorithm converges. Why the first \epsilon n update steps give a better candidate set is unclear to me. Whether the resulting model after \epsilon n update steps is any close to an optimum is unclear to me, too. Overall, there is little theoretical justification for the generation of the candidate attack in Alg. 1; it is for sure not the solution of (4). Hence, it's hard to conclude something like ""even under a simple defense, the XYZ datasets are resilient to attack"". And since the attack is model-based - it is constructed with a certain model (in this case, a linear model), loss function, and regularization scheme in mind - it is also hard to conclude ""the ABC dataset can be driven from 12% to 23% test error""; this might be true for the concretely chosen model, but a different model might be more robust to this kind of poising attack.

To me the value of the approach is that it allows to study the sensitivity of a concrete classifier (e.g., SVM) w.r.t. a rather broad set of attacks. In case of such sensitivity (for a concrete dataset), one could think about choosing a different defense model and/or classifier. However, in the absence of such sensitivity one should NOT conclude the robustness of the model.

In Section 3.2, the defenses are defined in terms of class means rather than centroids (as in (2)).

Eq. (8) requires F(\pi), though, F is defined over a set of data points. I don't fully understand this maximization term and I would like the authors to provide more details how to generalize Alg. 1 to this setting.

In the experimentation section, some details are missing to reproduce the results; for example, the algorithm has parameters \roh and \eta, the defense strategies have thresholds. It is neither clear to me how those parameters are chosen, nor which parameters were used in the experiments.

After authors response:

Regarding the correctness of correctness of Prop. 1: \hat {theta} is the minimizer of L w.r.t. D_c \cup D_p; but D_c \cup D_p is not simultaneously a maximizer of L. M states the value of L at a saddle point, but neither \hat{theta} nor D_c \cup D_p is necessarily part of a saddle point. However, it became clear to me that this is not required. We can replace
max_D min_\theta L(D, \theta)
by
max_D L(D, \hat
{\theta}_D)

where \hat{theta}_D is the minimizer w.r.t. D. Of course, this quantity is always smaller than M := L(\hat{D}, \hat{\theta}_\hat{D}) for any maximizer \hat{D} and saddle point (\hat{D}, \hat{\theta}_\hat{D}). I got confused from the two meanings of D_p, which is used as a free variable in the maximization in (4) and as the result of Alg. 1.","Title: 1988-Certified defenses for data poisoning attacks

Summary: The paper proposes a method of finding approximate upper bound and constructing candidate attacks for poisoning attacks on simple defenders that solves a convex loss minimization problems. 

Strengths: The paper proposes original and rigorous approach toward an important problem. Experiments were performed carefully.

Weaknesses: Not many, but there are several approximations made in the derivation. Also the strong assumptions of convexity and norm-based defense were made.


Qualitative evaluation

Quality:
The paper is technically sound for the most part, except for severals approximation steps that perhaps needs more precise descriptions. Experiments seem to support the claims well. Literature review is broad and relevant.

Clarity:
The paper is written and organized very well. Perhaps the sections on data-dependent defenses are a bit complex to follow.

Originality:
While the components such as online learning by regret minimization and minimax duality may be well-known, the paper uses them in the context of poisoning attack to find a (approximate) solution/certificate for norm-based defense, which is original as far as I know.

Significance:
The problem addressed in the paper has been gaining importance, and the paper presents a clean analysis and  experiments on computing an approximate upperbound on the risk, assuming a convex loss and/or a norm-based defense. 


Detailed comments

I didn't find too many things to complain. To nitpick, there are two concerns about the paper. 
1. The authors introduce several approximations (i -iii) which leaves loose ends. It is understandable that approximations are necessary to derive clean results, but the possible vulnerability (e.g., due to the assumption of attacks being in the feasible set only in lines 107-110) needs to be expanded to reassure the readers that it is not a real concern. 
2. Secondly, what relevance does the framework have with problems of non-convex losses and/or non-norm type defenses? Would the non-vanishing duality gap and the difficulty of maximization over non-norm type constraints make the algorithm irrelevant? Or would it still give some intuitions on the risk upperbound?

p.3, binary classification: If the true mean is known through an oracle, can one use the covariance or any other statistics to design a better defense?


","Given some assumptions on the learning procedure, this manuscript bounds the effectiveness of data poisoning attacks on the learner's loss and introduces an attack that nearly achieves that bound in two settings: a data-independent and data-dependent defense. The problem considered is important, and while much of the community's efforts have been devoted to test time attacks (e.g., adversarial examples), somewhat less attention has been given to causative attacks. 

The methodology followed is appropriate given the security settings: bounding the power of an attacker given a realistic threat model is a first step towards preventing the often inevitable arms race between attackers and defenders.

The writing could be improved by stressing two takeaways that are somewhat hidden in Sections 4 and 5. The first one is the observation made at lines 214-217. It seems like an important take-away from the experimental results and would benefit from being emphasized in the introduction and discussion. Likewise for the fact that data-dependent defenses are much less effective at mitigating data poisoning attacks than data-independent defenses. Although it may appear evident in hindsight, it is a useful observation for future efforts to build upon.

Some of the weaknesses of the manuscript stem from the restrictive---but acceptable---assumptions made throughout the analysis in order to make it tractable. The most important one is that the analysis considers the impact of data poisoning on the training loss in lieu of the test loss. This simplification is clearly acknowledged in the writing at line 102 and defended in Appendix B. Another related assumption is made at line 121: the parameter space is assumed to be an l2-ball of radius rho. 

The paper is well written. Here are some minor comments:
- The appendices are well connected to the main body, this is very much appreciated.
- Figure 2 and 3 are hard to read on paper when printed in black-and-white.
- There is a typo on line 237.
- Although the related work is comprehensive, Section 6 could benefit from comparing the perspective taken in the present manuscript to the contributions of prior efforts.
- The use of the terminology ""certificate"" in some contexts (for instance at line 267) might be misinterpreted, due to its strong meaning in complexity theory."
Towards Generalization and Simplicity in Continuous Control,"Aravind Rajeswaran, Kendall Lowrey, Emanuel V. Todorov, Sham M. Kakade",https://proceedings.neurips.cc/paper/2017/hash/9ddb9dd5d8aee9a76bf217a2a3c54833-Abstract.html,"This paper presents a policy search with Natural Gradient algorithm using linear and RBF features. It obtains state of the art results (compared to TRPO with neural networks) on OpenAI benchmarks. These results highlight the need for simplicity and generalization in RL.

The algorithm can be seen as an actor-critic algorithm where the actor (policy) is updated by a natural gradient step and the critic (value function) is estimated by matching the n-step returns. Then to compute the advantage function they use a GAE procedure. The authors choose a linear and RBF representations of the states to learn the policy. Those representations can also be interpreted as neural networks. So, one way to look at this paper is that the authors come up with simpler neural networks in order to achieve state of the art results on those continuous-control tasks. Thus, it will be very interesting to compare the number of the learnable parameters between this algorithm and TRPO with NN. In addition, it would be nice to add the number of generated trajectories used with this implementation and TRPO to compare the data efficiency of the algorithm.

Another point that should be addressed is that there is no clean proof of convergence of Q Learning (at my knowledge) with linear features (except tabular case) towards the optimal action-value function. So I do not think that it is a good argument to use. 

Finally, I think that the paper raises an interesting point about simplicity of the architectures. However, they should provide the ratio of the number of learnable parameters between their implementation and TRPO and give the number of trajectories used to compare data-efficiency. Those should be good arguments to prefer simpler neural networks architectures.
","This paper takes an critical view on recent results for deep reinforcement learning. The paper uses a simple natural gradient algorithm (comparable to TRPO) to either learn a linear policy or a policy linear in RBF features generated by a random Fourier basis. Despite the simplicity of the policy architecture, comparable results could be achieved with these linear policies. Moreover, the paper analyses the generalisation ability of the policies by making the task harder (action limits, more difficult and varying initial states and perturbations). The linear architecture using RBF features could learn also these more challenging tasks.

This paper does not provide any theoretical or algorithmic contribution. However, it is still an interesting paper as it takes a critical view at the results presented for recent deep reinforcement learning algorithms. It was generally believed that deep architectures are needed to learn such tasks. Hence, the results are quite surprising and interesting. Linear architectures do have several
advantages in RL to non-linear techniques. However, linear architectures where thought to be not expressive enough to learn these complex tasks. This does not seem to be true. Even if there are no theoretical or algorithmic contributions in this paper, it gives us the opportunity to rethink the ""deep is always good story"" for reinforcement learning. If a simple architecture works equally well, it should be preferred.

In summary, the paper is interesting and food for thought, but does not contain theoretical or algorithmic contributions. I would rate it nevertheless as borderline, leaning towards accepting the paper.","The paper evaluates natural policy gradient algorithm with simple linear policies on a wide range of “challenging” problems from OpenAI MuJoco environment, and shows that these shallow policy networks can learn effective policies in most domains, sometimes faster than NN policies. It further explores learning robust and more global policies by modifying existing domains, e.g. adding many perturbations and diverse initial states.

The first part of the paper, while not proposing new approaches, offers interesting insights into the performance of linear policies, given plethora of prior work on applying NN policies as default on these problems. This part can be further strengthened by doing ablation study on the RL optimizer. Specifically, GAE, sigma vs alpha in Eq. 5, and small trajectory batch vs large trajectory batch (SGD vs batch opt). In particular, it is nice to confirm if there are subtle differences from prior NPG implementation with linear policies that made the results in this paper significantly better (e.g. in TRPO [Schulman et. al., 2015], one of the major differences was sigma vs alpha). 

It could be unsurprising that a global linear policy may perform very well in narrow state space. The second part of the paper tries to address such concern by modifying the environments such that the policies must learn globally robust policies to succeed. The paper then again showed that the RBF/linear policies could learn effective recover policies. It would be great to include corresponding learning curves for these in Section 4 beyond Figure 4, comparing NN, linear and RBF. Are there figures available? 

Additional comment:
- A nice follow-up experiment to show is if linear policies can solve control problems from vision, with some generic pre-trained convolutional filters (e.g. ImageNet). This would further show that existing domains are very simple and do not require deep end-to-end optimization. "
Imagination-Augmented Agents for Deep Reinforcement Learning,"Sébastien Racanière, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra",https://proceedings.neurips.cc/paper/2017/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html,"This paper presents an approach to model-based reinforcement learning where, instead of directly estimating the value of actions in a learned model, a neural network processes the model's predictions, combining with model-free features, to produce a policy and/or value function. The idea is that since the model is likely to be flawed, the network may be able to extract useful information from the model's predictions while ignoring unreliable information. The approach is studied in procedurally generated Sokoban puzzles and a synthetic Pac-Man-like environment and is shown to outperform purely model-free learning as well as MCTS on the learned model.

---Quality---

I feel the authors have done an exceptional job of presenting and evaluating their claims. The experiments are thorough and carefully designed to tease issues apart and to clearly answer well-stated questions about the approach. I found the experiments to provide convincing evidence that I2A is taking advantage of the learned model, is robust to model flaws, and can leverage the learned model for multiple tasks.

I had one question regarding the experiments comparing I2A and MCTS. I2A was trained for millions of steps to obtain a good policy. Was a comparable amount of data used to train the MCTS leaf evaluation function? I looked in the appendix, but didn't see that detail. Also, out of curiosity, if you have access to a perfect simulator, isn't it a bit funny to make I2A plan with the learned model? Why not skip the model learning part and train I2A to interpret rollouts from the simulator?

---Clarity---

I felt the paper was exceptionally well-written. The authors did a good job of motivating the problem and their approach and clearly explained the big ideas. As is often the case with deep learning papers, it would be *extremely* difficult to reproduce these results based purely on the paper itself -- there are always lots of details regarding architecture, meta-parameters, etc. However, the appendix does a good job of narrowing this gap. I hope the authors will consider publishing source code as well.

---Originality---

As far as I am aware, this is a highly novel approach to MBRL. I have never seen anything like it, and it is fundamentally, conceptually different than existing MBRL approaches.

The authors did a good job of citing relevant work (the references section is prodigious). Nevertheless, there has been some other recent work outside the context of deep nets studying the problem of planning with approximate, learned models that the authors may consider discussing (recognizing of course the limitations of space). For instance, Talvitie UAI 2014 and AAAI 2016, Jiang et al. AAMAS 2015, Venkatraman et al. ISER 2016. The presented approach is unquestionably distinct from these, but there may be connections or contrasts worth making. Also, the I2A architecture reminds me of the Dyna architecture, which similarly combines model-based and model-free updates into a single value function/policy (the difference being that it directly uses reward/transition predictions from the model). Again, I2A is definitely new, but the connection may be valuable to discuss.

---Significance---

I think this is an important paper. It is addressing an important problem in a creative and novel way, with demonstrably promising results. It seems highly likely that this paper will inspire follow-up work, since it effectively proposes a new category of MBRL algorithms that seems well-worth exploring.

***After author response***

I have read the authors' response and thank them for the clarifications.
","Paper summary:

This paper introduces I2A model which has both model-free and model-based components. Authors consider the setting where the exact transition model is not available and the domain is complex. The goal is to design a model that will be robust to the errors in transition function approximation. To achieve this, I2A has a rollout based on imperfect transition model and a rollout policy (which is learnt simultaneously) and encode the information from rollout and add it to the model-free information. Authors show experiments on sokobon and pacman worlds.

My comments:

I like the idea of using policy distillation to simultaneously learn the rollout policy. This looks like the best possible approximation of the current policy.

1. Can authors report the comparison between test performance of the I2A model and the rollout policy model? I see that rollout policy is a distilled version of the I2A model. If it performs almost close to I2A, it can be used as a cheaper substitute in time-constrained domains.

2. Authors use a pre-trained environment model and freeze it during I2A training. Even though they say that random initialization doesn’t perform well, I would like to see the performance of I2A with randomly-initialized environment model and also pretrained environmental model with fine-tuning during I2A training. These results would help us to get better understanding of the model.

3. I don’t understand the intuition behind the copy model. If the authors’ goal is to check if the number of parameters have effect in performance, they should simply increase the number of parameters in the model-free baseline and see if there is increase in performance. I think this should be one of the baselines.

4. Figure 4(right) shows the effect of increasing the rollout length. Authors claim that increasing the rollout length only increase the performance and then saturates. Can you also report what is the performance curve for rollout lengths 10 and 15? That would help us to confirm that LSTM encoding indeed helps in avoiding the multiplicative prediction errors.

5. Silver et al. [34] is an important baseline that is missing in the paper. While I understand that you use a separate transition model and they learn the transition end to end, the comparison would help us to gauge which direction is superior. I am not saying that we should not pursue research in inferior direction, but just that it would help us benchmark different approaches. It is perfectly fine if you don’t do better than Silver et al. [34]. 

6. Experiment setup for section 4.2 is not clearly written. Do you remove model free path of all the models? Do you remove them only during testing or also during training?

7. Line 175-178 is either not clear or contradicting.

8. In figure 5(right) poor model finally reaches better performance. I am surprised that there is no discussion in the text explaining this.

9. Message from section 4.3 is not clear. No-reward IAA is close to baseline performance. This means that reward prediction is importance to achieve I2A performance. However authors say that reward prediction is not needed. Can you please comment on this?

10. Captions missing in table 1 and 2.

11. Your appendix is corrupted after page 4. So I cannot read from page 5.

I would increase my scores if you agree to add the experiments suggested in points 2,3,4,5.
","This paper introduces a novel method of combining model-based and model-free RL. The core of the architecture is a dynamics model that, when rolled out, predicts future observations and rewards. These imagined rollouts are then processed into a concise representation that is fed as additional input into a policy which is trained using model-free methods. Experiment ablations show the effect of various design choices and experimental comparisons show sample efficiency gains in complex environments compared to baseline methods.

Overall I find the paper result very compelling and clearly presented. This work combines and builds upon several prior ideas (such as learning actionable imperfect models, auxiliary rewards, boosting model-free with model-based, model-based for transfer learning) into a method that works on relevant, complex tasks. The experiments provide insight into the method, which will help the research community build upon this work. I therefore vote for a strong accept.

Major feedback:
- Section 6: The related work is lacking. Here are some additional references you may consider, but more than what I list should be added:
Dyna architecture (https://pdfs.semanticscholar.org/b5f8/a0858fb82ce0e50b55446577a70e40137aaf.pdf)
Accelerating model-free with model-based (https://pdfs.semanticscholar.org/b5f8/a0858fb82ce0e50b55446577a70e40137aaf.pdf)
Goal-driven dynamics learning
(https://arxiv.org/pdf/1703.09260.pdf)
- Supplementary B.2: The environment models being different for Sokoban and Minipacman is concerning because it indicates the environment models may be overfit to the particular environment. Therefore it is unclear how much engineering it takes to get I2A to work on a new domain, thus calling into question its practicality. Using one architecture in multiple domains (e.g., Atari suite) would address this concern.

Minor feedback:
- Section 2: Fig. 1 and 2 are very good
- Section 3.1: The method for generating rollouts is rather unorthodox. Further investigation/insight would be helpful.
- Section 4.1: Says that unrolling for > 5 did not increase performance, but would still be useful to see how much performance decreases
- Section 4.2: Fig. 5 (right) is convincing
- Section 4.3: If reward prediction just speeds up learning and doesn’t affect final performance, please have the learning curves show this
- Section 4.4: Tables need captions
- Section 4.4: When doing MCTS with I2A, does performance always increase when allowed to plan for longer? The text seems to indicate this, but if so I’m a bit surprised since the more you plan, the more likely you are to incorrectly exploit your model and get a maximization bias (motivation from double Q-learning)
"
Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,"Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html,"The paper investigates the use of an ensemble of neural networks (each modelling the probability of the target given the input and trained with adversarial training) for quantifying predictive uncertainty. A series of experiments shows that this simple approach archives competitive results to the standard Bayesian models. 

Finding an efficient and reliable way for estimating predictive uncertainty in neural networks its still an open and very interring problem. While the proposed approach is based on well-known models and techniques (and thus is not new itself), to the best of my knowledge it has not been applied to the problem of estimating predictive uncertainty so far and could serve as a good benchmark in future. A drawback compared to the Bayesian models is that the approach comes without mathematical framework and guarantees. 

Specific comments and questions: 

- While other approaches, like MC-dropout can also be applied to regression problems with multidimensional targets, it is not clear to me, if a training criterion (like that described in section 2.2.1) suitable for multidimensional targets does also exists (learning a  multivariate Gaussian with dependent variables seems not state forward).

- ""the advantage of VAT is that it does not require knowledge of the true target y and hence can be applied to semi-supervised learning"". Note, that adversarial training can also be applied with respect to a different than the correct label (e.g. see https://arxiv.org/pdf/1611.01236.pdf) 

-""since a base learner trained on a bootstrap sample sees only 63% unique data points"". Where are the 63% based on? 

- ""For ease of computing quantiles and predictive probabilities, we further approximate the ensemble prediction as a Gaussian whose mean and variance are respectively the mean and variance of the mixture.""  Is there a mathematical explanation for why this can be done? 

- While the likelihood can be estimated directly in the proposed method, it can only be accessed via MCMC methods for MC-dropout. It it nevertheless fair to make the comparison in Table 1? And what do you indicate by the fat printed values? That they are significantly different from each other based on a statistical test?   

Minor comments:
- line 19: natural language processing [36] and bioinformatics [1, 52] -> natural language processing [36], and bioinformatics [1, 52].
- line 61: the aberivation BMA needs to be introduced here. 
- line 85: and well suited ->  and is well suited
- line 87: and the first to do so -> and are the first to do so
- line 136: the loss is noted differently than before (i.e. in line 108) 
- line 163: strength and correlation between... ->  strength of and correlation between
- line 170: additional closing parenthesis. 
- line 175: helps reduce their bias and ensembling helps reduce the variance -> helps reducing/ helps to reduce 
- line 194: The Brier score was already defined (differently!) in line 113.
- Figure 13 in Abbendix B3: the legend is missing. ","The paper proposes a simple non-Bayesian baseline method for estimating predictive uncertainty. This is achieved by using ensembles of NNs, where M distinct NNs, from different random initializations, are trained independently through optimizing a scoring rule. The authors simply used a conventional loss function (i.e. softmax cross entropy) as the score rule for classification task, while for regression problems the authors replaced a regular loss function (i.e. MSE) by negative log-likelihood (NLL), which should capture predictive uncertainty estimates. In addition to this, an adversarial training (AT) schema is proposed to promote smoothness over the predictive distribution. To demonstrate the capacity of ensemble and AT to estimate predictive uncertainty, the authors evaluated the proposed method on a large number of datasets across different tasks, both for regression and classification (vision) tasks. Their method outperforms MC dropout for classification tasks while being competitive on regression datasets in term of NLL. Furthermore evaluations demonstrate the ability of the proposed approach to provide uniform predictive distributions for out-of-distribution (unknown) samples, for an object classification task (ImageNet). 

The paper proposed the use of NNs ensemble, which trained independently on ordinary training samples and their corresponding adversary, in order to estimate the predictive uncertainty. Although the proposed method is not significantly novel, nor very sophisticated, the paper is contributing to the domain by expliciting an idea that have not been very much exploited so far. However, the results obtained from the experimentations are not able to support well the proposal.

Training NNs by adversaries is one of the contributions of the paper, but the relationship between smoothness and obtaining a well-calibrated predictive uncertainty estimates is a bit vague. How smoothness obtained using AT can help to achieve a well-calibrated predictive uncertainty estimates? In other words, the effectiveness of AT on the predictive uncertainty estimates is not clearly justified, and the experiments are not supporting this well. For examples, the authors showed AT has no considerable effect for regression problems (Table 2 in Appendix), while it has a significant effect for vision classification problems. Why? Moreover, if we consider AT as data augmentation, can we demonstrate the positive effects of AT on the predictive uncertainty over other simple data augmentation approach, such as random crop? I think that having another experiment with simple data augmentation for training NNs and ConvNets can highlight whether the smoothness has a definite effect on the predictive uncertainty or not, and whether AT is truly key to achieve this.

Recognizing erroneous samples, which are confidently misclassified by NNs, is also another concern for AI safety. The high confidence of misclassified samples prohibit a human intervention then some major deficiencies (accidents) can occur. So, the question can be asked that whether this ensemble can recognize the misclassified samples by providing uniform distributions? The experiments should support that.

Estimating the predictive uncertainty is a key for recognizing out-of-distribution and misclassified samples.  However, a few non-Bayesian papers are missed from the literature review of the paper. For example, Hendrycks and Gimpel (2016) propose a baseline method by using softmax statistics to estimate the probability of error and probability of out-of-distribution samples. Also, ensemble of DNNs have been evaluated by Abbasi and Gagné (2017) for computing the uncertainty of the predictions for adversaries cases, a hard case of out-of-distribution examples.

References: 
Hendrycks, Dan, and Kevin Gimpel (2016). ""A baseline for detecting misclassified and out-of-distribution examples in neural networks."" arXiv preprint arXiv:1610.02136.
Abbasi, Mahdieh, and Christian Gagné (2017). ""Robustness to Adversarial Examples through an Ensemble of Specialists."" arXiv preprint arXiv:1702.06856.


** Update following reviewers discussions **

I likely underappreciated the paper, re-reading it in the light of other reviews and discussions with other reviewers, I increased my score to 6. I still maintain my comments on AT, which I think is the weakest part of the paper.
 ","This paper proposes to use ensembles of deep neural networks to improve estimates of predictive uncertainty in classification and regression. In classification the aggregation is done by averaging probabilistic predictions, obtained by minimising a proper scoring rule, such as cross entropy or Brier score. In regression each component of the ensemble is predicting both mean as well as Gaussian variance around it, and aggregation is done by approximating the mixture of Gaussians by a single Gaussian. The training is enhanced by adversarial training, i.e. creating a shifted copy of each instance, where the shift is adversarially in the direction of largest increase in loss. Extensive experiments demonstrate that this simple method outperforms other methods, including the recently proposed MC dropout by Gal and Ghahramani (ICML 2016).

The paper is very well written and a pleasure to read. The experiments are extensive and convincing, the literature review is strong. The only significant shortcoming that I found is that the paper does not discuss running time of the methods. MC dropout and the proposed methods are being compared on the number of dropout samples and number of models in the ensemble, respectively. However, the running time of obtaining a sample from MC dropout should be much faster than training an additional network in the proposed ensemble method. This makes the direct comparisons unfair. However, at least according to the figures, the proposed method still outperforms MC dropout, even if having a smaller ensemble."
Adaptive Active Hypothesis Testing under Limited Information,"Fabio Cecchi, Nidhi Hegde",https://proceedings.neurips.cc/paper/2017/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html,"The authors presented a new algorithm, named the IBAG algorithm, for active sequential learning. The main idea is that it 
requires only a limited knowledge of a set of target actions. The authors did an extensive theoretical formulation and evaluation of their approach, and showed that it outperforms, in most cases, the benchmark approach (Chernoff’s algorithm). 

Overall, I think that this paper adds value to existing literature on hypotheses testing under the active learning setting and in presence of limited information. ","The paper studies the problem of active hypothesis testing with limited information. More specifically, denoting the probability of ‘correct indication’ by q(j,w) if the true hypothesis is j and the chosen action is w, they assume that q(j,w) is bounded below by \alpha(w) for all j, and this quantity is available to the controller. They first study the incomplete-bayesian update rule, where all q’s are replaced with alpha’s, hence approximate, and show that with this IB update rule, one needs at least O(log(1/\delta)) to achieve (1-\delta) posterior error probability. Then, they show that their gradient-based policy achieves the order-wise optimal sample complexity. 

I have a few technical questions. 
On the upper bound: The authors claim that the upper bound is obtained by considering the worst case scenario where q(w,j) = \alpha(w) for every w if j is the true hypothesis. If that’s the case, how is different the analysis given in this paper from that for Bayesian update case? (If my understanding is correct, q(w,j) = \alpha(w) implies IB update rule = Bayesian update rule.) Further, what can happen in reality is that q(w,j) is arbitrarily close to 1, and alpha(w) is arbitrarily close to 1/2. In this case, can the upper bound provide any guarantee? 

On the lower bound: Can this bound recover the existing lower bounds for the case of complete information? 

On simulation results: It will be great if the authors can present the lower & upper bounds on E[T] and compare them with the simulation results. 
","This paper considers active hypothesis testing where the algorithm only receives an indirect feedback. It proposes a method based on incomplete Bayesian update, and at each step greedily chooses the action that maximizes a lower bound on the expected posterior of the true hypothesis. It gives matching lower bound and upper bound on the number of samples required w.r.t. delta, the confidence parameter.

This paper is clearly written, and I believe its results are correct (I didn't read the appendix). It considers a new model, and the algorithm proposed is new (but similar to the existing methods in [11] and [13])

My main concern is the motivation of this paper. This paper proposes a new model of active hypothesis testing, but gives little justification for it. The paper does provide an example in section 5 (I think it would be better to move it into section 1 or 2), but even that example is unnatural to me: why don't we just let the workers output the labels? How can you know the quality of the worker?

Another issue is that the analysis is limited:
1. The upper and lower bounds are only with respect to delta. What is the dependency on |J|, |W|, or some other quantities?
2. How is the sample complexity of the proposed method compared with other methods like the Chernoff algorithm, GBS algorithm, and a naive passive method? The paper provides numerical results, but I would expect to see theoretical comparisons."
Translation Synchronization via Truncated Least Squares,"Xiangru Huang, Zhenxiao Liang, Chandrajit Bajaj, Qixing Huang",https://proceedings.neurips.cc/paper/2017/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html,"Summary:
In this paper, the author proposes TranSync algorithm to deal with the 1D translation
synchronization problem. In contrast with many other methods, this work considers
employing the truncated least squares to alleviate noisy measurements.A theory is presented
to analyze the algorithm. Both synthetic and real data are used to demonstrate
the efficiency of the proposed approach.

Qualitative:
The paper is well written and reads easily. The theory is gently exposed while the proofs are left to supplementary
materials. The proposed method is demonstrated on a wide set of experiments. The exposition would benefit from a
glimpse at the main ideas of the proof in the main text. I like this work.","This work deals with translation synchronization that consists in recovering latent coordinate from these noisy pairwise observations. The proposed algorithmic procedure consists in solving a truncated least squares at each iteration, the convergence analysis and exact recovery condition are provided. Finally the experiments illustrate the interest of the proposed strategy compared to state-of-the-art methods. This paper is interesting and complete.","In this paper the authors describe a robust and scalable algorithm,TranSync, for solving the 1D translation synchronization problem. The algorithm is quite simple, to solve a truncated least squares at each iteration, and then the computational efficiency is superior to state-of-the-art methods for solving the linear programming formulation. On the other hand, the analyze TranSync under both deterministic and randomized noise models, demonstrating its robustness and stability. In particular, when the pair-wise measurement is biased, TranSync can still achieve sub-constant recovery rate, while the linear programming approach can tolerate no more
than 50% of the measurements being biased. The paper is very readable and
the proofs of main theorems are clear and appear correct. However, it will be good if the authors can provide more intuition behind the theorem. The numerical experiments are complete and clear. "
Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization,Yossi Arjevani,https://proceedings.neurips.cc/paper/2017/hash/a00e5eb0973d24649a4a920fc53d9564-Abstract.html,"This papers covers some theoretical limitations on variance-reduced stochastic algorithms
for finite sum problems (SAG, SDCA, SAGA, SVRG etc.). Such algorithms have had some important
impact in machine learning & convex optimization as they have been shown to reach a linear convergence
for strongly convex problems with lipschitz gradient (log term) while having a factor in $n + \kappa$ (and not as $n\kappa$).

A first result of this paper is that the sequential iterative algorithm needs to know
which individual function has been returned by the oracle to reach the linear convergence
rate. Typically SAG or SAGA need this knowledge to update the gradient memory buffer
so it is not terribly surprising that knowing the function index is necessary.

It is then argued that such variance-reduced methods cannot be used with data augmentation
during learning. This is only true if the data augmentation is stochastic and if there is
not a finite set of augmented samples.

A second result concerns the possibility to accelerate in the ""Nesterov sense"" such
variance-reduced algorithm (go from $\kappa$ to $\sqrt{\kappa}}$). It is shown that
""oblivious"" algorithms whose update rules are (on average) fixed for any iteration
cannot be accelerated.

A practical consequence of the first result in 3.2 is that knowing the value of the
strong convexity parameter is necessary to obtain an accelerated convergence
when the algorithm is stationary.

Such oblivious algorithms are further analysed using
restarts which are well known techniques to alleviate the non-adaptive nature of
accelerated algorithms in the presence of unknown local conditionning.

The paper is overhaul well written with some pedagogy.

Minor:

CLI is first defined in Def 2 but is used before.

Typo: L224 First, let us we describe -> First, let us describe
","This paper establishes a number of results concerning lower bounds for finite sum optimisation problems. Over the last 2 years a close to complete theory has been established, which this paper contributes further to. This is an important area of research in my opinion.

This paper is very well written and is more polished than most NIPS submissions. I don't have many comments:

• I'm not following the use of Fano's inequality in equation (10), could you please clarify?

• Should the last part of the equation between lines 134-135 be n^2/2 instead? I'm not following it.

• I can't find the definition of t^{*} in equation 13.

• I think the tilde is not used consistently when big-Omega notation is used in the paper.

• In definition 2, it would help if it's more clearly stated that \theta typically contains the function index. Currently you just have some text before the definition “the oracle may allow one to query about a specific individual function”. 

• How does the result of section 3.2 relate to Theorem 2 in reference 4? Is it just a strengthening by the additional n term? . The result of section 3.3 is also closely related to results in that paper. In the introduction you state that you improve on the results in [4], I think the exact nature of the improvement needs to be more clearly stated."
Recursive Sampling for the Nystrom Method,"Cameron Musco, Christopher Musco",https://proceedings.neurips.cc/paper/2017/hash/a03fa30821986dff10fc66647c84c9c3-Abstract.html,"The authors provide an algorithm which learns a provably accurate low-rank approximation to a PSD matrix in sublinear time. Specifically, it learns an approximation that has low additive error with high probability by sampling columns from the matrix according to a certain importance measure, then forming a Nystrom approximation using these kernels. The importance measure used is an estimate of the ridge leverage scores, which are expensive to compute exactly ( O(n^3) ). Their algorithm recursively estimates these leverage score by starting from a set of columns, using those to estimate the leverage scores, sampling a set of columns according to those probabilities, and repeating ... the authors show that when this process is done carefully, the leverage score estimates are accurate enough that they can be used to get almost as good an approximation as using the true ridge leverage scores. The cost of producing the final approximation is O(ns^2) computation time and O(ns) computations of entries in the PSD matrix.

This is the first algorithm which allows touching only a linear number of entries in a PSD matrix to obtain a provably good approximation --- previous methods with strong guarantees independent of the properties of the PSD matrix required forming all n^2 entries in the PSD matrix. The experimental results show that the method provides more accurate kernel approximations than Nystrom approximations formed using uniform column samples, and kernel approximations formed using Random Fourier Features. These latter two are currently the most widely used randomized low-rank kernel approximations in ML, as they are cheap to compute. A heuristic modification of the algorithm brings its runtimes down to close to that of RFFs and uniform sampled Nystrom approximations, but retains the high accuracy.

I see one technical problem in the paper: the proof of Lemma 5, in the display following line 528, the first inequality seems to use the fact that if W is a diagonal matrix and 0 <= W <= Id in the semidefinite sense, then for any PSD matrix A, we have WAW <= A. This is not true, as can be seen by taking A to be the rank one outer product of [1;-1], W to be diag(1, 1/2), and x = [1, 1]. Then x^TWAWx > x^tAx = 0. Please clarify the proof if the first inequality holds for some other reason, or otherwise establish the lemma.

Comments:
-In section 3.2, when you introduce effective dimension, you should also cite ""Learning Bounds for Kernel Regression Using Effective Data Dimensionality"", Tong Zhang, Neural Computation, 2005. He calls the same expression the effective dimension of kernel methods
-Reference for the claim on lines 149-150? Or consider stating it as a simple lemma for future reference
-line 159, cite ""On the Impact of Kernel Approximation on Learning Accuracy"", Cortes et al., AISTATS, 2010 and ""Efficient Non-Oblivious Randomized Reduction for Risk Minimization with Improved Excess Risk Guarantee"", Xu et al., AAAI, 2017
-line 163, the reference to (13) should actually be to (5)
-on line 3 of Alg 2, I suggest changing the notation of the ""equals"" expression slightly so that it is consistent with the definition of \tilde{l_i^lambda} in Lemma 6
-the \leq sign in the display preceding line 221 is mistyped
-the experimental results with Gaussian kernels are very nice, but I think it would be useful to see how the method performs on a wider range of kernels. 
-would like to see ""Learning Kernels with Random Features"" by Sinha and Duchi referenced and compared to experimentally
-would like to see ""Learning Multidimensional Fourier Series With Tensor Trains"", Wahls et al., GlobalSIP, 2014 cited as a related work (they optimize over frequencies to do regression versus 'random' features)","This work has two major contributions: 1. apply ridge leverage score (RLS) sampling to Nystrom method and establish several useful theoretical results, and 2. proposed an efficient and provable algorithm to approximate the ridge leverage scores. 

If the theories were correct, I would argue for acceptance. The proof of Theorem 8, which is the key to analyzing the efficient algorithm, seems specious. First, the proof is too terse to follow. Second, the proof seems to have errors. Third, the proof looks like induction, but it does not follow the basic rules of induction -- induction hypothesis, base case, and inductive step.

The presentation of the algorithms is difficult to follow. First, Algorithm 2 is written in an incursive way. However, the function interface is undefined. Readers of this paper, including me, cannot easily implement Algorithm 2. Second, the notations are confusing. In Algorithm 2 and the proofs, the notations S0, S1, S are abused and thus confuse me. I would recommend a subscript to indicate the level of recursion.

It is hard for the authors to answer my questions in short. So they'd better submit a *full* and *error-free* proof of Theorem 8. Make sure to write your mathematic induction by following a textbook example. Also make sure you do not skip any intermediate step in the proof. The authors may share me a Google Drive public link.


Details regarding the proof of Theorem 8.

1. The basis step of induction hypothesis does not hold.
In Algorithm 2, if S0 has 16 columns, you want to select $\sum_i p_i$ columns from the higher level which has 32 columns. However, $\sum_i p_i$ is not bounded by 32. Therefore, there won't be sufficient samples to guarantee the basis step.

2. I don't see why ""... S will satisfy Eq (6) ...""
Your argument just ensured one level of recursion, provided that it were correct! However, Eq (6) is the overall guarantee.

3. To establish the inductive step, you should guarantee $192 * d_eff * log (d_eff)$ be smaller than the data size in this step. It is unclear whether or not this holds.

4. Note that in each level of the recursion, since the matrix K is different, $d_eff^\lambda$ is different. You do not distinguish them.



Other Comments

1. Line 157: How can additive error be ""the strongest type of approximation""? Is it written in [GM13]? I think relative-error bound is.

2. Line 163: ""Note that (13)"". Where's (13)?

3. Do you have to describe Algorithm in the recursive way? It seems to be a bottom-up algorithm.


--- after feedback ---

I appreciate the authors' patient feedback. I increased my rating to weak acceptance.

Two further comments:

1. In Alg 2, the set \bar{S} seems to be a set of vectors. However, in L5, it's a set of indices.

2. The proof of Thm 8 is carefully laid out. I appreciate it. However, it is very involved, and I am not confident of its correctness. The authors and readers should use their own discretion.

","Recursive Sampling for the Nyström Method

The paper present the concept of recursive leverage
score calculation to ensure a linear time nystroem
approximation without negatively effectiving the approximation accuracy.
While the approach is interesting the presentation is not
always sufficiently clear and also the experimental part has
some missing points and could be further extended.

Comments:
- 'The kernel method' - there is no 'kernel method' by itself,
  a kernel is simple a mathemtical construct used in a number of methods
  leading to methods using a kernel or maybe the kernel-trick
  --> along this line check the remaining text to disinquish between
  a high dimensional feature space, a kernel function, a kernel matrix, the kernel trick
  and a hilbert space -- it would be quite imprecise to this all this together
- 'projecting K onto a set of “landmark” data points' - acc. to your notation K is
  the kernel matrix  this one is not actually projected onto some landmarks
  --> following sec 2 I see your point - but initially one expects to have K^{ns} * 1./K^{ss} * K^{sn}
- 'with high probability || K -K_approx||_2 \le \lambda - really (squared) euclidean norm
  or not Frobenius norm?
- 'using a uniform sample of 1/2 of our input points' - why 50%?
- eq 1 please introduce the notation for ()^+ -- pseudo-inverse?
- In Alg1 could you briefly (footnote) explain where you got the '16'
- 'If S 0 has > 16 columns' - why 16 columns - this actually immediately points to the question
  what you typically require for N (approximating small world problems is obviously a bit useless
  - but at the end this is a constraint which should be mentioned in the paper)
- If I got it right in Algorithm 2 ... S_0^T K S_0 ... - you expect that K (full) is available during
  the calculation of the algorithm? ... - this would in fact be quite unattractive in many
  practical cases as for e.g. N=1 Mill points it may even be impossible to provide a full kernel matrix
  - or very costly to calculate all the kernel function values on the fly. The nice part of the 
  uniform landmark nystroem approach is that a full K is never needed - although by bad sampling of the
  landmarks the accuracy of the approximation may suffer a lot
- Theorem 7 another magic constant '384' - please refer to the supplementary / or tech report / reference
  where the full derivation is provided (in the proof its not getting much better ' 192')
- I think it is not quite clear that the derivations/proofs (and hence the algorithm) lead to lower computational
  costs (I am not saying your are wrong - but I think it is not clear (enough) from the provided derivation)
- how many landmarks did you finally obtain / took and how many random fourier features have been used?
- it would also be good to show that you approach for non-rbf kernels - e.g. a simple linear kernel representation,
  a polynomial kernel or other (by prob. skipping random fourier features). It would also be good to study the behaviour
  if the intrinsic dimension (by means of non-vanishing eigenvalues) is changing -> refering to the point what happens
  if the kernel space is not really very low rank
- ', it never obtained high enough accuracy to be directly comparable' -- hm, if you increase the number of random
  fourier features - you can achieve any level of accuracy you like - so the strategy would be to increase the #features
  to the requested level and compare the runtime performance 

"
Early stopping for kernel boosting algorithms: A general analysis with localized complexities,"Yuting Wei, Fanny Yang, Martin J. Wainwright",https://proceedings.neurips.cc/paper/2017/hash/a081cab429ff7a3b96e0a07319f1049e-Abstract.html,"The authors proved an upper bound of early stopping strategy for kernel boosting algorithm and determined the number of iteration steps to achieve minimax optimal rate. The upper bounds are stated in terms of localized Gaussian complexity and lead to fast rates for kernels with fast decaying eigenvalues. These results are novel and promising.

A small concern is on the the authors' claim that it is straightforward to extend the results for empirical L2 norm to population L2 norm. I have a little bit reservation on this before seeing the detailed proof.","The paper establishes minimax optimal rates of estimation for early stopping of kernel boosting, where the stopping index is expressed in terms of localized Rademacher/Gaussian complexities.

Overall, this paper is well written and closes a gap in the literature: while lower bounds in terms of localized complexity measures are known, a similar result for the choice of the nb. of iterations (and therefore upper bounds) was unknown. 

Some comments:

***Remark after Th. 1 about random design setting should be more explained (at least for the not initiated reader); applies (11b) to squared population L^2-norm with f^T or \bar f^T ?

***l. 188: I think Yang et al. [30] state a lower bound involving the critical radius delta_n defined as the smallest positive solution such that R(delta)<=delta^2/sigma ,
with R(delta) given in (6) (rather than the worst-case constant sup delta^2_n; or are these two numbers equivalent ?).

***l.223/224: could you provide a reference?

***a possible weak spot: numerical experiments are provided only for the error involving f^T in distinction to Th. 1. The authors remark in l.282/283 that a rigorous proof is missing
(on the other hand, based on this simulations one can indeed strongly suspect that the same result holds for f^T as well)


A few very minor remarks about typesetting:

*** l. 155 ff. --> the function class {\mathcal E} and also delta depend on n, which could be indicated for better understanding; same for the effective noise level sigma for least squares
*** l.158: ""Gaussian complexity ... at scale sigma""-->should be ""delta""
"
Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning,"Shixiang (Shane) Gu, Timothy Lillicrap, Richard E. Turner, Zoubin Ghahramani, Bernhard Schölkopf, Sergey Levine",https://proceedings.neurips.cc/paper/2017/hash/a1d7311f2a312426d710e1c617fcbc8c-Abstract.html,"
* summary: This paper studies how to merge on-policy and off-policy gradient algorithms. The authors propose a new policy gradient framework that unifies many previous on-policy and off-policy gradient methods. Many previous policy gradient algorithms can not only be re-derived and but also get improved by the introduction of control variate. Even though this framework introduces bias to gradient updates but, they show in theoretical results that this bias can be bounded. Experiments are very well done and provide enough insights to understand the proposed framework. 

* In overall, the paper is very well written with excellent related work and discussions. The main contents are well presented as well, the theoretical results are very intuitive and make sense. 

* I think this is very strong paper. I have only few comments as following:

- I think that even if one is familiar with TRPO, he might still not be able to know how to transfer the technique of TRPO to approximately implement Algorithm 2. It would be nicer if the authors elaborate on this step.

- As seen in Eq. (6) and Theorem 1, the bias introduced by the difference between \beta and \pi is non-trivial. Therefore, I find that IPG does not really always outperform Q-Prop, sometimes they are comparable (when \beta is not equal to \pi). According to my understanding, v from 0 to 1 means a shifting gradually from on-policy to off-policy, therefore one could compensate the bias by gathering more data/larger batch-size. Therefore, it might be more interesting to see if Table 2 is expanded with more experiment setting: v increases gradually from 0 to 1 (with and without control variate), and also with the change of data-size/batch-size to see if bias can be compensated somehow.

- line 151, should it be $v=0$? ","As the title says, this paper proposes a framework to merge on-policy and off-policy gradient estimation for deep reinforcement learning to achieve better sample efficiency and stability. The topic addressed in the paper is of importance, the paper is overall well-written, the experiments done in the paper looks fair. 

Experimental results show that a reasonable setting for IPG tends to depends on tasks. Since it is always annoying to tune these setting, it would be nice to provide a default setting and provide when and how to modify these parameters.","I have read the author's rebuttal and chosen not to change any of my review below.

This paper presents a method for using both on and off-policy data when estimating the policy gradient. The method, called Interpolated Policy Gradient, is presented in Equation (5). In this equation the first term is the standard on-policy policy gradient. The second term is a control variate that can use samples from various different distributions (e.g., on or off policy). Different definitions of \beta, which denotes the policy that s_t is sampled according to, causes this algorithm to become equivalent to various previous methods.

The paper is well written, presents results that are novel, and clearly places the new method in the context of related literature. The empirical results support the theoretical claims. I recommend acceptance.

One thing that would have helped me a lot would be if there was more clarity in the use of $\pi$ and $\theta$. For example, in equation (5), I think of having my current policy parameters, $\theta$, and some other old policy parameters ,$\theta_old$. Then I read the first term and see E_{\rho^\pi}. This term really means $E_{\rho^{\pi_\theta}}$, since $\pi_\theta$ is the policy, not $\pi$. Dropping the \theta also hides the important term here - this first term uses samples from $\pi_\theta$, not $\pi_{\theta_old}$. That is, $\pi$ isn't the important term here, $\theta$ is, and $\theta$ isn't in the subscript on the expected value. I suggest using $E_{\rho^{\pi_\theta}}$ or $E_{\rho^\theta}$, and saying that $\rho^\theta$ is shorthand for $\rho^{\pi_\theta}$.

On line 107 ""IHowever"" should be ""However"".

The green text in Algorithm 1 is hard to see."
Parameter-Free Online Learning via Model Selection,"Dylan J. Foster, Satyen Kale, Mehryar Mohri, Karthik Sridharan",https://proceedings.neurips.cc/paper/2017/hash/a2186aa7c086b46ad4e8bf81e2a3a19b-Abstract.html,"Online learning has recently attracted much attention due to its involvement as stochastic gradient descent in deep learning. Oracle inequalities form an important part of online learning theory. This paper proposes an online algorithm, MultiScaleFTPL, in terms of a scale c_i of regret to expert i. Then an oracle inequality for regret bounds is presented. An essential term is \sqrt{n \log (n c_i/\pi_i)}, which means a tight complexity requirement on the hypothesis set and is caused by the linear design of p_t in Algorithm 3. This special design leads to a restriction of applying the main oracle inequality to supervised learning only to Lipschitz losses in Theorems 8 and 9. But the results in the paper are interesting in general. ","SUMMARY

While I am not heavily familiar with the literature on adaptive online learning, this paper seems to be a breakthrough, offering in the form of Theorem 1 a very strong result that can be leveraged to obtain adaptive (in the model complexity sense) online learning bounds in a number of settings. The efficiency, at least in the polytime sense, of the algorithms for the various settings makes these results all the more interesting. I was very surprised by the ``aside'' on the 1-mixability of logistic loss and the argument for circumventing the lower bound of Hazan, Koren, and Levy in the supervised learning setting. I wish that the authors could give more detail to this observation and the consequences, as the implications are so interesting that I would be (almost) sold on acceptance from this fact alone. I found the results of this paper to be very interesting, technically strong, and important, so I would strongly recommend acceptance.


DETAILED REVIEW

This paper is very well written and clearly identifies the core technical contribution. I do not really have any criticisms of the paper. I found Theorem 1 to be a very powerful result, as it readily gives rise to a number of corollaries by appropriately using the MultiScaleFTPL algorithm. I checked part of the proof of Theorem 1 (which is to say I did not look at the proof of Lemma 1), and I think the result is correct. The proof of Theorem 2 appears to be correct, and this result may be thought of as the main bound that can be specialized in the different applications. The ease of the analysis after starting from Theorem 2 is impressive.

Regarding the various applications: These all seem either interesting or provide useful examples of how to apply Theorem 2. However, for some of the results (like Theorem 3), even if the previous best results involved inefficient algorithms, it would be good to establish the proper context by providing citations to those results.

In the Discussion section, where you discuss ``Losses with curvature'', do you believe the $\log(n)$ factor arising from the $1/n$-cover to be necessary or just an artifact of using a cover?


MINOR COMMENTS

Line 154: is the norm $\|\cdot\|_i$ meant to be an $\ell_p$ norm for $p = i$? I suspect not. You are abusing notation right? This level of abuse is too high, and I'd perhaps put parenthese around the subscript $i$ to distinguish from an $\ell_p$ norm.

Line 193: ``convexity chosen'' --> ``convexity was chosen''

Line 194: ``incorporate generalize'' doesn't make any sense; fix this","The paper introduces a framework for deriving model selection oracle inequalities in the adversarial online learning setting. The framework is based on a new meta-algorithm, called MultiScaleFTPL, which is able to combine base algorithms (""experts""), incurring a regret to the i-th algorithm which is proportional to the range of its losses. This adaptation to the loss range leads to numerous applications of the algorithm in a wide range of settings, in many of which oracle bounds were previously unavailable. 

In particular, the authors consider adaptation to the comparator's norm in uniformly convex Banach spaces (which is a novel results, as previously only Hilbert space norms were considered), an algorithm with a bound which holds for any \ell_p norm of the comparator (quite surprising that such a generality in the bound is actually possible), adaptation to the rank for online PCA, adaptation to the norm of positive definite comparator matrix in the Matrix Multipcative Weight setup, combining algorithms in the online supervised learning setup, including online penalized risk minimization and online multiple kernel learning. Many of these results are impressive. The overall contribution is thus very solid.

The central result in the paper is the MultiScaleFTPL algorithm, essentially a ""prediction with expert advice"" algorithm. The main feature of the algorithm, the reason why it stands out from other previously considered methods, is its adaptation to the loss range: the algorithm guarantees a regret to the expert i (for all i), which scales proportionally to the range of the losses of expert i, rather than to the worst-case range among experts. The algorithms is an unusual version of Follow the Perturbed Leader algorithm, which samples the future and plays with a distribution which minimizes the worst-case (w.r.t. the current gradient) penalized regret. 

The technical quality of the paper is high. I verified some of the proofs (not all) in the appendix. I found one confusing aspect in the proof of the main theorem (Thm 1), which I was not able to understand: at the top of page 12 (appendix) what does ""choosing p_t to match e_i"" mean, given that i is the index over which the supremum is taken? (i.e., it sounds as if p_t matches all all e_i simultaneously...). I would be very to glad to get a feedback from the authors on this issue. 

In summary, I really like the new algorithms and numerous impressive novel results the algorithm leads to. I think the paper is definitely worth accepting to the conference.

Minor remarks:
- line 54-55: ""all losses ... are all bounded"" -> ""losses ... are bounded""
- Eq. (3) -- what is w_t^i on the l.h.s.? (i index not explained)
- line 121: ""with norm large as"" -> ""with norm as large as""
- Eq. below line 135: sigma's are denoted differently than in Algorithm 3
- line 187: 2-smooth norm -- this term is not explained (at least give a reference to the definition)
- appendix A.1, last line of page 11: < p_t,g_t >  ->  < p_t, g_t' >
- appendix A.1 (proof of Thm 1): would be good to say what \epsilon is.
- appendix A.1 (proof of Thm 10): domination is defined by saying that w_1 is at least w_2 plus sum of c_1 + c_2, but there is a factor of 2 in front of epsilon sigma_i c_i in the bound. Should I understand it that epsilon is distributed on {-1/2, 1/2}? But then it does not fit to the proof of Thm 1 (top of page 12) 
- appendix A.2 (proof of Thm 2): \cal{A} introduced without definition (class of base algorithms?)

-------

The rebuttal clarified my (minor) issues with the proofs. I keep my high score."
The Importance of Communities for Learning to Influence,"Eric Balkanski, Nicole Immorlica, Yaron Singer",https://proceedings.neurips.cc/paper/2017/hash/a36e841c5230a79c2102036d2e259848-Abstract.html,"This paper studies the problem of influence maximization in social networks. In the influence process, one selects a set of initially influenced seed nodes in the network, and the influence propagates following a random propagation process, and at the end the expected number of influenced nodes is the influence of the seed set. Influence maximization is then the task of selecting a seed set of a certain size to maximize the influence of the seed set, given samples from the influence process in the form of (a random seed set, the influence of the seed set in an instance of the random propagation process). The paper considers the classical independent cascade model for propagation, and assumes the sample seed set is from a product distribution. 

The paper proposes an algorithm that enjoys provable guarantees in certain random graph models that can capture the community structure in social networks, thus enjoys good performance in the experiments on both synthetic and real datasets. The idea is to avoid selecting nodes from the same community, since their potential influenced nodes can overlap a lot. This is detected by estimating the second order marginal contribution of the first node w.r.t. the second node. The authors provided performance guarantees under two settings. In the dense communities and small seed set setting, the approximation ratio is 1-o(1), while it is a constant in the setting with tight and loose communities and non-ubiquitous seed sets.

The presentation is clear and easy to follow.

minor:
--missing reference in Line 110, and the title of Figure 1","This work marries influence maximization (IM) with recent work on submodular optimization from samples. The work salvages some positive results from the wreckage of previous impossibility results on IM from samples, by showing that under an SBM model of community structure in graphs, positive results for IM under sampling are possible with a new algorithm (COPS) that is a new variation on other greedy algorithms for IM. It’s surprising that the removal step in the COPS algorithm is sufficient from producing the improvement seen between Margl and COPS in Figure 2 (where Margl sometimes does worse than random). Overall this is a strong contribution to the IM literature.

Pros:
- Brings IM closer to practical contexts by studying IM under learned influence functions
- Gives rigorous analysis of this problem for SBMs
- Despite simplicity of SBMs, solid evaluation shows good performance on real data
Cons:
- The paper is very well written, but sometimes feels like it oversimplifies the literature in the service of somewhat overstating the importance of the paper. But it’s good work.
- The paper could use a conclusion.

1) The use of “training data” in the title makes it a bit unclear what the paper is about. I actually thought the paper was going to be about something very different, some curious application of IM to understand how training data influences machine learning models in graph datasets. Having read the abstract it made more sense, but the word “training” is only used once beyond the abstract, in a subsection title. It seemed as though the focus on SBMs and community structure deserve mentioning in the title?

2) The SBM is a very simplistic model of “communities,” and it’s important to stay humble about what’s really going on in the theoretical results: the positive results for SBMs in this work hinge on the fact that within communities, SBM graphs are just ER graphs, and the simple structure of these ER graphs is what is powering the results. 

3) With (2) registered as a minor grievance, the empirical results are really quite striking, given that the theoretical analysis assumes ER structure within the communities. It’s also notable that the COPS algorithm really does quite well even graphs from on the BA model, which is far from an SBM. In some sense, the issue in (2) sets up an expectation (at least for me) that COPS won’t ""actually work"". This is quite intriguing, and suggests that the parts of the analysis relying on the ER structure are not using the strongest properties of it (i.e., something close to the main result holds for models much more realistic than ER). This is a strong property of the work, inviting future exploration.

Minor points:
1) The dichotomous decomposition of the IM literature is a little simplistic, and some additional research directions deserve mention. There has been several recent works that consider IM beyond submwdular cases, notably the “robust IM” work by [HK16] (which this submission cites as focusing on submwdular IM, not wrong) and also [Angell-Schoenebeck, arxiv 2016].
2) Two broken cites in Section 3.1. Another on line 292. And I think I see at least one more. Check cites.
3) The name COPS changes to Greedy-Prune in Section 5 (this reviewer assumes they refer to the same thing). 
4) I really prefer for papers to have conclusions, even if the short NIPS format makes that a tight fit.","The paper proposes a new influence maximization algorithm for the discrete-time independent cascade diffusion model. It also provides a formal approximation analysis on the stochastic block model adjustifying the intuition of the algorithm to exploit the community structure of the underlying networks. Experiments on synthetic and real datasets show that the performance of the new algorithm is close to the greedy upper bound achieved accordingly. The major problem of the paper is that although the analysis of the algorithm assumes a stochastic block model (SBM), in practice, the algorithm unfortunately does not utilize the insights learned from the SBM (since SBM can be fitted in the training data) As a result, it should compare with other heuristics based on community structures. To name a few, ""Maximizing Influence Propagation in Networks with Community Structure""; ""Correction: A Novel Top-k Strategy for Influence Maximization in Complex Networks with Community Structure""; ""A New Community-based Algorithm for Influence Maximization in Social Network""; Furthermore, on networks with known community structures, it should at least compare with one basic heuristic ""Scalable influence maximization for independent cascade model in large-scale social networks"" demonstrating that the new algorithm perform better compared to alternatives which do not exploit the community structures. "
Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra,"John T. Halloran, David M. Rocke",https://proceedings.neurips.cc/paper/2017/hash/a4666cd9e1ab0e4abf05a0fb232f4ad3-Abstract.html,"This paper introduces Theseus, an algorithm for matching MS/MS spectra to peptide in a D.B. 
This is a challenging and important task. It is important because MS/MS  is currently practically the only common high-throughput method to identify which proteins are present in a sample. It is challenging because the data is analog (intensity vs. m/z  graphs) and extremely noisy. Specifically, insertions, deletions, and variations due to natural losses (e.g. carbon monoxide) can occur compared to a theoretical peptide spectra. 

This work builds upon an impressive body of work that has been dedicated to this problem. Specifically, the recently developed DRIP (Halloran et al 2014, 2016)  is a DBN, i.e. a generative model that tries to match the observed spectra to a given one in the D.B. modeling insertions/deletions etc. and computes for it a likelihood and the most likely assignment using a viterbi algorithm.

A big issue in the field is calibration of scores (in this case - likelihoods for possible peptide matches from the D.B to an observed spectra) as these vary greatly between spectra (e.g. different number of peaks to match) so estimating which score is “good enough” (e.g. in terms of overall false positive rate) is challenging.  Another previous algorithm, Percolator, has been developed to address this by training an SVM to discriminate between “good” matching peptides scores and “negative” scores from decoy peptides. A recent work  (Halloran et al 2016) tried to then map the DRIP viterbi path to a fixed sized vector so Percolator could be applied atop of DRIP.

Here, the authors propose an alternative to the above heuristic mapping. Instead, they employ Fisher Kernels where two samples (in our case - these would be the viterbi best path assigned to two candidates matches) are compared by their derivative in the feature space of the model. This elegant solution to apply discriminative models (here Percolator) to generative models (here DRIP) was suggested for other problems in the comp bio domain by Jaakkola and Haussler 1998. They derive the likelihood and its derivatives and the needed update algorithms for this, and show employing this to several datasets improve upon state of the art results.

Pros:

This work tackles an important problem. It results in what seems to be a significant improvement upon state of the art. It offers a new model (Theseus) and an elegant application Jaakkola and Haussler’s Fisher Kernel idea to the problem at hand. Overall, this appears to be high-quality computational work advancing an important topic.


Cons:

Major points of criticism:

1. Writing: While the paper starts well (intro) it quickly becomes impenetrable. It is very hard to follow the terms, the variables and how they all come together. For example, Fig1 is referred to in line 66 and includes many terms not explained at that point. Sec. 2.1 refers to c_b which is not defined (is it like c(x) ?) How are the natural losses modeled in this framework, Lines 137-145 are cryptic, evaluation procedure is too condensed (lines 221-227). In general, the authors spend too much space deriving all the equations in the main text instead of actually explaining the model and what is new compared to previous work. This makes evaluating both the experiments and the novelty hard. To make things worse, the caption of the main figure (Fig3) refers to labels/algorithms (“Percolator”, “DRIP Percolator Heuristic”) which do not appear in the figure. 

Given the complexity of the domain and the short format of NIPS papers, we recommend the authors revise the text to make the overall model (Theseus) and novel elements more clear, and defer some of the technical details to a longer more detailed supplementary (which in this case seems mandatory).



","Summary:
Tandem mass spectrometry (MS/MS) is used to identify the proteins in a complex biological sample. In the past, discriminative classifiers were used to identify the top ranked peptides (subsequence of protein). Many heuristically derived features were shown to improve the performance. In this work, the author proposed a novel unsupervised generative model ""Theseus"" -- a Dynamic Bayesian Model with Fisher kernel. The author claim that providing the gradient-based features from Theseus as the input features to the discriminative models could substantially improve the prediction quality. Supportive real world experiments were provided to show that the proposed method leads the state-of-art identification accuracy.

Qualitative Evaluation:

Quality:
The author supported the proposed method with experiments on several real world datasets. The method is compared to many previous approaches and shows significant improvements. However, the paper lack of some discussion about the computational efficiency: how much overtime is needed when using proposed method to extract features?

Clarity:
The paper is well organized. The experiment details are well described. However, when introducing the dynamic bayesian model, it might be better if the author could better introduce the variables and symbols. For example, the author introduced spectrum s in section 2 line 68. But its definition was not introduced until section 3 in page 4. Also, line 130, O^{mz} and O^{in} is not well-defined, I think the (O^{mz},O^{in}) pair means a particular peak's m/z and intensity? When reading the paper with so many symbols, concepts and overloading functions, I think it might be helpful if the author could provide a table listing the symbols and their meanings.
Some minor issues:
In page 4, line 130, ""x a candidate peptide of length m"" should be ""x be a candidate peptide of length m""
In page 4, line 135, ""specrum"" -> ""spectrum""

Originality:
This work provides a novel unsupervised method on a traditional computational task. It follows the trend of generating better features to discriminative models to improve the accuracy. Relevances and comparisons to previous methods are well discussed in the paper.

Significance:
The works provides a new way to extract features based on unsupervised learning requires no heuristic features for protein identification in tandem mass spectrum. The experiment results shows substantial increase in prediction accuracy comparing to previous work and leads the new state-of-art. 

"
On the Optimization Landscape of Tensor Decompositions,"Rong Ge, Tengyu Ma",https://proceedings.neurips.cc/paper/2017/hash/a48564053b3c7b54800246348c7fa4a0-Abstract.html,"### Summary

The paper analyzes the landscape of tensor decomposition problem. Specifically, it studies random over-complete tensors. The associated objective function is nonconvex, yet in practice simple methods based on gradient ascent are observed to solve this problem. This paper proves why we should expect such outcome by showing that there is almost no local maxima other than the global maxima of the problem when the optimization is initialized by any solution that is slightly better than random guess. Importantly, it is shown that these initial points do not have to be close to the true components of the tensor. This is an interesting result and well written paper. The analysis involves two steps: local (points close to true components) and global (point far from true components). The number of local maxima in each case is analyzed and shown to be exactly 2n for the former and almost nonexistent for the latter. 


### Questions

1. Authors mention that in practice gradient ascent type algorithms succeed. Do these algorithms use any specific initialization that meets your criterion? More precisely, do they use a scheme to produce initialization which is slightly better than chance and thus the objective value is slightly greater than 3*n?
IF YES: Please elaborate briefly how this is done or provide a reference.
IF NO: Any idea how such initial points could be generated easily? Could it be a very hard problem on its own?

2. The analysis is performed on random tensors, where each true component is assumed to be drawn iid from Gaussian distribution N (0, I). It seems to be a strong assumption and not true for real world problems. While I am not disputing the importance of the result provided in this paper, I was wondering if the authors had thoughts about analysis scenarios that could potentially explain the success of local schemes in a broader setup? For example, performing a worst-case analysis (i.e. choosing the worst tensors) and then showing that these worst tensors form a negligible fraction of the space of tensors? This would go beyond the Gaussian setting and may be more pertinent to explaining why local search succeeds on real world problems.

### Minor Comments

1. Lemma 2.2. Eq. 2.1, the first E is missing a | at the begining right after [.
2. Lines 80, 84, 123 local minima should be local maxima?
3. You refer to hard optimization as nonconvex, but since your task in 1.1. is defined as a maximization, concave functions are actually ideal for that. To avoid confusion around minimization/maximization/nonconvexity, you might want to replace 1.1 from Max f(x) with Min -f(x) so that everything remains a minimization, and thus nonconvexity implies hardness. In that case, you can also ignore my comment 2.
   


","This paper addresses the problem of identifying the local/global maxima for tensor decomposition. Tensor decomposition is an increasingly important non-convex problem with applications to latent variable models and neural networks. One of the fundamental question in tensor decomposition is, when are the components identifiable? and is there efficient methods for finding those components. It has been noted that LOCALLY the geometry of this landscape is close to convex and a simple methods such as projected power method with good initializations can identify those components. The major gap in this picture is that, empirically randomly initialized methods seem to find those components just as well. This paper is an important step towards filling this gap. A breakthrough made in this paper is in bringing the Kac-Rice formula to fill in the missing piece in the puzzle. How is the local minima behaving GLOBALLY, that is away from any of the components? This is made precise by counting argument based on the expectation of the number of such minima. There are no such minima (with some technical conditions on the region) with high probability. This is an exciting new result, with minor weaknesses on 1) it is for Gaussian components 2) it is for symmetric 4-th order tensor only. 

The theorem provides a guarantee in the over complete regime where n \geq d \log d. Is there something fundamentally different between over and under complete regime from the perspective of the geometry of the objective function? One would expect under completed tensor to be easier to decompose, and hence at least the same guarantee as in the main theorem to hold. 

Similarly, the main result holds up to n \leq d^2/log d. Is this a fundamental limit on how many components can be learned? How does this compare to other (weaker) results known for tensor decomposition? For example, what are the conditions sufficient for simpler LOCAL convexity?"
Counterfactual Fairness,"Matt J. Kusner, Joshua Loftus, Chris Russell, Ricardo Silva",https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html,"This paper presents an interesting and valuable contribution to the small but growing literature on fairness in machine learning.  Specifically, it provides at least three contributions: (1) a definition of counter factual fairness; (2) an algorithm for learning a model under counter factual fairness; and (3) experiments with that algorithm. 

The value and convincingness of each of these contributions declines steadily. The value of the contributions of the current paper is sufficient for acceptance, though significant improvements could be made in the clarity of exposition of the algorithm and the extent of the experimentation with the algorithm.

Section 4.2 outlines why is is likely that very strong assumptions will need to be made to effectively estimate a model of Y under counterfactual fairness.  The assumptions (and the implied analysis techniques) suggest conclusions that will not be particularly robust to violations of those assumptions.  This implies a need for significant empirical evaluation of what happens when those assumptions are violated or when other likely non-optimal conditions hold.  Realistic conditions that might cause particular problems include measurement error and non-homogeneity of causal effect.  Both of these are very likely to be present in the situations in which fairness is of concern (for example, crime, housing, loan approval, and admissions decisions).  The paper would be significantly improved by experiments examining model accuracy and fairness under these conditions (and under errors in assumptions).

The discussion of fairness and causal analysis omits at least one troubling aspect of real-world discrimination, which is the effect of continuing discrimination after fair decisions are made. For example, consider gender discrimination in business loans.  If discriminatory actions of customers will reduce the probability of success for a female-owned business, then such businesses will have a higher probability of default, even if the initial loan decisions are made in an unbiased way. It would be interesting to note whether a causal approach would allow consideration of such factors and (if so) how they would be considered.

Unmeasured selection bias would seem to be a serious threat to validity here, but is not considered in the discussion.  Specifically, data collection bias can affect the observed dependencies (and, thus, the estimation of potential unmeasured confounders U and the strength of causal effects).  Selection bias can occur due to selective data collection, retention, or reporting, when such selection is due to two or more variables.  It is relatively easy to imagine selection bias arising in cases where discrimination and other forms of bias are present.  The paper would be improved by discussing the potential effects of selection bias.

A few minor issues: The paper (lines 164-166 and Figure 1) references Pearl's twin network, but the explanation of twin networks in caption of Figure 1 provides insufficient background for readers to understand what twin networks are and how they can be interpreted.  Line 60 mentions ""a wealth of work"" two sentences after using ""wealth"" to refer to personal finances, and this may be confusing to readers.

","Summary
The authors compare several ideas of ""fairness"" in the ML literature and focus in on counter-factual fairness (to be defined in a second). The problem in general is, I observe some features X, some outcomes Y and some protected class of a person say A. I don't want my predictor to be ""biased"" against certain classes. How can I do this?

I will focus on 3 key distinctions of fairness that the authors look at:
1) Unawareness (don't include A in model)
2) Individual fairness (make sure that if X is the same, then predictions are the same)
3) Causal Fairness: If we intervene and change A, does it change the prediction?

The authors argue that causal fairness is the ""right"" definition of fairness. I find this very intuitive and I like the specifically causal way of looking at things. I think some parts of the paper are quite nice (I think the red car/policing example is actually the most illuminating of the whole thing, it's a shame it comes in so late in the paper) but others are quite confusing. 

Overall I think this is a ""concept"" paper rather than a particular technique. For this reason I am a weak accept on this paper. I am positive because I think it is important, but negative because a concept paper should be much clearer/more well polished. I now discuss ways I think the authors can improve the presentation of the paper without changing the actual content.

Review
0) The paper can be made clearer by being more example driven
The red car/crime examples are excellent, but they come in too late. They should come first, right after the definitions to show how one or another can fail. That example also explains quite well why we cannot condition on things downstream of A (Lemma 1). Similarly, the example of LSAT/GPA/FYA prediction in Figure 2 really explains everything. 

I would rather see the definitions  + detailed explanation for the DAGS for the 2 examples + a simple linear version of the everything/the algorithm + the experiments + the experiment from the supplement on stops/criminality rather than the general formulation (put that in the supplement!). This would mean section 2.2 is not needed, one could write a simpler definition of Def 5.

1) It's hard to think about any guarantees of the fair learning algorithm. 
It wasn't clear to me what assumptions it requires for either a) some notion of asymptotic consistency of b) some notion of performing well (or failing).

2) It's not clear that the full algorithm is needed
Looking at the Fair Prediction problem in section 5 (for Figure 2, level 2), why can't we just residualize race/sex out of the LSAT/FYA/GPA scores and then compute knowledge as the first principal component of the residuals? That seems to be quite close to what the authors are doing anyway and doesn't require any Bayesian modeling. I understand that the Bayesian mdoel provides posterior estimates etc... but conceptually, is there anything more to the model than that?","This paper addresses the question of whether a machine learning algorithm is ""fair"". Machine learning algorithms are being increasingly used to make decisions about people (e.g. hiring, policing, etc.). In some cases, making such decisions based on certain “protected” attributes may result in better predictions, but unfair decisions. For example, suppose that we wish to predict whether a particular person is likely to default on a loan and use that prediction to decide whether to accept or reject his/her loan application. If employers are discriminatory, then the race of the individual affects their employment, which in turn affects their ability to repay the loan (see Figure 1). As a result, predictions are likely to be more accurate if the race of the individual is used. However, it is unfair (and illegal) to reject a loan application on the basis of race. 

This paper proposes a definition of fairness for a predictor, which they call “counterfactual fairness”. This definition formalizes the intuition that a prediction is fair with respect to a protected attribute, say race, if that prediction would be the same had that person been born a different race. The authors discuss provide several compelling examples illustrating how this definition works and how it compares to prior conceptions of fairness. Next, the authors describe how to derive counterfactually fair predictors using causal assumptions of varying strength. Lastly, they apply their method to fairly predict success in law school. As expected, the counterfactually fair predictors perform worse than a ""full"" predictor that uses all of the available data. However, the full predictor would provide differential treatment to individuals had they been born a different race or sex. Also as expected, using stronger causal assumptions results in better predictions. Both counterfactually fair predictors also performed than a ""fairness through unawareness"" predictor that does not explicitly use any protected features for prediction. 

As machine learning takes a more prominent role in decision making related to people, the question of fairness becomes increasingly important. The proposed definition is compelling and, in my opinion, better captures human conceptions of fairness compared to prior, non-counterfactual definitions. Additionally, the paper is well written, easy to follow, and technically correct."
Efficient Online Linear Optimization with Approximation Algorithms,Dan Garber,https://proceedings.neurips.cc/paper/2017/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html,"This paper proposes online linear optimization approaches using approximation algorithms, where the authors consider the set of feasible actions is accessible. They explain how their approach captures natural online extensions of well-studied offline linear optimization problems.

The paper is clearly written, easy to follow and addresses a very interesting problem. It seems to be theoretically sounds. However, the paper is very theoretical and neither uses synthetic or real data to validate the veracity of the formulated claims. And, I do not clearly see how these algorithms would perform with real world datasets.

When we look at the paper from algorithms lenses we see that algorithm 1 is relatively classical (Online Gradient Descent Without Feasibility) easy to understand and accept theoretically, algorithm 2 ((infeasible) Projection onto...) seems to be convincing, however in practical situations they depend on a number of factors that are not necessarily captured/or cannot be captured by the alpha-regrets. To disprove this, it would be good to demonstrate that the proofs are congruent with some simulated data.   

I also believe that it would be good to demonstrate in a practical situation (even with simulated data) the performance of algorithm 3. Otherwise, the paper might not seem very compelling to the NIPS community.","The authors present a theoretical paper on online linear optimization with the application of online learning. The aim is to produce an algorithm which reduces computational complexity (aka number of required calls to an 'oracle', meaning a nearly-perfect optimizer - itself infeasible as NP but approximable, while providing a fast diminishing alpha-regret bounds, improving on computationally demanding procedures such as FPL. This paper assumes a constant (stationary) structure of adversary / expert action. The paper is entirely without numerical results (esp. comparisons to Abernethy et al.) and as opposed to oracle complexity (aka speed) the actual alpha regret bounds do not considerably improve on state-of-the-art: while theoretical bounds might be similar, oracle calls are not presumably simply 'wasted', so expected losses (average losses on a process) might still compare favorably for competing methods. In short, while this reviewer cannot refute the validity of the analysis or its relevance in the literature, feels that some experimental validation on well simulated data would be very useful in shedding further light on the exact loss/computational trade-off on offer. Otherwise the paper is well written, clear attention to detail, motivation, proofs etc and is quite commendable in that respect.","This paper addresses the problem of online linear optimization when the feasible set is such that doing exact projections into it is NP-hard but there is an approximate algorithm for maximizing linear functions in the feasible set with a constant approximation ratio. This paper is an extension to ""Playing games with approximation algorithms"", by Kakade et al with a similar algorithm with substantially better computational complexity.

The main issue with the previous work (kakade et al)  addressed in this paper is the linear (in the length of the online prediction history) number of calls to the approximation algorithm required in each online prediction round. Needing to essentially remember the entire history and have ever more expensive computation in each iteration is prohibitive and doesn't match most other work in online learning.

This paper proposes a new infesible projection algorithm which given an input either returns its approximate projection into the feasible set or returns an infeasible point which is a convex combination of a known set of feasible points and is not too far from the input. Using this infeasible projection algorithm it's possible to make a stochastic online optimization algorithm which has better complexity (roughly because projecting a point which is near from an existing projection is easy).

It is unclear what is applicability of this algorithm, however, as well as what would be the interest of it to the community. This seems to be of at most theoretical interest, and the paper should provide better justification of why this result is useful if it is to belong in nips."
Inhomogeneous Hypergraph Clustering with Applications,"Pan Li, Olgica Milenkovic",https://proceedings.neurips.cc/paper/2017/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html,"This paper considers the hypergraph clustering problem in a more general setting where the cost of hyperedge cut depends on the partitioning of hyperedge (i.e., all cuts of the hyperedge are not treated the same). An algorithm is presented for minimizing the normalized cut in this general setting. The algorithm breaks down for general costs of the hyperedge cut; however the authors derive conditions under which the algorithm succeeds and has provable approximation guarantees. 


Detailed comments:
==================

The main contributions of the paper are 

+ Generalization of hypergraph partitioning to include inhomogeneous cut of the hyper edge; the motivation for this is clearly established.

+ A novel technique to minimize the normalized cut for this problem. Similar to the existing techniques [e.g., 11], the algorithm tries to derive appropriate pairwise edges so that the problem can be transformed to the standard graph clustering (the approximation guarantees then follow from those of the standard spectral clustering).

Few comments on the positives/negative of the paper/algorithm:

+ The paper is well written and the motivation for the inhomogeneous cut of the hyper edge is clearly established. The algorithm details are easy to follow.

- The procedure that finds the approximate graph edge weights does not succeed in all cases as clearly illustrated in the paper.

+ I liked the part where they derive the conditions under which the procedure is feasible. However, it is not clear how practical it is to enforce such constraints in practice. 

- The main drawback is that they transform the problem on hypergraphs to graphs and then use the graph algorithms to solve the resulting problem. Although this is one of the standard approaches, it is shown that an exact representation of the hypergraph via a graph retaining its cut properties is impossible (E. Ihler, D. Wagner, and F. Wagner. Modeling hypergraphs by graphs with the same mincut properties. Information Processing Letters, 1993). So a purely hypergraph based algorithm similar to (Hein et. al. The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited. NIPS 2013) would be really interesting in this general setting.
","In this paper the authors consider a novel version of hypergraph clustering. Essentially, the novelty of the formulation is the following: the cost of a cut hyperedge depends on which nodes go on each side of the cut. This is quantified by a weight function over all possible subsets of nodes for each hyperedge. The complexity of such formulation may at first glance seem unnecessary, but the authors illustrate real-world applications where it makes sense to use their formulation. The paper is nicely written. My main critique is that some key ideas of the paper -as the authors point out themselves- are heavily based on [11]. 
Also some theoretical contributions are weak, both from a technical perspective and with respect to their generality. For instance, Theorem 3.1 is not hard to prove, and is true under certain conditions that may not hold. On the positive side, the authors do a good work with respect to analyzing the shortcomings of their method.Also, the experimental results are convincing to some extent. 

My comments follow:
1. The authors provide a motivational example in the introduction. I suggest that the caption under figure  1 should be significantly shortened. The authors may refer the reader to Section 1 for details instead. 
2. The authors should discuss in greater details hypergraph clustering, and specifically the formulation where we pay 1 for each edge cut. Τhere is no canonical matrix representation of hypergraphs, and optimizing over tensors is  NP-hard. The authors should check the paper ""Hypergraph Markov Operators, Eigenvalues and Approximation Algorithms"" by Anand Louis (arxiv:1408.2425)  
3.  As pointed out by [21] there is a random walk interpretation for motif-clustering. Is there a natural random walk interpretation of the current formulation in the context of motif clustering? 
4.  The experimental results are convincing to some extent. It would have been nicer to see more results on motif-clustering. Is there a class of motifs and graphs where your method may significantly outperform existing methods [10,21]? 

Finally, there is a typo in the title, and couple of others (e.g., ""is guaranteed to find a constant-approximation solutions""->""is guaranteed to find a constant-approximation solution"") along the way. Use ispell or some other similar program to detect all typos, and correct them. "
Runtime Neural Pruning,"Ji Lin, Yongming Rao, Jiwen Lu, Jie Zhou",https://proceedings.neurips.cc/paper/2017/hash/a51fb975227d6640e4fe47854476d133-Abstract.html,"The paper proposes a run-time pruning algorithm for deep networks where pruning is performed according to the input image and features. RL is used to judge the importance of each convolutional kernel and conduct channel-wise pruning. The method can be applied to off-the-shelf networks and a tradeoff between speed and accuracy can be achieved.

Positives
- The idea of doing dynamic pruning of neural networks by learning the decision to prune using RL is fairly novel. 
- The idea is justified well using empirical evaluation on Imagenet and CIFAR datasets with relevant comparisons to other pruning methods and trade-off between speed and accuracy.","**summary: This paper proposes to prune convolutional layers of CNN dynamically at runtime. The authors formulate the pruning as a MDP. Specifically, at each layer the feature map is embedded as a fixed length code.
	  A decoder then produces the Q-value for decision. The actions correspond to which group of channels to compute and which to prune and the reward relates to the cost as well as the loss of the prediction. Experiments on CIFAR10/100 and ImageNet 
	  show improvement over other CNN pruning techniques.
	  **other comments: The paper applies the ideas of deep reinforcement learning to CNN pruning. The procedure is clearly presented and the intuition well-explained. The idea of using MDP techniques to solve resource-constrained classification 
	  is not new. The authors may want to cite the paper by Benbouzid et al 2012: Fast classification using sparse decision DAGs. A potential drawback of the presented method is its complexity. To train a deep Q network together with a CNN seems highly non-trivial.	  
	  In practice, simpler network compression methods might be preferred. Have the authors compared to other model compression methods such as in Quantized Convolutional Neural Networks for Mobile Device by Wu et al 2016? 
	  **overall: the paper is well developed and novel in its application area. It's a good paper if it has thoroughly compared to recent model compression papers for CNN to justify its added complexity.
	  ","Update: I updated my score after RNN overhead clarification for more recent networks and their remark on training complexity.

The authors propose a deep RL based method to choose a subset of convolutional kernels in runtime leading to faster evaluation speed for CNNs. I really like the idea of combining an RNN and using it to guide network structure. I have some doubts on the overhead of the decision network (see below) would like a comment on that before making my final decision.

-Do your computation results include RNN runtime (decision network), can you comment on the overhead? 
The experiments focus on VGG16 which is a very heavy network. In comparison, more efficient networks like GoogleNet may make the RNN overhead significant. Do you have time or flops measurements for system overhead? Finally, as depth increases (e.g. Resnet151) your decision overhead increases as well since the decider network runs at every step.

- Do you train a different RNP for each p? Is 0.1 a magic number?

-The reward structure and formulating feature (in this case kernel) selection as an MDP is not new, check Timely Object Recognition Karayev, Imitation Learning by Coaching, Hal Daume III. However, application of it to internal kernel selection is interesting.

line 33 ""Some other samples are more difficult, which require more computational resources. This property is not exploited in previous neural pruning methods, where input samples are treated equally"" This is not exactly true check missing references below.

Missing references on sample adaptive inference in DNNs:
-The cascading neural network: building the Internet of Smart Things Leroux et al.
-Adaptive Neural Networks for Fast Test-Time Prediction Bolukbasi et al.
-Changing Model Behavior at Test-Time Using Reinforcement Learning Odena et al.

misc:
table 1 3x column should have Jaderberg as bold (2.3 vs 2.32)
line 158 feature -> future
line 198 p=-0.1 -> p=0.1 ?"
"Train longer, generalize better: closing the generalization gap in large batch training of neural networks","Elad Hoffer, Itay Hubara, Daniel Soudry",https://proceedings.neurips.cc/paper/2017/hash/a5e0ff62be0b08456fc7f1e88812af3d-Abstract.html,"
I think the paper provides some clarity on a topic that has seen a bit of attention lately, namely that of the role of noise in optimization and in particular the hypothesis of sharp minima/flat minima.  From this perspective I think this datapoint is important for our collective understanding of training deep networks. I don’t think the observation made by the authors come as a surprise to anyone with experience with these models, however the final conclusion might. I.e. we know the progress in learning is proportional to the number of updates (and not amount of examples seen). We know that when having large minibatches we have lower variance and hence we should use a larger learning rate, etc. I think one practical issue that people have got stuck in the past is that with larger minibatches the computational cost of any given gradient increases. Nowadays this is mediated to some extend by parallelization though maybe not completely. However, in this light, because of the additional computational cost, one would hope for there to be a gain, and hence one to get away with fewer steps (i.e. such that clock wall time is lower) otherwise why bother. I think this practical consideration might stopped people to reach the natural conclusion the authors have. 

Some comments:

1. I don’t fully follow the use of sharp/flat minima used by the authors. For example they say that you need more steps to reach a flat minima. I’m not sure about this statement. I think the crucial observation is that you might need more steps (because the distance you travel is proportional with log number of steps) to reach a minima. I’m not sure the diffusion process says anything about sharp vs flat. I would rather think this view is more natural to think in terms of saddle local minima. And to reach a saddle with a lower index one needs to take more steps.  I think this marginally affects the message of the paper, but I think strictly phrasing this paper in terms of Keskar et al might not be the best choices.

2. Regarding my comment with respect to wall-clock time. If using larger minibatches we need to correct such that we do the same number of updates, what is the gain (or reason) for pushing for large minibatches in practice? I understand the theoretical argument (that helps understanding generalization in neural nets). But does this imply also that at most we might have a similar time cost as we increase the minibatch size? Can we do something about it? Maybe this is a more important question rather than figuring out how to parallelize large minibatches, which is the message I took from lines 270-271.

3. Ghost Batch Normalization. Is my understanding that this is mostly an empirical finding? What is the justification for one to require noisy estimates of mean and variance? It feels like a failing of Batch Norm rather than SGD (and I assume that SGD without batch norm reaches same performance with small batches and large batches by just playing with learning rate and number of updates). I feel that maybe a better understanding of this would be useful. 

4. Line 91, did you mean \hat{g} instead of g_n? 

5. Any news on the ImageNet results with everything?
","
Summary of the paper and some comments:

This paper investigates the reasons for generalization gap, namely the detrimental effect of large mini-batches on the generalization performance of the neural networks. The paper argues that the reason why the generalization performance is worse when the minibatch size is smaller is due to the less number of steps that the model is trained for. They also discuss the relationship between their method and recent works on flat-minima for generalization. The main message of this paper is that there is no fundamental issue with SGD using large minibatches, and it is possible to make SGD + large minibatches to generalize well as well. The authors propose three different methods to overcome these issues,

1) Ghost BN: They have proposed to use smaller minibatch sizes during the training to compute the population statistics such as mean and the standard deviation. During the test time, they use the full-batch statistics. I think computing the population statistics over the small minibatches, probably have some sort of regularization effect coming from the noise in the estimates. I feel like this is not very well justified in the paper.
2) Learning rate tuning, they propose to rescale the learning rate of the large mini-batches proportionally to the size of the minibatch. This intuitively makes sense, but authors should discuss more and justify it in a better way.
3) Regime adaptation, namely trying to use the same number of iterations as the model being trained. This is justified in the first few sections of the paper.

Firstly, the paper at some points is very verbose and feel like uses the valuable space to explain very simple concepts, e.g. the beginning of section 3 and lacks to provide enough justifications for their approach.

The paper discusses mainly the generalization of SGD with large minibatches, but there is also an aspect of convergence of SGD as well. Because the convergence results of SGD also depends on the minibatch size as well, please see [1]. I would like to see a discussion of this as well.

In my opinion, anyone that spend on monitoring the learning process of the neural networks would notice easily notice that the learning curves usually have a logarithmic behavior.  The relationship to the random-walk theory is interesting, but the relationship that is established in this theory is very weak, it is only obtained by approximating an approximation.

Equation 4 is interesting, for instance, if we initialize w_0 to be all zero, it implies that the norm of the w_t will grow logarithmically with respect to the number of iterations. This is also an interesting property, in my opinion.

Experiments are very difficult to interpret, in particular when those plots are printed on the paper.

The notation is a bit inconsistent in throughout the paper, some parts of the paper use bracket for the expectation and some other parts don't. Authors should explicitly state what the expectation is taken over.

In Figure 3, can you also consider adding the large minibatch cases without regime adaptation for comparison?

It would be interesting to see the learning curves of the models with respect to different wallclock time as well.

I can not see the AB in Table 1 and 2. The results and the improvements are marginal, sometimes it hurts.  It seems like largest improvement comes from RA.

The paper could be better written. I think the paragraph sections in the abstract are unnecessary. The section 7 could be renamed as ""Discussion and Conclusion"" and could be simplified.


[1] Li, Mu, et al. ""Efficient mini-batch training for stochastic optimization."" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.","The paper addresses the recently observed generalization gap problem between large and small batch methods. It has been hypothesized that methods with small and large batches converge to very different optima. This work relates the process of gradient descent with an ultra slow diffusion process and show that this gap can be explained by lower diffusion time for the batch method. Based on this observation a correction is devised that makes the diffusion curves match for different batch sizes. Furthermore, this could help explain why training long past the time where the validation error has plateaued is necessary and still improves performance. This work is quite interesting and will stimulate further work into understanding certain strange properties of SGD.

The experimental section contains experiments on MNIST, CIFAR-10 and 100 as well as Imagenet. They compare against the results of (Keskar et al, 2016) and show that their tricks allow the large batch methods to generalize better than the baseline. One key finding is that the models should be trained much longer with large batches - which in a sense negates their purpose. What is not clear from the experiments is how necessary Ghost-Batch normalization is. Would good results be obtained with regime adaptation only? This would strengthen the story of the paper a bit more.

the Section 4 proposes a multiplicative noise gradient but this is not addressed in the experimental section. It would have been interesting to see some results on this. Even just in Figure 2."
Monte-Carlo Tree Search by Best Arm Identification,"Emilie Kaufmann, Wouter M. Koolen",https://proceedings.neurips.cc/paper/2017/hash/a6d259bfbfa2062843ef543e21d7ec8e-Abstract.html,"This work uses best arm identification (BAI) techniques applies to the monte-carlo tree search problem with two-players (in a turn-based setting). The goal is to find the best action for player A to take by carefully considering all the next actions that player B and A can take in the following rounds. Access to a stochastic oracle to evaluate the values of leaves is supposed, hence the goal is to find approximately (with precision \epsilon) the best action at the root with high confidence (at least 1-\delta). Algorithms based on confidence intervals and upwards propagation (from leaf to the root) of the upper (for the MAX nodes, action by player A) and lower (for the MIN nodes, action by player B the opponent) confidence bounds are proposed. The algorithms are intuitive and well described, and well rooted in the fixed confidence BAI literature. In particular, recent refinements in this line of work are used, but also well separated from the new contributions (Section 3.1) which makes the paper very clear. A theorem on the number of leaf evaluations is given, and the complexity H (3) exhibited and improves on previous work. The comparison with existing algorithms and strategies is precise. Finally, experiments show a big improvement over existing methods, and some insights are given on lower bounds and future directions, which again are grounded in recent results in the BAI fixed confidence setting.

The paper is very well written, and the contributions are made easy to understand and contextualize, thank you. 

Some comments to maybe further improve the clarity of the work. Figure 1 should be pointed to in the text (it is easier to understand immediately what’s going on than line 111). There could be a discussion to explain why you need two candidates a_t and b_t at time t+1, which can be surprising at first. In the experiments, why do you set \delta = 0.1 |\mathcal L|? It is strange in Figure 4 to have \delta = 2.7 (though experimentally this of course does not seem to be a problem).  Is it common in practice to run these algorithms with such little confidence parameter? I would suggest running the experiments with \delta = 0.01*27 instead to avoid confusion, or just \delta = 0.1 (without L). Lemma 7 is hard to parse in its current form, but the example makes this task easier. Could it be possible to add a line in Figure 3 with the optimal proportions (making sure the same \delta is used)? (extremely minor but 259.9 x 1.76 = 457.424, maybe remove the numerical value of kl(\delta, 1-\delta)) Figure 4 is unreadable without 300% zoom on my computer, maybe it could be removed.  I suggest the following change to the figures: adding two columns with name of the algorithm and the total number of samples used, and then displaying proportions instead of raw numbers (same as w^* ine line 281).

Overall, I believe this is a clear and significant contribution with respect to the existing literature.","This paper treats the problem of Monte Carlo Tree Search in depth-k game trees. Even though the problem was considered last year for depth-two trees in 'Optimal best arm identification with fixed confidence' (COLT 2016), the present paper considerably extends this previous work that could not directly be generalized to higher depths. 

They propose a new type of algorithm that can be run with different Best Arm Identification (BAI) algorithms and they provide the analysis for LUCB and UGapE. The idea relies on two main ingredients : First, the BAI algorithm leads the learner in its first decision departing from the root of the tree and then, the subsequent 'representative leaf' is sampled. This leaf is chosen by following a somehow greedy path along the tree, each time choosing the action that maximizes (resp. minimizes) the expected value. 

This line of work sheds a new light on MCTS problems by bringing the tools developed in the framework of Best Arm Identification. The proposed method has several qualities :
1. It is fairly intuitive and easy to understand for the reader having some ground knowledge of the BAI literature;
2. It is computationally efficient, allowing to conduct convincing experiments;
3. It outperforms the existing algorithm FindTopWinner (see comment below).

The authors also prove a problem-dependent lower bound following the now classic technics from e.g. (Garivier and Kaufmann, 2016), (Kaufmann et al., 2015). Even though the final result of Lemma 7 is not fully explicit, the optimization problem can be numerically solved. This allows in particular to have a comparison basis for the experimental section, on top of existing algorithms. Also, as mentioned in the comments of Lemma 7, such a lower bound result gives a hope towards finally obtaining an asymptotically  optimal algorithm following the Track-And-Stop technique from (Garivier and Kaufmann, 2016). This is indeed an exciting perspective, coming with a few other new problems opened by this paper.

I only have minor comments:
- The Lemmas 13 and 14 seems to be the key tools that allow to control the behavior of the algorithm with respect to the choice of the 'representative leaf'. However, this intuition is not really clear in the proof of Theorem 3 that is sketched in the main paper. Space requirements don't make it easy but this could make the idea of the proof even clearer.
- The 'update information about the arms' in the Figure 2 is quite unclear. Actually, the exact action of the algorithm is a bit tricky, I had to read several times to understand whether the algorithm only chooses the first step or all the path of actions and where the information/feedback finally shows up. This could be made a bit clearer in the algorithm pseudo-code maybe.
- l 255 : 'uniformLY at random';
- Lemma 11 in Appendix, please chose i or k to index s;


Overall, I strongly recommend accepting this paper as it opens a new path towards the understanding of MCTS problems and may lead to future major advances. ","\noindent The paper considers the game tree of a two-person zero-sum game. The payoff at each leaf of the tree is assumed to be stochastic and the tree can be of arbitrary length. The goal is to identify the best move at the root of the tree that gives the highest payoff. A PAC setting is considered in this paper where the move can be sub-optimal within a parameter $\epsilon$. Notice that the Best Arm Identification (MAI) problem is such a tree with depth 1. Two algorithms are proposed which are based respectively on UGapE and LUCB. They are essentially the same as in the MAI problem. However, in the present context, the algorithms need to deal with the structure (i.e. depth, branches) of the tree. The algorithms maintain confidence intervals at each leaf node and propagate them upwards to the root. They also maintain a ``representative"" leaf for each internal node of the tree, which corresponds to the leaf that should be sampled if the confidence interval of that internal node needs to be updated. Then the algorithm proceeds similarly as in MAI. In each round, the algorithm identifies the two marginal children of the root, and samples the represetative leaf of one (or both, depending on the algorithm) of them. The stopping rule is exactly the same as the one typically used for MAI. Theoretical upper and lower bounds are derived. Upper bounds are similar to the results of LUCB in MAI and lower bounds are also similar to that in MAI.

In general, the paper is well-motivated and the results presented in the current paper indeed improves upon earlier results both theoretically and experimentally (especially experimentally, in which the new algorithms take only about 1/7 the number of samples by the state-of-the-art algorithm). Moreover, the paper is clear written and easy to follow. However, while the problem considered is a generalization of the MAI problem, the algorithms and all the analysis are too similar to that in MAI. In addition, while the paper by Jamieson et al. 2014 on LIL is listed in the reference, it is not mentioned if that techinique will help to improve the results both experimentally and theoretically. It it known that LIL can improve the CBlike bound in MAI setting. It would be better to investigate this point.

To sum up, the paper is in general a good paper, especially given the significant improvement in experimental results.
"
Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model,"Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun WOO",https://proceedings.neurips.cc/paper/2017/hash/a6db4ed04f1621a119799fd3d7545d3d-Abstract.html,"This paper introduces TrajGRU, an extension of the convolutional LSTM/GRU. Contrary to convLSTM/GRU, TrajGRU aims at learning location dependant filter support for each hidden state location. TrajGRU generates a flow field from the current input and previous hidden state and then warp the previous hidden states through bilinear sampling following this flow field.

Author evaluate their proposal on a video generation on two datasets, MovingMNIST having 3 digits at the same time and HKO-7 nowcasting dataset, where TrajRU outperforms their convolutional counterpart.

Few specific question/remarks:
Did you compare TrajGRU with ConvGRU having a larger support than just a 5x5 kernels? 

Does the TrajGRU requires more computation than a convGRU due to its warping operation? It would be informative to provide the overall parameter, number of operation and running time for the models evaluated in the experiment section.  

Why did you trained the model for a fix number of epoch rather than doing early stopping, could the performances of some model be improved by stopping the training earlier?


- Quality
The paper seems technically sound.


- Clarity
The paper is clear overall. It would be nice to specify the warp method to have more complete view of the TrajGRU. Also it is not clear what are the number of examples and training/validation/test splits for the HKO-7 datasets.


-Originality
Few other works have explored the use of warping for video model. See ""Spatio-temporal video autoencoder with differentiable memory"" for instance. It would to compare/contrast TrajGRU to this approach. 




-Significance/Conclusion
Designing model that learn good video representation is still an ongoing research problem.
This paper propose a novel model  that propose to learn the filter support in addition to filter weight which is an interesting step toward better video model. 

However, the approach is only tested so far on one synthetic dataset (MovingMnist) and one specialized nowcasting dataset. It would be good to see if this model lead to better video representation for more traditional video task such as human action classification with generic videos.
","Summary
This paper describes a new GRU-based architecture for precipitation nowcasting, a task which can be seen as a video prediction problem with a fixed camera position. The authors also describe a new benchmarking package for precipitation nowcasting and evaluate their model on both this benchmark and an altered version of moving MNIST.

Technical quality
I think this is a good application paper while the proposed architecture is interesting as well. For the sake of completeness I would have liked to see some comparisons with LSTM versions of the networks as well and with fully connected RNNs but the authors already did look at a nice selection of baselines. 

Clarity 
The paper is generally well-written and coherent in structure. While I found it easy to follow the definition of the ConvGRU, I found the description of the proposed TrajGRU much harder to understand. The most important change in the new model is the replacement of the state-to-state convolutions/multiplications with a transformation that seems to be a specific version of the type of module described by Jaderberg et al. (2015) (reference 10 in the paper). It is not clear to me how the structure generating network gamma produces the flow fields U and V exactly though. It would help if this was made more explicit and if there would be a more clear emphasis on the differences with the transformation module from Jaderberg et al. (2015). At least the goal of the trajectory selection mechanism itself is clear enough for the rest of the narrative to make sense. 

Novelty
While the general architecture is the same as the ConvLSTM and the paper is somewhat application oriented, the idea of learning trajectories with spatial transformer inspired modules is quite original to me.

Significance
The new data set seems to be a useful benchmark but I know too little about precipitation nowcasting to judge this. I also don't know enough about the state-of-the-art in that field to judge the performance gains very well and since the benchmark is a new one I suppose time will need to tell. The work would have been stronger if there was also a comparison on another existing precipitation nowcasting benchmark. I think that the idea to learn sparsely connected temporal trajectories may have more widespread applications in other domains where the distributions to be modelled are of high dimensionality but display certain independence properties and redundance. 

pros:
Interesting model
Nice baselines and practical evaluation
The paper is generally well-written.
cons:
The description and motivation of the model is a bit short and hard to follow.
The novelty is somewhat limited compared to ConvGRU/LSTM

EDIT: The rebuttal addressed some of the concerns I had about the paper and I think that the additional experiments also add to the overall quality so I increased my score.","This paper makes novel contributions in the following two aspects: proposing a new dataset for nowcasting (if the dataset goes public, the reviewer is not able to find such a claim in the paper though) and applying the idea from 'Spatial Transformer Network' in a convRNN setup to predict the next few frames given a recent history.

The introduction is well written, and the problem is well motivated. The rest of paper is clearly structured, but with following two points that are less satisfactory: 
1. The use of \mathcal{U}_t, and \mathcal{V}_t and their corresponding \gamma function deserves much more detailed discussion as, according to the rest of the paper, this is the only technical difference from the Spatial Transformer Network where a 2*3 affine transformation matrix is learned instead. It is quite unclear why such scheme is abandoned and resorted to continuous optical flow.
2. The experiments are to some certain degree convincing. The measured difference are often statistically not significant (my guess as a reviewer as no confidence interval is given). In addition, as baselines, Conv3Ds often underperform Conv2Ds, which the reviewer finds strange given the nature of the problem. 

The reviewer would like the authors to properly address my above concerns to revise the feedback. 

Addition: The author feedback helps to resolve my questions."
Scalable Model Selection for Belief Networks,"Zhao Song, Yusuke Muraoka, Ryohei Fujimaki, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/a6ea8471c120fe8cc35a2954c9b9c595-Abstract.html,"The authors propose an approach for learning multi-layered sigmoid belief networks, where sparsity is enforced by imposing a regularizer on the number of nodes.  In particular, they employ a variational lower bound on the marginal likelihood, and then applying a Laplace approximation.  The resulting form includes a term that can be viewed as a regularizer on the model.  The experiments show some improvements against other approaches, obtaining better likelihoods.

On the plus side, the basic approach appears to be simple conceptually.  The authors show some desirable properties under some assumptions.  The synthetic experiments show that the approach seems to be doing reasonable things.  On the real world experiments, the authors show that their approach achieves some improvements in terms of (bounds on the) test likelihood, but the improvements appear to be somewhat modest in this regard.  The authors' approach also appears to learn more compact models.  It was compared with other approaches that impose some sparsity, but it was stated several times that other approaches did not observe any shrinkage of nodes.  It would be helpful here, if possible, to get a sense or intuition of why the authors' approach may be more successful at imposing sparsity here compared to other approaches.

I also wonder how this work relates to work such as:

  ""Learning the Structure of Deep Sparse Graphical Models.
  By R. Adams, H. Wallach, and Z. Ghahramani. 
  In AISTATS, 2010.

Here, they proposed the continuous Indian buffet process, that allows the number of nodes in each layer of a belief network to be learned, but also the depth.  In contrast, the authors mostly cite prior works as being able to impose sparsity only on edges, and not on nodes.
","The ""FABIA"" algorithm is used to maximize (a lower bound) on generalized
factorized information criterion (gFIC). gFIC is a model score (here
for SBNs). The presented method can be viewed as an approximate
Bayesian approach where a variational approach is used for
approximation of marginal log-likelihood. (Priors on model structure
aren't considered, so we are implicitly using a uniform one.)

gFIC is well-motivated and the FABIA approach to bounding it is
sensible. The derivations in Section 3 constitute a strong part of the
paper. The big issue is whether this method delivers the promised 4
benefits mentioned at the end of the abstract.


The authors say of Fig 2(a) that:
""In both one- and two-layer cases, the inferred model size is very
close to the ground truth.""
but, in fact, what we see is that the *mean* size is close to the
ground truth, the s.d. is reasonably large.
2(b) does show that the FABIA does better than the NVIL (when NVIL is
not initialised with the correct size).

On Table 1, there is a small win in terms of VLB, but the smaller size
is some advantage. I'm not convinced that compactness per se is
something people care about too much. No one looks at the SBN model
structure to get 'insight'. These SBNs are used for predictions. On
the other hand, as the authors note, a smaller SBN allows ""faster
testing"", ie the SBN output is more rapidly computable. On topic
modelling we again get small wins when comparing to NVIL.

There is progress here. The issue is whether there is enough for a
venue like NIPS. I think there is, but only just.
","The authors propose a variational Bayes method for model selection in sigmoid belief networks. The method can eliminate nodes in the hidden layers of multilayer networks. The derivation of the criterion appears technically solid and a fair amount of experimental support for the good performance of the method is provided.

I have to say I'm no expert in this area, and I hope other reviewers can comment on the level of novelty.

detailed comments:
- p. 1, l. 21: ""Model selection is here the task of selecting the number of layers [...]"": I got the impression that the proposed algorithm only eliminates individual nodes, not entire layers. Please clarify.
- p. 2, ll. 76--77: Are there really no weights on the first layer? You forgot to define b.
- p. 5, ll. 173--175: If nodes with expected proportion of 1's very small can be eliminated, why doesn't the same hold for nodes with expected proportion of 0's equally small? ""When the expectation is not exact, such as in the top layers, [...]"": Please clarify. How can we tell, when the expectation is not exact? And do you really mean 'exact' or just 'accurate', etc. What is the precise rule to decide this.
- p. 6, ll. 226--227: Please explain what 10-5^2 and 25-15 means (is there a typo, or why would you write 5^2 instead of simply 25?).
- p. 6, l. 239: ""Our performance metric is the variational lower bound of the test log-likelihood."" Why use a variational bound as a performance metric? Sounds like variational techniques are an approach to derive a criterion, but using them also to measure performance sounds questionable. Should the evaluation be based on a score that is independent of the chosen approach and the made assumptions?
- references: please provide proper bibliographic information, not just ""JMLR"" or ""NIPS"". Add volume, page numbers, etc."
Collaborative Deep Learning in Fixed Topology Networks,"Zhanhong Jiang, Aditya Balu, Chinmay Hegde, Soumik Sarkar",https://proceedings.neurips.cc/paper/2017/hash/a74c3bae3e13616104c1b25f9da1f11f-Abstract.html,"This paper proposes a variant of stochastic gradient decent for highly decentralized networks where communication is only possible through ""neighbors"" is a distributed device graph.  While this computation model is not realistic for conventional deep learning systems, it addresses the interesting case of trying to learn using a highly distributed network of weak devices, such as cell phones or IOT devices.   This is a setting where distributed methods are not as well studied as in the centralized case, and I think the proposed class of algorithms is more practical in this setting than, say, distributed asynchronous ADMM methods that require updates of dual parameters.  The Lyapunov function analysis presented is fairly elegant and concise.

One criticism I do have of this paper is that convergence requires a fairly complex and uninterpretable assumption (Assumption 3) that seems fairly non-standard for SGD analysis.  Also, It would be nice to see some experiments on convex models (like logistic regression),  where the assumptions are verifiable. ","This paper considers the problem of distributed optimization on networks where machines can only directly communicate with neighboring machines in rounds, as specified by a weighted incidence matrix. 

The method they propose is a version of the algorithm from Nedic and Ozdaglar [17], with the deterministic gradients replaced by stochastic ones. The authors do not clearly explain this in their related work, just stating that “the proposed algorithms have similarities”.

As it currently is, I'm not happy with the statement of the algorithm. It is not sufficiently detailed for me to know exactly what is intended. In particular, the quantity g_j is referred to as a stochastic gradient without any further elaboration. My best guess is that it's intended to be chosen via without-replacement sampling, as they refer to shuffling the data in their pseudo-code. I think the proof assumes that the data are sampled without-replacement though (please clarify if this is the case). In either case, the pseudo-code of the algorithm must make this clear.

The momentum variant is interesting, I think the paper would be much stronger if a theoretical analysis of it was included.","This paper explores a fixed peer-to-peer communication topology without parameter server. To demonstrate convergence, it shows that the Lyaounov functions that is minimized includes a regularizer term that incorporates the topology of the network. This leads to convergence rate bounds in the convex setting and convergence guarantees in the non-convex setting.

This is original work of high technical quality, well positioned with a clear introduction. It is very rare to see proper convergence bounds in such a complex parallelization setting, the key to the proof is really neat (I did not check all the details). As we know, insight from convex-case proofs usually apply to the non-convex case, especially as the end convergence should be approximately convex. Thanks to the proof, one can predict the impact of the topology over convergence (through the eigenvalues). This proof should also open to further work. Experiments are also well designed, thought they do not cover the hypothetical situation where a parameter server could fail.

However, several critical limitations will impact the short term significance. Some of them could probably be addressed by the time of publication.

First, the loss in both training speed and convergence accuracy of CDSGD compared to plain SGD is very large, and is only mitigated with the use of a momentum. While the authors show how the algorithm can be modified to support both Polyak and Nesterov momentums, I could not find anywhere which momentum CDMSGD actually use! There is no explanation of why this momentum helps performance so much.


Second, compared to other data-distributed SGD approaches such as Federated Averaging, the main practical contribution of the paper is to get rid of the parameter server, at some efficiency cost on controlled experiments (figure 2.b). So this work significance would greatly improve if the authors clearly identified the reasons for not using a parameter server. In the introduction, they hint at better privacy preserving (if I understood right), but Federated Averaging also claims privacy preserving WITH a parameter server. In the experiments, final accuracy with momentum is better than Federated averaging: is this because of the decentralized approach or the use of a momentum? Note also that this paper does not seem to mention the main reason I have seen for using decentralized approach, which is robustness in case of failure of the central server.

Last, the stochastic Lyapunov gradient (eq 7) shows that the regularization can be very strong especially with small learning rates, and the effects are visible in the experiments. What this regularization amounts to would be worth studying, as this is a striking side-effect of the algorithm.

Potential issues:

Line 92: f_j(x)=1/N f(x) does not make sense to me. 
It should be  f_j(x)=1/N sum_i f^i_j(x) ??

Line 119: it implies that all eigenvalues are non-zero, which contradicts the fully connected topology where they are all zero but one.

Line 143: ""Gradient of V(x) also has a Lipshitz"": you either remove gradient or replace 'has' with 'is'.
"
On the Complexity of Learning Neural Networks,"Le Song, Santosh Vempala, John Wilmes, Bo Xie",https://proceedings.neurips.cc/paper/2017/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html,"The authors proved a lower bound suggests that even the combination of small network size, smooth, standard activation functions, and benign input distributions is insufficient to make learning neural networks easy, which is quite interesting. However, we expect the authors provide more discussions on how to avoid such poor situations actively.","The paper gives an exponential lower bound to the complexity of learning neural networks in the statistical query model. The result is significant in that
it applies to a large class of natural learning algorithms (such as SGD) and its assumptions are also natural. The proof is based on extending the statistical
dimension approach to the present framework.

The paper is a technically solid, elegant piece of work which brings together several research directions. 

One small comment: the connection to [12] described in Section 1.3 could be more specific.
","The paper presents a new information-theoretic lower bound for learning neural networks. In particular, it gives an exponential statistical query lower bound that applies even to neural nets with only one hidden layer using common activation functions and log-concave input distributions. (In fact, the results apply to a slightly more general family of data distributions.) By assuming log-concavity for the input distribution, the paper proves a lower bound for a more realistic setting than previous works with applied to discrete distributions.

To prove this SQ lower bound, the paper extends the notion of statistical dimension to regression problems and proves that a family of functions which can be represented by neural nets with one hidden layer has exponentially high statistical dimension with respect to any log-concave distribution. (I was surprised to read that statistical dimension and SQ lower bounds have not been studied for regression problems before; however, my cursory search did not turn up any previous papers that would have studied this problem.) It is not clear to me if there are any novel technical ideas in the proofs, but the idea of studying the SQ complexity of neural networks in order to obtain lower bounds for a more realistic class of nets is (as far as I can tell) novel and clever.

The specific class of functions which are used to prove the lower bound is, like in most lower bound proofs, pretty artificial and different from functions one would encounter in practice. Therefore the most interesting consequence of the results is that reasonable-looking assumptions are not sufficient to prove upper bounds for learning NN's (as the authors explain in the abstract). On the other hand, the class of functions studied is not of practical interests, which makes me question the value of the experiments in the paper (beyond pleasing potential reviewers who dislike papers without experimental results). With the tight page limits of NIPS, it seems like the space taken up by the experimental results could have a better use.

My main problem with the paper is the clarity of the write-up. I find the paper hard to read. I expect technical lemmas to be difficult to follow, but here even the main theorem statements are hard to parse with a lot of notation and formulas and little intuitive explanation. In fact, even the ""Proof ideas"" section (1.4), which I expected to contain a reader-friendly, high-level intuitive explanation of the main ideas, was disappointingly difficult to follow. I had to reread some sentences several times before I understood them. Of course, it is possible that this is due to my limitations instead of the paper's, but the main conceptual ideas of the paper don't seem so complex that this complexity would make it impossible to give an easily readable overview of the theorems and proofs.

Overall, this seems to be a decent paper that many in the NIPS community would find interesting to read but in my opinion a substantial portion of it would have to be rewritten to improve its clarity."
A Sample Complexity Measure with Applications to Learning Optimal Auctions,Vasilis Syrgkanis,https://proceedings.neurips.cc/paper/2017/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html,"Recently there has been a lot of interest in the Algorithmic Game Theory community in designing approximately revenue optimal auctions given a small number of samples of the buyer's valuations. A pertinent question is what is the minimum number of samples required to design a truthful auction that provides an additive epsilon approximation to the optimal revenue. A bound on this quantity is usually obtained by relating it to learning theory concepts like VC dimension and pseudo-dimension. This paper proposes a new notion of split-sample complexity and applies to obtain bound on the sample complexity for common auction scenarios. 

The notion of split-sample complexity is as follows: Given a set of m samples, for any given subset of size m/2, we can identify an optimal hypothesis. Denote by \hat{H}_S the set of optimal hypotheses over all size m/2 subsets of S. The split sample growth rate is how large the set \hat{H}_S can get over all sets of size S. 

The authors show that given m samples, the expected revenue obtained from the optimal hypothesis over these m samples, can be related to the optimal revenue for the given distribution and an additive error term. This additive error term relates to the split-sample complexity as a function of m. Then for specific classes of auctions sample complexity bounds can be obtained by bounding the split-sample complexity.

This new approach allows the author to strengthen previous bounds for bundle and item pricing mechanisms by Morgernstern and Roughgarden. 

Overall this paper is well written and provides a new technique for tackling an ongoing research agenda. 



","The paper introduces a new hypothesis class complexity measure called ""split-sample growth rate"", which counts the number of possible hypotheses ERM can output on a subsample of half the data, and proves generalization bounds using this measure. The utility of this measure is demonstrated by application to optimal auctions, which have particularly simple split-sample growth rates.

I found the simplicity of the approach compelling, and the results significant, though the paper was quite dense and not easy to read in places. A major omission in my opinion is a conclusion which comments on the potential for ""split-sample growth rate"" to find applications beyond auctions, and I ask the authors to comment on this in the author response.

Line 23- remove first comma
Line 106- by doing so- not show
Lemma 1- statement and proof are fairly dense and difficult to understand
Line 160- missing closing }
Line 162- it would be nice if the authors could show this derivation","Summary:
Rademacher complexity is a powerful tool for producing generalization guarantees. The Rademacher complexity of a class H on a sample S is roughly the expected maximum gap between training and test performance if S were randomly partitioned into training and test sets, and we took the maximum over all h in H (eqn 11).  This paper makes the observation that the core steps in the standard generalization bound proof will still go through if instead of taking the max over all h in H, you only look at the h's that your procedure can possibly output when given half of a double sample (Lemma 1).  While unfortunately the usual final (or initial) high-probability step in this argument does not seem to go through directly, the paper shows (Theorem 2) that one can nonetheless get a useful generation bound from this using other means.  The paper then shows how this generalization bound yields good sample complexity guarantees for a number of natural auction classes.  In several cases, this improves over the best prior guarantees known, and also simplifies their analysis.

Evaluation:
While the core idea is to some extent an observation, I like the paper because it is a nice, cute idea that one could actually teach in a class.  And it allows for a simpler sample complexity analysis for auctions, which is great.  It may also have other uses too.  On the negative side, I wish the paper were able to answer whether the linear dependence on 1/delta is really necessary (see below).  That would make the paper feel more complete to me.  Still, I am overall positive.

Questions:
Do you know if the linear dependence on delta in Theorem 2 is required?  I see why you need it in your proof (due to using Markov) and you explain nicely why the usual use of McDiarmid to get an O(log(1/delta)) bound doesn't go through.  But this leaves open whether (a) there is a different way to get an O(log(1/delta)) bound, or on the other hand (b) there exist D,H for which you indeed have a more heavy-tailed chance of failure.   If this were resolved, the paper would feel more complete.

On a related note, do you know if you can get a bound along the lines of Theorem 2 in terms of the maximum # of hypotheses that can be produced from half of the *actual* sample S? (That is, replacing the sup over S of size m in the definition of \hat{\tau} with the actual training sample S?)

Suggestions: 

- If I am not mistaken, The proof of Lemma 1 basically goes through the textbook Rademacher bound proof, changing it where needed to replace H with \hat{H}_{S \union S'}.  That's fine, but you should explicitly tell the reader that that is what you are doing up front, so they know what's new vs what's old.  E.g., you have a nice discussion of this form for Theorem 2, where you talk about where it differs from the usual argument, that I think is quite useful.

- In Section 4 you examine several interesting scenarios (single bidder, multiple iid regular,...) , and for each one you say ""In this case, the space of hypotheses H is ..."".  I think what you mean to say is more that ""In this case, it is known that an *optimal auction belongs to* the hypothesis class H of ..."".  In other words, the generalization guarantees apply to any H you want, but now you are saying what the meaningful H is in each scenario, right?  I think this is worth saying explicitly, since otherwise the reader might get confused why H is changing each time.

Also I'll point out that the idea of counting the number of possible outputs of an auction mechanism in order to get a sample complexity bound has been used in the auction literature before. The first I know of is the FOCS 2005 paper ""Mechanism design via machine learning"" by Balcan, Blum, Hartline, and Mansour, though that paper operates in a transductive setting where an argument of this type is much easier to make. "
On Optimal Generalizability in Parametric Learning,"Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, Vahid Tarokh",https://proceedings.neurips.cc/paper/2017/hash/a82d922b133be19c1171534e6594f754-Abstract.html,"As is well known, the leave-one-out CV (LOOCV) error can be efficiently computed for ridge regression, thanks to efficient formulas for adjusting the loss function value when one sample point is removed. The authors propose an approximate formula for LOOCV for other models by generalizing this idea.

I haven't seen this proposed before. Looks like the idea has plenty of potential.

The paper is really well written and polished, and appears technically solid.

I could't find any typos or other minor details to fix, except:
- references: please use capital initials in ""Bayesian"" and for all words in book and journal titles","This paper proposes an efficiently computable approximation of leave-one-out cross validation for parametric learning problems, as well as an algorithm for jointly learning the regularization parameters and model parameters. These techniques seem novel and widely applicable.

The paper starts out clearly written, though maybe some less space could have been spent on laying the groundwork, leaving more room for the later sections where the notation is quite dense.

Can you say anything about the comparsion between ALOOCV and LOOCV evaluated on only a subset of the data points (as you mention in l137-140), both in terms of computation cost and approximation accuracy?

Other comments:
l75: are you referring to PRESS? Please name it then.
l90: ""no assumptions on the distribution"" -- does that mean, no prior distribution?
Definition 7: the displayed equation seems to belong to the ""such that"" in the preceding sentence; please pull it into the same sentence. Also, I find it odd that an analytic function doesn't satisfy this definition (due to the ""there exists one and only one""). What about a two-dimensional function that has non-differentiabilities in its uppper-right quadrant, so that along some cross sections, it is analytic?
l186-187: (relating to the remarks about Def7) This sounds a bit odd; it might be better to say something like ""We remark that the theory could be extended to ..."".
l250: Are you saying that you are not jointly learning the regularization parameter in this second example? If the material in section 4 doesn't apply here, I missed why; please clarify in that section.

Typographic:
l200: few -> a few
References: ensure capitalization of e.g. Bayes by using {}"
K-Medoids For K-Means Seeding,"James Newling, François Fleuret",https://proceedings.neurips.cc/paper/2017/hash/a8345c3bb9e3896ea538ce77ffaf2c20-Abstract.html,"The authors propose to use a particular version of the K-medoids algorithm (clarans - that uses iterative swaps to identify the medoids) for initializing k-means and claim that this improves the final clustering quality. The authors have also tested their claims with multiple datasets, and demonstrated their performance improvements. They have also published code that will be made open after the review process.

Comments:
1. The paper is easy to read and follow, and the authors have done a good job placing their work in context. I appreciate the fact that the optimizations are presented in a very accessible manner in Section 4. As the authors claim, open source code is an important contribution.

2. The authors should consider the following work for inclusion in their discussions and/or comparisons:
Bahmani, Bahman, et al. ""Scalable k-means++."" Proceedings of the VLDB Endowment 5.7 (2012): 622-633.


3. While the authors have compared actual run times of different algorithms in Section 5, can they also provide some idea of the theoretical complexity (whenever possible)?

4. Related to previous comment: how much impact would the actual implementation have on the runtimes, particularly given that the “bf” algorithm uses BLAS routines and hence can be implicitly parallelizing matrix operations. Is there any other place in the implementations where parallel processing is done?

5. Please number the data sets in Table 3 to make it easy to compare with table 4.

6. In column 1(km++) of Table 4, why is it that km++ does not run unto 90 iterations always given that TL was based on this time?

7.  In 5.2, the claim that “clarans often finds a globally minimal initialization” seems a bit too speculative. If the authors want to claim this they must run many many rounds at least in a few datasets, and show that the performance cannot get better by much. 
","This manuscript provides good motivation for why existing initialization schemes for k-means (such as k-means++) are susceptible to getting stuck in local minima and propose the use of a k-mediods initialization scheme named ‘clarans’ (originally published in 1994) for k-means initialization. The authors provide intuition and explanation for why clarans would be a good initializer. The manuscript also details a series of computational optimizations to the clarans algorithm to address the complexity of the original clarans algorithm. The thorough evaluation supports the claims of the authors and the evaluation decisions and details are very clearly explained in the manuscript, making it very easy to reproduce if needed. 

However, in my opinion, this manuscript lacks any novelty unless (a few of) the optimizations to the clarans presented  in the manuscript are new. That does not mean that I think this manuscript does not make a significant contribution. 

Another concern I have is the following: While the empirical evaluation in the manuscript appears to be quite thorough, a crucial angle missing in the relative performance of clarans is dependence on the number of clusters k. For example, questions such as the following should be addressed (especially in a empirical only paper): does clarans outperform all other initialization schemes for all value ranges of k (on the same dataset) or is clarans better for large k while being at par with k-means++ for small k? ","This paper studies the utilization of CLARANS method  (proposed by Ng and Han in 1994) for k-means initialization. 
CLARANS is a k-medoids method, which finds clusters with medoids as cluster centers. These medoids can be considered as the most representative data samples, as they minimize the sum of dissimilarities between themselves and their cluster members.
The initialization of k-means has been aware of a key to guaranteeing the optimal clustering results. Various approaches have been proposed to find good initialization for k-means. Since k-medoids can find the k most representative data samples, they can be naturally a set of good initial centers for k-means to start.
In this paper, authors experimentally show that CLARANS is a better initializer for k-means than other algorithms on 23 data sets, in terms of the minimization of MSE. 

The paper is easy to follow. The extensive evaluation results support the claim that CLARANS is a good initializer for k-means. However, there are several concerns:
1. The novelty of this paper is marginal. CLARANS and k-means are already well-known clustering algorithms. 
2. The study of CLARANS + k-means and comparison with other approaches are mostly from empirical evaluation perspective. The theoretical soundness is weak.
3. CLARANS is computationally expensive. Although different levels of complexities are discussed, CLARANS is difficult to be applied on large-scale data sets for selecting good initial centers.
 
Considering the above weak points, reviewer suggests that the paper should be highly improved before publishing at NIPS.
"
Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction,"Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang, Nicu Sebe",https://proceedings.neurips.cc/paper/2017/hash/a869ccbcbd9568808b8497e28275c7c8-Abstract.html,"This paper presents an interesting approach for structured prediction by introducing gated attention model in learning and fusing the multi-scale features for image labeling. The method was specifically illustrated on edge/boundary detection and promising results have been demonstrated.

Using both gating function and attention models have been very popular lately but this paper tackles the multi-scale learning and fusion problem from a new angle. The proposed algorithm is well motivated and a reasonable formulation within fully convolutional neural networks. Promising results on various edge detection benchmarks have been shown. However, it will be more convincing if the generic task of image labeling such as PASCAL and COCO can be shown.

This is an interesting paper with a good idea and promising results. It will be more convincing to see how it is applied to the generic image labeling task. ","The main contribution of this paper is adding gates to a probabilistic graphical model to control message passing. Application wise, the author claims they're the first paper adding attention to contour detection.

In terms of the contribution, as far as I know, there're already previous works on adding gates, or attention into probabilistic graphical model in a variety of ways. I just listed a few here:

[1] How to generate realistic images using gated MRF’s
[2] Modeling Natural Images Using Gated MRFs 
[3] Semantic Object Parsing with Graph LSTM
[4] Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition
[5] Gated Graph Sequence Neural Networks
[6] Interpretable Structure-Evolving LSTM

The author did a good rebuttal. I agree that this paper provided a new type of feature CRF with mathematically grounded attention. Although the model has quite a overlap with previous works, it still provides some new ideas. The experimental results also show corresponding improvement. Authors should express the model and figure in a more clear way though (e.g. the chain graph). The hidden variable ""h"", label ""y"" should all be in the formulation.","This paper proposes a gating mechanism to combine features from different levels in a CNN for the task of contour detection. The paper builds upon recent advances in using graphical models with CNN architectures [5,39] and augments these with attention. This results in large improvements in performance at the task of contour detection, achieving new state-of-the-art results. The paper also presents an ablation study where they analyze the impact of different parts of their architecture.

Cons:
1. Unclear relationship to past works which use CRFs with CNNs [5,39] and other works such as [A,B] which express CRF inference as CNNs. The paper says it is inspired from [5,39] but does not describe the points of difference from [5, 39]. Is only attention the difference and thus the novelty? Does the derivation in Section 2 follow directly from [5,39] (except taking attention into account) or it requires additional tricks and techniques beyond [5,39]? How does the derivation relate to the concepts presented in [A]? Not only will these relationships make it easier to judge novelty, it will also allow readers to contextualize the work better.

2. Intuitively, I dont understand why we need attention to combine the information from different scales. The paper is fairly vague about the motivation for this. It would help if the rebuttal could provide some examples or more intuition about what a gating mechanism can provide that a non-gating mechanism can't provide.

3. The paper presents very thorough experimental results for contour detection, but I will encourage the authors to also study at the very least one other task (say semantic segmentation). This will help evaluate how generalizable the technique is. This will also increase the impact of the paper in the community.

4. The paper is somewhat hard to read. I believe there are some undefined variables, I haven't been able to figure out what f_l^{D,C,M} in figure 2 are.

5. There are minor typos in Table 2 (performance for [14] is misreported, ODS is either 70.25 or 71.03 in [14] but the current paper reports it as 64.1), and these numbers are not consistent with the ones in Figure 4. I would recommend that the authors cross-check all numbers in the draft.

[A] Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials
Philipp Krähenbühl and Vladlen Koltun
NIPS 2011

[B] Conditional Random Fields as Recurrent Neural Networks. ICCV 2015"
Geometric Descent Method for Convex Composite Minimization,"Shixiang Chen, Shiqian Ma, Wei Liu",https://proceedings.neurips.cc/paper/2017/hash/a8abb4bb284b5b27aa7cb790dc20f80b-Abstract.html,"Summary: The paper extends the recent work of Bubeck et al. on the geometric gradient descent method, in order to handle non-smooth (but “nice”) objectives. Similar work can be found in [10]; however, no optimal rates are obtained in that case (in the sense of having a square root condition number in the retraction factor). The core ideas and motivation origin from the proximity operators used in non-smooth optimization in machine learning and signal processing (see e.g., lasso).

Quality: The paper is of good technical quality - for clarity see below. The results could be considered “expected” since similar results (from smooth to non-smooth convex objectives with proximal operators) have been proved in numerous papers the past decade; especially under convexity assumptions.

Technical correctness: I believe that the results I read in the main are correct.

Impact: The paper might have a good impact on the direction of demystifying Nesterov’s accelerated routines like FISTA (by proposing alternatives). I can see several ramifications based on this work, so I would say that the paper has at least some importance.

Clarity: My main concern is the balance of being succinct and being thorough. The authors tend towards the former; however, several parts of the paper are no clear (especially the technical ones). Also, the whole idea of this line of work is to provide a better understanding of accelerated methods; the paper does not provide much intuition on this front (update: there is a paragraph in the sup. material - maybe the authors should consider moving that part in the main text).

Comments:
1. Neither here, nor in the original paper of Bubeck it is clear how we get some basic arguments: E.g., I “wasted” a lot of time to understand how we get to (2.2) by setting y = x^* in (2.1). Given the restricted review time, things need to be crystal clear, even if they are not in the original paper, based on which this work is conducted.
Maybe this can be clarified by the rest of the reviewers. Assuming y = x++, this gives the bound x* \in B(x++,  2/\alpha^2 * || \nabla f(x) ||_2^2 - 2/\alpha (f(x) - f(x++))), but f(x*) \leq f(x++), so we cannot naively substitute it there. Probably I’m missing something fundamental.   

Overall: There is nothing strongly negative about the paper - I would say, apart from some clarity issues and given that proofs work through, it is a “weak accept”.","The paper proposes an extension of the geometric descent method of Bubeck, Lee and Singh proposed [1]. The extension is to the case of composite optimization objectives, something which was already achieved by Drusvyatskiy, Fazel and Roy [10] for a (slightly suboptimal) variant of the algorithm.

The theory part of the paper finally provides a useful discussion of backtracking (which is an ok but not earth-shaking contribution), and ends with a limited memory discussion which however seems not to give much novel on top of [9,10].

The experiments are comparing to Nesterov's accelerated method, and show in my view very promising results for the geometric method. The experiments are performed on very relevant machine learning models and real datasets.
For reproducibility: What was used to stop algorithm 1 + 2 (the internal subroutines? See also next comment).

Summary: I find this topic highly interesting, and it could prove fruitful in view of the huge popularity but limited theoretical understanding of accelerated methods and momentum in deep learning and other applications.
My main concern is if the paper here makes enough new contributions on top of the already existing work of [1,9,10], some of which have already covered the composite case. The paper though is nicely written and I find the experiments convincing.

Comments: For satisfying the necessary conditions in each step, the authors proposed the Brent-Dekker method and semi-smooth Newton for two univariate subproblems. Please comment more clearly that those subproblems are univariate, and give more details why/when these two algorithms are suitable, along with discussing the complexity and impact for overall complexity, in contrast to e.g. binary search. Crucially: What accuracy is needed for the convergence to hold? (Exact root finding will be intractable in both Algo 1 and 2 I assume).

== Update after author feedback ==
Thanks for answering my question; keeping my assessment for now"
Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications,"Qinshi Wang, Wei Chen",https://proceedings.neurips.cc/paper/2017/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html,"The paper studies the stochastic combinatorial semi-bandit problem. The authors use a general framework in which the actions trigger the arms in a stochastic fashion. Previous studies have that the minimal probability that an arm is triggered appears in the upper bounds proved on the expected regret of the state-of-the art-algorithm. This paper shows that under some smoothness condition this annoying term will not appear in the bound. Intuitively, if the contribution of the arm to the expected rewards of any solutions is small then we don't need much information about it. Moreover, they prove that in general the minimal probability term is unavoidable by providing a lower bound that contains this term. 

The paper is clearly written. The contribution of the paper is mostly technical but is of interest. Providing a lower bound and a nice condition in which case the minimal probability term disappears is nice though still somehow unsurprising. The analysis of the paper applies to a large general framework and make a flurry of contributions to a series of related works.

 Can you comment of the assumption in line 182. How big is this assumption? It seems to me that it is a pretty constraining assumption that limits the reach of the paper. Is there a lot of interesting examples where the probability of triggering base arm i with action S is determined by the expectation vector μ ? 

Line 54-55: weird formatting.","
The authors condider the problem of Combinatorial Semi-Bandits with triggered arms. By introducing a new assumption based on triggered probability, they can get rid of a factor 1/p in the regret bounds, where p* is the smallest triggered probability.

The literature is well-cited and the paper is well-written. I really enjoyed how there general general analysis allows to recover and improve several existing settings: CMAB, influence maximization, cascading bandits or linear bandits. There assumption is furthermore quite intuitive: the less an arm can be triggered, the lower is his impact on the expected reward. I however did not have time to read the proofs in the appendix.

1. Wouldn't the analysis could be performed for arbitrary unknown non-parametric set of distribution D, since the algorithm only uses the mean estimates.

2. Your lower bound uses rewards in [0,1/p]. What about the case were the rewards are bounded in [0,1]? Do you think, the factor 1/p is still necessary?

3. Do you have examples, where the set of super-arms is infinite or continuous while there are only a finite number of arms?

Typos:
- l55: ""both"" -> ""Both""
- l236: ""[1,1]"": is it necessary?
- l237: I did not understand why a set of super-arms j, only generate regret at most ""O(2^{-j} log T/\delta_i^2)""? Isn't it the regret of a single super-arm in this set?","Overview: The paper studies combinatorial multi-armed bandit with probabilistically triggered arms and semi-bandit feedback (CMAB-T). The paper improves the existing regret bounds by introducing a new version of the bounded smoothness condition on the reward functions, called triggering probability modulated bounded smoothness condition. The intuition behind this is that hard-to-trigger arms do not contribute much to the expected regret. With this new condition, the authors are able to remove a potentially exponentially large factor in the existing regret bounds. The paper then shows that applications such as influence maximization bandit and combinatorial cascading bandit both satisfy this condition. Finally, the paper gives a lower bound and shows that for general CMAB-T problems, the exponential factor is unavoidable. 

Quality: The paper provides rigorous proofs for all its claims. 

Clarity: The paper is well-organized and clearly-written. Everything is well-defined and well-explained. The paper also has a nice flow to it. My only complaint is that a concluding section is missing. The authors may also want to proofread for typos and grammatical errors. 

Originality: The paper strengthens an existing condition and gives a tighter regret bound for CMAB-T problems. The paper also applies this new condition to influence maximization bandit and combinatorial cascading bandit. The authors introduce several new techniques in their proofs. 

Significance: The paper greatly improves the regret bound for CMAB-T problems with a new condition that has many applications. 
"
Matching neural paths: transfer from recognition to correspondence search,"Nikolay Savinov, Lubor Ladicky, Marc Pollefeys",https://proceedings.neurips.cc/paper/2017/hash/a8ecbabae151abacba7dbde04f761c37-Abstract.html,"The work proposed a method for correspondence search at the lowest level (e.g., pixels in images) using the activation paths in neural networks (CNN in particular). The high level idea is that if two pixels are corresponding ones in a reference and a searched image if they lead to similar activation patterns (parallel paths) from deep neural networks. The work uses backward dynamic programming (polynomial time algorithm) to compute the matching score between two pixels with exponential number of such paths. Experimental results on the stereo matching task showcase that the proposed method is competitive among non-learning-based methods. 

The proposed method is intuitive. The experiments have been limited to stereo matching, finding camera correspondence. Will the proposed method work with finding correspondence of objects placed under different background? 

The authors suggested that using the full hierarchy of neural networks improves the performance of the proposed method. How big an impact does the structure of the neural network itself play in the performance? Whether or not it would be beneficial to re-tune the CNN on the images from the experiment tasks? 

Minor comment: 
1. Please explain the figures in more details. What does the coloring in the third row in Figure 2 and 3 mean? ","The paper proposes a method to use a pre-trained network for image parts matching.
This is an interesting and new way to transfer knowledge learned for recognition to a
the quite different task of correspondance search. The method aims at comparing 
the activation values of several layers for two images and searching for similar 
activation paths across the layers. Since the number of possible paths is exponential
in the number of layers, this work introduces an algorithm to aggregate all of them 
and solve the correspondance search in polynomial time.

+ the paper is really well written and curated in the details
+ the method is new and sound
+ the experiments demonstrate the effectiveness of the proposed approach

Among the methods that deal with correspondence and has a learning phase, this 
recent work 
C. B. Choy, J. Gwak, S. Savarese, M. Chandraker, Universal Correspondence Network, in Neural Information Processing Systems (NIPS), 2016. 
was not cited and should be discussed in the text. Since two other learning-based
methods are considered in the experiments, also this one could be used as reference.

One high level question: how is this approach dependend on the original 
pre-trained network? Here the paper builds over VGG16, but could it be used 
on other architectures (simples ones: alexnet, more complicated: Inception)
and how would it influence the results?

Among the ","This paper presents a method for determining spatial correspondences between two images using a pre-trained convolutional network, without needing to train on the specific correspondence task.  Given two images, the matching similarity at each pixel location is defined as the sum total similarity between all pairs of possible paths through network, starting at each location and its corresponding offset location in the other image.  For a single offset value, this results in a recurrence relation dependent on the next layer up, which enables a dynamic-programming-like algorithm to compute all costs in linear time using a single backwards pass.  In this way, a multi-layer matching is performed for each offset, accounting for a hierarchy of scales and features.

This algorithm is applied to the task of stereo matching, where it achieves performance slightly lower than SOA supervised methods that require retraining, but better than a baseline computing correlation between stacked feature maps.

This idea is an interesting way to use a pretrained network for additional uses.  However, I think it could be further explored and evaluated, and perhaps more simply explained.  In particular, I would like to see whether it can be applied to other dense matching tasks, e.g. video optical flow, and compared to other dense correspondence methods, such as Deep Flow and SIFT Flow (in both speed and accuracy).  In addition, the notation seems a little overly complex and can be hard to follow (though I was able to understand it) --- I think it would be good to try simplifying this if possible.

Overall, I think this is an interesting method, but feel it could be further explored for more image correspondence applications.


Additional detail:  Twice the paper says the derivation of the algorithm makes use of the ""associative"" property, but this is actually the distributive property (not associative).
"
Convergence Analysis of Two-layer Neural Networks with ReLU Activation,"Yuanzhi Li, Yang Yuan",https://proceedings.neurips.cc/paper/2017/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html,"This paper provides convergence guarantees for two-layer residual networks with ReLU activations using SGD. This is an important problem and will be interesting to NIPS community. In particular, authors show that when the input is Gaussian and the ground truth parameters (the residual part W^*) are small enough, SGD will converge to the global optimal, which is also the ground truth in the considered setting. Preliminary experiments verify the claim on both synthetic data and real-world data. The paper is in general well written and a nice proof flowchart is provided to help readers. 

The two-phase analysis is interesting and novel, where in the first phase a potential function is constructed and proved to decrease with the iterations of SGD and in the second phase a classical SGD analysis is employed following one-point strong convexity. 

Potential issue in proof: We found a potential flaw in the proof for Lemma 2.5 (the convergence analysis of SGD). In line 778 of the appendix, the second equality should lead to $\langle W_t - W^*, \eta G_t \rangle$ instead of $\langle W_t - W^*, \eta \nabla f(W) \rangle$. These two formulations are equivalent to each other if they are in expectation, but the current results in Lemma 2.5 are not in expectation.

Comparison to result of [ZSJ+2017]: a) it's important to note that the works are independent as that paper has not been presented so far, b) however, both the papers provide very similar Guarantees as this paper require ""small"" norm of the residual parameter while [ZSJ+2017] require good enough initialization, c) this paper however provides analysis of SGD which is more practical algorithm than GD. 

Overall, the paper presents an interesting result for two-layer neural networks and should be of interest to the NIPS community. However, the proof might have an issue that requires clarification. 

Refs:
[ZSJ+ 2017] Recovery Guarantees for One-hidden-layer Neural Networks, ICML 2017, https://arxiv.org/pdf/1706.03175.pdf
","This paper discusses the convergence property of SGD for two layer NN with ReLU and identity mapping.

The authors argue that SGD converges to a global minimizer from (almost) arbitrary initialization in two phases: 
* Phase 1: W is drifting in wrong directions but a potential function g(W) = || W* + I ||_{2, 1} -  || W* + I ||_{2, 1} decreases
* Phase 2: W converges to W* at a sublinear rate

The analysis is a combination of local one-point restricted strong convexity (used to show Phase 2) + guarantees for entering this benign region (Phase 1).

I didn't check the proofs line by line, but I'm a bit unclear about the statements of Lemma 5, I would kindly ask the authors to elaborate the following:

First, W^t needs to satisfy the one-point strongly convex property for all t. We need to control ||W^t||_2 < gamma -- the upper bound for ||W^*||_2. I think for this you need some smoothness assumption. Could you please point me where how do you ensure this?

Second, the authors assumes that the iterates W^t stays in the one-point RSS region with diameter D.  In line 174, you mentioned D = sqrt(d)/50, where d is the input dimension.  This means the number of iterations you need must be larger than dimension d. I think this might be a bit weak, as many convergence results for nonconvex optimization are dimension free.  

Thanks!","This paper proves the convergence of a stochastic gradient descent algorithm from a suitable starting point to the global minimizer of a nonconvex energy representing the loss of a two-layer feedforward network with rectified linear unit activation. In particular, the algorithm is shown to converge in two phases, where phase 1 drives the iterates into a one-point convex region which subsequently leads to the actual convergence in phase 2. 

The findings, the analysis, and particularly the methodology for proving the convergence (in 2 phases) are very interesting and definitely deserve to be published. 

The entire proof is extremely long (including a flowchart of 15 Lemmas/Theorems that finally allow to show the main theorem in 25 pages of proofs), and I have to admit that I did not check this part. 

I have some questions on the paper and suggestions to further improve the manuscript:
- Figure 1 points out the different structure of the considered networks, and even the abstract already refers to the special structure of ""identity mappings"". However, optimizing for W (vanilla network) is equivalent to optimizing for (W+I), such that the difference seems to lie in different starting points only. If this is correct, I strongly suggest to emphasize the starting point rather than the identity mapping. In this case, please shorten the connection to ResNet. 
- There is a fundamental difference between the architecture shown in figure 1 (which is analyzed in the proofs), and the experiments in sections 5.1, 5.4, and 5.5. In particular, the ResNet trick of an identity shortcut behaves fundamentally different if the identity is used after a nonlinearity (e.g. BatchNorm in Figure 6 and ReLu in the ResNet Paper), opposed to an equivalent reformulation of the original network in Figure 1. Moreover, sections 5.1, 5.4, and 5.5 consider deep networks that have not been analyzed theoretically. Since your theoretical results deal with the 2-layer architecture shown in Figure 1, I suggest to reduce the numerical experiments that go well beyond the theoretically studied case, and instead explain the latter in more detail. It certainly contains enough material. In particular, it would be interesting to understand how close real-world data is to satisfying the assumptions of Theorem 3.1. 
- For a paper of the technical precision such as the one presented here, I think one should not talk about taking the derivative of a ReLU function without a comment. The ReLU is not differentiable whenever any component is zero - so what exactly is \nabla L(W)? In a convex setting one would consider a subgradient, but what is the right object here? A weak derivative?
- Related to the previous question, subgradient descent on a convex function already requires the step size to go to zero in order to state convergence results. Can you comment on why you can get away with constant step sizes?
- The plots in Figures 7 and 8 are difficult to read and would benefit from more detailed explanations, and labeled axes. 
- I recommend to carefully check the manuscript with respect to third person singular verbs and plural nouns. Also, some letters are accidently capitalized after a comma. "
Quantifying how much sensory information in a neural code is relevant for behavior,"Giuseppe Pica, Eugenio Piasini, Houman Safaai, Caroline Runyan, Christopher Harvey, Mathew Diamond, Christoph Kayser, Tommaso Fellin, Stefano Panzeri",https://proceedings.neurips.cc/paper/2017/hash/a9813e9550fee3110373c21fa012eee7-Abstract.html,"
This manuscript proposes a new information-theoretic measure for quantifying the amount of information about the stimuli carried in the neural response, which is also used for behavior for a perceptual discrimination tasks. This work builds on the previous literature of Partial Information Decomposition. The information-theoretic measure is tested using a simulation and two published experimental datasets.

Overall, I find that this is a potentially interesting contribution. The manuscript is generally well-written, except it misses the definitions of a few key quantities. The general approach the authors use is principled and potentially lead to better way to examine the relation between the neural response, behavior and the stimuli.

I have a few general as well as technical concerns about the applicability of this measure in typical experiments which I will detail below. 

Major concerns:

Eq.(1) is a key equation, however, the terms at the r.h.s are not defined, and only intuitions are given. For example, how to compute SI(C:{S,R})? I think it would be useful to have that spell out explicitly. Minor comment- there is a typo in the last two terms, and now they appear to be identical.

Eq. (2) gives the definition of the proposed new measure. However, what is the difference between this measure, and the bivariate redundancy measure proposed in Harder et al. (PRE, 2013; reference (21) in the submission)? In Harder et al., a measure with similar structure is proposed, see their Eq. (13). It is not clear to me how the ""new definition"" the authors proposed here is different from the Eq. (13) there.  Many of the properties of the measure proposed here seems to be shared with the measure defined there. However, since the definition of SI(C:{S,R}) is not given in the current paper, I might be missing something important here. In any case, I'd like to understand whether the measure defined in Eq. (2) is indeed different from Eq. (13) of Harder et al (2013), as this seems to be critical for judging the novelty of the current paper.


Regardless of whether the measure the authors proposed is indeed novel, there is a crucial question in terms of  practically how useful this method is. I'd think one of the main challenges for using this method is estimating the joint probably of the stimulus, activity and the choice. But in the Supplementary Information, the authors only explained how to proceed after assuming this joint probability has been estimated. Do the authors make any important assumptions to simplify the computations here, in particular for the neural response R? For example, is the independence across neurons assumed? Furthermore, in many tasks with complex stimuli such as in many vision experiments, I'd think it is very difficult to estimate even the prior distribution on the stimulus, not to say the joint probability of the stimulus, the activity and the behavior. I understand that the authors here only focus on simple choice tasks in which the stimuli are modeled as binary variables, but the limitations of this approach should be properly discussed.

In the experimental section, it seems that the number information is a bit small, e.g., ~10^{-3} bit. Can these number map to the values of choice probability and are they compatible? In general, it might be useful to discuss the relation of this approach to the approach of using choice probability, both in terms of the pros and the cons, as well as in which cases they relate to each other.

In summary, my opinion is that the theoretical aspects of the paper seems to be incremental. However, I still find the paper to be of potential interests to the neurophysiologists. I have some doubts on the general applicability of the methods, as well as the robustness of the conclusions that can be drawn from this method, but still this paper may represent an interesting direction and may prove useful for studying the neural code of choice behavior.



Minor:

In eq. (2), the notation for I_{II}(R) is confusing, because it doesn't contain (R,C), but nonetheless it depends on them.  This further leads to the confusion for I_{II}(R1,R2) at the top of line 113. What does it mean?



","The paper proposed a novel of information-theoretic criterion to describe the intersection information between sensory coding and behavioral readout. Compared to previous studies, this criterion doesn’t require specific choice of decoding algorithm and considers the full structure in the measured statistical relationships between stimulus, responses and choice. The paper starts from the Partial Information Decomposition(PID), claiming that none of these components fit the notion of intersection information. Then it defines the new criterion to be the minimum of two shared informations. This new notion of intersection information can successfully rule out the statistical relationships when responses are neither decoded by choice nor informative about stimulus. Moreover, intersection information is zero when responses affect the behavior but are not relevant to the stimulus.

The paper tested the new measure intersection information with simulated data based on a scheme of perceptual discrimination task. By changing choice and sensory noise level, intersection information can reveal its desired property when the statistical relationship of S,R and C is different. Furthermore, the paper used intersection information to rank candidate neural codes for task performance with two experimental dataset.

The paper appears to be technically sound. Properties of intersection information are clearly proved in the supplementary information. The problem and approach is not completely new, but it is a novel combination of familiar techniques. It's clear how this work differs from previous contributions. The related work is mostly referenced.
 
In summary, the intersection information criterion in this paper can offer single-trial quantification and capture more subtle statistical dependencies. It can help us to rank candidate neural codes and map information flow in brain. This is interesting work on a important question in neuroscience. But it would benefit from more thorough comparisons and analysis so that other people can be convinced to use this newly defined criterion.
 
Questions:

Most importantly, the numerical calculation of intersection information is not fully discussed. This makes it hard to evaluate the practicality of the results.

There should be more theoretical comparison between newly defined intersection information and shared information SI(C:{S;R}). It was shown in the section 2.1 that intersection information can rule out four scenarios of statistical relationship. The example in supplementary is very helpful and nicely designed. But I want to see the results of SI(C:{S;R}). It is not clear whether they are nonzero in scenario c and d. Moreover, in the framework of Partial Information Decomposition (PID) (Williams and Beer 2010), Interaction Information I(S;R;C) is defined to measure if the system has synergy or redundancy. This is equal to CI–SI in this paper. What is the advantage of the newly defined intersection information versus this Interaction Information?

The test with simulated data showed some desired properties for intersection information. For example, when the choice noise increases, intersection information will decrease when intersection level is high, and the intersection information is smaller when sensory noise is larger. But why is there a crossing point under low sensory noise between high intersection scenario and low intersection scenarios, whereas under high sensory noise, there's no crossing point?","This paper addresses the issue of trying to determine whether neural activity recorded during perceptual discrimination is related to ultimate behavioral choice.  The authors define a novel measure of intersection information that captures the amount of information between the presented stimulus S and the behavioral choice C that can be extracted from neural activity features R. They test their measure using simulated data and show that they recover the expected patterns with respect to choice and sensory noise and across two conditions of high and low intersection. Finally, they validate using experimental data from 2 perceptual discrimination tasks.

It's generally well-written and does a good job of characterizing its various components regimes. I really liked the validation on real data and wish they would have explained these in more detail. The simulation was good to show as a proof of concept but I'm not sure I understood the main point, as there were too many conditions and parameters to keep track of (ie high/low intersection, choice noise, sensory noise). What are the relevant regimes for real data? Would you typically have a good estimate for sensory/choice noise and would thus know what regime of the model you're in?

This was a dense paper and I'm not sure I understood it all (and it was a lot of content including the SI), but it seems like a useful, novel method of analyzing perceptual discrimination tasks. It'd be nice to see a discussion on whether this can be extended to other types of tasks as well, such as memory or RL tasks.

A few minor errors: 
1. Equation 1 has two identical terms; should one of these be UI(C: {R\S})?
2. Figure 4d legend should refer to panel c, not panel b
3. Line 209: ""ensured that the timing variable did contain any rate information"" should include a ""not""?"
Self-supervised Learning of Motion Capture,"Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, Katerina Fragkiadaki",https://proceedings.neurips.cc/paper/2017/hash/ab452534c5ce28c4fbb0e102d4a4fb2e-Abstract.html,"This work addresses the problem of motion capture for monocular videos in the wild, where we are interested in estimating human poses in RGB videos. Instead of directly optimizing the parameters of a 3D human model to reduce the error measured by the metrics such as segmentation, optical flow, or the coordinates of key points, this paper proposes an end-to-end trainable framework to predict the parameters of the SMPL model. The optimization objective includes the projection error of key points, optical flow, and foreground-background segmentation. The model is tested on Surreal dataset and H3.6M dataset, compared with a pre-trained model and a direct optimization framework.

[Strengths]
- The paper proposes a set of promising objectives which can be utilized to fine-tune the models on real-world datasets, which are pre-trained on synthetic datasets.
- Figures are well-drawn and do help readers understand the proposed framework.

[Weakness]
- This paper is poorly written. 

   - The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.
   - Sharing the style of citations and bullet items is confusing.
   - Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i},  z^{i} ] ^{T}$ is usually more preferred.
- The experimental results are not convincing. 
   - The descriptions of baseline models are unclear.
   - Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.
   - Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary. 
- The reason of claiming that it is a supervised learning framework is unclear. In my opinion, the supervision signals are still labeled.

[Reproducibility]
The proposed framework is very simple and well explained with sufficient description of network parameters and optimization details. I believe it's trivial to reproduce the results.

[Overall]
In term of the proposed framework, this paper only shows the improvement gained of fine-tuning the model based on the proposed losses defined by the reprojection errors of key points, optical flow, and foreground-background segmentation.

Taking into account that this work does show that fine-tuning the model pre-trained on synthetic datasets on real-world video clips improves the performance especially, it's still a convicting article. 
In sum, as far as I am concerned this work makes a contribution but is insufficient.","This paper proposes to do motion capture from monocular video using a neural network to directly predict 3D body configurations.  The proposed model achieves state-of-the-art performance on benchmark datasets.


Pros:

- The overall proposed model with a network trained using a combination of supervision and self-supervised 3D/2D consistency checks appears to be novel.  (Though I'm not very familiar with prior work in this area.)

- The paper is well-written overall with good discussion of prior work on and motivation for the self-supervised losses used.  The proposed techniques appear to be well-motivated with strong domain-specific knowledge of the motion capture problem.

- The model achieves SotA results across two benchmark datasets and multiple evaluation metrics, and reasonable ablation studies are done to confirm the importance of each component of the self-supervised learning problem.


Cons:

- The paper focuses on a single application area which may not be of wide interest to the NIPS audience -- CVPR/ICCV/ECCV might be more appropriate venues.  The novelty seems to be mainly application-driven rather than in any new learning or optimization techniques, by using existing domain-specific consistency checks to train a parametric model.

- Some implementation details seem to be missing from the ""Implementation details"" section -- e.g., what is the input image resolution? When and how does spatial downsampling occur? (The stride of the convolutions is not mentioned.)  Any data augmentation? Momentum? It would also be nice to see a figure or table summarizing the exact outputs of the model.

- (Minor) Some of the losses include (non-squared) L2 norms -- is this correct? In standard linear regression the loss is the squared norm.","The paper proposes an end-to-end network for predicting the 3D human model, from monocular videos in the wild. Without having a direct supervision for the target (3D shape), the network is supervised using projections of the target in other spaces, with better results for videos in the wild (skeletal keypoints, dense motion and segmentation), for which they use state of the art implementations to obtain the labels. The functions used to build the projections from the 3D model are differentiable and are used to define the cost of the network. Because there might be multiple solutions in the 3D space that match the projections, the authors first calibrate the network with a synthetic dataset.

The results are very good, showing an improvement of 5-6 times over previous implementations and 20%-50% over the pretrained solution. Regarding the results for components of the cost, the authors could add more plots (eg: Ls, Lk+Lm, Ls+Lm), in order to see what kind of information is missing from certain combinations. For future work they could consider finding other differentiable self-supervision cues, with higher state of the art results in those ""missing information"" zones (maybe tracking).

The solution is not unsupervised because it clearly uses supervision from other state-of-the-art tools, with whom supervision the neural nets parameters that predict the 3D model are updated (maybe if the projections were obtained in an unsupervised manner). This is important in future work when the authors want to extend to other classes. But the formulas used to transform the 3D model into the 3 projections encloses only human supervision and is general enough to transfer to other classes of objects."
Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System,"Chengxu Zhuang, Jonas Kubilius, Mitra JZ Hartmann, Daniel L. Yamins",https://proceedings.neurips.cc/paper/2017/hash/ab541d874c7bc19ab77642849e02b89f-Abstract.html,"The study represents a systematic approach to simulation and modeling of the rodent vibrissal system all the way from sensory transduction to behavior (discrimination of object classes).  Thanks to the high ethological relevance and the richness of existing empirical knowledge about the coding of information by the vibrissal system, this direction of work will likely yield new insights about the principles of sensory coding, generate new hypotheses for empirical studies, and allow capturing such principles in artificial systems and algorithms.  The comprehensive nature of the approach will allow iterative refinement and validation of each aspect of the model in one framework.

As the authors indicate, the present study is a proof-of-concept investigation paving the way for future studies to establish close correspondence between biological and artificial sensory systems.  Their framework will allow future projects to incorporate more realistic empirically derived physical models of the whiskers, the empirically measured properties of various mechanoreceptor types, the known functional differences of the distinct pathways with their respective sensory transformations, integration of active whisking, etc.

Since this is a proof-of-concept study, it seems inappropriate to criticize the study for not integrating many of the known properties of the vibrissal system (e.g. characteristics of the mechanoreceptors) -- these will be integrated in future iterations.

The networks were trained used a supervised approach.  It would be great to see a discussion of the biological counterparts of the error signals.

I disagree with the authors' assessment that models with vastly different numbers of parameters should not be directly compared.  This concern only holds when training and testing on the same data, giving an unfair advantage to models with more degrees of freedom.  However, with proper cross-validation and testing procedures prevent such mis-characterizations.   Furthermore, regularization schemes complicate the estimation of the true number of free parameters in the models.

As the authors suggest, the use DRMs indeed may hold the promise of matching brain areas or layers with corresponding level of representation. 

Overall, I find that the study outlines several directions for further investigation in both computational and empirical studies and I recommend accepting the paper.","The authors develop several models of the Whisker-Trigeminal system, each with a different neural architecture, trained to solve a shape-recognition task. The models are composed of a module capturing simplified but biologically-motivated whiskers physics, dynamics and transduction, and a deep neural network loosely inspired in the hierarchical structure of the first stages of the somatosensory system. Several neural network architectures are compared in terms of task performance, and a few of those achieve reasonable performance. Overall, the paper is technically sound, and is clearly written, with the methods and assumptions clearly exposed. However, the paper would have gained if a tighter link would have been established between the models and the specific neural areas, and if more concrete experimental predictions would have been provided:


-in lines 255-258, the authors write “The key biologically relevant follow-up question is then: how should we distinguish between the elements in the shortlist?”. Arguably, as important as this question is to ask what the successful models have in common. Indeed, the authors list the architectural features that seem to correlate with good performance (lines 235-251), but it would be important to also discuss how these features map into the neural system and could be tested experimentally;

-the proposal to experimentally validate the different network architectures by comparing the Representational Dissimilarity Matrices in models and neural recordings seems reasonable but more details should be provided: what are the specific predictions for the different models? Also, could a quick analysis be done on publicly available experimental datasets to test which architecture leads to the closest results?

Some typos:
-Figure 1 caption, second to last line, instead of “then measure to the”, “then measure the”;
-Figure 2 caption, instead of “to to the follicle”, “to the follicle”;
-Figure 2 caption, instead of “variable axes is been”, “variable axes has been”;
-line 73, instead of “to the reasonably”, “to be reasonably”;
-line 138, instead of “implementary”, “supplementary”;
-line 267, instead of “in general are comparatively”, “in general comparatively”;
-line 320, instead of “might would be”, “might be”.

-- after the rebuttal --
I read the rebuttal and my colleagues reviews, and would like to raise the score from 6 to 7, therefore recommending acceptance.","* Summary

  The authors of the paper ""Toward Goal-Driven Neural Network Models
  for the Rodent Whisker-Trigeminal System"" train networks for object
  detection on simulated whisker data. They compare several
  architectures of DNNs and RNNs on that task and report on elements
  that are crucial to get good performance. Finally they use a
  representational dissimilarity matrix analysis to distinguish
  different high performing network architectures, reasoning that this
  kind of analysis could also be used on neural data. 

* Comments
  What I like most about the paper is direction that it is
  taking. Visual datasets and deep neural networks have shown that
  there are interesting links between artificial networks and
  neuroscience that are worthwhile to explore. Carrying these
  approaches to other modalities is a laudable effort, in particular
  as interesting benchmarks often have advanced the field
  (e.g. imagenet). The paper is generally well written, and the main
  concepts are intuitively clear. More details are provided in the
  supplementary material, and the authors promise to share the code
  and the dataset with the community. 

  I cannot judge the physics behind simulating whiskers, but the
  authors seem to have taken great care to come up with a realistic
  and simulation, and verify that the data actually contains
  information about the stimulus. I think a bit more nuisance
  variables like slight airflow, as well as active whisking bouts
  would definitely make the dataset even more interesting, but I can
  see why it is a good idea to start with the ""static"" case first. 

  With regard to benchmarking, the authors made an effort to evaluate
  a variety of different networks and find interesting insights into
  which model components seem to matter. I would have wished for more
  details on how the networks are setup and trained. For example, how
  many hyperparameters were tried, what was the training schedule, was
  any regularization used, etc? The code will certainly contain these
  details, but a summary would have been good.

  The part about the representation dissimilarity matrix analysis is a
  bit short and it is not clear to me what can be learned from it. I
  guess the authors want to demonstrate that there are detectable
  differences in this analysis that could be used to compare neural
  performance to those networks. The real test for this analysis is
  neural data, of course, which the paper does not cover. 

  In general, I see the main contribution of the paper to be the
  dataset and the benchmarking analysis on it. Both together are a
  useful contribution to advance the field of neuroscience and machine
  learning. 

"
Clustering Billions of Reads for DNA Data Storage,"Cyrus Rashtchian, Konstantin Makarychev, Miklos Racz, Siena Ang, Djordje Jevdjic, Sergey Yekhanin, Luis Ceze, Karin Strauss",https://proceedings.neurips.cc/paper/2017/hash/ab7314887865c4265e896c6e209d1cd6-Abstract.html,"AIM
In this paper, the authors  present a string clustering problem motivated by DNA data storage. 
They introduce a novel distributed clustering algorithm, with a new hashing scheme for edit distance. They present results on simulated and real data in terms of accuracy, performance, scalability, and robustness.

Comments
The paper is well written. It is clear and it reads well. The authors clearly state the starting point of the problem and elucidate the characteristics of the clustering problem. Moreover, they consider strategies to reduce computational time such as hashing based on the edit distance/q-gram distance. 

The authors should clarify how they evaluate the performance in terms of the benchmark used when not using the synthetic dataset.
","The background is to store data in DNA sequences, where one can encode information into specially designed DNA sequences. The data is then retrieved using DNA sequencing machine. Since there can be sequencing errors in the retrieving process, the DNA sequences need to be specially designed so that information can be robustly retrieved. 

Such datasets have a lot of small clusters that grows linearly with size of information to be coded.  Each cluster contain only a few sequences. The clusters differs a lot to each other, which is determined in the design process. More importantly, not all clusters need to be correctly identified in order to retrieve the original information, which mean the data storage system is very robust.

The dataset is easy in the sense that the clusters are well separated. It is challenging in the sense that the size is in the magnitude of billions.

The ms proposed a clustering method based on a Hashing technique described on page 4, section 3.2.  The reads are then grouped locally and globally by their Hashing values, repeated for a predefined iterations.

The ms provides justifications about how to construct the Hash function and the experiment result shows the method does its job. The ms also use q-gram distance to approximate the edit distance to avoid some unnecessary computations.

In general I like this paper and propose to accept it. Since I am not familiar with related literature and I wander if it is possible to split the dataset in small chunks and run fast kNN (e.g. http://www.cs.ubc.ca/research/flann/)? 






","The paper presents a solution to a new type of clustering problem that has emerged from studies of DNA-based storage. Information is encoded within DNA sequences and retrieved using short-read sequencing technology. The short-read sequencer will create multiple short overlapping sequence reads and these have to be clustered to establish whether they are from the same place in the original sequence. The characteristics of the clustering problem is that the clusters are pretty tight in terms of edit distance (25 max diameter here - that seems quite broad given current sequencing error rates) but well separated from each other (much larger distance between them than diameter).

I thought this was an interesting and timely application. DNA storage is a hot topic and this kind of clustering is one of the computational bottlenecks to applying the method. I guess the methods presented will also be of general interest to the NIPS audience as an example of a massive data inference problem. 

The methods look appropriate for the task and the clustering speed achieved looks very impressive. 

The paper is well written and the problem, methods and results were easy to follow. 

My main concern with the work centred around how realistic is the generative model of the data. The substitution error rate and the insertion/deletion error rates are set to be the same parameter p/3. I think that current Illumina high-throughput sequencing is associated with much higher substitution error rates than insertion/deletion rates, I think  perhaps 100-fold less insertion/deletion than substitution for some illumina technologies. If insertion/deletion rates are much lower than substitution errors then perhaps the problem is less challenging than stated here since there would be less requirement for dealing with edit distances in the case insertions/deletions are rare. Also, for most technologies the error rates from sequencing are can be well established and therefore methods can be fine-tuned given knowledge of these parameters. 



"
AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms,"Marco Cusumano-Towner, Vikash K. Mansinghka",t,"[after feedback]

Thank you for the clarifications. I've updated my score.

I do still have some concerns regarding applicability, since a gold standard ""ground truth"" of inference is required. I do appreciate the situation described in the feedback, where one is trying to decide on some approximate algorithm to ""deploy"" in the wild, is actually fairly common. That is, the sort of setting where very slow MCMC can be run for a long time on training data, but on new data, where there is e.g. a real-time requirement, a faster approximate inference algorithm will be used instead.

[original review]

This paper introduces a new method for benchmarking the performance of different approximate inference algorithms. The approach is based on constructing an estimator of the “symmetric” KL divergence (i.e., the sum of the forward and reverse KL) between an approximation to the target distribution and a representation of the “true” exact target distribution.

The overall approach considered is interesting, and for the most part clearly presented. I would agree that there is lots of potential in approaches which directly consider the auxiliary random variables which occur within an approximate inference algorithm. The meta-inference performed here (as described in definition 3.2) relates estimation of the probability density assigned to a particular algorithm output to a marginal likelihood computation, marginalizing out the latent variables in the algorithm; SMC and AIS are then natural choices for meta-inference algorithms.

The primary drawback of this paper is that the proposed divergence estimate D-hat provided by AIDE does not seem to lend itself to use as a practical diagnostic tool. This symmetric KL does not allow us to (say) decide whether one particular inference algorithm is “better” than another, it only allows us to characterize the discrepancy between two algorithms. In particular, if we lack “gold standard” inference for a particular problem, then it is not clear what we can infer from the D-hat — plots such as those in figure 3 can only be produced contingent on knowing which (of possibly several) inference algorithms is unambiguously the “best”.

Do we really need a diagnostic such as this for situations where “gold standard” inference is computationally feasible to perform? I would appreciate it greatly if the authors could clarify how they expect to see AIDE applied. Figure 5 clearly demonstrates that the AIDE estimate is a strong diagnostic for the DPMM, but unless I am mistaken this cannot be computed online — only relative to separate gold-standard executions. The primary use case for this then seems to be for providing a tool for designers of approximate inference algorithms to accurately benchmark their own algorithm performance (which is certainly very useful! but, I don’t see e.g. how figure 5 fits in that context).

I would additionally be concerned about the relative run-time cost of inference and meta-inference. For example, in the example shown in figure 4, we take SMC with 1000 particles and the optimal proposal as the gold standard — how does the computational cost of the gold standard compare to that of e.g. the dashed red line for SMC with optimal proposal and 100 meta-inference runs, at 10 or 100 particles?

Minor questions:

• Why are some of the inner for-loops in algorithm 2 beginning from m = 2, …, M, not from m = 1?
"
Information-theoretic analysis of generalization capability of learning algorithms,"Aolin Xu, Maxim Raginsky",https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html,"This paper follows up on the line of work introduced by Russo and Zou, that uses information theoretic bounds to provide bounds for adaptive data analysis. Here, the authors focus on learning algorithms. The authors use the input-output mutual information instead of the collection of empirical risks as in Russo—applied on adaptive composition of learning algorithms, this later allows for stronger connections to the hypothesis space’s VC dimension.  

The paper was a pleasure to read with only a single grievance regarding typography (below under ‘minor points’). The work was scoped carefully and clearly in the context of prior work. The results are intersting: they tighten some known analysis, but also provide new, high probability results.

A nice part of the paper is where the authors use the info theoretic framework to demonstrate that the Gibbs algorithm effectively is ERM plus a (upper bound on the) regularizer of the input-output mutual information. Similarly, they motivate noisy ERM as a way to control generalization error. This last, algorithmic part is what got me the most excited, and left me wanting more. I would like to see more prescriptive discussion and methodology. How can this translate into instructions for practitioners? Even a discussion of open problems/questions in that direction would be good to have.


Minor points:
- A bit confusing that the authors use uppercase W both for the hypothesis space and the output hypothesis. I can see that the typefaces are different, but I feel that they’re not different enough to help me read this more smoothly. 
","This paper derives upper bounds on the generalization error of learning algorithms in terms of the mutual information between the data S and the hypothesis W. Using Lemma 1, their first result is to show that the generalization error can be upper-bounded by a quantity that is proportional to the square root of the mutual information between S and W, as long as the loss is \sigma-subgaussian. Lemma 1 also enables the authors to immediately recover a previous result by Russo and Zou. They furthermore provide a high-probability bound and an expected bound on the absolute difference between the population risk and the empirical risk. 

The next section then applies these bounds to specific algorithms: two of them have an hypothesis set that can be easily upper bounded; the next two have a bounded mutual info between S and W that can be upper bounded; then the authors discuss practical methods to effectively bound I(S;W); and finally, the generalization error of a composite learning algorithm is analyzed. Finally, a similar analysis is then employed to “adaptive data analytics”.

PROS:

- The authors attempt to bridge information-theoretic ideas with learning theory. This is something that I believe would enormously favor both communities.

- The first bounds on the generalization error based on the mutual information are intuitively appealing. (It is however unclear to me how novel they are relative to Russo and Zou's prior work).

CONS:

- The paper is very dense as it is, and would have benefited from more concrete, lively examples that the NIPS community can relate to in order to illustrate the results. In particular, I feel it's a terrible shame that most likely only minute fraction of the NIPS community will find interest in the results of this paper, just because of the way it is written. This is the main reason why I felt obliged to decrease my score.

- All the results seem to require the sigma-subgaussianity of the loss. How restrictive is this? Can you give concrete examples where the analysis holds and where it fails? ","Authors provide upper bounds on the  generalization error of supervised learning algorithms via the mutual information between the chosen hypothesis class and the sample set. These bounds can be further used to recover the VC dimension bounds up to a multiplicative factor of sqrt(log n). The approach provides an alternate proof technique to the traditional Radamacher complexity bounds for binary classification. I enjoyed reading the paper and I recommend acceptance.
"
MarrNet: 3D Shape Reconstruction via 2.5D Sketches,"Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill Freeman, Josh Tenenbaum",https://proceedings.neurips.cc/paper/2017/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html,"This paper presents an interesting algorithm to recover 2.5 and 3D shapes from 2D images. 
The paper is well written and very clear. I have only a few concerns:
1- I think that reconstruction can be only done from a monocular view up to scale. In the proposed algorithm I think that this issue can be overcome by the use of shape prior information. I think the authors should comment this issue in the problem

2- Related to the previous point, the success of the algorithm seems to depend deeply on the object prior information. So one of the main limitations of the usability of the algorithm is that it only works for specific trained objects classes. If this is the case it should be mentioned in the paper

3- I think that it should also be interesting a motivation paragraph. Who should be interested in this? (besides the academic interest), why this approach is more interesting than stereo, depth cameras..... I think that an interesting object not tackled in the paper are human shapes and faces. Can this technology be usable in this cases? 

","This paper describes a technique for estimating voxelized 3D shapes from single images. As directly predicting 3D shapes from images is hard, the paper proposes to separate the problem into 2 tasks - inspired by Marr's theory about vision. In the first part the method takes the image as input and predicts several intrinsic quantities (""2.5D sketch""), in particular depth, surface normals and the 2D silhouette using an encoder-decoder architecture. This information is fed into a second encoder-decoder network which predicts the volumetric 3D representation at 128^3 resolution. The advantage of the method is that the second part can be trained on synthetic data allowing for ""domain transfer"" based on the 2.5D sketches which are invariant to the actual appearance. Experiments (mainly qualitative) on Shapenet, IKEA and Pascal3D+ are presented.

Overall, I liked the idea. The paper is clear, well motivated and written and the qualitative results seem convincing. However, I have some concerns about the fairness of the evaluation and the self-supervised part which I like to have answered in the rebuttal before I turn towards a more positive score for this paper.

Positive aspects:

+ Very well written and motivated
+ Relevant topic in computer vision
+ Interesting idea of using intermediate intrinsic representations to facilitate the task
+ Domain transfer idea is convincing to me (though the self-supervision and fine-tuning of the real->sketch network are not fully clear to me, see below)
+ Qualitative results clearly superior than very recent baselines

Negative aspects:

- Self-supervision: It is not clear to me why this self-supervision should work. The paper says that the method fine-tunes on single images, but if the parameters are optimized for a single image, couldn't the network diverge to predict a different shape encapsulated in the shape space for an image depicting another object while still reducing the loss? Also, it is not very well specified what is finetuned exactly. It reads as if it was only the encoder of the second part. But how can the pre-trained first part then adapted to a new domain? If the first part fails, the second will as well.
- Domain adaptation: I agree that it is easy to replace the first part, but I don't see how this can be trained in the absence of real->sketch training pairs. To me it feels as the method profits from the fact that for cars and chairs pretraining on synthetic data already yields good models that work well with real data. I would expect that for stronger domain differences the method would not be able to work without sketch supervision. Isn't the major problem there to predict good sketches from little examples?
- Baselines: It is not clear if the direct (1-step) baseline in the experiments uses the same architecture or at least same #parameters as the proposed technique to have a fair evaluation. I propose an experiment where the same architecture is used but insteads of forcing the model (ie, regularizing it) to capture 2D sketches as intermediate representations, let it learn all parameters from a random initialization end-to-end. This would be a fair baseline and it would be interesting to see which representation emerges. Another fair experiment would be one which also has 2 encoder-decoder networks and the same number of parameters but maybe a different distribution of feature maps across layers. Finanlly a single encoder-decoder architecture with a similar architecture as the proposed one but increased by the number of parameters freed by removing the first part would be valuable. From the description in the paper it is totally unclear what the baseline in Fig. 4. is. I suggest such baselines for all experiments.
- DRC: It should be clarified how DRC is used here and if the architecture and resolution is adopted for fairness. Also, DRC is presented to allow for multi-view supervision which is not done here, so this should be commented upon. Further the results of the 3D GAN seem much more noisy compared to the original paper. Is there maybe a problem with the training?
- Quantitative Experiments: I do not fully agree with the authors that quantitative experiments are not useful. As the paper also evaluates on ShapeNet, quantitative experiments would be easily possible, and masks around the surfaces could be used to emphasize thin structures in the metric etc. I suggest to add this at least for ShapeNet.

Minor comments:

- Sec. 3.1: the deconvolution/upconvolution operations are not mentioned
- It is unclear how 128^3 voxel resolution can be achieved which typically doesn't fit into memory for reasonable batch sizes. Also, what is the batch size and other hyperparameters for training?
- Fig. 8,9,10: I suggest to show the predicted sketches also for the real examples for the above mentioned reasons.
","This paper has potential but is currently written in a confusing way. For example the paper claims the model is self-supervised in several places. This makes no sense. The model is trained using full supervision (3d models, 2.5 depths, normals and segmentations). What happens is that at test time reconstruction is achieved through optimization.

Here's how it could have been better written: 

There are two neural networks, a) predicts object 2.5D depth, normals and foreground-background segmentation, b) predicts a voxel-based 3D reconstruction of the object from the 3 outputs of the first neural network. The networks are trained independently. At test time, reconstruction is computed through gradient descent over the weights of the voxel-net's encoder, with the rest of the model parameters fixed. 

This perspective sheds light on merits and limitations of the approach. Limitations first: it must be very slow (how slow?), it isn't trained for the task at hand, e.g. the model isn't trained to optimize the reconstruction. Merits: the idea of having a fixed voxel-net decoder is interesting and seems to produce good results. The idea of the 2.5D net, which can more easily be trained on real data, as setting constraints on the voxel-net is also interesting.

The technical section begins with a hand-waving explanation of the architectures. How are the 3 inputs to the voxel net combined ? This basic information should be in the paper, so it is minimally self-contained. One of the most important parts of the paper is fit into 10 lines (section 3.4) without much discussion or details, making it hard on the reader.

The rebuttal addressed some of my concerns and I think it can be accepted."
ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching,"Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/ade55409d1224074754035a5a937d2e0-Abstract.html,"Rather than providing an understanding of joint-distribution matching using adversarial network, the paper essentially proposes two specific algorithms based on cycle-consistency (one unsupervised, one semi-supervised) using an approach that casts interesting lights on the existing joint-distribution matching algorithms and in particular conditional entropy regularization. Besides a section discussing these similarities in detail, the authors also provide extensive experiments.

I found this paper generally insightful and well thought.  People unfamiliar with the topic may find it hard to read, especially because equation (5) is not really meaningful (since \pi is a product of the optimization process, it cannot be part of its specification.)  However I can see what is meant by the authors, and it makes sufficient sense in my mind.

I am not familiar enough with the most recent result to asses the quality of the empirical results. The experimental process however seems serious.


","Adversarially Learned Inference (a.k.a. Adversarial Feature Learning)
is an interesting extension to GANs, which can be used to train a
generative model by learning generator G(z) and inference E(x)
functions, where G(z) maps samples from a latent space to data and
E(x) is an inference model mapping observed data to the latent
space. This model is trained adversarially by jointly training E(x)
and G(z) with a discriminator D(x,z) which is trained to distinguish
between real (E(x), x) samples and fake (z, G(z)) samples. This is an
interesting approach and has been shown to generate latent
representations which are useful for semi-supervised learning.

The authors highlight an issue with the ALI model, by constructing a
small example for which there exist optimal solutions to the ALI loss
function which have poor reconstruction, i.e. G(E(x)) can be very
different to x. This identifiability issue is possible because the ALI
objective imposes no restrictions on the conditional distributions of
the generator and inference models. In particular, this allows for
solutions which violate a condition known as cycle consistency, which
is considered a desirable property in unsupervised learning. They
propose adding a cross-entropy term to the loss, which enforces a
constraint on one of the two conditionals.

The ALICE training objective is shown to yield better reconstructions
than ALI on toy data. They also evaluate on the inception score, a
metric proposed in Salimans et al. (2017), which also shows better 
performance for the model trained with the ALICE objective.

The description of how the model can be applied to semi-supervised
learning seemed a little strange to me. The authors consider a case
where the latent space z corresponds to the labels and semi-supervised
learning involves training a model with the ALICE objective, when some
paired (z,x) samples are available. They claim that these labelled
samples can be leveraged to address the identifiability issues in
ALI. While this setting holds for the experiments they present in
section 5.2, a more common application of this kind of model to
semi-supervised learning is to use the inference network to generate
features which are then used for classification. The results in the
ALI paper follow this approach and evaluate ALI for semi-supervised
learning on some standard benchmarks (CIFAR10 and SVHN). This is
possibly the strongest evidence in the ALI paper that the model is
learning a useful mapping from data space to latent space, so it would
be useful to see the same evaluation for ALICE.

In the ALI and BIGAN papers, it is shown that cycle consistency is
satisfied at the optimum when the E and G mappings are
deterministic. Similarly, in this paper the authors show that the
optimal solution to a model trained with cross-entropy regularisation
has deterministic mappings. In lines 228-233, the authors suggest that
a model trained with deterministic mappings is likely to underfit and
taking the alternative approach of training a model with stochastic
mappings and a cross-entropy regularisation gives faster and more
stable training. While this is an interesting claim, it was not
immediately clear to me why this would be the case from the
description provided in the paper.

This paper is on an interesting topic, highlighting a significant
issue with the ALI training objective and proposing a solution using
cross-entropy regularisation. It would be more convincing if the
authors had evaluated their model on the same set of experiments as
the original ALI paper, especially the semi-supervised learning
benchmarks, and provided more justification for the use of
cross-entropy regularisation over deterministic mappings","GANs have been receiving a lot of attention lately. They are good at generating samples using an adversarial approach. However they cannot infer the mapping from data to the latent space. One solution to this has been to use adversarial learning to distinguish between pairs from two joint distributions: the model joint and the joint defined as the product of the approximate posterior and the empirical data distribution. This approach has been used in several works one of which is ALI (Dumoulin et al. 2016). This paper casts many existing works as performing joint-distribution matching. It also discusses and proposes a solution to the identifiability issue that arises in these settings using entropy regularization. Few remarks regarding this paper are below.

1. Although I found that the experiments were not fully developed and seemed ""toy"", the issue raised by this paper, namely the identifiability problem in joint distribution matching is interesting. Unifying several existing works as performing this same joint distribution matching is interesting. The entropy regularization approach is interesting. I only wish the experiments were carried out more thoroughly. 

2. It was not clear to me what type of restrictions are being referred to in the paper (example at the bottom of page 2). Discussing the nature of these restrictions would have been a good insight.

3. The paper does not solve the identifiability issue that arises because one latent sample z may lead to any element in the sample space. What it does is use an auxiliary discriminator that distinguishes reconstructed data from real data. This procedure, according to the paper is equivalent to entropy regularization. However this still does not ensure that one latent sample z leads to one data sample x from the data space. This would happen only in the optimal case which is usually never attained in adversarial approaches that are minimax problems. This brings back the question of what types of restrictions should be enforced to ensure identifiability.

4. Some notations are not defined in the paper. See for example section 3.2. There are also some typos: for example page 5 ""p_{\theta}(x | z) is an approaximation to \pi(x , z)"" should be \pi(x | z). 

 "
Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization,"Pan Xu, Jian Ma, Quanquan Gu",https://proceedings.neurips.cc/paper/2017/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html,"The authors present an efficient algorithm for latent variable Gaussian graphical models (LVGGMs) in which the precision matrix of the observations is the sum of a sparse and a low rank matrix. Their approach is based on matrix factorizations while previous methods were based on nuclear norm penalized convex optimization schemes. The advantage of this approach is that the resulting nonconvex optimization problem is much easier to optimize. The authors prove that their algorithm (basically an alternating gradient method for the sparse and low rank pieces separately) has a linear rate of convergence in a region of the global optimum. The authors provide an initialization method that produces a good initialization with high probability, and the authors conclude with an experimental evaluation that compares their method to the competing convex optimization methods (their algorithm often performs comparably or better while running much faster than the competing methods).  The paper is well-written but feels defensive in parts.

-  Just a thought:  I recall reading somewhere that estimating the sample precision matrix by inverting the sample covariance matrix is a bad thing to do, especially in higher dimensions.  It may be worthwhile to replace the inverse with something else (perhaps something that doesn't have the same O(d^3) complexity).

-  A variety of fast SVD methods that approximately compute the full SVD are available.  It would be more interesting to compare to the exact methods where these additive approximations have been swapped in.  I believe that they often have much less than O(d^3) complexity while still providing a quality approximation.

","The paper considers learning the dependency structure of Gaussian graphical models where some variables are latent. Directly applying the usual assumption of sparsity in the precision matrix is difficult because variables that appear correlated might actually both depend on a common latent variable. Previously, Chandrasekaran et al. proposed estimating the model structure by decomposing the full precision matrix into the sum of of a sparse matrix and a low-rank matrix. Likelihood is maximized while the components of the sparse matrix are penalized with an l1 regularizer and the low-rank matrix is penalized with a nuclear norm. Computing the proximal operator to update the low-rank component requires performing SVD in O(d^3) time at each iteration.

The authors propose replacing the low-rank component with its Cholesky decomposition ZZ^T and finding Z directly. The main technical contribution is that they prove linear converge of an alternating gradient algorithm, up to optimal statistical precision.

Experiments show that by avoiding the need for SVD at each iteration, the proposed method is orders of magnitude faster than the method of Chandrasekaran et al. or a related approach that uses ADMM to estimate the model. It is also more accurate on synthetic and gene network data.

Is Huang, Jianhua Z., et al. ""Covariance matrix selection and estimation via penalised normal likelihood."" Biometrika 93.1 (2006): 85-98 related? They propose regularizing the Cholesky factorization for precision matrix estimation, although without any latent variables. Their motivation is that it avoids needing to check that the estimated precision matrix is PSD, which takes O(d^3) time.","The contribution is interesting but not on the top of the NIPS list.  The paper builds a non-convex estimator for a latent variable Gaussian Graphical Model under structural assumption that the resulting precision matrix is a superposition of a sparse matrix and a low rank matrix, and then suggest an empirical iterative algorithm. 

Based on numerical computations the authors claim the algorithm is fast and efficient. However, it is not clear to the reviewer why only experiments with a relatively small latency with, $r$, much smaller than $d$ is considered.  The algorithm would certainly be much more sounds if extensions to the case with $r\sim d$ would be claimed.

Wrt the theory. The main theory statement on the algorithm convergence is stated under a number of rather non-intuitive assumptions, which dilute the results. It would be useful to get some intuition and clarification on the nature of the imposed conditions. "
Sparse convolutional coding for neuronal assembly detection,"Sven Peter, Elke Kirschbaum, Martin Both, Lee Campbell, Brandon Harvey, Conor Heins, Daniel Durstewitz, Ferran Diego, Fred A. Hamprecht",https://proceedings.neurips.cc/paper/2017/hash/aebf7782a3d445f43cf30ee2c0d84dee-Abstract.html,"The paper proposes a new method for the identification of spiking motifs in neuronal population data.  The authors’ approach is to use a sparse convolutional coding model that is learned using an alternation between LASSO and matching pursuit.  The authors address the problem of false positives and compare their algorithm to 3 competing methods in both simulation experiments and real neural data.

It appears that the basic model is not original, but that the regularization has been improved, leading to a method that performs well beyond the state of the art.

I believe that the method could be of some use to the neuroscience community as it may be powerful enough to determine if population codes fire in coordinated assemblies and if so, what is the nature of the assemblies?  This is a step towards addressing these long-standing questions in the neuroscience community.

Overall, the paper is clearly written, the method is reasonably conceived and described, and the simulation studies were sufficiently conceived for a paper of this length. Some of the methods for evaluating performance were unclear.  I am not totally certain how to assess Figure 4, for example since lines 169-175 where the “functional association” metric is described is too impoverished to determine what was actually done and what is being compared across detection thresholds.

I also am of the opinion that the authors are over-interpreting their data analysis results when they claim that the motif appearance is related to re-activation of former assemblies (do they mean replay?).

Also, it appears that few of the motif activations in Figure 7 include all of the cells that are a part of the motif.  This is concerning as it is not a part of the model description and makes it appear as though there is a poor model fit to the data.
","This submission introduces an algorithm to extract temporal spike trains in neuronal assemblies, based on a sparse convolutive non-negative matrix factorization (scNMF). This algorithm is evaluated on synthetic datasets and compares favorably with PCA, ICA and scNMF.

The difference with scNMF is not obvious and, despite the supplementary material, the update rules for the motifs and the activities are not obvious. The text mentions a difference in the regularization scheme, it should be emphasized. The contribution could be reinforced, the interests of the algorithm could be better explained by formally describing the difference with state of the art.

Another remark is about the name of the algorithm and the article title: why changing the name ""sparse convolutive"" NMF to ""convolutional""? Without specific reason, it may be more adequate to keep the terminology ""sparse convolutive"" to keep the consistency with the existing literature. ","* Summary
  The paper ""Sparse convolutional coding for neuronal assembly
  detection"" proposes a new approach to detecting stationary
  occurring spike motifs across neurons. The proposed algorithm uses a
  non-negative matrix factorization of the data matrix into
  ""occurrences"" and ""motifs"". The authors test their algorithm on
  synthetic and real data from cell cultures, compare their algorithm
  against PCA and ICA, and give strategies to
  identify spurious motifs and determine the best parameters of the
  algorithm. 

* Comments
  The algorithm is a nice application of non-negative matrix
  factorization (NMF). The paper is generally well written, and the
  algorithm and the mathematics look sound. The algorithm seems to
  work well on synthetic data and produce stable results on real
  data. The only additional point I would have been interested in, is
  run time, in particular the maximal time length that can be run in
  reasonable time and how the algorithm scales with the number of
  neurons. For instance, the optimization problem (3) seems very
  costly when converting the activity signals into Toeplitz
  matrices. However, the matrices are very sparse, so the algorithm
  might still scale well. I few words on that would be good. 

* Minor Comments
  - All plots should be re-generated (esp. figures 3 and 4) because
    all fonts are basically unreadable.

"
Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons,"Nikhil Parthasarathy, Eleanor Batty, William Falcon, Thomas Rutten, Mohit Rajpal, E.J. Chichilnisky, Liam Paninski",https://proceedings.neurips.cc/paper/2017/hash/b0169350cd35566c47ba83c6ec1d6f82-Abstract.html,"This paper describes a NN architecture to enhance results of a linear decoder for retinal ganglion cell activity, incorporating natural image statistics to improve the results of the neural decoder. The method is related to recent work in super-resolution, inpainting, and denoising. To produce enough training data, the authors first trained an encoder to model RGC responses (4 kinds of cells, several thousand cells in all). They applied this encoder on images from ImageNet. From these simulated RGC responses, the paper learns a linear decoder of the RGC responses, then trains a CAE to enhance the linearly decoded image. This approach was assessed with pixel-wise MSE and image structural similarity.

The paper is clearly written and presents an interesting idea of relevance to both the NN and the visual neuroscience communities. I'm curious about how this architecture would perform at different spatial scales? And if it may be extended to improve decoding performance for other visual features like motion?","The paper describes a method to decode natural images from retinal-like activities, using convolutional neural networks. The retinal-like activities are generated by a constructed lattice of linear-nonlinear-Poisson models (separately fitted to RGC responses to natural scenes in a macaque retina preparation) in response to natural static images. After a simple linear decoding of the images from the retinal-like activities, a convolutional neural network further improves on the reconstruction of the original natural images. The paper is clearly written and the results seem sound. A few comments to clarify the motivation, assumptions and impact of the work:

-the method proposed is compared to a linear decoder and shown to perform substantially better. However, the performance of the decoding stage will most likely depend on the performance of the encoding stage. A more explicit discussion on the fact that the results of the CAE will ultimately depend on the encoding would clarify the scope and limitations of the study;

-on a related note, the authors should be explicit about the reasons for choosing a synthetic encoding stage: why didn’t the authors directly fit their method to the data without the synthetic encoding stage? Too few cells recorded? Too few natural images for fitting the CAE? An attempt at excluding the temporal component of the cells responses, so that the problem reduces to decoding static natural images instead of movies (which would be a more complex task)? This should be more explicitly stated;

-in Macintosh et al. 2017 (https://arxiv.org/abs/1702.01825), a convolutional neural network is shown to be a good encoding model of the RGCs in response to natural movies. The authors should discuss their method in the context of such study; 

-a plot illustrating how the performance of the CAE decoder depends on the amount of data used for training the network would be instrumental for assessing how powerful and practical the approach is; 

-the authors show that the CAE decoder is able to capture phase structure, unlike the linear decoder. Is the CAE better than the linear decoder mostly because it captures the phase structure? How does the performance between CAE and linear decoder compare in phase scrambled images? It would be helpful to comment on this, and perhaps explicitly plot such comparison.

Some typos/minor comments:
-lines 83-84, the authors write “resulting response of each neuron…displayed in Figure 3”, however no neural responses are displayed in Figure 3;
-line 104, the authors write “all tuned through an exhaustive grid-search”. Providing more details about this tuning would improve the clarity of the paper, and allow for a better assessment of the usability of the method presented;
-line 112, the authors write “i = {1,2,…,n}”. However, ’n’ is the number of neurons. Is ’n’ also the number of training epochs?

-- After the rebuttal --
The rebuttal has addressed satisfactorily all my concerns/comments, and therefore I would like to increase my score to a clear accept (score: 8).","A new LN neural decoding architecture for decoding images using the neural response from a population of retinal neurons is proposed. The N-stage does super-resolution type operation in the natural images context, and implemented by convolutional autoencoder (CAE). The CAE was trained end-to-end to mimic an VAE with a recognition model. The L-stage was simply a least-square decoder. They showed proof of concept on a simulated population (no real data). Two main data sets were used. The example of network performance used ImageNet images, and clearly showed that the two-stage reconstruction method both qualitatively and quantitatively outperformed just linear decoding. A few small examples in sec. 3.1.1-2 remarked that simply adding a nonlinearity to the linear decoder could not math the improvement by using the CAE, and additionally the linear+CAE approach had difficulty reconstructing with phase-scrambled images. These observations led the authors to believe that the CAE component contains more than just a simple nonlinear transformation of pixel intensities, and that is is specifically picking up on phase relationships in the image. The final data set tested the importance of context-dependent training, in which the CelebA face data set was used to see if a CAE trained specifically for face recognition would outperform a generally-trained CAE for image decoding.  They found that while the face-trained network did perform better, the improvement was surprisingly small, and that the generally-trained CAE did a good job, suggesting that this type of approach could be a reasonable candidate for general image decoding given enough training data."
"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models","Caglar Gulcehre, Francis Dutil, Adam Trischler, Yoshua Bengio",https://proceedings.neurips.cc/paper/2017/hash/b030afbb3a8af8fb0759241c97466ee4-Abstract.html,"The authors have addressed the problem of sequence-to-sequence modelling. Specifically, they investigate a mechanism which allows planning future alignments while computing current alignment of input and output sequences. The proposed method outperforms the baseline methods in three different tasks with faster convergence and fewer parameters.

Pros:
* The paper is very well written and easy to follow.
* The idea of using a planning mechanism on sequence-to-sequence models seems novel.
* The proposed method is able to outperform baseline methods on different tasks which suggests that it can be a good fit for general sequence-to-sequence modeling tasks.
* Also, the proposed method requires less parameters and shows faster and better convergence.

Cons:
* It would have been nicer to see the effect of number of planning steps on the performance. The choice of 10 planning steps is not justified.
","The paper introduces the planning mechanism into the encoder-decoder model with attention. The key idea is that instead of computing the attention weights on-the-fly for each decoding time step, an attention template is generated for the next k time steps based on the current decoding hidden state s_{t-1}, and a commitment vector is used to decide whether to follow this alignment plan or to recompute it. 

Overall, the paper is well written, and technical discussion are clear. The effect of the planning mechanism is demonstrated by Figure 2, where planning mechanism tends to generate smoother alignments compared to the vanilla attention approach. In this paper, the attention template (matrix A) is computed as eq(3). It would be interesting to see if some other type of features could be used, and if they could control the language generation, and produce more diverse and controllable responses. This might be evaluated on a conversation task, however, I understand that it is out of the scope of this paper.

In general, this is an interesting idea to me. A couple of minor questions:

-- In Table 1, why the Baseline + layer norm is missing? PAG and rPAG seem only outperform the baseline when layer norm is used.

--You used k=10 for all the experiments. Is there any motivation behind that? 10 time steps only correspond to 1 to 2 English words for character MT.

--Do you have any results with word-level MT? Does the planning approach help?

--Can you say anything about the computation complexity? Does it slow down training a lot? ","This paper addresses the problem of using neural models to generate long sequences.
In particular, the authors seek to modify the decoder of the encoder-decoder architecture
as used in sequence to sequence models.
The encoder is fixed, as is used as in prior work
What the authors propose is a plan-ahead alignment mechanism. A commitment vector is used
to decide wether to update the commitment plan or to follow  existing the plan ahead alignment.

Experiments were carried out on a random graphs task, a question generation task, and character level
machine translation.

 
1. In terms of the contribution of the paper, a lookahead alignment mechanism, while interesting, and well motivated in the paper, is a bit unclear if it is significant enough a contribution to carry the paper.

2. The experimental evaluation also does not given one enough confidence. First, the first two experiments
are not very insightful, in particular, for the experiment on question generation, the main result given 
is a convergence plot. The third experiment, on NMT is equally minimal, and has only a very small comparison to prior work.

3. Overall, the paper’s contribution is somewhat limited, and one wishes it was better evaluated, 
the current experiments are somewhat minimal.
"
Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems,"Yonatan Belinkov, James Glass",https://proceedings.neurips.cc/paper/2017/hash/b069b3415151fa7217e870017374de7c-Abstract.html,"The paper examines the extent to which a character-based CTC system (an “End to End” system) uses phonemes as an internal representation. To answer this question the hidden activations of the network are classified using either a feed forward neural network or a nearest neighbour classifier. Visualizations and through analysis are presented.
 
The paper is clearly written and the methodology is sound. I believe it will be of interest for the speech community. 
 
I especially like the analysis in section 5.5 which sheds some light onto classification of phonemes with longer temporal structured. 

Other than that I have the following constructive comments:

- Section 4: The baseline ASR system performance should be reported on both LibriSpeech and on TIMIT, to assure the reader the task is carried out on a mature system.

- The message of section 5.3 is not clear to me. There are small trends all over the place, none are really explained. 

- Line 207: “...hidden states carry information from past observations.” This is not accurate as the system at hand is bidirectional and also the future is observed. 

- Line 210: It's not clear what is “the original CTC model”?

- Section 5.4: Figure 5.3 is too small and dense, you can unify legend for all plots and remove values from the axis. 
","This paper tries to analyze the speech representations learned by a deep end-to-end model. I have the following comments:

1. The paper uses a phone classification task to evaluate the quality of the speech representation. But phone classification cannot directly be translated into tasks like automatic speech recognition. It is only one aspect of the speech representation, and I don't think it's a good idea to use it as the primary metric of the representation quality.

2. It is not super clear what the authors learned from the analysis, and how those points can be used to improve actual speech related tasks such as automatic speech recognition.

3. Figure 1 shows the classification accuracy using features from different layers of the neural network. Does it imply that the phone classification accuracy may not be a good metric for speech representation?","The authors conduct an analysis of CTC trained acoustic models to determine how information related to phonetic categories is preserved in CTC-based models which directly output graphemes. The work follows a long line of research that has analyzed neural network representations to determine how they model phonemic representations, although to the best of my knowledge this has not been done previously for CTC-based end-to-end architectures.

The results and analysis presented by the authors is interesting, although there are some concerns I have with the conclusions that the authors draw that I would like to clarify these points. Please see my detailed comments below.

- In analyzing the quality of features learned at each layer in the network, from the description of the experiments in the paper it would appear that the authors only feed in features corresponding to a single frame at time t, and predict the corresponding output label. In the paper, the authors conclude that (Line 159--164) ""... after the 5th recurrent layer accuracy goes down again. One possible explanation to this may be that higher layers in the model are more sensitive to long distance information that is needed for the speech recognition task, whereas the local information which is needed for classifying phones is better captured in lower layers."" This is a plausible explanation, although another plausible explanation that I'd like to suggest for this result is the following:

It is know from previous work, e.g., (Senior et al., 2015) that even when predicting phoneme output targets, CTC-based models with recurrent layers can significantly delay outputs from the model. In other words, the features at a given frame may not contain information required to predict the current ground-truth label, but this may be shifted. Thus, for example, it is possible that if a window of features around the frame at time t was used instead of a single frame, the conclusions vis-a-vis the quality of the recurrent layers in the model would be very different. An alternative approach, would be to follow the procedure in (Senior et al., 2015) and constrain the CTC loss function to only allow a certain amount of delay in predicting labels. I think it is important to conduct the experiments required to establish whether this is the case before drawing the conclusions in the paper. 

Reference: A. Senior, H. Sak, F. de Chaumont Quitry, T. Sainath and K. Rao, ""Acoustic modelling with CD-CTC-SMBR LSTM RNNS,"" 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Scottsdale, AZ, 2015, pp. 604-609. 

- In analyzing phoneme categories I notice that the authors use the full 61 label TIMIT phoneme set (60 after excluding h#). However, it is common practice for TIMIT to also report phoneme recognition results after mapping the 61 label set down to 48 following (Lee and Hon, 89). In particular, this maps certain allophones into the same category, e.g., many closures are mapped to the same class. It would be interesting to confirm that the same trends as identified by the authors continue to hold in the reduced set.  

Reference: Lee, K-F., and H-W. Hon. ""Speaker-independent phone recognition using hidden Markov models."" IEEE Transactions on Acoustics, Speech, and Signal Processing 37.11 (1989): 1641-1648.

- There have been many previous works on analyzing neural network representations in the context of ASR apart from the references cited by the authors. I would suggest that the authors incorporate some of the references listed below in addition to the ones that they have already listed:
* Nagamine, Tasha, Michael L. Seltzer, and Nima Mesgarani. ""On the Role of Nonlinear Transformations in Deep Neural Network Acoustic Models."" INTERSPEECH. 2016.
* Nagamine, Tasha, Michael L. Seltzer, and Nima Mesgarani. ""Exploring how deep neural networks form phonemic categories."" Sixteenth Annual Conference of the International Speech Communication Association. 2015.
* Yu, Dong, et al. ""Feature learning in deep neural networks-studies on speech recognition tasks."" arXiv preprint arXiv:1301.3605 (2013).

- In the context of grapheme-based CTC for ASR, I think the authors should cite the following early work:
* F. Eyben, M. Wöllmer, B. Schuller and A. Graves, ""From speech to letters - using a novel neural network architecture for grapheme based ASR,"" 2009 IEEE Workshop on Automatic Speech Recognition & Understanding, Merano, 2009, pp. 376-380.

- Line 250: ""... and micro-averaging F1 inside each coarse sound class."" What is meant by ""micro-averaging"" in this context?

- Minor comments and typographical errors:
* Line 135: ""We train the classifier with Adam [22] with default parameters ..."". Please specify what you mean by default parameters.
* In Figure 1 and Figure 2: The authors use ""steps"" when describing strides in the convolution. I'd recommend changing this to ""strides"" for consistency with the text.
* In Figure 1. d: The scale on the Y-axis is different from the other figures which makes the two figures in (c.) and (d.) not directly comparable. Could the authors please use the same Y-axis scale for all figures in Figure 1."
Multi-Task Learning for Contextual Bandits,"Aniket Anand Deshmukh, Urun Dogan, Clay Scott",https://proceedings.neurips.cc/paper/2017/hash/b06f50d1f89bd8b2a0fb771c1a69c2b0-Abstract.html,"Summary.

The paper is about contextual bandits with N arms. In each round the learner observes a context x_{ti} for each arm, chooses
an arm to pull and receives reward r_{ti}.

The question is what structure to impose on the rewards. The authors note that

E[r_{ti}] = < x_{ti}, theta > is a common choice, as is 
E[r_{ti}] = < x_{ti}, theta_i >

The former allows for faster learning, but has less capacity while the latter has more capacity and slower learning. The natural question
addressed in this paper concerns the middle ground, which is simultaneously generalized by kernelization.

The main idea is to augment the context space so the learner observes (z_{ti}, x_{ti}) where z_{ti} lies in some other space Z.
Then a kernel can be defined on this augmented space that measures similarity between contexts and determines the degree of sharing
between the arms.

Contribution.

The main contribution as far as I can tell is the idea to augment the context space in this way. The regret analysis
employs the usual techniques.

Novelty. 

There is something new here, but not much. Unless I am somehow mistaken the analysis of Valko et al. should apply directly to the augmented
contexts with more-or-less the same guarantees. So the new idea is really the augmentation.  

Impact.

It's hard to tell what will be the impact of this paper. From a theoretical perspective there is not much new. The practical experiments
are definitely appreciated, but not overwhelming. Eg., what about linear Thompson sampling?   

Correctness. 

I only skimmed the proofs in the supplementary material, but the bound passes plausibility tests.

Overall.

This seems like a borderline paper to me. I would increase my score if the authors can argue convincingly that the theoretical results are really doing
more than the analysis in the cited paper of Valko et al.

Other comments.

- On L186 you remark that under ""further assumption that after time t, n_{a,t} = t/N"". But this assumption is completely unjustified, so how meaningful
  are conclusions drawn from it?

- It's a pity that the analysis was not possible for Algorithm 1. I presume the sup-""blah blah"" algorithms don't really work? It could be useful to 
  show their regret relative to Algorithm 1 in one of the figures. Someone needs to address these issues at some point (but I know this is a tricky problem).


Minors.

- I would capitalize A_t and other random variables.
- L49: ""one estimate"" -> ""one estimates"".
- L98: ""z_a"" -> ""z_a \in \mathcal Z"".
- L110: The notation t_a as a set that depends on t and a is just odd.
- L114: Why all the primes?
- There must be some noise assumption on the rewards (or bounded, or whatever). Did I miss it?
- L135: augmented context here is also (a, x_a).







","This paper introduces a multitask bandit learning approach. It takes after two existing contributions:  Valko et al. 2013 on kernelised contextual bandits, Evgueniou and Pontil, 2004 on regularized multitask learning. The authors of the present paper provide a way to estimate the similarities between the tasks if it is not given, which is essential for real-world data.

Pros of the paper:
- the problem of learning multitask contextual bandits is of importance for many practical problems (e.g. recommendation);
- the mathematical anaylsis, as far as I have checked, is correct;
- results from numerical simulations are convincing.

Cons:
- I would point out that the paper provides an incremental contribution and/or that the novelty is not well sold. For instance, it seems a lot of the work provided here is similar to the work of Valko et al 2013. What if that work is implemented with multitask kernels ? Would the resulting algorithm be very different from that proposed in the present paper ?
- there is the question of the computational complexity induced by the growing kernel matrix K_{t-1}: something should be said here.
- there is the frustrating proof fo the regret that mentions two algorithms SupKMTL-UCB and BaseKMTL-UCB that are only given in the supplementary material: the authors should at least provide the main lines of these algorithms in the main text. Otherwise, Theorem 4.1 cannot be understood.

All in all, the paper addresses an interesting problem. However, there is some drawbacks regarding 1) the incrementality of the contribution, 2) some algorithmic points (e.g. growing kernel matrix) and 3) the presentation of Theorem 4.1.


"
Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks,"Prateep Bhattacharjee, Sukhendu Das",https://proceedings.neurips.cc/paper/2017/hash/b166b57d195370cd41f80dd29ed523d9-Abstract.html,"This method provides 2 contributions for next frame prediction from video sequences. 
The first is the introduction of a normalized cross correlation loss, which provide a better similarity score to judge if the predicted frame is close to the true future. The second is the pairwise contrastive divergence loss, based on the idea of similarity of the image features. Results are presented on the UCF101 and Kitti datasets, and a numerical comparison using image similarity metrics (PSNR, SSIM) with Mathieu et al ICLR16 is performed. 


Comments:

The newly proposed losses are interesting, but I suspect a problem in the evaluation.

I fear that the evaluation protocol comparing the present approach to previous work [13] is different to [13]: In the table, the authors don't mention evaluating their method on 10% of the test set and only in the moving area as Mathieu et al. did.
Did the authors performed the sanity check of comparing the obtained PSNR/SSIM score of frame copy to see if they match the ones reported in [13]?
Please be more specific about the details of the evaluation (full test set? Full images? -> in this case, what are the scores for a frame copy baseline?)

The authors answered my concerns in the rebuttal.
- Providing generated videos for such paper submission would be appreciated.
In the displayed examples, including the ones from the supplementary material, the movements are barely visible. 

- Predicting several frames at a time versus one?
In Mathieu et al. work, results showed that predicting only one frame at the time worked better. Have you tried such experiment?

l.138: I don't understand the h+4?


Minor

The first sentence, ""Video frame prediction has always been one of the fundamental problems in computer vision as it caters to a wide range of applications [...]"" is somehow controversial in my opinion: The task was, in the best of my knowledge, introduced only in 2014. As for the applications, even if i believe they will exist soon, I am not aware of systems using next frames predictions yet.

l. 61 uses -> use
l. 65 statc -> statistical ?
l 85. [5]is -> [5] is
Eq 4: use subscripts instead of exponents for indexes
l 119 (b)P -> (b) P
l 123-125 weird sentence
l. 142 ,u -> , u
l. 153 tries to -> is trained to
l. 163 that, -> that
l. 164 where, -> where   
[13] ICLR 16
[17] ICLR 16
please check other references","
      
      The paper presents a method for predicting the future frames of video sequences. It contributes with a multi-stage GAN approach, including new loss functions to the GAN community specialized to the video domain. These loss functions enforce local and global smoothness over time. The paper has several positive and negative sides. 

      Positive sides:

      1. It is addressing an interesting and relevant problem, that is going to be of wide interest, especially in the computer vision community. 

      2. It is relatively novel. The new things are comming through the usage of the contrastive loss and the cross-correlation loss as well as the multi-stage GAN idea. 

      3. Relatively well written and easy to read. 

      4. Positive experimental evaluation. The results on the UCF dataset suggest that the combined method is better than the baselines, and the different components do contribute to a better overall performance. 


      Negative sides:
      1. Shallow experimental section. While the paper has an extensive experimental evaluation in the supplementary material, the experiments section in the paper is quite short and shallow. The paper could leave out some of the details from the previous sections and dedicate more space for the experiments. The paper says that it conducts experiments on three datasets, however, only one is reported in the paper. 

      2. Questionable experimental conduct. The reported results in the paper are on UCF, but the model is trained on Sports1M. Why the cross-dataset transfer? Is that comparable to the method of [13]? The paper should make sure the setup is the same as for the previous works. If cross-dataset transfer is needed, then how about seeing results on other pairs of datasets? 

      3. State-of-the-art comparison. The paper compares to state-of-the-art on the UCF dataset and to [13] only. It would be beneficial to see compatisons to other works on other datasets too. 

      3. Analysis. The paper would greatly benefit from a deeper analysis of different segments of the method. What are also the typical bad, failure cases? What is the maximum reasonable temporal extent at test time? How deep in the future can the method generalize when it is trained on X frames? 
      
      ","This paper proposes a coarse-to-fine pipeline with two-stage GANs for video frames prediction. For each stage, the generator takes as input a sequence of image patches of low-resolution and outputs a sequence of higher resolution patches. To further alleviate issues of unstable training of GAN and blurry frames, authors also compute the sum of discriminator's decisions on all frames and add two terms in the loss to encourage temporal coherence. Quantitative and qualitative evaluation on real-world video datasets demonstrate effectiveness of the proposed method.

1. 
In Figure 1, output of stage 2 has size of 128x128 but input size is only 32x32. According to section 2.1, height and width of output should be 2 times of input, therefore it should be 64x64 instead of 128x128.
2. 
As for NCC loss, I wonder why not compute that for the current predicted frame and the current ground-truth frame instead of the previous one. Besides, how is the square patch size h determined? How to handle the case when the motion of a patch is beyond the small local neighborhood? How much time does it take to compute NCC loss for a minibatch?
3.
In section 6.2, authors mentioned that random sequences of 32x32 patches are extracted from frames with significant motion for evaluation, but they did not explain how many sequences of patches are extracted from those frames for each video.
In addition, there is very little analysis about the experimental result, especially the qualitative result. 
4. 
It is better to add some intermediate visualization results to get more insight of the proposed two loss terms. For example, authors can visualize NCC similarity of a sequence and get a a heatmap from which the influence of NCC similarity can be checked. Author can also visualize p_i, p_{i+1} and corresponding Y^{\hat}_i and y^{\hat}_{i+1}.
"
Improving the Expected Improvement Algorithm,"Chao Qin, Diego Klabjan, Daniel Russo",https://proceedings.neurips.cc/paper/2017/hash/b19aa25ff58940d974234b48391b9549-Abstract.html,"In this paper the authors propose an optimal algorithm for the best arm identification problem, where the rewards of arms are drawn from normal distributions with unknown means and with a common known variance. 
The algorithm called Top-Two Expected Improvement (TTEI) is inspired by Expected Improvement algorithm, by the recent work of Russo [19] on top two-sampling and those of Garivier and Kauffman [8]on Chernoff’s stopping rule.
The Expected Improvement criterion is modified in order to take into account the expected improvement of an arm over another one. Then the top two sampling idea is used to choose two arms: the first is chosen using the Expected Improvement criterion, and the second one is chosen using the modified criterion with respect to the first chosen arm. A coin is tossed to choose the first arm with a probability \beta, and the second with a probability 1-\beta.  \beta is the parameter of TTEI algorithm. This smart algorithm can be used for the two settings of best arm identification problem: fixed confidence or fixed budget. In the case of fixed confidence the Chernoff’s stopping rule is used. 
The analysis establishes formal guarantees of optimality for algorithms such as TTEI, which tend to sample the arms with respect to the optimal proportions.  Another strong point, the analysis reveals that even if \beta is far from its optimal value (in worst case \beta=1/2 and \beta^*=1 or 0), the sample complexity bound is at most twice the optimal one.
The weak point of TTEI algorithm is that it is specialized on one distribution of the exponential family (the normal distribution), while the works of [19,8]  handle all exponential distributions with optimal algorithms. The experiments done on normal random variables show a small improvement in comparison to TTTS [19] and a more substantial improvement in comparison to TO [8].  The paper will be stronger, if the analysis of TTEI provided evidences (or clues) on the fact that TTEI performs better on Normal distribution than TO [8] as expected. Also, it would be interesting to test and compare TTEI with state-of-the-art on other distributions of the exponential family.
Overall, it is a very good paper. I vote for acceptance.

","The authors propose a new best arm identification algorithm called TTEI that is an extension of ""expected improvement"" (EI), a known heuristic from the Bayesian optimization community. The motivation for the extension is to match the asymptotic optimalities (rate of posterior convergence & expected stopping time), which are important properties of an algorithm. The authors analyze the asymptotic property of TTEI, show its (near)-optimality, and present its superior empirical performance in synthetic datasets.

Overall, well-written paper on an interesting problem. I highly rate the novelty of the algorithm. On the other hand, the asymptotic guarantee often does not capture the reality well in my opinion (described later, though I think this isn't a reason for rejection). I think the experiment part is the weakest in that they do not describe how and why their algorithm work better. Also, I would like to see how much TTEI behaves differently from TTTS since they share many asymptotic properties. This was not captured in the experiments.

Question on the rate of posterior convergence: The authors claim at line 97 that the analysis takes place in a frequentist setting. That is true, but isn't the analysis on posterior convergence essentially on the posterior probability ""computed by the algorithm""? Sure, we can talk about a quantity that is computed by the algorithm, but if one considers a frequentist setting then what is the meaning of Theorem 2 and how it is solving the problem in the frequentist setting? That is, shouldn't one also talk about what it means to have (1-\alpha_{n,1}) approach to 0 in the frequentist setting?

One weakness would be that the guarantee is limited to Gaussian (unlike other papers like the lil' ucb paper that applies to sub-Gaussian RVs), so it does not apply to bernoulli case that is very popular in practice. I wonder if one can relax the random variables to be sub-Gaussian whose scale parameter is bounded below a known constant. What is the authors' opinion on this? I wonder if there are critical steps in the proof that works for Guassian but not for generic sub-Gaussian.

Any moderate-confidence regime results? Recent analysis on the top arm identification (such as Chen and Li On the Optimal Sample Complexity for Best Arm Identification, 2016) tells us that there are some important terms that does not depend on \log(1/\dt), so there might be some practically relevant terms the asymptotic analysis is missing. I appreciate the asymptotic optimality, but what happens in practice is that no one wants to use \delta that is too small and in most cases \log(1/\delta) can be treated as constants in the set of problems we care about. I also wonder whether this fact can affect the practical performance.

Still, I like to emphasize that I don't think the lack of non-asymptotic analysis should be the reason for rejection in any ways as such analysis is often hard and I believe people should go beyond that to explore new algorithms.

Could you provide a more detailed comparison between Russo'16? Are there any difference in the asymptotic optimality? Any different sampling behavior that one can observe in practice?

Below L194, why do we need |mu_n,i - \mu_i| \le \epsilon there? If so, what is the key mathematical reason for that? (or, is it for convenience of mathematical derivation?) As far as intuition goes, it seems like if N is large and the sampling proportion is controlled, then one should be able to control |mu_n,i - \mu_i|. Let me know what the authors think.

In experiments, why do we use ""max_i \alpha_{n,i} \ge c"" as the stopping criterion? I thought the good stopping criterion is the one from Garivier & Kaufmann. How is this stopping rule used for the experiments justified in the frequentist setting? Also, I think the authors must have compared the actual probability of misidentification, especially as a graph in P(misid.) vs the # of samples, which measures 'anytime' performance that I think is practically relevant. Could the authors comment on this?

Also, I was expecting the reason why we see RSO works worse than TTEI. 

L274: f => of
L219: allocation => allocate
L260: guarantees -> guarantee
L294: what is the definition of the bold-faced numbers in Table 2? Does that coloring involves some statistical significancy test?","This paper studies the problem of best arm identification for stochastic bandits with a finite number of Gaussian arms. The asymptotic as well as fixed-confidence settings are considered. The author(s) introduce(s) a refined (randomised) version (TTEI) of the Bayesian ""Expected Improvement"" algorithm  (EI) and show that, in contrast to the original algorithm, it is optimal in the sense that all arms have optimal asymptotic sampling ratios (asymptotic setting) and essentially optimal sampling complexity (fixed-confidence setting). Moreover, the paper includes some experimental results comparing the empirical performance of TTEI with that of other algorithms suited for the problem and mentions possible future directions for investigating the problem.

I consider this to be an outstanding paper. It is truly a pleasure to read since the author(s) manage(s) to offer a well-organised, lucid and honest account although the topic is fairly complicated. This applies in particular to the long and difficult proofs.

I would like to make a few minor remarks that might be worth considering, but are not at all meant as criticism:
- The assumption that all arms have common known variance seems to be very optimistic. I would appreciate a comment on how this assumption can be relaxed or when it is realistic.
- Since $\beta$ is introduced as a tuning parameter of TTEI, it is not immediately clear why this parameter appears in the general optimal sampling proportions, see e.g. the first paragraph on page 4. Similarly, I feel it is slightly misleading that the proof of Theorem 3 begins with a remark on TTEI although the statement refers to arbritrary algorithms.
"
Towards Accurate Binary Convolutional Neural Network,"Xiaofan Lin, Cong Zhao, Wei Pan",https://proceedings.neurips.cc/paper/2017/hash/b1a59b315fc9a3002ce38bbe070ec3f5-Abstract.html,"The paper describes a new method for training binary convolutional neural networks. The scheme addresses both using binary weights and activations. The key insight here is to approximate the real valued weights via a linear combination of M binary basis weights. The coefficients for reconstructing the real weights can be found using least squares in the forward pass, and then pulled outside the convolution to allow for fast binary convolution at test time. A similar approach is taken for the activations, but in this case the weights and shifts are trained as normal during backpropagation. The result is a network that requires M more binary convolutions than a straightforward binary neural network, but it is expected that these will be significantly more hardware friendly. Experiments on ImageNet show that the approach outperforms competing approaches.

Binary convolutional networks are an important topic with with obvious commercial applications. The idea in the paper is good, and the results on ImageNet are encouraging.

Comments

1. Given that you approximate a real valued convolution with M binary convolutions, it seems to me that the approximate cost would be similar to using M binary CNNs like XNOR nets. With this in mind, I think the approach should really be compared to an ensemble of M XNOR nets trained with different initial weights, both in terms of total train time and prediction accuracy.

2. The paper suggests the v_i shifts are trainable. Why are values for these given in table 4 and the supplementary material. Are these the initial values?

3. The paper uses fixed values for the u_i's but mentions that these could also be trained. Why not train them too? Have you tried this?

4. It's not clear from the paper if solving regression problems for the alphas during training adds much to the train time.

5. It would be nice to have an estimation of the total cost of inference for the approach on a typical net (e.g. ResNet-18) given some realistic assumptions about the hardware, and compare this with other methods (perhaps in the supplementary material)","This paper proposes a scheme to approximate the weights of a neural network layer by the linear combination of multiple layers with binary {1, -1} weights. The proposed solution affects the inference time performance of the network only, but it relies on batch-normalization at training time to improve its binarizability.

The solution makes use of both local optimization of the weight matrices as well as using backpropagation to optimize them in their global context as well. A nice property of the approach is that even it can result on a speedup even on current hardware, but it has very good potential for even greater speed and power gains with custom hardware as it relies on very cheap and well parallelizable operations.

Given the fact that this is the first solution that can give comparable recognition performance while massively cutting the raw computational cost of the network (using 5 binary operations for each floating point), and its great potential when combined with custom hardware, this paper could en up having a great impact on the design of custom accelerators and mobile vision in general.

The only weakness is that the quality was tested against a relatively weak baseline: their full precision reference model reaches ~69% top-1 accuracy on the ILSVRC2012 dataset while the current state of the art is over 80% top-1 accuracy.","This paper extends previous work done for binary CNNs using multiple binary weight/activation matrices. Although it is presents incremental additions to prior work, the paper shows strong large-scale results on the standard ImageNet benchmark as well as thorough experimentations, and a clear presentation. This paper would entertain a wide interest among NIPS audience. 

Comments and suggestions for improvements:
-	You stated that the B and v parameters of activations are learned by the NN, but in table S5, the learned Bs are all 1.0 except for one row, while v values are almost symmetric. Could you elaborate further on this point, and update the text to state the exact procedure you use for learning/setting these parameters.
-	Could you provide a histogram of the discovered alpha values for weight matrices, to gain more insight on how sparse they are?
-	How bad are the results if you don’t initialize by the FP CNN? Do you need to start from a fully converged model, or just couple epochs are sufficient?
-	Given the strong correlation between the number of binary weight matrices M and final performance (as shown in Table 1, and figure S4), Could you provide further experimental results to show the performance drop when M > 5 (as stated in section 3.3) and the potential effect of regularization to fix it?
-	In section S1, algorithm 1: you need to add gradient and update for the v parameters (based on the text you learn them by backpropagation).
-	In section S5, please state if you fix the activations to FP.

"
Spectrally-normalized margin bounds for neural networks,"Peter L. Bartlett, Dylan J. Foster, Matus J. Telgarsky",https://proceedings.neurips.cc/paper/2017/hash/b22b257ad0519d4500539da3c8bcf4dd-Abstract.html,"Updates after rebuttal: some of my concerns are addressed, so I updated the rating a bit. However, the last point, quoting the rebuttal ""Spectral complexity can be compared across architectures, and indeed
part of our aim in obtaining a bound with no explicit scaling in the
number of layers was to compare architectures with drastically different
numbers of layers.""

What I meant is actual experiments that apply the proposed technique to empirically compare networks with very different architectures. It would be great if at least one such experiment could be included in the final version if this paper get accepted.

------------------------

This paper investigate the question of measuring the effective complexity or generalization capability of a network and propose to do it via the notion of margin normalized by a spectral complexity measure. 

With a covering number based analysis, a margin based generalization bound is derived to suggest that, if the training could be achieved with large margin and bounded R_A, then the test accuracy can be controlled effectively. Based on the theoretical results, the normalized margin (by the spectral complexity measure) is proposed as a measure on various networks. Among with other results, it is shown that this metric could effectively distinguish the networks trained on random labels and true labels (while the unnormalized margin could not).

Although a lot of open questions remains (as stated in the end of the paper), the current paper is already quite interesting. The reviewer is willing to change the rating if the following technical details are addressed:

- Please add ticks to all the plots. Currently it is very hard to interpret without knowing the scale of the axis. For example, in Figure 1, is the mean of the two margin distributions further apart in (c) than in (b)?

- Could you also plot the mean of the normalized vs unnormalized margins as training goes (i.e. like Figure 1(a)) on cifar and rand label?

- Could you add caption to Figure 3?

- What is W in Theorem 1.1?

- F_A in line 63 is a typo?

- Could this measure be used to compare across architectures? Different architectures could have different test accuracy on cifar. Could you take a number of different architectures and compare their test accuracy vs. their normalized margins?","This paper proposes analysing the margin distributions of deep neural network to analyse their generalization properties. In particular they propose a normalized margin estimator, which is shown to reflect generalization properties of certain neural networks much better than the naive margin computation.

Your notion of margin is different than what I'm used to. (I usually hear margin defined as the minimum distance in input space to a point of a different class, as SVMs are commonly depicted). Maybe just take a few words to stress this?

Why does the x axis on some (if not all?) your figures not start at 0? Shouldn't the margin always be positive? Unless you're not taking y to be the argmax of F(x) but instead the true class, in which case it would be negative for a missclassification. Maybe mention this.
What is the data distribution in your plots? Train, validation, or test? I general your plots should have clearer axes or captions.

In Theorem 1.1 it's not clear to me what B and W are (without the appendix), maybe just briefly mention what they are.

CIFAR and MNIST are acronyms, and should be capitalized. Ditto for SGD.
There are some typos throughout, and I feel like the writing is slightly ""casual"" and could be improved.

Your section on adversarial examples is somewhat puzzling to me. The margins, let's say M1, that you describe initially are in Y-space, they're the distance between the prediction f(x)_y and the next bess guess max_i!=y f(x)_i. This is different than the input space margin, let's say M2, which is something like `min_u ||u-x|| s.t. f(u) \neq f(x)`. As such when you say ""The margin is nothing more than the distance an input must traverse before its label is flipped"", you're clearly referring to M2, while it would seem your contribution revolves around M1 concept of margin.
While intuitively M1 and M2 are probably related, unless I'm missing something about your work relating them, claiming ""low margin points are more susceptible to adversarial noise than high margin points"" is a much stronger statement than you may realise.


Overall I quite like this paper, both in terms of the questions it is trying to answer, and in the way in does. 
On the other hand, the presentation could be better, and I'm still not convinced that the margin in output space has too much value in the way we currently train neural networks. We know for example that the output probabilities of neural networks are not predictive of confidence (even to an extent for Bayesian neural networks). I think more work relating the input margin with the output margin might be more revealing.

I've given myself a 2/5 confidence because I am not super familiar with the math used here, so I may be failing to get deeper implications. Hopefully I'll have time to go through this soon."
Consistent Multitask Learning with Nonlinear Output Relations,"Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, Massimiliano Pontil",https://proceedings.neurips.cc/paper/2017/hash/b24d516bb65a5a58079f0f3526c87c57-Abstract.html,"The paper tackles multi-task learning problems where there are
non-linear relationships between tasks. The relationships between
tasks is encoded as a set of non-linear constraints that the outputs
of each task must satisfy (e.g . y1^2 + y2^2 = 1). In a nutshell, he
proposed technique can be summarized as: use kernel regression to make
predictions for each task independently, then project the prediction
vector onto the constrained set.

Overall, I like the idea of being able to take advantage of non-linear
relationships between tasks. However, I am not sure how practical it
is to specify the non-linear constraints between tasks in practice.
The weakest point of the paper is the empirical evaluation.

It would be good if the authors can discuss real applications where
one can specify a reasonable set non-linear relationships between
tasks. The examples provided in the paper are not very
satisfactory. One imposes deterministic relationships between tasks
(i.e. if the output for task 1 is known then the output for task 2 can
be inferred exactly), and the other approximates the relationship
through a point cloud (which raises the question why was the data in
the point cloud not used as training data. Also to obtain a point
cloud one needs to be in a vector-valued regression setting).

The paper could be significantly improved by providing a more thorough
empirical evaluation section that analyses the performance of the
proposed method on multiple real problems.  One of the main
contributions of the paper, as stated by the authors, is the extension
of the SELF approach beyond vector-valued regression. But evaluation
is done only on vector-valued regression problems.

The technique depends on using a kernel K for the kernel
regression. Is there any guidance on how to select such a kernel? What
kernel was used in the experimental section?

It would also be good if the authors analyze the effect of various
design and parameter choices. How sensitive are the results to the
kernel K, to the regularization parameter \lambda, to the parameters
\delta and \mu?

How were all the above parameters set in the experimental section?

Looking at figure 1, I am very surprised that the performance of STL
seems to converge to a point significantly worse than that of
NL-MTL. Given a large amount of data I would expect STL to perform
just as well or better than any MTL technique (information transfer
between tasks is no longer relevant if there is enough training
data). This does not seem to be the case in figure 1. Why is that?
","Summary: 
The authors propose a method for multi-task learning with nonlinear output relations using kernel ridge regression combined with manifold/discrete optimization. They provide detailed theoretical analysis on the generalization properties of their method. Experiments show that the method works well on two synthetic data and a nonlinear dynamical systems dataset of a robot arm. 


Comments: 
- In general the paper is well-written and clear. I like the careful analysis on generalization bounds and convergence rate of the excess risk. That is a main contribution of the current paper. 

- The main difficulty of applying the current method is the identification of the constraint set C, which describes the nonlinear relationship between the tasks. For example, in the synthetic data the user needs to know the exact equation governing the curve, and in the SARCOS task extra examples are required to estimate the non-linear manifold of the outputs. If the constraint set C is mis-specified, which could be common in actual applications, the end result could be worse than STL, just like the mis-specified linear MTL models in the two synthetic datasets. There are many more ways to mis-specify a nonlinear relationship than a linear one. If there are not enough examples from the output space to estimate the joint relationship between tasks, the end result could be worse than not applying MTL.  
","This paper presents a formulation and algorithm for multi-task learning with nonlinear task relations.   A set of constraints are required to be satisfied while optimizing the joint functions.  Extensions are considered when the constraints cannot be all satisfied. The work is novel in the sense that most previous multitask works considered only linear relations among the tasks.  An algorithms and synthetic as well as a robotic domain experiments are shown to evaluate the approach.

The paper is theoretically sound and I find the algorithm interesting, although one question in my mind is whether using linear relations to approximate the nonlinear ones is already sufficient in many real world domains.  While Table 1 shows significant improvements of the nonlinear MTL approach, it is not clear whether the nonlinear MTL models suffer from overfitting due to increased model complexity.  Perhaps more experiments in different problem domains will convince the reviewer. "
Deep Recurrent Neural Network-Based Identification of Precursor microRNAs,"Seunghyun Park, Seonwoo Min, Hyun-Soo Choi, Sungroh Yoon",https://proceedings.neurips.cc/paper/2017/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html,"In this paper, the authors address the prediction of miRNA precursors from both RNA sequence and secondary structure with a recurrent neural network. 

More precisely, the secondary structure is predicted using RNAfold. Hence each position in the sequence is annotated with two characters, one among {A,T,C,G} to encode the sequence itself and the other among {(,:,)} corresponding to the secondary structure according to classical dot-bracket notation. These are transformed into a 16-dimensional vector using one-hot encoding. Now that the problem is about classifying sequences, it makes sense to use state-of-the-art units for sequence classification, that is to say, LSTMs with an attention mechanism. Attention weights then show which parts of the sequences are activated in each of the positive and negative classes. The core contribution of the paper hence build quite naturally on the literature, but to the best of my knowledge this had never been done before and this appears to be well done. I have a few concerns nevertheless, namely:

- I am not very familiar with LSTMs, and I have a hard time understanding the architecture of the network, in particular, relating Figure 2 with the equations in Section 3.2. 

- In the Introduction, the authors mention two neural-network-based algorithms for the same task, namely MiRANN and DP-miRNA. Why aren't those evaluated in the experiments?

- Regarding Table 2, are those AUCs cross-validated or reported on the test set? It seems strange that the proposed method outperforms the others in terms of AUC on 'human' but not in terms of any other metric.

- At the end of Section 4.1, the authors report p-values on the significance of their results. I would rather like to see whether their method significantly outperforms the closest one (for instance, is the AUC of 0.9850 on human significantly better than that of 0.9802?)


","The paper presents an LSTM model with an attention mechanism for classifying whether an RNA molecule is a pre-microRNA from its sequence and secondary structure. Class weights are incorporated into log-loss to account for class imbalance in the datasets used. The proposed method is extensively evaluated against 5 other existing methods on 3 datasets, and is shown to outperform the existing methods in most cases. The paper then attempts to give some insight into the features that are important for achieving good performance. First, by showing that secondary structures are largely responsible, but sequence features give a small boost, and second, by interpreting the attention weights using an adapted version of class activation mapping (proposed in an earlier CVPR paper). Results using different architectures and hyperparameters are also shown.

The paper presents an interesting application of LSTMs to biological data, and has some novel elements for model interpretation. The main issue I have with this paper is in how the method is evaluated. If the goal is to find new pre-microRNAs, especially those that have noncanonical structures (as stated in the abstract) special care must be taken in the construction of the training and test sets to make sure the reported performance will be reflective of performance on this discovery task. This is why in ref [10] describing miRBoost, methods are evaluated based on ability to predict completely new pre-microRNAs that were not previously found in previous versions of miRBase (included as 'test' dataset in this paper). While holding out 10% of the dataset as a test set may seem to also be sufficient, it results in a far smaller evaluation set of positives (1/5 - 1/10) than using the 'test' dataset, as well as possibly allowing the models to learn structural features of newly discovered RNAs, some of which may have been randomly placed in the training set.

An even stricter evaluation would use ""structurally nonhomologous training and test data sets"", as proposed in Rivas et al. (PMID:22194308), that discusses the importance of doing so for properly evaluating methods on the related RNA secondary structure prediction task. If a model is able to perform well on this stricter evaluation, where test RNA sequences are structurally different from training sequences, one can then be more confident in the ability to discover structurally novel pre-microRNAs.

A related comment on the evaluation is that the AUROC metric is unsuitable for tasks where only performance on the positive class is important (as in this case) and the area under the precision-recall curve (AUPR), or even better, the area under the precision-recall-gain curve (AUPRG; Flach and Kull NIPS 2015) should be used.

The adaptation of the class activation mapping method to the LSTM model is interesting and the paper hints that it could indeed be useful in uncovering new biological insight. It would strengthen the section if features common to the negatively predicted examples could be discussed as well.

The attention mechanism seemed to make a big difference to the performance - it would be interesting if it was possible to dissect how/why this is occurring in the model (and on this task). Does the attention mechanism make a bigger difference for longer sequences? And is it possible to show some of the long range dependencies that the attention mechanism picks up?


Other questions for the authors:

- In section 4.3 were the chosen examples actually correctly classified as positives and negatives respectively?

- Line 175 - how were the dropout rates selected empirically? Using cross validation?

- In the discussion it is mentioned that additional data modes can be added as separate network branches. Why was this approach not taken for the structure data and instead an explicit encoding that simultaneously considers structure and sequence was used?


Typos:

Line 91: ""long-term dependency"" -> ""long-term dependencies""

Table 1: ""Positive exmaples"" -> ""Positive examples""

Table 2 caption: ""under curve of receiver operation characteristic"" -> ""area under receiver operating characteristic curve""

Table 3: microPred (test), cross-species, 0.985 should be highlighted instead

Figure 4 caption: ""RNN ouputs"" -> ""RNN outputs""

Line 237: ""technqiue"" -> ""technique""

Reference 10 should be Tran et al. not Tempel et al.
","The authors present a nice paper on the first deep learning attempt at pre-miRNA identification, and demonstrate results (marginally) better than state of the art approaches. The main strength of the paper is the originality of use of RNN for this task (given that this is the first such attempt), and the fact that they are able to demonstrate even subtle improvements over methods that employ hand-crafted features, indicating that their approach effectively extracts features in a manner that is superior to the effectiveness of human experts with domain expertise, a compelling demonstration.  Some positive attributes: The paper is well written and clear, I liked the analysis of seq vs structure vs both (demonstrating that seq doesn't add much!), and I am insufficiently familiar with RNA prediction to speak to the novelty of this, but I liked their method for encoding seq+structure as a matrix of one hot vectors. 

Additional comments:
-Section 4.3, in which an effort is made to extract the useful features, is pertinent and interesting, but a bit poorly developed. What insights were gained from this analysis? It seemed only to confirm what the researchers would have guessed a priori, validating that the method works, but not providing any useful insights. 

-The main weakness of the paper is that it appears as an assembling of components in order to try out RNN on this prediction task, precisely the opposite of what the authors claim at the bottom of section 1. In what way was this study a careful crafting of a pipeline optimized to this task (as claimed), rather than a somewhat arbitrary compilation of layers? Similarly, a number of settings are presented with neither reference nor justification (e.g. last line of section 3.2. Why are those activation functions used for their respective layers?). All together, this results in a possibility that the approach is overfit, not the model itself (as nicely demonstrated, the final model does NOT overfit these data) but the arbitrary-seeming nature of the path towards the final model makes it seem like the process itself overfits and 'cherry picks' this model. 

The authors could address this weakness by clearly stating their contributions in this study (how each component of the pipeline was chosen, how it fits this specific task), by explaining why they tried the configurations listed in Table 4, and by explaining their 'design decisions' a bit more thoroughly. Table 4 summarizes their architecture exploration on one of the datasets. It was unclear to me if they used only that one dataset to select among the architectures? Or if they are only presenting one set of data in the table, but all 3 were used in selecting the final model architecture? They should clarify this point. If in fact all 3 datasets were used,  if it is possible to leave one out for this stage and re-select the architecture, then check its ability to generalize to the final dataset, this would go far to assuage concerns of the process leading to an overfit model. 
"
Boltzmann Exploration Done Right,"Nicolò Cesa-Bianchi, Claudio Gentile, Gabor Lugosi, Gergely Neu",https://proceedings.neurips.cc/paper/2017/hash/b299ad862b6f12cb57679f0538eca514-Abstract.html,"
      	Pros:
      	- A systematic study on the classical Boltzmann exploration heuristic in the context of multi-armed bandit. The results provide useful insights to the understanding of Boltzmann exploration and multi-armed bandits

      	- The paper is clearly written

      	Cons:

      	- The technique is incremental, and the technical contribution to multi-armed bandit research is small.

      	The paper studiee Boltzmann exploration heuristic for reinforcement learning, namely use empirical means and exponential weight to probabilistically select actions (arms) in the context of multi-armed bandit. The purpose of the paper is to achieve property theoretical understanding of the Boltzmann exploration heuristic. I view that the paper achieves this goal by several useful results. First, the authors show that the standard Boltzmann heuristic may not achieve good learning result, in fact, the regret could be linear, when using monotone learning rates. Second, the authors show that, if the learning rate remain constant for a logarithmic number of steps and then increase, the regret is close to the optimal one. This learning strategy is essentially explore-then-commit strategy, but the catch is that it needs to know the critical problem parameter \Delta and T, which are typically unknown. Third, the authors propose to generalize the Boltzmann exploration by allowing individual learning rates for different arms based on their certainty, and show that this leads to good regret bounds. 

      	The above serious of results provide good understanding of Boltzmann exploration. In particular, it provides the theoretical insight that the naive Boltzmann exploration lacks control on the uncertainty of arms, so may not preform well. This insight may be useful in the more general setting of reinforcement learning.

      	The paper is in general well written and easy to follow.

      	The technical novelty of the paper is incremental. The analysis are based on existing techniques. The new technical contribution to the multi-armed bandit research is likely to be small, since there are already a number of solutions achieving optimal or new optimal regret bounds. 

      	Minor comments:

      	- line 243, log_+(\cdot) = min{0, \cdot}. Should it be max instead of min?
      	





      ","This paper revisits Boltzmann exploration in stochastic MAB. It discusses when Boltzmann exploration could be done wrong (Section 3), and also discusses how to do it right (Section 4).

This paper is interesting and well-written in general. Boltzmann exploration is a commonly used exploration scheme in MAB and RL, and this paper might improve the RL community's understanding of this important exploration scheme. That said, I recommend to accept this paper. However, I only give a ""marginal acceptance"" for the following reasons:

1) One key insight of this paper is not new. It is well-known in the RL community that a naive implementation of the Boltzmann exploration could lead to bad performance, and the main reason is that Boltzmann exploration does not consider the uncertainty of the empirical reward estimates. This reduces the novelty of this paper.

2) This paper has proposed one way to fix this issue (Section 4, ""Boltzmann exploration done right""). However, there are many other ways to fix this issue by using a different exploration schemes (e.g. UCB1, Thompson sampling). It is not clear to me why we should use the modified Boltzmann exploration scheme proposed in this paper. In particular, no experimental comparisons have been provided in this paper.

One question: in the first equation of Theorem 3, the regret is O(log^2(T)/\Delta^2). However, in the second equation of that theorem, the regret becomes O(log^2(T)/\Delta). Please explain.","[I have read the other reviews and the author feedback, and I maintain my accept rating.
Looking forward to seeing the additional expt comparisons the authors alluded to in their feedback.]

This paper studies several variants of Boltzmann exploration for stochastic bandit problems theoretically and convincingly demonstrates when (and why) it is sub-optimal as well as simple modifications to the exploration strategy to achieve (poly)logarithmic regret. These modifications differ from the classic learning-rate schedules in reinforcement learning (e.g. Singh et al'00) that attenuate the temperature of the Boltzmann distribution adaptively. Instead, this paper introduces per-action ""learning rates"" (reminiscent of per-action confidence intervals in UCB strategies) to achieve low regret. It is debatable if the resulting (implicit) distribution over arms looks anything like a Boltzmann distribution.

A small experiment (e.g. a simulation study) will be very informative -- does this strategy remain robust when we get the scale of \beta's wrong (by poorly setting C)? does attenuated Boltzmann exploration (e.g. Singh et al'00) lose substantially to this approach for practical problem sizes? Since this approach precludes the use of importance weighted estimators (computing the posterior prob. of picking an arm seems to be hard) whereas attenuated Boltzmann exploration can also be used with importance weighting, a more careful comparison seems necessary.

Minor:
113: Use >= in the eqns
Does the analysis of Eqn(6) also apply to importance-weighted reward estimates (thereby providing a unified treatment of Seldin&Slivkins'14 also)?"
End-to-end Differentiable Proving,"Tim Rocktäschel, Sebastian Riedel",https://proceedings.neurips.cc/paper/2017/hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html,"The paper describes a unified approach to learning deep neural networks with a symbolic AI approach. Specifically, based on prolog's backward conditioning approach, the neural network is constructed recursively to prove facts in a Knowledge base. The authors develop differentiable operations for unification, AND and OR. Vector representations of symbols help in sub symbolic matching for symbols that are similar but not identical. Experiments are conducted on three KBs.

Overall the paper is quite good. It unifies two very popular areas in AI/ML and therefore I would expect this to be significant and something that could potentially result in a lot of follow-up work. The writing is good, maybe the background is a little too short. The results seem to be on-par with ComplEx, but the interpretability of a the results is a plus in the proposed system. The experiments are not run on particularly large KBs, so is scalability an issue. From what I understand, the computational complexity of the proposed system could be quite high. One thing that was not so clear to me is, it turns out that NTP-lambda is often better than NTP. Since this uses, ComplEx, I was not so sure of its role in the learning. Specifically, are a subset of the parameters learned by ComplEx, if so which subset, and how much effect does NTP learning have in this combined model.","Summary of paper
----------------
The paper presents a novel class of models, termed Neural Theorem Provers (NTPs), for automated knowledge base completion and automated theorem proving, using a deep neural network architecture. The recursive construction of the network is inspired by the backward chaining algorithm typically employed in logic programming (i.e., using the basic operations unification, conjunction and disjunction). Instead of directly operating on symbols, the neural network is employed to learn subsymbolic vector representations of entities and predicates, which are then exploited for assessing the similarity of symbols. Since the proposed architecture is fully differentiable, knowledge base completion can be performed using gradient descent. Thus, following the fundamental philosophy of neural-symbolic systems, the paper aims at combining the advantages of symbolic reasoning with those of subsymbolic inference. The paper briefly describes and discusses possible optimizations of the initial, computationally demanding algorithm (i.e., parallel evaluation of proofs and a truncation heuristic to speed up the gradient calculation), and proposes an interesting and apparently very effective training/regularization procedure based on the joint training of NTP together with a state-of-the-art neural link prediction method called ComplEx. Finally, it is shown that the performance of the proposed method on four benchmark knowledge base completion tasks is competitive against ComplEx. Also, it is argued that in contrast to previous approaches, the proposed algorithm is highly interpretable since it is capable of inductive logic programming, i.e., inducing concrete symbolic, first-order logic rules from parameterized templates. The paper concludes with some reasonable and likely worthwile ideas for future investigation.


Short summary of review
-----------------------
This is a solid paper. It aims at solving an important open problem and proposes a creative and effective solution by combining the efforts and thus aggregating the advantages of two very active lines of research. Particularly the dedication to interpretability of the method is commendable. The manuscript is rigorous and at the same time easy to follow. While there are a few issues concerning the quality and clarity, the overall article clearly has merit to be published at a venue like NIPS, and likely is of interest for several of its sub-communities.


Quality
-------
In general, the paper follows a high quality standard. While not being very theoretical in nature, the few technical details provided appear sound. While the experimental section impressively shows how NTPs excell at multi-hop reasoning and inducing interpretable first-order logic rules, it is not particularly successful at convincing the reader of the superiority of NTPs as compared to the state-of-the-art. This is because there is only a single competitor, ComplEx; employing a broader variety of competing algorithms (e.g. from the extensive pool of statistical relational learning methods) would certainly be commendable. However, this might be acceptable in the light of the contribution/focus of the paper being more on interpretability, rather than on significant performance increase. Another central issue with the paper is that while it prominently claims to have designed a system capable of 'theorem proving', the reader is actually never exposed to any evidence that the architecture at hand has indeed the ability to prove mathematical theorems. While this lack is acknowledged in the last sentence of the paper, the fact that said phrase is used in the first sentence of the abstract and even the title may provoke suspicions of deliberatley exploiting its catchiness for marketing purposes. Related work is discussed in commendable detail, providing a solid overview of the field, and extensive references are provided.

Clarity
-------
Most of the paper is well-organized and written in a clear, concise manner, making it rather easy to follow. Figure 1 is very helpful in understanding the general NTP architecture and the chosen running example (i.e., grandfatherOf) is simple enough to enable quick and intuitive understanding and at the same time expressive enough to convey and illustrate the central ideas of the method (also, the subtle reference to popular culture increases the enjoyability of reading the manuscript). However, the structure of section 3 is not quite reasonable: The paragraph directly following the heading '3 Methods' is only refering to subsections 3.1-3.3 (i.e., the modular building blocks of the architecture), but not to subsections 3.4-3.7, which appear to have been placed there rather arbitrarily. Maybe splitting section 3 into two distinct sections would help (e.g. '3 Architecture' and '4 Training & Inference' or something). Also, it would in my view make more sense to put the section on regularization (i.e., 3.7) immediately after the one on training (i.e., 3.4) since these are highly related and the former builds upon the latter (this would also enhance reading flow since one wouldn't have to scroll up to recall the formula for L_ntp, and to identify that the ntp-term and the complex-term share the same structure). Furthermore, the very short background section only defines terminology and notation, but does not provide any details on the fundamental mechanisms of logic programming (e.g. the backward chaining algorithm) that are required to be able to follow the manuscript. Given the tight page limit at NIPS it is evident that extensive coverage of preliminaries is not feasible, but the paper should at least provide pointers to relevant literature for those not familiar with the matter.
Some further, specific issues:
- it is not described how exactly the artificial neural network is created/constructed; it is stated in section 3.1 that 'The module unify [...] creates a neural network [...].' (l. 105-106), the specifics of which are not quite obvious; it should be detailed in what sense the min operation and the RBF kernel correspond to a neural network, and how the recursive construction of the network is to be understood; a graphical representation may be helpful here
- there appear to be inconsistencies regarding the employed notation of data structures: atoms seem to be represented as lists (of three terms), meaning that rule bodies should be represented as lists of lists; however, in the definition of the AND module (section 3.3), sets (denoted by Euler script) are used instead of lists of lists for the first argument, which is also contradicting the signature of AND ('L is the domain of lists of atoms', l. 153); furthermore, the :: operator doesn't make sense on sets; the quickest fix for this issue would be to just define Euler scripts to denote lists of lists instead of sets
- in the example for the unification module (section 3.1), where did the exponentials from the RBF kernel in the proof success score go?
- in Figure 1, it is not clear why S_33, S_313 and S_323 fail; the UNIFY module can only fail if the arity of the atoms doesn't match, which doesn't appear to be the case here; thus, the computation can only fail due to reaching the maximum proof depth; however, as I see it, in that case further paths beyond (for example) S_33 must be explored first, meaning that so the operation cannot fail at S_33 already; this should be clarified
- it would be helpful to have an inference example, i.e. an example of how the success score of a given statement is calculated, given the computational tree/graph in Figure 1; some kind of visualization of this would be helpful as well

Originality
-----------
As is discussed in detail, the paper is built upon the ideas of extensive prior work, and its main contribution essentially boils down to effectively combining two previously independent lines of research: 1) neural-symbolic systems for reasoning in knowledge bases, e.g. as developed in the field of statistical relational learning, and 2) the lately emerging end-to-end differentiable neural network architectures aiming at mimicking some of the capabilities of modern computers. Thus, the paper does not really provide any fundamental novel insight, but rather takes inspiration from said fields and symbiotically combines them in a novel and seemingly effective manner, succeeding at exploiting the advantages of both classes of approaches. The first line of work, i.e. neural-symbolic systems, aims at combining the benefits of subsymbolic similarity-based reasoning (and thus generalization) ability of neural link prediction / statistical relational learning with the symbolic multi-hop reasoning ability of automated theorem provers / logic programming systems. However, previous neural-symbolic approaches are either a) not trainable end-to-end, b) not interpretable and/or c) not capable of incorporating prior domain-specific knowledge. This is where the second line of work, i.e. fully differentiable computing neural network architectures, comes into play, enabling NTPs to handle all the features a), b) and c). Another contribution is the proposal of the recursive construction of the neural network in terms of modules (for unification, conjunction and disjunction operations), similar to dynamic neural module networks, and inspired by the backward chaining algorithm used for example in Prolog.

Significance
------------
The creation and modeling of large knowledge bases/graphs is becoming increasingly important due to their potential support of practical tasks such as information retrieval, question answering etc. Recent efforts in distilling humanities knowledge into large-scale triple-based knowledge graphs (such as Freebase, YAGO, Google KG) are still challenged by difficulties in achieving high amounts of completeness, which is a natural product of the tremendous volume of true facts. Since hand-engineering these systems is clearly not feasible, methods capable of automatically augmenting knowledge bases/graphs based on logical reasoning and statistical inference are highly warranted. The method proposed in this paper has the potential to provide valuable insights - both for scientists as well as for practitioners - into the non-trivial task of knowledge base completion. Especially the ability of NTPs to induce human-readable, logical rules, setting it apart from the majority of previous work, may open up tremendous possibilities, particularly in the light of the recently emerging, significant interest in interpretability in AI / machine learning systems (e.g. as desired by end users, policy makers / governmental bodies and AI critics).

Minor issues
------------
- the explanation of the four rules of the AND module (l. 157-162) is referring to 'line(s)' multiple times; however, the rules aren't formatted in distinct lines, so the formulation is a bit confusing; rather use 'rule(s)' instead of 'line(s)'
- the last sentence of the first paragraph of section 3.4 (i.e., l. 176-177) is gramatically flawed
- in l. 199, it should read 'predicates with indices r, s and t, respectively'

","The paper introduces a new method for neural-symbolic reasoning on relational data by constructing neural networks that emulate the backward chaining algorithm of Prolog. 

In general, the paper addresses a very interesting problem, since the combination of neural networks/embedding methods and rule-based systems for reasoning on symbolic data is a promising research direction that has a long history in AI. What I found especially interesting about this method it that it allows to compute the gradient with regard to the proof success. This a very interesting property that enables, for instance, to search for rules in an ILP setting via gradient descent. It also allows to integrate this method with other gradient-based learning methods, e.g., for visual reasoning tasks.

The paper is written well and reasonably good to understand. However, the description of the modules and the construction of a full NTP could be described in more detail (e.g. via additional pseudocode in the supplementary material). Currently, it would be challenging to exactly reproduce the model only from the information in the paper.

Overall, there are many aspects that I like about the paper and the ideas that it pursues. My relatively low-score stems from the following concerns:

First, I agree that the method could be used for the task of theorem proving. Unfortunately, in its current form, the paper actually doesn't show this. The experimental evaluation is done for simple link prediction tasks and, in addition, the paper shows qualitative results with regard to rule induction. However, in both cases no real theorem proving is required. Based on these experiments, I don't see how it can be claimed that the proposed method is a functioning theorem proving system. I'd like to emphasize that this concern is not related to the method itself, but to the generality in which it is currently presented.

My second concern is the intended use of NTPs. If we are only concerned about link prediction (i.e. what is evaluated in the experiemnts), using NTPs would introduce a lot of heavy machinery but the improvements compared to a much simpler method such as ComplEx are seemingly small. Moreover, due to the large computational cost, NTP are limited to small datasets while methods like ComplEx scale very well. Furthermore, with regard to rule induction it would be important to compare the rules that are learned with NTP to those that can be extracted from embeddings or graph based methods (e.g. [1,2,3]) or classical methods ILP such as FOIL. NTPs seem to be a powerful method, but unfortunately its full benefits are currently not really tested or evaluated.

Overall the paper introduces interesting ideas and a promising approach for neural-symbolic reasoning. Due to this I am currently leaning towards accept. However, I'd ask the authors to address my above concerns in their response.

[1] Yang et al. ""Embedding entities and relations for learning and inference in knowledge bases"", ICLR.
[2] Lao et al. ""Random Walk Inference and Learning in A Large Scale Knowledge Base"", EMNLP, 2011.
[3] Neelakantan et al. ""Compositional Vector Space Models for Knowledge Base Completion"", EMNLP, 2015."
Tomography of the London Underground: a Scalable Model for Origin-Destination Data,"Nicolò Colombo, Ricardo Silva, Soong Moon Kang",https://proceedings.neurips.cc/paper/2017/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html,"The authors develop a novel model for a real-world problem of predicting the number of people traveling through stations at a given time, given dataset of origin-destination pairs of timestamps. The paper presents maximum-likelihood inference for an approximate model (to make it tractable) and variational inference for the true model. The ML method is shown to perform adequately on synthetic data. On real data visualizations (fig 3 and 4) of predictions on a massive real dataset are provided as evidence that it works well. 
The model is bound to be of much interest to network researchers. 

The main criticism of the paper is it's evaluation of the predictive ability of their method on the real dataset is lacking. They present scatter plots and time-series requiring the user to verify visually that their method works.  These need to be quantified (using e.g., predictive L1 error or something more appropriate), even if only a proxy for the ground-truth is available. Competing methods (e.g., [18]) are not evaluated on the real data; if they're not appropriate for this data, it needs to be addressed in the text.  They need to discuss when their method works and when it fails using exploratory analysis. Further, they present the VI method even though they appear not to have succeeded in scaling it to the size of the real dataset.

1. what are the units for figure 2? minutes? is the x-axis ""starting time""?
2. you mention stochastic gradient descent in 173-178 but it's unclear if this was actually implemented or used in evaluation?
3. in figure 3, the scatter plots suggest that the initialization does as well as the ML method? what is going on here?
4. unnormalized distance scores are presented in several figures. these are hard to interpret.

","I thank the authors for the clarification in their rebuttal. It is even more clear that the authors should better contrast their work with aggregate approaches such as Dan Sheldon's collective graphical models (e.g., Sheldon and Dietterich (2011), Kumar et al. 2013, Bernstein and Sheldon 2016). 

Part of the confusion came from some of the modeling choices: In equation (1) the travel times added by one station is Poisson distributed?! Poisson is often used for link loads (how many people there are in a given station), not to model time. Is the quantization of time too coarse for a continuous-time model? Treating time as a Poisson r.v. requires some justification. Wouldn't a phase-type distribution(e.g., Erlang)  be a better choice for time? Such modeling choices must be explained.

The main problem, I fear, is not in the technical details. The paper is missing a bigger insight into inferring traffic loads from OD pairs. Something beyond solving this particular problem as a mixture of Poissons with path constraints. 

It is important to emphasize that the evaluation is not predictive (of the future) but rather it gives estimates of hidden variables. It is a transductive method as it cannot predict anything beyond the time horizon given in the training data. I am OK with that.

Figure 4 is missing a simple baseline using the shortest paths.

The results of the model in Figure 5 don't seem to match the data in the smaller stations (e.g., Finchley Road).

Minor comments:
What is the difference between the distributions Poisson and poisson in eqs (7) and (9)?
Figure 3: botton => bottom

      A. Kumar, D. Sheldon, and B. Srivastava. 2013. Collective Diffusion over Networks: Models and Inference. In Proceedings of International Conference on Uncertainty in Artificial Intelligence. 351–360.
      Garrett Bernstein, Daniel Sheldon,Consistently Estimating Markov Chains with Noisy Aggregate Data, AISTATS, 2016.
      Du, J., Kumar, A., and Varakantham, P. (2014, May). On understanding diffusion dynamics of patrons at a theme park. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems (pp. 1501-1502). International Foundation for Autonomous Agents and Multiagent Systems.
      ","This paper is about using probabilistic modeling to infer traffic patterns along a network given only the origin-destination observations. The motivating example is finding the paths of travelers on the London Underground given their “tap in” and “tap out” stations. 

The problem studied is of important theoretical interest for graph analysis and has applications to human mobility, computer networks, and more. The model the authors specified was innovative and practical, I particularly liked the conditioning of each leg of the journey on the absolute time (Eq 1) and the initialization procedure suggested. The model was described at just the right level of generality for solving a variety of problems. 

Here are ideas for improvements. Some of the details of the variational inference procedure (parts of Section 3) could have been moved to an appendix to make way for more room in the empirical evaluation. The empirical evaluation could have been more convincing: I did not see any strong differences between VI and maximum likelihood in the inferred station loads, and there was missing any reasonable benchmark methods. Some of the plots were a little confusing (not to mention small; if they are to be included in the main paper itself, and I think they should, then they need to be readable). For example, what do the columns of Figure 4 represent? Please ensure to label all axes and columns in the plots. It would also be helpful to state the questions that you want the empirical evaluation to answer (other than just “testing the model”). "
Gaussian process based nonlinear latent structure discovery in multivariate spike train data,"Anqi Wu, Nicholas A. Roy, Stephen Keeley, Jonathan W. Pillow",https://proceedings.neurips.cc/paper/2017/hash/b3b4d2dbedc99fe843fd3dedb02f086f-Abstract.html,"I was not expecting to read a paper claiming state-of-the-art results with GPLVM in 2017, but here we are. The model is a very solid application of nonparametric inference, including a novel extension of the Laplace approximation appropriate for GPLVMs with Poisson observations, and the results are interesting in that they seem better able to decode known behaviorally-relevant variables in the one real dataset they look at.

A few objections/questions:
-In the abstract, the authors claim their results are superior to those of variational autoencoder based models like LFADS. However, no experimental results are presented. Did the authors forget to remove this claim from the abstract? The authors do note that GP-based models tend to outperform deep neural networks in the small data limit, which seems accurate, but some experimental comparison would be appreciated. And given the constantly growing volume of neural data, a comparison on larger datasets would be appreciated as well.
-Experimental results are judged entirely based on r^2 scores with a known behavioral variable. While this may be of more interest for neuroscientists, a proper comparison of held-out data likelihood or approximate marginal likelihood of the data would be appreciated.
-In the experiments with hippocampal data, how are the latent variables aligned with behavioral variables? Linear regression? Or did the latent variables discovered just happen to align perfectly with x and y directions in the maze? The latter seems highly unlikely.","Here the authors introduce a latent variable model for population neural activity with GP latent state and, also, GP mapping (tuning curves) between the latent state and Poisson observations. This is an interesting model that nicely extends work in this area.
I have a couple points of concern…
Identifiability – if I understand correctly rotations and reflections of x’=Rx have the same likelihood. How can the latent variables that the model estimates be directly compared to extrinsic variables in this situation. Why does r^2 work? In general, why should the estimated latent variables match the extrinsic location one-to-one? In other contexts (e.g. Vidne et al. J Comp Neuro 2012), the low-dimensional latent state has been interpreted as common input, for instance, that can be completely unrelated to some covariate.
I’m also a bit confused how the latent variables from PLDS and P-GPFA can be directly compared with those from this new method, since the mapping function is linear for those methods. In Fig 2, where are the tuning curves for PLDS/P-GPFA coming from, for instance?
Minor issues:
In section 2/3 – it’s mostly clear from the context, but you may want to explicitly state that \rho and \delta are the marginal variance and length scale for k_x.
It could be argued that the task dimensionality is often unclear. Even in hippocampus, phenomena such as head-direction cells, speed tuning, and phase locking would suggest that neural populations are acting in more than 2d.


** After author comments **
The authors have addressed all of my concerns. Using regression to map the latent variables to the extrinsic variables is a nice approach.","Summary
---------
The authors propose a gaussian process latent variable model with poisson observations (P-GPLVM). The demonstrate how to perform inference using a decoupled Laplace approximation. They apply the model to simulated data as well as rat hippocampal data.

The paper is well written and clearly presented. I appreciated the nice summary of the model structures in Table 1.

I would have liked to see more applications of the technique, and/or a larger improvement on real data compared to previous methods.

Questions/concerns
--------------------
- The performance benefit of the P-GPLVM is quite small over PLDS and the GPLVM on real data. For example, in Figure 2C, there is only a weak improvement over these methods (especially given the fact that the y-scale in Fig 2C shows a small range, and lack error bars), lessening the overall impact of the results.

- How does the algorithm scale with the size of the latent dimension, the number of neurons, and the length of the recording? The examples in the paper have latent dimension <= 2, on the order of tens of neurons, and 500 time points (50s binned at 100ms). Is the technique feasible for larger datasets?

- The authors ran multiple simulations and show the mean across repeats--the authors should also show the variability (standard error or deviation) across repeats, both for the r^2 performance (error bars) as well as estimated trajectories (error bands).

- If I understand correctly, the estimated latent trajectory in both the simulation and hippocampal data tracked the animal's position? That seems surprising--what if you set the latent dimension to 3? or 4, or 1?

Minor comments
-----------------
- line 103: remove the 'as' in ""our model is as a doubly nonlinear ..."" "
Multi-Objective Non-parametric Sequential Prediction,"Guy Uziel, Ran El-Yaniv",https://proceedings.neurips.cc/paper/2017/hash/b432f34c5a997c8e7c806a895ecc5e25-Abstract.html,"This paper presents an asymptotic analysis for nonparametric sequential prediction when there are multiple objectives. While the paper has some relevance to a certain subset of the machine learning community, I have some concerns about the paper's relevance to NIPS, and I also am unclear on some of the basic setup of the paper. I discuss these issues in the remainder of the review. Overall, I think the results in the paper are technically strong and, with the right motivation for $\gamma$-feasibility (see below) I would be weakly positive on the paper.

Relevance:

While I can see that the contents of the paper have some relevance to machine learning, I feel that this sort of paper would fit much better in a conference like COLT or ALT, given the technical level and given how much this paper can benefit from the additional space offered by those venues. I am not convinced that the problem addressed by this paper is interesting to the broader machine learning community and even many theoretical researchers within online learning. I can understand that an asymptotic analysis may be interesting as it allows one to obtain a first approach to a problem, but if the asymptotic nature of the analysis is fundamental, as it is in this case (the authors mention that positive non-asymptotic results simply are not possible without additional assumptions), then is there hope to finally achieve learning algorithms that can actually be used? I do not see any such hope from this paper, especially in light of the construction of experts that should be fed into the MHA algorithm.

Motivation for the objective:

This paper lacks sufficient motivation for the formal objective; while the high-level objective is clear, namely, to obtain a strategy that minimizes the average $u$-loss subject to the average $c$-loss being at most $\gamma$, the formal rendering of this objective is not sufficiently motivated. I understand why one would want to use strategies that are $\gamma$-bounded (as defined in Definition 1). However, the authors then further restrict to processes that are $\gamma$-feasible with no motivation for why we should restrict to these processes. How do we know that this restriction is the right one, and not too great of a restriction? If we restrict less, perhaps we would arrive at a different $\mathcal{V}^*$ that is larger than the one arising from $\gamma$-feasible processes. In particular, with additional space, I feel that the authors should be able to properly explain (or at least give some explanation, as none whatsoever is given in the current paper) why we should care about the conditional expectation of $u(y, X_0)$ (conditional on the infinite past). It is paramount to motivate the $\gamma$-feasibility restriction so that $\mathcal{V}^*$ can be understood as a quantity of fundamental interest.


Other comments on the paper:

While the MHA algorithm seems interesting, without an explanation of what experts are used in the main paper, it is difficult to get a sense of what the algorithm is really doing. The authors should devote some space in the main text explaining the construction of the experts. I wonder if the authors could have provided proof sketches in the main paper for the main results, Theorems 1 and 3, and bought themselves about 3 pages (each of those proofs is about 1.5-2 pages), as well as an exposition that would be clearer to the audience. 



MINOR COMMENTS

I looked through part of the proof of Theorem 1 and it seems fine, but I am not an expert in this area so I am trusting the authors.

In the math display between lines 68 and 69, I think the a.s. should not be there. If it should be there, what does it mean? All the randomness has been removed by the outer expectation.

Line 131: ``an histogram'' --> ``a histogram''

This paper is at times sloppily written. Examples:

    Line 131, you write $H_{h,k}$ and then later write $k, l = 1, 2, \ldots$, and this swapping of $h$ and $l$ happens throughout the paper.

    Math display between lines 107 and 108: The minus sign in front of the last summation on the first line should be a plus sign.

  ``Optimallity'' -> ``Optimality''



UPDATE AFTER REBUTTAL

I am satisfied with the authors' response regarding the formal framing of the problem and the conditional expectation, provided that they provide details (similar to their rebuttal) in the paper.","
The paper studies online learning in the multi-objective setting. The paper builds upon previous result by Mahdavi et al, and extends the multi-objective framework to the case of stationary and ergodic processes, allowing for the dependencies among observations. The authors first give a lower bound for all algorithms and present an algorithm which achieves the optimal value.

I found the paper to be well-written and relatively easy to follow, despite my lack of familiarity with ergodic theory. The authors succeeded at separating discussion and interpretation of their result from technical details, which made the paper readable. The assumptions on the sequence of outcomes (stationarity and ergodicity) are much weaker from the i.i.d. assumption in the past work. It is hard for me to judge the significance of the paper, as I not familiar with previous work on multi-objective learning, but I found the results interesting, and the theoretical contribution to be strong. In particular, the main results are: 
- Theorem 1, which states that despite adaptive nature of the prediction strategy, no such strategy can beat the best prediction which knows the (infinite) past. This is something one can expect, but it is nice to see it shown. 
- Convergence of Algorithm 1, a specific version of exponential weights algorithm, to the optimal solution for stationary and ergodic sequences.

I think it would be beneficial to describe the construction of the experts in a bit more detail in the main part of the paper. It is not entirely clear what is the size of the expert set? From what I understand, the set is countable, but not necessarily finite. Can then the algorithm be actually run on this set?

Can anything be said about the performance of the algorithm for finite samples (apart from ""regret bounds"" (9) and (10))?

What are the constants C_{k,h}, C'_{k,h} in the proof of Theorem 3? Are they related to the prior probability \alpha assigned to a given ""expert""?

-------

I have read the rebuttal. ","The paper addresses prediction of stationary ergodic processes with multiple objective loss functions. In this setting the average loss incurred according to one loss function cannot be exceeded above a certain threshold, while the average loss incurred due to a second loss function needs to be minimized.
This problem has been solved for i.i.d. processes with multiple objective loss functions and for stationary ergodic processes with a single objective function. This paper tackles multiple constraints and stationary ergodic processes at the same time.

The paper is clear and well written. The results appear to be correct and the problem is well motivated."
Optimal Sample Complexity of M-wise Data for Top-K Ranking,"Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh",https://proceedings.neurips.cc/paper/2017/hash/b4d168b48157c623fbd095b4a565b5bb-Abstract.html,"Paper 1058 
Inspired by the pairwise framework for top k ranking aggregation in [17], this paper mainly explores the optimal sample complexity for M-wise top-K rank aggregation in a minmax sense based on the PL-model. To reach that complexity, the authors first derives a lower bound of O(nlgn/(delta_K^2*M)) and then construct a slightly modified Rank Centrality algorithm, which is upper bounded by exactly the same order. Even more surprisingly, unlike [17], such a result is guaranteed without employing the MLE method at all, which is a great improvement of efficiency. 

The experiments for synthetic datasets show that the performance of Rank Centrality could meet that of Spectral MLE under dense regime, and become slightly worse than Spectral MLE under sparse regime. However, the slight disadvantage of Rank Centrality under sparse regime is not of a big issue considering the efficiency gain. Moreover, the curving fitting of sample complexity perfectly matches $1/M$, the rule suggested by the theoretical results. On the other hand, the experiments for the real world simulated data also suggest the possibility to generalize the main results of this paper to more complicated applications where the PL model assumption doesn’t necessarily hold.

Overall, it’s a good paper featured with elegant theoretical analysis and solid experimental supports. It’s an amazing reading experience for me! Thank you so much for sharing your idea and your contributions to this area!	
 
There’s only one problem from me: Seeing that the complexity is simultaneously governed by n $delta_K$ and M, what’s the influence of n and $delta_K$ on the practical sample complexity besides M?","Summary:
    The authors consider the problem of optimal top-K rank aggregation from many samples each containing M-wise comparisons. The setup is as follows: There are n items and each has a hidden utility w_i such that w_i >= w_{i+1}. w_K-w_{K+1} has a large separation Delta_K. The task is to identify the top K items. There is a series of samples that are given. Each samples is a preference ordering among M items out of the n chosen. Given the M items in the sample, the preference ordering follows the PL model. Now there is a random hyper graph where each set of M vertices is connected independently with probability p. Further, every hyperedge gives L M-wise i.i.d preference order where M items come from the hyperedge.
  
     The authors show that under some density criterion on p, the necessary and sufficient number of samples nis  O( n log n/M Delta_K^2 ) . The notable point is that it is inversely proportional to M.

     The algorithm has two parts: For every M-wise ordered sample, one creates M pariwise orderings that respect the M-wise ordering such that these pairwise orderings are not correlated. This is called sample breaking. Then the problem uses standard algorithmic techniques like Rank Centrality that use the generated pairwise ordered samples to actually generate a top K ranking.

    The proofs involve justifying the sample breaking step and new analysis is needed to prove the sample complexity results.

 Strengths:
  a) For the dense hypergraph case, the work provides sample optimal strategy for aggregation which is the strength of the paper.
  b) Analyzing rank centrality along with path breaking seems to require a strong l_infinity bound on the entries of the stationary distribution of a Markov Chain whose transition matrix entries have been estimated upto to some additive error. Previous works have provided only l_2 error bounds. This key step helps them separate the top K items from the rest due to the separation assumed.


I did not have the time to go through the whole appendix. However I only quickly checked the proof of Theorem 3 which seems to be quite crucial. It seems to be correct. 

Weaknesses:

      a) Spectral MLE could be used once sample breaking is employed and new pairwise ordered samples generated. Why has that not been compared with the proposed approach for the experiments in section 4.2 ? Infact that could have been compared even for synthetic experiments in Section 4.1 for M > 2 ?
      b) The current method's density requirements are not optimal (at least w.r.t the case for M=2). I was wondering if experimentally, sample breaking + some other algorithmic approach can be shown to be better ?","This paper considers the problem of performing top-k ranking when provided with M-wise data assuming the Plackett-Luce (PL) model. They provide necessary and sufficient conditions on the sample complexity. The necessary condition employs Han and Verdu's generalized Fano's inequality. For the sufficient condition, the authors give a simple algorithm that essentially ""breaks"" the M-wise sample to produce pairwise samples, and then uses these in a rank-centrality algorithm. The analysis that this algorithm has the right sample complexity is where the major work of this paper is. 

The main part of the paper is well written, however, the organization of the supplementary material can be improved. I appreciate the authors clearly indicating the dependencies of their results, but the organization still seems quite fractured. In all, the proof sketch is clearly presented in the text and the discussions are very helpful. 

This is a solid paper, and I recommend its acceptance. "
From which world is your graph,"Cheng Li, Felix MF Wong, Zhenming Liu, Varun Kanade",https://proceedings.neurips.cc/paper/2017/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html,"(I have been asked by the area chairs to provide an extra set of eyes for this paper. Because this review is entering the discussion after the rebuttal phase, discussants were asked to consider anything raised here in light of this lack of an opportunity to give a rebuttal, as it is possible that I have misunderstood things that a rebuttal would have addressed.) 

The work presents itself as contributing a unification of two widely studied random graph models, the stochastic block model and small world model, and developing an efficient algorithm for recovering latent representations for this unified model. A problem with this claim is that, as is acknowledged in the work, this “new” unified approach is really just a (sampled) graphon model, as the authors make clear on Lines 113-114. It’s plain to any reader familiar with graphons that adaptations of both the SBM and small world model fit within the graphon framework. The SBM-graphon connection is obvious and well-studied; the SW-graphon connection is obvious in-so-much-as the small world model is really just a latent space model, though most attention for latent space graphons has been on e.g. logistic latent space models. Denoting that model the ""small world” model invokes expectations of navigability results, so in general the results would be better explained in terms of a latent space model with an inverse polynomial distance kernel. All that said, the work is considered on it's merits as a contribution to the graphon literature.

Detailed comments:

1) The way the paper presents it’s results does a disservice to their strengths. The paper contributes a careful analysis of a complex algorithm that appears to bring earlier results for sampled dense graphons over to sparse graphons (Line 255: ""We generalize the results in [24] for the sparse graph case.”) That said, while the results in [24] are explicitly given for dense graphs, they are sketched there for the sparse case as well (in the conclusions, with the comment ""A precise statement and formulation of the results in Section 4 for the sparse setting might require some care, but should be for the most part straightforward.”).

It is possible that the details of the sketch in [24] are not nearly as “straight-forward” as the authors there claim. But engaging in a clear dialogue with the literature is a reasonable requirements for acceptance, in the eyes of this reviewer. If the paper had a (1) title and abstract and (2) related work section that made clear the position of the paper in the sampled graphon literature, specifically related to [24], that would _greatly_ improve the ability of a reader to appreciate the work. 

As is, the results pertaining to recovering both SBM and SWM networks claimed in the abstract and introduction are essentially results achieved by [24], except for (a) the sparse graph case (the polynomial small world kernel is a universal kernel in the language of the results of that work) and (b) the pairwise distance recovery. I will now focus on the pairwise distance recovery. Basically, [24] is quite general, as is the core of this work. And the focus of the title and introduction on mis-specification between the SBM and SWM feels like the wrong problem to focus on when putting forward this work.

2) Connecting the extended analysis of [24] -- roughly, to get the kernel PCA to work in the sparse regime -- is a nice if modest contribution. More significant is connecting the output of this machinery to MDS machinery to obtain the pairwise distances. This appears to be a novel point, and it seems quite useful. The analysis here does rely on some specific details of the small-world kernel, but it would be really helpful if the authors clarified what the general conditions are on the latent space kernel for some version of pairwise distance recovery to go through. It seems that the long-range links allowed by the heavy polynomial tails may be critical; is that so? Do any version of the results work for kernels with exponential tails?

3) In line with the odd focus of the introduction, the empirical comparisons made in the work are a bit off, but I'm willing to let this slip as the work isn't really an empirical paper, and should get positive ""credit"" for even trying to do empirical work here. The empirical work focuses on comparisons with algorithms that aren’t quite trying to do what the present algorithm is trying to do, while not benchmarking itself against methods that are designed for some of the same tasks. First, the SBA algorithm of Airoldi-Costa-Chan and SAS algorithm of Chen-Airoldi, and related network histogram methods are specifically designed for the estimation problem in Figure 4. But not compared. Second, the methods used in comparison are not really reasonable. Majority vote label propagation from [56] is an algorithm for categorical labels, not continuous embeddings. It’s not clear why [54], a paper about conformal Isomap, is called a random walk label propagation method, so that might be a crossed-up citation. It is likely that the modularity method [55] is finding a core-periphery cut with it’s main eigenvector; for this reason it would be interesting to see how a semi-supervised method does (e.g. ZGL, given the labels for the presidential candidates discussed as being given to other methods). 

As an overall comment, the empirical question is a terrific one, with high quality data, but the evaluation is not up to the standards of a thorough empirical investigation. But again, I realize that the main contribution of this work is theoretical, but I still want to register what I see as issues. I am most concerned about the lack of comparison to the other methods in the estimation in Figure 4 and Figure S9.

3) As a small comment about Figure (S)9:  for the methods under consideration it would be more honest to show the performance of the Abraham et al. algorithm in Fig 9d and the Newman spectral algorithm in Fig 9b, as opposed to just showing the cross-objectives (where the two methods fail).

4) One point that confused this reader for some time was the fact that the paper studies sparse graphons without any discussion of the tricky issues for such models. It should be made very clear that the model under consideration is the Bollobas-Janson-Riordan (2007) model (also studied by Bickel-Chen-Levina (2011), which is a sampled dense graphon. Some pointers to the emerging literature on sparse graphons defined through edge-exchangeable ideas (Crane-Dempsey 2016 building on Caron-Fox 2014, Veitch-Roy 2015, Borgs et al. 2016) would be useful to provide. 

5) The related work section overlooks that [22], Airoldi et al., makes computational improvements (in terms of complexity) on the USVT method (but still for dense graphs).

6) Ref [19], the latent space model of Hoff et al., is pegged as a “Graphon-based technique” where it should be raised as a broad citation for latent space models such as the small world model discussed herein.

7) The discussion of results for both the unipartite and bipartite graph model is nice. The bipartite model discussed here is sometimes called an affiliation network, see ""Affiliation networks” by Lattanzi and Sivakumar, perhaps worth connecting to.

I think there are a lot of interesting connections made in the work. The management of errors in the isomap algorithm for recovering latent space graph models is a nice contribution.
","Summary of paper:
The paper first proposes a unified model for graphs that generalises both stochastic block model  (SBM) and small-world model (SWM). Two versions of the unified model are presented, and algorithm and analysis are given for estimating the models. 
-> Model 1 (Simplified model): A minor modification of the sparse graphon as in [R1, see below]. Here, one would sample n points x_1, … , x_n from [0,1] according to some distribution D (that may not be uniform), and then probability is given by C(n)*k(x_i,x_j), where C controls sparsity and k is the kernel function that is assumed have a specific form.
-> Model 2 (Bipartite model): This is essentially a generalisation of sparse graphon to bipartite graphs. Here, one samples x_1, … , x_n ~ D on [0,1] and also y_1, … , y_m ~ D on [0,1], and in the graph, edges only occur between x-nodes and y-nodes with probability C(n)*k(x_i,y_j). This model is presented in main text, but estimation is given in appendix.
Algorithms for estimating the models are given and analysed. Main idea is to first get a spectral embedding from adjacency matrix, then apply Isomap to get distance matrix from embedding. Finally, apply line embedding algorithm to get x_i’s from distances. The first two steps are analysed and the last is assumed to work well based on existing results. I have not checked all the proofs thoroughly since most intermediate claims seem intuitively correct to me. 

Pros:
1. The algorithm is somewhat novel though its individual components are well known. While most works end at graphon kernel estimation, this paper estimate distances under assumed structure of kernel. Some parts of proof also seem novel, but I am not very familiar with the literature on Isomap.
2. There is limited work on estimating sparse bipartite graphon. 

Cons:
1. The simplified model can be derived from sparse graphon [R1] and is same as [29]. Also density regime is not very sparse. So spectral embedding analysis has limited novelty. 1. 
2. The bipartite model is interesting, but not entirely new. Bipartite graphs have been studied in the context of partitioning [R2,R3], and dense bi-partite graphons are also known [R5]. 
Some techniques for bipartite model is somewhat similar to [R2].   
3. The main paper is written in a very confusing way (particularly Section 2), and many interesting parts and potential contributions can be found only in Appendix.

Detailed concerns: 
Line 81: Recovery is possible when p,q = Omega(log(n)/n), not polylog as claimed in the paper. Polylog(n)/n is an easier regime where trace method type bounds are applicable (see [R4]). [5,9] study log(n)/n case. This claim repeats throughout the paper, and the authors actually use much simpler techniques for spectral embedding. For sparse graphs, one needs to consider regularisation of the adjacency matrix [R4].
Line 103-126: It took me long time to realise that the paper considers two very different estimation problems. A reader normally would imagine everything goes into making of a big graph with both influencers and followers. The paper has no clearly stated problem definition.  
Line 118-126: This model follows from sparse graphon [R1]. Let F be the cumulative distribution of D. Then x_i = F^{-1}(z_i) for z_i ~ Unif[0,1]. So we can define the kernel in terms of z_i which is sparse graphon model. Similar argument can also be used for bipartite case.
Line 140-141: For bipartite graph, [R2] also studies decomposition of B’B. They suggest a regularisation by deleting the diagonal, which is quite similar to the diagonal diminishing approach used here. Though [R2] only studied partitioning, it allowed more disparity between m and n, and more sparse regime.
Line 199-203: Though line embedding is used as a blackbox, its effect on the overall guarantee must be accounted for. Also, do results of [48,49] hold when distances are known upto some error?
Figure 1: Do we assume \rho(n) is given? Shouldn’t this be estimated as well? If \rho(n) is assumed known, then results of [R1] are applicable.
Line 348-366: This section, and the main paper in general fails to motivate why is it more important to estimate latent position instead of graphon kernel. For instance, Figure 4 still shows only the kernel matrix -- then why shouldn’t we use the more general theory of graphon estimation? The only motivation can be found at the end in Appendix G.
Line 324-332: No guarantee is known for unclear points. Why doesn’t this cause any problem in this part? Maybe, I just missed this point in the proof. So a pointer would suffice.
Line 64: USVT works in the polylog(n)/n regime. See proof in [28]. So in the assumed density regime, one could also use USVT instead of estimating d via DecideThreshold.
Line 36-39,99: It is quite inconvenient if a paper refers to figures in appendix for introductory discussion.

[R1] Klopp, Tsybakov, Verzelen. Oracle inequalities for network models and sparse graphon estimation.
[R2] Florescu, Perkins. Spectral Thresholds in the Bipartite Stochastic Block Model.
[R3] Feldman, Perkins, Vempala. Subsampled Power Iteration: a Unified Algorithm for Block Models and Planted CSP’s.
[R4] Le, Vershynin. Concentration and regularization of random graphs.
[R5] Choi. Co-clustering of nonsmooth graphons.","This paper studies the latent position model for networks. The authors assume a small world kernel in the latent space. By combining techniques like spectral analysis and isomap, they are able to provide a provable procedure to recover the latent structure of the nodes in the graph. They illustrate their method on a twitter dataset for political ideology estimation. 

Overall I think the paper is well-written, the theory looks correct and proof ideas are provided which makes it very easy to follow. Below are some detailed comments and questions.

1. The algorithm and analysis can be broken down into two parts, first is recovering the low rank feature map, and the second is to control the error in isomap. The analysis for the first part is standard, and similar to those in [24]. Also this part could be very general, it should hold for many other latent position models such as random dot product graph etc. The second part of the analysis is tailored to the small world kernel and seems novel. It would be nice to see some discussions about how this technique generalizes to other type of kernels.

2. I have some confusion making sense of the bipartite model assumption. I understand that one wants to divide the nodes into high-degree and low-degree parts, but why is it necessary to assume that there is no connection within high degree nodes? In most real world networks the celebrity nodes also have edges within themselves. In fact, it seems to me having more inter-cluster edges would only help the inference. Is this an assumption that mainly for theoretical convenience or can the author provide more insights in the rebuttal?

3. The authors criticize the degree-corrected block model on the fact that, ""the subgraph induced by the high degree nodes is dense, which is inconsistent with real-world networks"" (line 146-147). However if I understand correctly, Proposition 3.1 also only holds for dense graphs (\rho(n)=\omega(\log n)), which means for the simplified model, a graph consists of all celebrities is dense. (just to make sure we are on the same page, I'm referring to sparse when the average degree is O(1)). How is this different from DC-SBM? I failed to see the sparsity claim the authors made in the abstract (line 9) and line 255. Please clarify.

4. The authors claim that they can make a distinction between SBM and SWM, but the main theorem only talks about the recovery of the pairwise distances. I was wondering if someone would like to do a model selection, for instance, to figure out whether a SBM algorithm applies, how the authors would suggest him to do based on the inferred latent positions. In other words, is there any procedure that tells me whether the latent distribution is discrete or continuous, up to a certain accuracy?

5. The experiments provides very interesting insight on this dataset. It is very impressive that the authors manually labeled the data for classification task. For G 2.1 it is seems that majority with modularity initialization does a fairly good job, but as said in the paper, the proposed approach delivers more information on the finer granularity than a binary labeling, which is nice.

6. It is important to make the paper self-contained. For instance, line 36 talks about figures in the supplementary, and figure 8 in theorem 2.2 is actually not in the main paper. Also as I mentioned, I like the fact that the authors put a lot of intuition for the proof in the main paper, but the current layout is a bit too crowded, there's barely any space under caption of figure 2 to distinguish it from the text. I would suggest moving more things to supplementary to improve readability.

7. Line 849 requires some elaboration.

Other minor issues:
typo in line 159 ""involved""
in line 250-251, the W is undefined, might be some unitary matrix I assume?
line 292, the curve is defined by \Phi capitalized?


"
An Empirical Bayes Approach to Optimizing Machine Learning Algorithms,James McInerney,https://proceedings.neurips.cc/paper/2017/hash/b60c5ab647a27045b462934977ccad9a-Abstract.html,"
This paper describes an alternative to Bayesian Optimization (BO). In BO, only the hyperparameter setting corresponding to the “best” model (evaluated on validation data) is kept and all others are discarded. The authors argue that this is wasteful, given the cost of training and evaluating several models. As a solution, they proposed an empirical-Bayes-inspired approach consisting in averaging hyperparameters across Bayesian optimization evaluations.

The paper is well written and the contribution is clear. Indeed, keeping only the “best performing” evaluation is wasteful and discards most of the uncertainty associated with the stochasticity arising from subsampling the training set to generate a validation set.

The main benefit of this method seems to be the ability to work without a validation set. In fact, the method works better when no validation set is used. The authors seem to give two reasons for this: 1) not holding out a validation set gives more data to train on 2) evaluating on a (necessarily small) validation set introduces variance. 

Although this is an interesting paper, the comparison with Bayesian optimization methods is not as exhaustive as it should be. For instance, to see if the improvement over BO methods is arising from point (1) above, the authors could take the best candidate from BO and simply retrain on validation+training data (concatenated). Another obvious comparison that is still missing is to simply average the prediction of all settings considered during a BO run. 


Minor: 

“validation” is misspelled in Figure 1
","# Update after author feedback

I maintain my assessment of the paper. I strongly urge the authors to improve the paper on the basis of the reviewers' input; this could turn out to become a very nice paper.

Specially important IMHO
- retrain on validation+train (will come as an obvious counter-argument)
- clarify the use of $\lambda$; R2 and R3 got confused
- variance in experiments table 2+3 (again, would be an easy target for critique once published)


# Summary of paper
Bayesian optimisation (BO) discards all but the best performance evaluations over hyperparameter sets; this exposes to overfitting the validation set, and seems wasteful. The proposed method ""averages"" over choices hyperparameters $\eta$ in a Bayesian way, by treating them as samples of a posterior and integrating over them. 
This requires introducing an extra layer of hyper-hyperparameters (introducing $\lambda$, subjected to empirical Bayes optimisation). 
An algorithmic choice is required to define an ""acquisition strategy"" in BO terms, ie a model for $r^{(s)}$, as well as a model for hyperprior $p(\eta | \lambda)$.
The proposed method does not require a separate validation set.

# Evaluation
The paper is very clearly written, without any language issues. It represents the literature and the context of the methods well. The problem statement and resolution process is clear, as is the description of the adopted solutions and approximations. The authors take care in preventing misunderstandings in several places. 
The discussion of convergence is welcome. 
The experiments are non-trivial and help the argumentation, with the caveat of the significance of results mentioned above.
The method is novel, it applies the idea of EB and hierarchical Bayes to the BO method. I believe the approach is a good, interesting idea.

# Discussion

Discussion lines 44-47 and 252 sqq: it may sound like the case presented here was cherry-picked to exhibit a ""bad luck"" situation in which the leftmost (in fig 1) hyperparameter set which is selected by BO also exhibits bad test error; and in a sense, it is indeed ""bad luck"", since only a minority of hyperparameter choices plotted in fig 1 seem to exhibit this issue. However, dealing with this situation by averaging over $\eta$'s seems to be the central argument in favour of the proposed EB-Hyp method. I buy the argument, but I also recognise that the evidence presented by fig 1 is weak.

The following advantage does not seem to be discussed in the paper: since a separate validation set is not needed, the training set can be made larger. Does this explain the observed performance increase? An analytical experiment could help answer this. Does the comparison between BO with/without validation in tables 2 and 3 help?

In algo 1, I am not clear about the dependence on $\hat{\lambda}$. I am assuming it is used in the line ""return approximation to ..."" (I suggest you write $p(X^*|X_\textrm{train}, \hat{\lambda})$ to clarify the dependence.)
I assume that in line ""find optimal..."", we actually use, per lines 200-208, $\hat{\lambda} = \arg \max \frac{1}{T} \sum_s p(\eta ^{(s)} | \lambda) f^{(s)}_X $. This is worth clarifying. 
But then, I'm confused what $p(\eta)$ in line ""calculate hyperparameter posterior..."" and eq 5 is? Is this an input to the algorithm?

EB-Hyp seems to carry the inconvenient that we must consider, as the ""metric of interest"" in BO terms, the predictive likelihood $f^{(s)}_X$, while BO can use other metrics to carry out its optimisation. This restriction should be discussed.

## Experiments

How many steps/ evaluations of $A$ do we use in either experiment? We are using the same number for all configurations, I suppose? How does the performance increase with the number of steps? Does EB-Hyp overtake only after several steps, or is its performance above other methods from the start? 

I find it hard to assign significance to the improvements presented in table 2 and 3, for lack of a sense of performance variance. The improvements are treated as statistically significant, but eg are they immune against different random data splits ? As it stands, I am not convinced that the performance improvement is real.



# Suggestions for improvement, typos

## Major
Fig 2 is incomplete: it is missing a contour colour scale. In the caption, what is a ""negative relationship""? Please clarify. What is the point of fig 2 ? I can't see where it is discussed?

## Minor
fig 1a caption: valiation
fig 1: make scales equal in a vs b
algo 1: increment counter $s$. Number lines.
line 228: into into
line 239: hyerparameters","This is a great paper demonstrating the importance of *integrating out* hyperparameters instead of optimizing them.

 I liked the point of not needing a separate validation set and appreciate the risk of overfitting to the validation set when using BayesOpt. These are all mistakes practitioners of deep learning make regularly, including myself. I also really liked the idea of using Thomson sampling instead of a closed for acquisition function in order to generate better samples while performing the integral. Obviously sampling from the prior would be much more inefficient, and I like how Thomson sampling unites Bayesian optimization with efficient Monte Carlo techniques. 

Now for the criticisms:

1) It is quite unfortunate that the results with fixed \lambda are not competitive. Have the authors tried partial runs to get a noisier estimate of the loglik for a particular \eta? This could increase the number of samples going into the GP. 

2) The optimization over \lambda in Algorithm 1 appears weird -- isn't the loop over a fixed lambda? How is this optimization performed in practice?

3) In it unfortunate that the only experiments are for unsupervised learning, you could apply this to the supervised learning setting. It would be nice to have an experiment with deep convnets or RNNs on a large dataset like ImageNet for example. "
Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction,"Zhan Shi, Xinhua Zhang, Yaoliang Yu",https://proceedings.neurips.cc/paper/2017/hash/b6e710870acb098e584277457ba89d68-Abstract.html,"This paper shows that ""certain"" adversarial prediction problems under multivariate losses can be solved ""much faster than they used to be"". The paper stands on two main ideas: (1) that the general saddle function optimization problem stated in eq. (9) can be simplified from an exponential to a quadratic complexity (on the sample size), and (2) that the simplified optimization problem, with some regularization, can be solved using some extension of the SVRG (stochastic variance reduction gradient) method to Bregman divergences.

The paper is quite focused on the idea of obtaining a faster solution of the adversarial problem. However, the key simplification is applied to a specific loss, the F-score, so one may wonder if the benefits of the proposed method could be extended to other losses. The extension of the SVRG is a more general result, it seems that the paper could have been focused on proposing Breg-SVRG, showing the adversarial optimization with the F-score as a particular application.

In any case, I think the paper is technically correct and interesting enough to be accepted.


","I found this an interesting paper with strong technical results (although it is far from my areas of expertise, do I don't have high confidence in my review).

A few comments:

* Sections 3 and 4 feel a little bit disjointed - they are two very separate threads, and it feels like the paper lacks a little focus. Would it make sense to put section 4 first?

* The inclusion of the regularizer ||\theta||^2 in (10) is a little abrupt - not very well explained - more explanation/justification would be helpful.

* The application to LP boosting using entropy regularization would benefit from more discussion: how does the resulting algorithm compare to the methods introduced in the original paper?","The paper extends a saddle point derivative of the SVRG algorithm to the case of entropy regularization (rather than Euclidean). The algorithm is applied to the problem of adversarial prediction, where the authors simplify the objective function so that it can be solved efficiently. The algorithm is then evaluated experimentally.

The only comment that I have about the paper is its presentation and writing. It seems the authors have tried to cram a lot of material into such a short paper. This makes the paper hard-to-follow at times and its reading tiresome.

Nonetheless, the contributions are novel albeit straight-forward, and as such I suggest its acceptance.

References
========

P. Balamurugan and F. Bach. Stochastic variance reduction methods for saddle-point problems."
Perturbative Black Box Variational Inference,"Robert Bamler, Cheng Zhang, Manfred Opper, Stephan Mandt",https://proceedings.neurips.cc/paper/2017/hash/b75bd27b5a48a1b48987a18d831f6336-Abstract.html,"The authors present a novel variation of the black-box variational inference algorithm that uses a non-standard optimization target derived using perturbation theory. The paper is well-written and carefully explains its relationship to existing work in this area. The exposition is easy to follow and the technical results are sound. The experiments are sensible and compare the method described by the authors to current state-of-the-art, in terms of either accuracy or speed as appropriate, in typical settings where variational inference would be employed. The paper should be interesting broadly to users of variational inference.

I found the authors' perspective on variational inference as a form of importance sampling to be illuminating, although there is something that bothers me about it. If we view variational inference as a method to approximate the marginal likelihood the whole story about bias-variance trade-off makes perfect sense. However, if we view variational inference as a method to approximate the posterior distribution, things are much less clear to me. In particular, f = id corresponds to an unbiased estimator so it should be the one that performs best in the limit of infinite computation where the expectation with respect to q can be evaluated exactly. But in that case the lower bound is exact regardless of the proposal/variational distribution chosen so the gradient with respect to variational parameters is always zero and we can't perform any optimization. Am I missing something here?","Summary:  
The authors present a new variational objective for approximate Bayesian inference.  The variational objective is nicely framed as an interpolation between classic importance sampling and the traditional ELBO-based variational inference.  Properties of the variance of importance sampling estimator and ELBO estimators are studied and leveraged to create a better marginal likelihood bound with tractable variance properties.  The new bound is based on a low-degree polynomial of the log-importance weight (termed the interaction energy).  The traditional ELBO estimator is expressed as a first order polynomial in their more general framework. 
The authors then test out the idea on a Gaussian process regression problem and a Variational autoencoder. 

Quality:  I enjoyed this paper --- I thought the idea was original, the presentation is clear, and the experiments were convincing.  The paper appears to be technically correct, and the method itself appears to be effective. 

Clarity:  This paper was a pleasure to read.  Not only was this paper extremely clear and well written, the authors very nicely frame their work in the context of other current and previous research.

Originality:  While establishing new lower bounds on the marginal likelihood is a common subject at this point, the authors manage to approach this with originality. 

Impact:  I think this paper has the potential to be highly impactful --- the spectrum drawn from importance sampling to KLVI is an effective way of framing these ideas for future research.

Questions/Comments: 

  - Figure 1: should the legend text ""KLVI: f(x) = 1 + log(x)"" read ""f(x) = log(x)"" ?  I believe that would cohere with the bullet point on line 124. 
  - How does the variance of the marginal likelihood bound estimators relate to the variance of the gradients of those estimators wrt variational params?  KLVI reparameterization gradients can have some unintuitive irreducibility (Roeder et al, https://arxiv.org/abs/1703.09194); is this the case for PVI? ","This work casts black box variational inference as a biased importance sampling estimator of the marginal likelihood. In this view, the role of the optimal variational posterior is to minimize the bias. The central contribution is a bound defined by a third order Taylor expansion of the importance weight. The experiments explore the properties of this bound in inference in GPs and maximum likelihood estimation in VAEs. The paper is well written overall and easy to understand. Still, the contribution is relatively minor and doesn't really provide large empirical wins in large data settings.

Specific questions/comments for the authors:

(1) In what sense is KLVI at the low variance large bias end of the bias/var spectrum? You can clearly have bounds that have 0 variance and infinite bias. More importantly, are there sensible lower variance and higher bias bounds than KLVI?

(2) tilde L on line 173 is not apparently defined anywhere.

(3) Can you give any characterization of the optimal q as it relates to the true posterior? That is one of the nice features of the ELBO.

(4) It seems like the convergence comparison of PVI and alpha-VI could use a few more points of comparison. Why was alpha=2 chosen? how does it compare to other choices of alpha? Is it not possible to use smaller positive alphas?

(5) Can the authors comment a bit more on the convergence of PVI to KLVI in the large data limit? I don't fully follow the argument, and  seems to be a relatively negative result for the context in which this may be used.

"
Kernel Feature Selection via Conditional Covariance Minimization,"Jianbo Chen, Mitchell Stern, Martin J. Wainwright, Michael I. Jordan",https://proceedings.neurips.cc/paper/2017/hash/b7fede84c2be02ccb9c77107956560eb-Abstract.html,"In this paper, authors propose a new nonlinear feature selection based on kernels. More specifically, the conditional covariance operator has been employed to measure the conditional independence between Y and X given the subset of X. Then, the feature selection can be done by searching a set of features that minimizing the conditional independence. This optimization problem results in minimizing over matrix inverse and it is hard to optimize it. Thus, a novel approach to deal with the matrix inverse problem is also proposed. Finally, the consistency of the feature selection algorithm is proved. Through experiments, the authors showed that the proposed algorithm compares favorably with existing algorithms.

The paper is clearly written and the proposed algorithm is technically sound. Overall, I like the algorithm.

Detailed comments:
1. In the proposed algorithm, some information about tuning parameters are missing. So, please include those information in the paper. For example, lambda_2 in Eq. (20). Also, how to initialize the original w?
2. It would be great to compare with other kernel based feature selection algorithms such as HSIC-LASSO and KNIFE. Also, related to the first comments, is it possible to further improve the proposed algorithm by properly initializing the w? For instance, HSIC-LASSO is a convex approach and can have reasonable features. 
3. The results of TOX-171 is pretty interesting. It is nice to further investigate the reason why the proposed approach outperforms rest of it.
4. It is nice to have comparison of redundancy rate. In feature selection community tends to use this measure for evaluation. 

Zhao, Z., Wang, L., and Li, H. (2010). Efficient spectral feature se lection with minimum redundancy. In AAAI Conference on Artificial Intelligence (AAAI) , pages 673–678.
","In this paper, authors propose a feature selection method that minimizes the conditional covariance of the output Y in RKHS given the selected features. It can be understood as minimizing the ""scatterness"" of Y given X. Clearly, a good feature selection should minimize such a variance. Several relaxations are also introduced to mitigate the computational cost. 

The methodology is sound and the paper is well-written. It may contain an interesting contribution to the field of kernel feature selection. 

I have only a few concerns: 

1. According to the definition of conditional independence, PY|X = PY|X_T. Can't we just minimize the difference between PY|X and PY|X_T? If the mapping from the distribution to the mean embedding is one-to-one, can't we just minimize the distance between these two conditional mean embeddings, using conditional MMD? What is the advantage of using a covariance operator or considering the second order moment? 

2. The computation of the proposed method looks heavy (see line 189) even after using the kernel approximation. How does it compare with the HSIC based methods? The performance of different relaxations should also have been tested in experiments. 

In general, this paper contains an interesting solution to kernel feature selection problems, though the computational advantage versus HSIC based methods is still not very clear to me. 
","Feature selection based on conditional covariance operator in RKHS proposed. The basic idea of the proposed method is in line with kernel dimension reduction (reference [9] in the manuscript), and the approach for optimization is simple (put weights for each feature dimension and minimize the proposed trace criterion of conditional covariance.)
Namely, the concept and methodology and not very new(but well thought-out), but the most important contribution I think of this paper is proving the feature selection consistency (Theorem 2).
I feel it is a significant contribution and this paper is worth accepted.

In ""Gradient-based kernel method for feature extraction and variable selection,"", NIPS2012, a kernel-based feature selection method is also presented. The approach is different but the one in NIPS2012 and the one in the current submission both are based on conditional covariance operator in RKHS. It would be informative to mention the work in NIPS2012 and give some comments on the similarity and differences.


Minor issues:
- line 94:  The notation ""[d]"" should be ""\mathcal{S}"", as it is introduced in line 48.
- line 131,132: F_m should be \mathcal{F}_m, and I couldn't find H_1^k in eq.(12). 
- line 146: ""Q(T)"" should be ""\mathcal{Q}(\mathcal{T})"", and |T|\leq m in line 147 should be |\mathcal{T}| \leq m.
- line 147: Isn't it better to refer to eq.(15) instead of eq.(17)?
- line 173: \|w_1\| \leq m

- The epsilon parameter is set to 0.001 and 0.1 for classification and regression tasks, resp., but I'd like to know the sensitivity of the proposed methods on the parameter. 

"
Active Learning from Peers,"Keerthiram Murugesan, Jaime Carbonell",https://proceedings.neurips.cc/paper/2017/hash/b87470782489389f344c4fa4ceb5260c-Abstract.html,"The authors propose an extension of the active learning framework to address two key issues: active learning of multiple related tasks, and using the 'peers' as an additional oracle to reduce the cost of querying the user all the time. The experimental evaluation of the proposed learning algorithms is performed on several benchmark datasets, and in this context, the authors proved their claims. 

The main novelty lies in the extension of the SHAMPO algorithm so that it can leverage the information from multiple tasks, and also peer-oracles (using the confidence in the predictions), to reduce the annotation efforts by the user. A sound theoretical formulation to incorporate these two is proposed. Experimental results show the validity of the proposed. 

The authors did not comment on the cost of using the peers as oracles, but assumed their availability.  This should be detailed more. Also, it is difficult to assess the model's performance fully compared to the SHAMPO algorithm as its results are missing in Table 1. The accuracy difference between peer and one version of the model is marginal. What is the benefit of the former? 

Overall, this is a nice paper with a good mix of theory and  empirical analysis. 
Lots of typos in the text that should be corrected. ","This paper proposes an online multitask learning algorithm. It claims that it can jointly learn the parameters and task-relatedness. However, there is a lack of comparison with other related work that can also do the joint work. Thus the major contribution of this work is not clear.

I would like to see a clear comparison with other papers that can learn the task relatedness.

Minor typos:

line 242 algorithm ?
","This paper proposes a novel active learning approach in an online multitask setting, where the algorithm can optionally query labels from peers or oracles.

For a specific task model, its parameters are estimated by utilizing information from peer models with the joint learning inter-task relationships.

Experiments are performed on three benchmark datasets, and results validated the superiority of the proposed method over baselines.

Theoretical analysis on the error bound is present. However, many key steps are directly following existing studies.

The reference is insufficient. More related studies should be cited.

The presentation should be significantly improved.
"
On Fairness and Calibration,"Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q. Weinberger",https://proceedings.neurips.cc/paper/2017/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html,"The paper deals with an increasingly important topic of fairness in predictive analytics.
The focus of the paper is on studying analytically and experimentally the model calibration effect on fairness.

The paper presentation is excellent. The problem is well presented and positioned with respect to existing work. The authors characterize the calibration and error-rate constraints with simple geometric intuitions and link theoretical findings to that interpretation.

It is an interesting result that it is difficult to enforce calibration and there are conditions under which it is feasible and a unique optimal solution exists and can be found efficiently through post-processing of existing classifiers.

In the experimental study the authors study the impact of imposing calibration and an equal cost constraint on real-world datasets. The results are promising.","This paper explores the relationship between Equal Opportunity and calibration constraints on models.  They show that even with relaxed Equal Opportunity constraints it can be problematic to enforce calibration.  They provide necessary and sufficient conditions for achieving this. They also create an algorithm to achieve a relaxation on Equal Opportunity that requires only one constraint (False positive or false negative or a weighted sum) be met by withholding information.  They also demonstrated the effect of this algorithm on several relevant datasets.  I found this paper to be well-written, easy to follow and also very thorough.  I think the topic is an important one for machine learning in practice, and the paper definitely adds something to the discussion.   I appreciated both the work on showing the required conditions and as well as the algorithm, which helps to understand the effect of this in practice.  ","Note: Equal opportunity as used in this paper was originally defined as “Equalized odds” in Hardt et al. 2017. This needs to be fixed in multiple places. 

The paper analyzes the interaction between relaxations of equalized odds criteria and classifier calibration. The work extends and builds on the results of a previous work [23]. 

1. Incompatibility of calibration and non-discrimination: The previous work [23] already establish incompatibility of matching both FPR and FNR (equalized odds) and calibration in exact as well as approximate sense (except in trivial classifiers). This result has been extended to satisfying non-discrimination with respect to two generalized cost functions expressed as a linear combinations of FPR and TPR of respective groups. This generalization while new is not entirely surprising . 

2. The authors show that if the equalized odds is relaxed to non-discrimination with respect to a single cost function expressed as a linear combination of FPR and FNR, then the relaxation and calibration can be jointly achieved under certain feasibility conditions.  

3. Additional experiments are provided to compare the effects of enforcing non-discrimination and/or calibration. 

While there are some new results and additional experiments, the conclusions are fairly similar to [23], essentially establishing incompatibility of satisfying equalized-odds style non-discrimination and classification calibration. 


Additional comments:
1. Why was EO trained not implemented and compared for the heart disease and income datasets?
2. Line 165-166: there seems to be a missing explanation or incorrect assumption here as if only one of a_t or b_t is required to be nonzero then g_t can be zero with just FPR=0 OR FNR=0, respectively. 

[Post rebuttal] Thanks for the clarifications on the contributions and the condition on a_t,b_t. 

I see the significance of the quantification of degradation performance of classifiers even when training is mindful of fairness and calibration conditions -- a similar quantification of accuracy of ""best"" fair predictor for exclusively EO condition (without calibration) would form a good complementary result (I dont expect the authors to attempt it for this paper, but may be as future work). That said, given the existing literature, I am still not very surprised by the other conclusions on incompatibility of two fairness conditions with calibration. "
One-Shot Imitation Learning,"Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba",https://proceedings.neurips.cc/paper/2017/hash/ba3866600c3540f67c1e9575e213be0a-Abstract.html,"Summary
---

Complex and useful robotic manipulation tasks are difficult because of the difficulty of manipulation itself, but also because it's difficult to communicate the intent of a task. Both of these problems can be alleviated through the use of imitation learning, but in order for this to be practical the learner must be able to generalize from few examples. This paper presents an architecture inspired by recent work in meta learning which generalizes manipulation of a robot arm from a single task demonstration; i.e., it does one-shot imitation learning.

The network is something like a seq2seq model that uses multiple attention mechanisms in the style of ""Neural Machine Translation by Jointly Learning to Align and Translate"".  There is a demonstration network, a context network and a manipulation network.  The first two produce a context embedding, which is fed to a simple MLP (the manipulation network) that produces an action that tell the arm how to move the blocks.  This context embedding encodes the demonstration sequence of states and the current state of arm in a way that selects only relevant blocks at relevant times from the demonstration.

More specifically, the demonstration network takes in a sequence (hundreds of time steps long) of block positions and gripper states (open or closed) and embeds the sequence through a stack of residual 1d convolution block. At each layer in the stack and at each time step, an attention mechanism allows that layer to compare its blocks to a subset of relevant blocks.  This sequence embedding is fed into the context network which uses two more attention mechanisms to first select the relevant period of the demonstration to the current state and then attend to relevant blocks of the demonstration.  None of the aforementioned interpretations are enforced via supervision, but they are supported by manual inspection of attention distributions.

The network is trained to build block towers that mimic those built in a demonstration in the final arrangement of blocks where the demonstration and test instance start from initially different configuration. DAGGER and Behavior Cloning (BC) are used to train the above architecture and performance is evaluated by measuring whether the correct configuration of blocks is produced or not. DAGGER and BC perform about the same. Other findings:
* The largest difference in performance is due to increased difficulty. The more blocks that need to be stacked, the worse the desired state is replicated.
* The method generalizes well to new tasks (number and height of towers), not just instances (labelings and positions of blocks).
* The final demonstration state is enough to specify a task, so a strong baseline encodes only this state but performs significantly worse than the proposed model.


Strengths
---

This is a very nice paper. It proposes a new model for a well motivated problem based on a number of recent directions in neural net design. In particular, this proposes a novel combination of elements from few-shot learning, (esp meta learning approaches like Matching Networks), imitation learning, transfer learning, and attention models. Furthermore, some attention parts of the model (at least for examples provided) seem cleanly interpretable, as shown in figure 4.


Comments/Suggestions/Weaknesses
---

There aren't any major weaknesses, but there are some additional questions that could be answered and the presentation might be improved a bit.

* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?

* How does this setting relate to question answering or visual question answering?

* How does the model perform on the same train data it's seen already? How much does it overfit?

* How hard is it to find intuitive attention examples as in figure 4?

* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.

* The related works section would be better understood knowing how the model works, so it should be presented later.

"," The main contribution of the paper is a new architecture for the block stacking task.Although the results seem to look good, I think that the contribution in terms of insight and novelty that I gather from reading the paper ultimately seem insufficient. Since neural netaork architecture seems to be the factor that leads to good performance, I think it'd be nice to see how the same architecture would fare in some other domain as well?"
Triangle Generative Adversarial Networks,"Zhe Gan, Liqun Chen, Weiyao Wang, Yuchen Pu, Yizhe Zhang, Hao Liu, Chunyuan Li, Lawrence Carin",https://proceedings.neurips.cc/paper/2017/hash/bbeb0c1b1fd44e392c7ce2fdbd137e87-Abstract.html,"-----
Post-rebuttal:

I think R2 brought up several important points that were not completely addressed by the rebuttal. Most importantly, I agree that the characterization of Triple GAN is somewhat misleading. The current paper should clarify that Triangle GAN fits a model to p_y(y|x) rather than this density being required as given. The toy experiment should note that p_y(y|x) in Triple GAN could be modeled as a mixture of Gaussians, although it is preferable that Triangle GAN does not require specifying this.
-----

This is a nice paper. The objective comes down to conditional GAN  + BiGAN/ALI. That is an intuitive and perhaps simple thing to try for the semi-supervised setting, but it’s nice that this paper backs up the formulation with theory about behavior at optimality. The results look good and it gives a principled framework for semi-supervised learning with GANs. The experiments are thorough and convincing. I was impressed to see state-of-the-art semi-supervised results not just on ""generative"" tasks like synthesizing images, but also on CIFAR label prediction (Table 1). The paper is clearly written, appears to be technically correct, and overall well executed. I also appreciated the careful comparison to Triple GAN, with the toy experiment in Figure 2 being especially helpful.

Section 4.3: it would be nice to also compare to the setting of having 0% labeled data, here and perhaps elsewhere. Here especially since DiscoGAN doesn’t use labels. The CycleGAN paper actually already includes this experiment (it reduces to just BiGAN), so that paper could also just be referred to.

It’s surprising that DiscoGAN does badly on the transposed MNIST metric, since DiscoGAN is explicitly trained to minimize reconstruction error on X—>Y—>X problems, which it sounds like is what this metric is measuring. Even if DiscoGAN does terribly at X—>Y, it should still do well at X—>Y—>X, if the reconstruction cost is set high enough. Therefore, this evaluation metric doesn’t seem the most appropriate for showing the possible advantages of the current method over DiscoGAN. Fig 4 is compelling though, since I can actually see that on X—>Y, DiscoGAN fails.


Minor comments:
1. Line 33: “infinite” —> “infinitely”
2. Line 138: “strive” —> “strives”
3. Line 187: I would separate DTN from DiscoGAN, CycleGAN, and DualGAN. The latter all use an essentially identical objective, whereas the former is a bit different.","In this paper, the authors have presented the Triangle Generative Adversarial Network, a new GAN framework that can be used for semi-supervised joint distribution matching. The proposed approach learns the bidirectional mappings between two domains with a few paired samples. They have demonstrated that Triangle-GAN may be employed for a wide range of applications. I have the following concerns.
(1) The training time of this new network is not reported. The authors may want to have some discussion on the training time. 
(2) Is this network stable for training? Or is there any trick for efficiently training this network?
(3) The performance gain of Triangle GAN is not significant compared to ALI[7]. The authors made this network much more complex but failed to achieve satisfied performance improvement.
(4) Fig. 3 is not well demonstrated. I understand that the proposed network can be used to generate samples. However, the improvement is not well explained.","Triangle GAN is an elegant method to perform translation across domains with limited numbers of paired training samples. Empirical results seem good, particularly the disentanglement of known attributes and noise vectors. I'm worried, though, that the comparison to the closest similar method is somewhat unfair, and in general that the relationship to it is somewhat unclear.


Relation to Triple GAN:

Your statements about the Triple GAN model's requirements on p_y(y | x) are somewhat misleading. Both in lines 53-56 and section 2.5, it sounds like you're saying that p_y(y | x) must be known a priori, which would be a severe limitation to their model. This, though, is not the case: rather, the Triple GAN model learns a classifier which estimates p_y(y | x), and show (their Theorem 3.3) that this still leads to the unique desired equilibrium. In cases where p_y(y | x) does not have a density, it is true that their loss function would be undefined. But their algorithm will still simply estimate a classifier which gives high probability densities, and I think their Theorem 3.3 still applies; the equilibrium will simply be unreachable. Your Propositions 1 and 2 also break when the distributions lack a joint density, and so this is not a real advantage of your method.

The asymmetry in the Triple GAN loss function (8), however, still seems potentially undesirable, and your approach seems preferable in that respect. But depending on the complexity of the domains, it may be easier or harder to define and learn a classifier than a conditional generator, and so this relationship is not totally clear.

In fact, it seems like your image-label mappings (lines 164-173) use essentially this approach of a classifier rather than a conditional generator as you described earlier. This model appears sufficiently different from your main Triangle GAN approach that burying its description in a subsection titled Applications is strange. More discussion about its relationship to Triangle GAN, and why the default approach didn't work for you here, 

Similarly, the claim that Triple GAN cannot be implemented on your toy data experiment (section 4.1) seems completely wrong: it perhaps cannot with exactly the modeling choice you make of using G_x and G_y, but p_x(x | y) and p_y(y | x) are simple Gaussians and can certainly be modeled with e.g. a network outputting a predictive mean and variance, which is presumably what the Triple GAN approach would be in this case.


Theory:

The claim that ""p(x, y) = p_x(x, y) = p_y(x, y) can be achieved in theory"" (line 120) is iffy at best. See the seminal paper

Arjovsky and Bottou. Towards Principled Methods for Training Generative Adversarial Networks. ICLR 2017. arXiv: 1701.04862.

The basic arguments of this paper should apply to your model as well, rendering your theoretical analysis kind of moot. This subarea of GANs is fast-moving, but really I think any GAN work at this point, especially papers attempting to show any kind of theoretical results like your section 2.3, needs to grapple with its relationship to the results of that paper and the followup Wasserstein GAN (your [22]) line of work.

As you mention, it seems that many of these issues could be addressed in your framework without too much work, but it would be useful to discuss this in the theory section rather than simply stating results that are known to be of very limited utility.


Section 4.4:

In Figure 6, the edited images are clearly not random samples from the training set: for example, the images where you add eyeglasses only show women. This adds a natural question of, since the images are not random, were they cherry-picked to look for good results? I would strongly recommend ensuring that all figures are based on random samples, or if that is impractical for some reason, clarifying what subset you used (and why) and being random within that subset.


Smaller comments:

Lines 30-33: ALI and BiGAN don't seem to really fit into the paradigm of ""unsupervised joint distribution matching"" very well, since in their case one of the domains is a simple reference distribution over latent codes, and *any* pairwise mapping is fine as long as it's coherent. You discuss this a little more in section 3, but it's kind of strange at first read; I would maybe put your citations of [9] to [11] first and then mention that ALI/BiGAN can be easily adapted to this case as well.

Section 2.2: It would be helpful to clarify here that Triangle GANs are exclusively a ""translation"" method: you cannot produce an (x, y) sample de novo. This is fine for the settings considered here, but differs from typical GAN structures enough that it is probably worth re-emphasizing e.g. that when you refer to drawing a sample from p(x), you mean taking a training sample rather than some form of generative model."
Learning Populations of Parameters,"Kevin Tian, Weihao Kong, Gregory Valiant",https://proceedings.neurips.cc/paper/2017/hash/bc4e356fee1972242c8f7eabf4dff517-Abstract.html,"The paper considers a statistical framework where many observations are available but only a few concern each individual parameter of interests. Instead of estimating each parameter individually, following an idea of Stein, the idea is to estimate ``population parameters"", that is to provide an estimate of the empirical distribution of these parameters.

Theoretically, an estimation of the empirical distribution based on its first moments is proposed and a control of the risk of this estimator with respect to the Wasserstein distance is proved. 

Then, an algorithm for practical implementation of this estimator is proposed and illustrates the performances of the estimator on both synthetic and real data-sets. The interest of the statistical problem is shown on political tendency and sports performances at the end of the paper.

I think that the statistical problem, the way to analyse it and the moment solution are interesting and I strongly recommand the paper for publication. Nevertheless, I think that many references and related works could be mentionned. For example, it seems that the estimator would provide an interesting relevant prior for an empirical Bayes procedure, the connection with latent variables in mixed effects models should be discussed, as well as the connection of this procedure with ``hidden"" variables model like HMM. Finally, a moment procedure was proposed recently to estimate the unobserved ``random environment"" of a Markov Chain. Comparisons with these related works could improved the paper.","This paper establishes an interesting new problem setting, that of estimating the distribution of parameters for a group of binomial observations, gives an algorithm significantly better (in earth-mover distance) than the obvious one, and shows both a bound on its performance and an asymptotically-matching lower bounds. It also gives empirical examples on several datasets, showing the algorithm's practical advantages.

I did not closely verify the proofs, but they seem reasonable. Presentation-wise, they are somewhat disorganized: it would be helpful to label the sections e.g. Proof of Theorem 3, and provide a formal statement of the proposition being proved in the noise-free case, etc.

Unfortunately, the algorithm does involve solving a fairly large linear program. Explicitly characterizing the computational complexity and accuracy effects of m would be helpful.

It would also be much nicer to not need a hard cutoff on the moment estimators s. Perhaps you could weight the loss function by the standard error of their estimators, or something similar? The estimators should be asymptotically normal, and you could e.g. maximize the likelihood under that distribution (though their asymptotic correlations might be difficult to characterize).

Though it's an interesting problem setting, it also doesn't seem like one likely to take the world by storm; this paper is probably of primarily theoretical interest. Some interesting potential applications are suggested, however.

A question about the proof: in lines 400-401, I think some more detail is needed in the proof of the variance bound. Choosing the t/k independent sets would give the stated bound, but of course there are \binom{t}{k} - t/k additional sets correlated to those t/k. It seems likely that averaging over these extra sets decreases the variance, but this is not immediate: for example, if all of the other sets were identical, the variance would be increased. I think some work is needed here to prove this, though maybe there's an obvious reason that I'm missing.


Minor notes:

Line 47: should be ""Stein's phenomenon.""

Line 143, ""there exists only one unbiased estimator"": If you add N(0, 1) noise to any unbiased estimator, it's still unbiased. Or you could weight different samples differently, or any of various other schemes. Your proof in Appendix E (which should be referenced by name in the main text, to make it easier to find...) rather shows that there is a unique unbiased estimator which is a deterministic function of the total number of 1s seen. There is of course probably no reason to use any of those other estimators, and yours is the MVUE, but the statement should be corrected.

Lines 320-322: (6) would be clearer if you established t to be scaled by c explicitly.

y axis limits for CDF figures (2, 3, 4) should be clamped to [0, 1]. Probably the y lower bound in Figure 1a should be 0 as well.","In this paper, authors study the problem of learning a set of Binomial parameters in Wasserstein distance and demonstrate that sorted set of probabilities can be learned at a rate of (1/t) where t is the parameter in the Binomial distribution. The result seems interesting and I had a great time reading it. However, I would like to understand the technical novelty of the paper better.

Can the authors please expand on the differences between this paper and one of the cited papers: “Instance optimal learning of discrete distributions”, by Valiant and Valiant? To be more specific:

In many of the discrete learning problems, Poisson approximation holds i.e., Binomial (t,p_i) ~ Poisson (tp_i). Hence, the proposed problem is very similar to estimating n Poisson random variables with means tp_1, tp_2, \ldots tp_n.  Similarly the problem in the VV paper boils down to estimating many Poisson random variables. Hence, aren't they almost the same problem?

Furthermore, in VV’s paper, they use empirical estimates for large Poisson random variables (mean > log n), and use a moment based approach for small Poisson random variables (mean < log n). In this paper, authors use moment based approach for small Poisson random variables (mean < t). In both the cases, the moment based approach yields an improvement of 1/poly(mean) ~1/poly(t) improvement in earthmovers / Wasserstein’s distance.

Given the above, can you please elaborate the differences between the two papers?

Few other questions to the authors:

1. In Theorem 1: What is the dependence of t in the term O_{delta, t}(1/\sqrt{n}).
2. In Theorem 1: s is not known in advance, it is a parameter to be tuned/ supplied to the algorithm. Hence, how do you obtain min_s in the right hand side of the equation?  
3. Why Wasserstein’s distance? 
4. Good-Turing approach (McAllester Schapire ‘00) seems to perform well for instance optimality of discrete distributions in KL distance (Orlitsky Suresh ‘15). However, VV showed that Good-Turing does not work well for total variation distance. In your problem, you are looking at Wasserstein’s distance. Can you please comment if you think Good-Turing approaches work well for this distance? Can you given example / compare these approaches at least experimentally?

Edits after the rebuttal:

Changed to Good paper, accept.

> For ANY value of t, there exist dists/sets of p_i's such that the VV and Good-Turing approaches would recover dists that are no better (and might be worse?) than the empirical distribution (i.e. they would get a rate of convergence > O(1/sqrt(t)) rather than the O(1/t) that we get).

Can you please add these examples in the paper, at least in the appendix? It would be also nice if you can can compare with these approaches experimentally.

> Suppose p_i = 0.5 for all i, t=10, and n is large. The analogous ""poissonized"" setting consists of taking m<--Poi(5n) draws from the uniform distribution over n items, and the expected number of times the ith item is seen is distributed as Poi(5). Poi(5) has variance 5, though Binomial(10,0.5) has variance 10/4 = 2.5, and in general, for constant p, Poi(t*p) and Binomial(t,p) have constant L1 distance and variances that differ by a factor of (1-p). 

While I agree that they have a constant L1 distance, I still am not convinced that these problems are 'very different'. Do you believe that your algorithm (or a small variation of it)  would not work for the problem where the goal is to estimate n Poisson random variables? If so / no, please comment. If not, at least say something about it in the paper. 

> [A ""bad"" construction is the following: consider a distribution of p_i's s.t. the corresponding mixture of binomials approximates a Poisson, in which case VV and Good-Turing will conclude wrongly that all the p_i's are identical.]

I do not agree with this comment too. Note that for this argument to work it is not sufficient to approximate each Poisson coordinate with a  mixture of binomials coordinate. One has to approximate a Poisson product distribution over n coordinates, with a mixture of binomial random variables with n coordinates. I believe this task gets harder as n-> infty.

> It depends on the distribution (and is stated in the theorem), but can be as bad as 2^t.

Please mention it in the paper. 




"
Structured Embedding Models for Grouped Data,"Maja Rudolph, Francisco Ruiz, Susan Athey, David Blei",https://proceedings.neurips.cc/paper/2017/hash/bd686fd640be98efaae0091fa301e613-Abstract.html,"This work describes a method for learning different word embeddings for different groups in data, such as speeches made by senators in different states or parties. The motivation being that senators from different parties use the same word in different contexts or meanings.

The work is generally an incremental extension of EFEs to grouped data where the groups are known a priori. The test set log-likelihoods show that the organisation of the groups do indeed improve the predictive accuracy of the model, however, the baselines are quite weak. The analysis of the learnt variations between embeddings are also interesting though table 3 would benefit from a comparison with the words obtained via frequency analysis between groups as hinted in line 316.

The amortization approach is quite interesting and makes this approach practical without a blow up in the number of parameters.

Would it be possible to extend this approach to discover the groups in the data?

Minor comments:
L34: Typo, (that) how
L53-54, repeat of lines 43-44.
L68: Typo: differntly, wheter
L141,143: Bad reference: eq 9
L302: Eventhough","
This paper presents a word embedding model for grouped data. It extends EFE to learn group-specific embedding vectors, while sharing the same context vector. To handle groups limited data, the authors propose two methods (hierarchical and amortization) to derive group-specific embedding vectors from a shared one. The paper is clearly written, but the novelty is a bit limited since it is an incremental work beyond EFE.

1. From Table 2, there is no clear winner among the three proposed models (hierarchical, amortiz+feedf, and amortiz+resnet), and the performance differences are subtle especially on the shopping data. If one would like to use S-EFE, do you have any practical guidance on choosing the right model? I guess we should prefer amortiz+resnet to amortiz+feedf, since amortiz+resnet always outperforms amortiz+feedf. Line 276 mentions that hierarchical S-EFE works better when there are more groups. Why?

2. Why Separate EFE performs worse than Global EFE?

3. The authors proposed hierarchical and amortization methods for the reasons in lines 128-131. It is interesting to see how S-EFE performs w.r.t. various data sizes. From this experiment, we might understand if S-EFE is going to surpass hierarchical/amortiz S-EFE, provided more and more data. If yes, would it be better to promote S-EFE as the leading method? If not, why?

4. I understand that pseudo log likelihood is a standard metric for evaluating embedding methods. However, it would be great to see how embedding helps in standard language understanding tasks, like text classification, machine translation, etc.

5. lines 141 and 143: Eq. 9 to Eq. 4

      ","This paper proposes an extension to exponential family embeddings (EFE) by adding group structures (S-EFE) via hierarchical modeling or amortization. Specifically, S-EFE uses a shared context vector across all groups and a separate embedding vector to construct exponential family embeddings of the dataset. Statistical sharing is further imposed either by using a common prior distribution from which per group embedding vectors (hierarchical model) are drawn from or by learning a (neural network) function to map global embedding vectors to group embedding vectors (amortization). Experiments on a Congressional speech dataset, arXiv dataset, and groceries dataset show that S-EFE outperforms global and separate EFE in terms of test log likelihood. 

The paper is well written and the motivation is clear. While the extensions (hierarchical model and amortization) are not excitingly novel, I think the proposed method can potentially be useful in various setups, as demonstrated by the authors in their experiments. 
- For words that are used differently across fields (e.g., intelligence), S-EFE is able to discover variations in its usage. 
For words that have relatively similar usages across fields, is the method able to keep their embeddings consistent or there are cases when the context vectors are different enough (e.g., due to different topics in different fields) that they confuse the model?
- In terms of training time, how much slower is S-EFE compared to EFE?
- Experiment results for hierarchical vs. amortized are mixed. Do the authors have any insights on when one is preferred over the other?"
Conservative Contextual Linear Bandits,"Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi Yadkori, Benjamin Van Roy",https://proceedings.neurips.cc/paper/2017/hash/bdc4626aa1d1df8e14d80d345b2a442d-Abstract.html,"
	This paper studies a kind of contextual linear bandits with a conservative constraint,
	which enforces the player's cumulative reward at any time t to be at least (1-alpha)-times
	as larger as that of the given baseline policy for a give alpha.
	They consider two cases, the cases with known and unknown baseline rewards.
	For each case, they propose a UCB-type algorithm based on an existing algorithm for contextual linear bandits and prove its regret upper bound, which are composed of
	the regret caused by the base algorithm and the regret caused by satisfying the conservative constraint.
	They also conducted simulations of the algorithm with known baseline rewards, and checked that the algorithm really satisfies the conservative constraint.

	The conservative constraint, their algorithms and regret bounds seem natural.
	The graph of Figure 1(a) should be improved so as to be able to see the initial conservative phases of CLUCB more.
	The same experiments for CLUCB2 should be done because it works in a more realistic setting.
      ","POST-REBUTTAL:

The authors have answered my concerns and will clarify the point of confusion. I'm changing from a marginal accept to an accept.

OLD REVIEW:

Summary of the paper

This paper proposes a ""safe"" algorithm for contextual linear bandits. This definition of safety assumes the existence of a current ""baseline policy"" for selecting actions. The algorithm is ""safe"" in that it guarantees that it will only select an action that differs from the action proposed by the baseline policy if the new action produces larger expected reward than the action proposed by the baseline policy. Due to the random nature of rewards, this guarantee is with high probability (probability at least 1-delta).





Summary of review

The paper is well written. It is an extremely easy read - I thank the authors for submitting a polished paper. The proposed problem setting and approach are novel to the best of my knowledge. The problem is well motivated and interesting. Sufficient theoretical and empirical justifications are provided to convince the reader of the viability of the proposed approach.

However, I have some questions. I recommend at least weak acceptance, but would consider a stronger acceptance if I am misunderstanding some of these points.






Questions

1. Definition 1 defines a desirable performance constraint. The high probability nature of this constraint should be clarified. Notice that line 135 doesn't mention that (3) must hold with high probability. This should be stated.

2. More importantly, the statement of *how* (3) must hold is critical, since right now it is ambiguous. During my first read it sounds like (3) must hold with probability 1-\delta. However this is *not* achieved by the algorithm. If I am understanding correctly (please correct me if I am wrong), the algorithm ensures that *at each time step* (3) holds with high probability. That is:

\forall t \in \{1,\dotsc,T\}, \Pr \left ( \sum_{i=1}^t r_{b_i}^i - \sum_{i=1}^t r_{a_i}^t \leq \alpha \sum_{i=1}^t r_{b_i}^i \right )

NOT

\Pr \left ( \forall t \in \{1,\dotsc,T\},  \sum_{i=1}^t r_{b_i}^i - \sum_{i=1}^t r_{a_i}^t \leq \alpha \sum_{i=1}^t r_{b_i}^i \right )

The current writing suggests the latter, which (I think) is not satisfied by the algorithm. In your response could you please clarify which of these you are claiming that your algorithm satisfies?

3. If your algorithm does in fact satisfy the former (the per-time step guarantee), then the motivation for the paper is undermined (this could be addressed by being upfront about the limitations of this approach in the introduction, without changing any content).

Consider the actual guarantee that you provide in the domain used in the empirical study. You run the algorithm for 40,000 time steps with delta = 0.001. Your algorithm is meant to guarantee that ""with high probability"" it performs at least as well as the baseline. However, you only guarantee that the probability of playing a worse action will be at most 0.001 *at each time step*. So, you are guaranteeing that the probability of playing a worse action at some point will be at most 1-.999^40000 = (approximately) 1. That is, you are bounding the probability of an undesirable event to be at most 1, which is not very meaningful. This should be discussed.

For now, I would appreciate your thoughts on why having a per-step high probability guarantee is important for systems where there are large numbers of time steps. If a single failure is damning then we should require a high probability guarantee that holds simultaneously for all time steps. If a single failure is not damning, but rather amortized cost over over thousands of time steps is what matters, then why are we trying to get per-time step high probability guarantees?","This paper presents a variant of linear UCB method for the contextual linear bandit problem, which is ""conservative"" in the sense that it will not violate the constraint that its performance is to be above a fixed percentage of a given baseline method. The authors prove some basic regret bounds on their proposed conservative method to establish their soundness. The idea of the method is straightforward, and the proofs are not particularly surprising, and the techniques used therein not overly innovative. Nonetheless, the problem formulation and proposed method are practical, and the results of the paper will likely benefit real world applications. "
Regularized Modal Regression with Applications in Cognitive Impairment Prediction,"Xiaoqian Wang, Hong Chen, Weidong Cai, Dinggang Shen, Heng Huang",https://proceedings.neurips.cc/paper/2017/hash/bea5955b308361a1b07bc55042e25e54-Abstract.html,"Summary:
In this paper the authors extend the modal regression framework with sparse regularization (combining modal regression and LASSO). They provide learning theory analysis for regularized modal regression (RMR). They evaluate the method on simulated toy data and in addition they apply the method to a clinical problem in Alzheimer’s disease: Predicting cognitive scores from brain imaging data. As a data source they use the publicly available ADNI database. 
The proposed method is compared to a suitable set of alternative and established method (least squares regression, ridge and lasso, and median regression). Overall the proposed method performs slightly better than competing methods. However, the improvement over related methods such as ridge regression is minuscule, and looking at the accompanying standard deviation, not statistically significant. This may have to do with the task at hand and the method may be more suitable for other problems.
	
Comments:
1. The ADNI databse contains more than 1500 subjects with imaging and scores from cognitive tests, why were only 795 subjects used?
2. Page 7: that is a very strong statement given the observed small differences between methods “The reason that least square models do not work well is that the assumption of symmetric noise may not be guaranteed on the ADNI cohort.” E.g., for “Fluency” ridge regression achieved an RMSE 0.3269+/-0.006 and RMR-L2 0.3276+/-0.0049.
3. Figure 1: the text is too small to read the axis annotation (i.e., feature names).

Minor:
1. Page 3 in 2.2: “consider consider”
2. Page 5 in 3.3. “Before return arbitrary” -> “Before returning arbitrary”

","This manuscript formulates modal regression as a penalized linear regression with a new loss, applied to prediction of Alzheimer's disease status from brain images. The new loss function captures the mode, rather than the mean, of the output variable. Such modal regression has been developped previously, but the manuscript studies it in the context of risk minimization with l1 or l2 regularization, and gives interesting theoretical results in those settings.

The theoretical contribution is interesting, as the resulting formulation seems quite easy to work with.

In the empirical validation, the difference in RMSE seems non significant. I wonder if this is not bound to happen: the RMR methods do not seek to minimize RMSE. I find the NeuroImaging validation not convincing: although I myself do a lot of neuroimaging data analysis, the sample sizes are small, and it would be interesting to confirm the effects on other data with more samples.

Also, it would be interesting to compare to a huber loss, which is also robust. The intuition is that huber would probably be less robust.

","The authors present a regularized modal regression method. The statistical learning view of this proposed method is studied and the resulting model is applied to Alzheimer's disease studies. There are several presentation and evaluation issues for this work in its current form. 

Firstly, the authors motivate the paper using Alzheimer's disease studies and argue that modal regression is the way to analyze correlations between several disease markers of the disease. This seems very artificial. The necessity of use conditional mode for regression has nothing specific for the Alzheimer's application. The motivation for RMR makes sense without any AD related context. The authors then argue that the main contribution for the work is designing modal regression for AD (line 52). However, for the rest of the methods the discussion is about statistical learning -- no AD at all. This presentation is dubious. The authors need to decide if the work is statistical learning theory for modal regression OR an application of modal regression for AD (for the latter more applied journals are relevant and not this venue).

Moving beyond this overall presentation issue, if we assume that modal regression for AD is the focus of the paper, then the results should show precisely this performance improvement compared to the rest mean-based regression. However, as shown in section 4 this does not seem to be the case. In several cases the performance of LSR is very close to RMR. It should be pointed out that when reporting such sets of error measures, one should perform statistical tests to check if the performance difference is significant i.e., p-value differences between all the entries of Tables. Beyond, the tables the message is not clear in the Figures -- the visualizations need to be improved (or quantified in some sense). This non-appealing improvements of AD data evaluations is all the more reason that the authors should choose 'standard' regression datasets and show the performance gains of RMR explicitly (once this is done, the results of AD can be trusted and any small improvement can be deemed OK). "
Adversarial Ranking for Language Generation,"Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, Ming-ting Sun",https://proceedings.neurips.cc/paper/2017/hash/bf201d5407a6509fa536afc4b380577e-Abstract.html,"For generative adversarial network (GAN) training, the paper presents an approach for replacing the binary classifier in the discriminator with a ranking based discriminator. This allows better training especially for problems which are not binary, such as (discrete) sequence generation, as e.g. in sentence generation.

Strength:
-	Novel, well motivated approach
-	Extensive experiments on four datasets, a synthetic dataset, Chinese poem generation, coco image caption generation, and Shakespeare’s plays generation.
-	The results show improvements over the recent RankGAN [33], as well as a maximum likelihood estimation (MLE) baseline. The paper shows the improvements w.r.t. automatic as well as human evaluation measures.
-	The paper is clear and to my understanding sound.

Weaknesses:
1.	Related Work: As the available space allows it, the paper would benefit from a more detailed discussion of related work, by not only describing the related works, but also discussing the differences to the presented work.
2.	Qualitative results: To underline the success of the work, the paper should include some qualitative examples, comparing its generated sentences to the ones by related work.
3.	Experimental setup: For the Coco image cations, the paper does not rely on the official training/validation/test split used in the COCO captioning challenge.
3.1.	Why do the authors not use the entire training set?
3.2.	It would be important for the automatic evaluation to report results using the evaluation sever and report numbers on the blind test set (for the human eval it is fine to use the validation set).

Conclusion:
I hope the authors will include the coco caption evaluation server results in the rebuttal and final version as well as several qualitative results.
Given the novelty of the approach and strong experiments without major flaws I recommend accepting the paper.

It would be interesting if the authors would comment on which problems and how their approach can be applied to non-sequence problems.
","The paper proposes an alternative parameterization of the GAN discriminator based on relative ranks among a comparison set. The method shows better quantitative results for language generation on three (relatively small) language datasets.

Mathematically, there are some potential problems of the proposed method. 
- Firstly, the original GAN identified the equilibrium of Eqn. (1) only for R(s|U,C) being a proper probability over a Bernoulli variable. However, the definition in Eqn. (4) does not guarantee this condition. Thus, it is difficult to see whether the proposed criterion can ensure the generator distribution to match the true data distribution. 
- However, if one regards the expected log probability in Eqn. (4) as a definition of energy, the practically used criterion Eqn. (7) is more similar to the energy-based GAN. From this perspective, if one completes the two expectations on U and C omitted in Eqn. (7), (i.e., the full formula for the a term should be E_{s ~ P_h} [ E_{U ~ bar{U}} E_{C ~ bar{C}} log R(s|U,C) ] where bar{U} and bar{C} represent all possible reference and comparison sets respectively), and plugs Eqn. (4) into the full formula, it becomes E_{s ~ P_h} [ E_{U ~ bar{U}} E_{C ~ bar{C}} ( E_{u ~ U} log P(s|u,C) ) ] <= E_{s ~ P_h} [ log P(s) ], which is a lower bound of the ranker-defined likelihood. In other words, the energy function is defined as a lower bound of the likelihood, which is a little bit strange and not conventional. 
- Putting things together, I don’t really understand what the proposed criterion is doing, and why it leads to better empirical performances. 

In terms of the experiment result, despite the good quantitative results, it would be nice if some qualitative analysis were conducted to understand why and how the generated sentences could achieve higher scores. Also, is the result sensitive to the size of the comparison set |C| or the size of the reference set |U|? 

In summary, the proposed method achieves nice empirical results in improving language generation under the GAN setting. However, given the content of the paper, it is still not clear why this method should lead to such improvement. ","This paper propose to use ranking loss to replace original binary classification loss in GAN, which is novel. All the evaluation scores are better than state-of-art sequence generating GAN, however, there is no sample of generated text, which is hard to say whether this method is overfitting the metric. "
Diving into the shallows: a computational perspective on large-scale shallow learning,"SIYUAN MA, Mikhail Belkin",https://proceedings.neurips.cc/paper/2017/hash/bf424cb7b0dea050a42b9739eb261a3a-Abstract.html,"
The paper studies the effect of decay of eigenvalues on function approximation using Gradient Descent(GD). It argues that Gradient Descent machinery may lead to large number of iterations when infinitely diff kernels are used to approximate non-smooth functions. 
To remedy this one could use second order methods or use an explicit regularisation. While explicit Regularisation leads to bias 
the alternative of deploying second order methods are too expensive. To remedy the situation the paper proposes Eigen-Pro.

The result showing that GD is ineffective when one wishes to approximate a not so smooth function using an infinitely differentiable  kernel is novel and insightful. It is linked to the decay of eigen structure of the kernel functions is again insightful. The use of eigen-pro which achieves a sort of implicit regularisation is an interesting alternative where the computational challenge is finessed with randomisation which should have practical significance

While the above mentioned paragraph mentions the pluses there are two negative points that needs to be mentioned.
Firstly, the implications of the derived results are not clear. If it is speed-up we are concerned with then the comparisons with PEGASOS Is only marginal and often Eigen-pro is slower. It is commendable that the method can match performance with state of the art algorithms with kernel methods more needs to be done to have an explicit comparison to understand time and accuracy 
trade-offs. Would it make sense to show results by training a Deep Network(since this is what is used as the state of the art ) and comparing it with the kernel approximation in a synthetic setting. This would strengthen the understanding of time and accuracy trade-offs.

Secondly, the longer version of the paper is easy to read but the submitted version is not very easy to read. 

Overall, a nice paper, but lacking in explaining the experimental results and readability.  


","The paper presents an analysis of gradient descent as learning strategy for kernel methods. The main result of the paper shows that there is a restriction on the functions that can be effectively approximated by gradient descent. In particular, the paper defines the concept of 'computational reach' of gradient descent and shows that for smooth kernels the 'computational reach' includes only a small fraction of the function space. This limitation is overcame by a new method called ""EigenPro"" which uses a precondition strategy. The method is efficiently implemented and evaluated over standard datasets. The results show systematic improvements by the proposed algorithm, however the statistical significance of these differences is not evaluated. 

In general, the papers is well written, the ideas are clearly presented, and the experimental setup, as well as the results are convincing. Even though I didn't check the details of the mathematical claims, they seem to be sound. Overall, I think the work contributes important insights on how large-scale kernel learning works and how it could be improved.
","This paper looks at analyzing the problem of long times required for convergence of kernel methods when optimized via gradient descent. They define a notion of computational reach of gradient descent after t iterations and the failure of gradient descent to reach the epsilon neighborhood of an optimum after t iterations. They give examples of simplistic function settings with binary labels where gradient descent takes a long time to converge to the optimum. They also point out that adding regularization improves the condition number of the eigenspectrum (resulting in possible better convergence) but also leads to overregularization at times.
They introduce the notion of EigenPro, where they pre-multiply the data using a preconditioner matrix, which can be pre-computed, improves the time for convergence by making the lower eigenvalues closer to the largest one as well as making cost per iteration efficient. They do a randomized SVD of the data matrix/kernel operator to get the eigenvalues and generate the pre-conditioning matrix using the ratio of the eigenvalues.They show that the per-iteration time is not much higher than kernel methods and demonstrate experiments to show that the errors are minimized better than standard kernel methods using less GPU time overall. The acceleration provided is significant for some of the Kernels. 
The paper is dense in presentation but is well written and not very difficult to follow. However there would be a lot of details that can be provided to compare how the pre-conditioning matrix can influence gradient descent in general and whether it should always be applied to data matrices every time we try to train a linear model on data. The authors also provide the argument of how lowering the ratio of the smaller eigenvalues compared to the larger one makes the problem more amenable to convergence. It would be good to see some motivation/geometric description of how the method provides better convergence using gradient descent. It would also be interesting to explore if other faster algorithms including proximal methods as well as momentum based methods also can benefit from such pre-conditioning and can improve the rates of convergence for kernel methods. "
Integration Methods and Optimization Algorithms,"Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d'Aspremont",t,"The paper provides an interpretation of a number of accelerated gradient algorithms (and other convex optimization algorithms) based on the theory of numerical integration and numerical solution of ODEs. In particular, the paper focuses on the well-studied class of multi-step methods to interpret Nesterov's method and Polyak's heavy ball method as discretizations of the natural gradient flow dynamics. The authors argue that this interpretation is more beneficial than existing interpretations based on different dynamics (e.g. Krichene et al) because of the simplicity of the underlying ODE. Notice that the novelty here lies in the focus on multistep methods, as it was already well-known that accelerated algorithms can be obtained by appropriately applying Euler's discretization to certain dynamics (again, see Krichene et al).

A large part of the paper is devoted to introducing important properties of numerical discretization schemes for ODEs, including consistency and zero-stability, and their instantiation in the case of multistep methods. Given this background, the authors proceed to show that for LINEAR ODES, i.e., for gradient flows deriving from the optimization of quadratic functions, natural choices of parameters for two-step methods yield Nesterov and Polyak's method. The interpretation of Nesterov's method is carried out both in the smooth and strongly convex case and in the smooth-only case. 

I found the paper to be an interesting read, as I already believed that the study of different numerical discretization methods is an important research direction to pursue in the design of more efficient optimization algorithms for specific problems. However, I need to make three critical points, two related to the paper in question and one more generally related to the idea of using multistep methods to define faster algorithms:

1) The interpretation proposed by the authors only works in explaining Nesterov's method for functions that are smooth in the l_2-norm and is not directly applicable for different norms. Indeed, it seems to me that different dynamics besides the gradient flow must be required in this case, as the method depends on a suitable choice of prox operator. The authors do not acknowledge this important limitation and make sweeping generalizations that need to be qualified, such as claiming that convergence rates of optimization algorithms are controlled by our ability to discretize the gradient flow equation. I believe that this incorrect when working with norms different from l_2. 

2) The interpretation only applies to quadratic functions, which makes it unclear to me whether these techniques can be effectively deployed to produce new algorithms. For comparison, other interpretations of accelerated methods, such as Allen-Zhu and Orecchia's work led to improved approximation algorithms for certain classes of linear programs.

3) Regarding the general direction of research, an important caveat in considering multistep methods is that it is not completely clear how such techniques would help, even for restricted class of problems, for orders larger than 2 (i.e. larger than those that correspond to Nesterov 's AGD). Indeed, it is not hard to argue that both Nesterov's AGD (and also Nemirovski's Mirror Prox) can leverage smoothness to fully cancel the discretization error accrued in converting the continuous dynamics to a discrete-time algorithm. It is then not clear how a further reduction in the discretization error, if not properly related to problem parameters, could yield faster convergence.

In conclusion, I liked the intepretation and I think it is worth disseminating it. However, I am not sure that it i significant enough in its explanatory power, novel enough in the techniques introduced (multistep methods are a classical tool in numerical systems) or promising enough to warrant acceptance in NIPS."
The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings,"Krzysztof M. Choromanski, Mark Rowland, Adrian Weller",https://proceedings.neurips.cc/paper/2017/hash/bf8229696f7a3bb4700cfddef19fa23f-Abstract.html,"
The paper examines embeddings based on structured matrices. In particular the paper analyzes the expected reconstruction error of a class of pointwise non-linear gaussian kernels computed using the embedded vectors. 

Embeddings based on structured matrices are well known in literature, [Sarlos, Smola '13, Yu et al '16] and have been studied from a practical and a theoretical viewpoint. In particular it is proven that they achieve an error, that is equal, up to constants to the one of unstructured matrices. The main contribution of this paper is to show that the constant is smaller than one ( it tends to 1 when the ambient dimension tend to infinity). 

The paper is technically correct.

Note that the crucial aspect for which the structured methods are preferred with respect to the unstructured ones is that they require O(n log n) instead O(n^2) to be computed, while having a comparable accuracy with respect to the unstructured ones, as widely proven in literature.

The proposed bound give a constant smaller than one. However the constants of the previous bounds comparing the error of structured and unstructured methods are already small and universal and the proposed bound does not reduce the error rate w.r.t. the ambient dimension or the number of random features. So the contribution consists in a minor improvement on the knowledge of the topic.


-------
Reading the rebuttal didn't change my point of view on the paper. Again I remark that the paper provides a result that is of interest and I think it should be accepted. However the proposed result is more on the technical side and does not consist in a major improvement on the topic (e.g. compared to [Yu et al '16], which indeed received an oral presentation). ","The paper analyses the theoretical properties of a family random projections approaches to approximate inner products in high dimensional spaces. In particular the authors focus on methods based on random structured matrices, namely Gaussian orthogonal matrices and SD-matrices. The latter are indeed appealing since they require significantly less computations to perform the projections thanks to their underlying structure. The authors show that the methods considered perform comparably well (or better) with respect to the Johnson-Lindenstrauss transform (baseline based on unstructured Gaussian matrices). Moreover they show that further improvements can be achieved by extending SD-matrices to the complex domain. The authors extend their analysis to the case random feature based approximation of angular kernels. 

The paper is well written and clear to read, however the discussion of some of the results could be elaborated more. For instance after Thm 3.3, which characterizes the MSE of the orthogonal JL transform based on SD-matrices, it is not discussed in much detail how this compares to the standard JL baseline. Cor. 3.4 does not really help much since it simply states that Thm 3.3 yields to lower MSE, without clarifying the entity of such improvement. In particular it appears that the improvement in performance of the OJLT over JLT is only in terms of constants (w.r.t. the number of sub-sampled dimensions m). 

I found it a bit misleading in Sec. 4, to introduce a general family of kernels, namely the pointwise nonlinear Gaussian kernels, but then immediately focus on a specific instance of such class. The reader expects the following results to apply to the whole family but this is not the case. Reversing the order, and discussing PNG kernels only at the end of the section would probably help the discussion.

I found the term 'Unreasonable' in the title not explained in the text. Is it not reasonable to expect that adding structure to an estimator could make it more effective?

The Mean Squared Error (MSE) is never defined. Although being a standard concept, it is also critical to the theoretical analysis presented in the paper, so it should be defined nevertheless.

","This paper analyzes a few extensions of the random orthogonal embeddings proposed by Yu et al 2016. In particular, the authors focus on the Gaussian orthogonal matrices and SD-product matrices (which slightly generalize the HD-product matrices seen in the past). The authors are able to show several results regarding the unbiasedness and the MSE of the embeddings, for the linear kernel and the angular kernel (recall that the Yu et al paper performed analysis on the Gaussian kernel). One interesting result is that for SD-product matrices, the MSE for linear kernel exhibits osculation when the number of SD blocks alternate between odd and even. Such a result supports the empirical finding that taking 3 blocks works well.

There are quite a few technical developments in the paper (30 pages of appendix) and it is impossible to verify every piece given the short turnaround. I read some proofs and they are correct. I would suggest giving a concrete mathematical definition of the Gaussian orthogonal matrix in Section 2.
"
A KL-LUCB algorithm for Large-Scale Crowdsourcing,"Ervin Tanczos, Robert Nowak, Bob Mankoff",https://proceedings.neurips.cc/paper/2017/hash/c02f9de3c2f3040751818aacc7f60b74-Abstract.html,"The paper proposes a novel PAC algorithm for the multi-armed bandit problem that combines ideas from KL-LUCB and lil-UCB. For the proposed algorithm (called lil-KLUCB) the authors show a sample complexity upper bound that improves on the corresponding result of both its predecessors. The core of this result is a novel concentration bound which uses the Chernoff-information - similiarly as in case of KL-LUCB, but this new approach results in a much tighter bound. Finally, the paper presents some experimental results demonstrating that the theoretical improvement has practical implications as well.

The problem I have found with the paper is that the proof of Theorem 1 contains errors that make it completely confusing, and thereby the correctness of the results remains unclear. What is exactly A_k and t_j? Their current definition do not makes sense: why should t in the definition of A_k run over the whole set of natural numbers? And why should t_k be bigger than, but still proportional to 2^k? And which part of the proof corresponds to the first term on the RHS of the equation below line 135, and which part to the rest of the terms?
Additionally, the proof sketch of Theorem 1 is is rather messy too (unfortunately, I was not able to check the corresponding part in the supplementary material):
- In line 180, what is U(f_t(\epsilon))? It cannot be deduced from the definition of U(t,\epsilon).
- Still in line 180, the equation in the second half of the line does not seem right either: the first term depends on gamma, whereas the second term depends on \epsilon. What exactly is going on there?
- In line 181 you are referring to some result from Theorem 1 - however, you are proving Theorem 1. (Presumably this reference is to the theorem in the supplementary.)

Nevertheless, the results are nice and plausible, and imprive significantly on previous bounds (although they are still somewhat worse than existing lower bounds); in fact, the novel concentration bound could be of interest on its own. Therefore, I would recommend acceptance given the issues discussed above could be taken care of in a convincing fashion.

Further remarks and questions:
- There does not seem to be a definition for D(z,x).
- How do you calculate L_i(t,\epsilon) and U_i(t,\epsilon)?
- In line 126: it seems that 1-\delta should be 1-2\delta.
- Line 142: the left bracket is missing in the term ""D\mu,\mu)"".
- In the line before line 142: ""exp(\lambda S_t)"" should be ""exp(\lambda S_{t_j})"".
- In the line below 135: S_t is not yet defined.
- In lines 194 and 195 you are referring to Fig. 5, whereas you only have Fig. 1.
- How close do you think the bounds are to the actual optimum of the problem?","
The authors introduce a new algorithm for simple regret minimization with a sample complexity which combines the better dependency of KL-UCB on the arms’ value and the better dependency of lil-UCB on the number of arms. The authors provide an analysis of the sample complexity and experiments with real-world data. 


The contribution is clearly stated: The authors properly explain in Section 1 the advantages of KL-UCB and lil-UCB. 

In Section 2, they introduce the KL-LUCB algorithm which is simple and practical (total computational time is linear with the number of arms pulled). 

The insights on the advantage of KL-UCB (with a low number of arms) and KL-LUCB over lil-UCB are clear even though I have a question: How is the last bound on the Chernoff information before line 86 obtained? This could be more detailed. (maybe in the appendix?). 

Is it possible to easily compute kappa(N) ? It seems that by using kappa(N) and the value of the second sum in the definition of kappa(N) (Thm2) it is possible to compute kappa(N+1) in constant computational time. Is this the case? In any case, I think the authors should provide a way to get the value of kappa and c in the algorithm description so that the people could directly implement the algorithm without digging in the proof. 

Under line 54 definition of U_i : inf -> sup

Line 56: I think your bound is even stronger and applies with probability 1-epsilon simultaneously for all t. If this is the case, you should mention it (exchange “for all t” with “with prob 1-eps” and replace mu_i by mu_i,t). 

Line 88 of the appendix: “It is easy to check that… “ is indeed easy but kind of long computation involving computing a bound on kappa, …  Adding some steps there would make the proof easier to check. 

AFTER REBUTTAL

I have read the rebuttal and I am satisfied with the answers of the authors. After considering other reviews, I reduced the review score due to lack of clarity in the technical parts of the paper. Yet, I still think that the main results and the paper are strong and deserve attention. 




","This paper propose a new algorithm for the problem of best-arm identification with fixed confidence in multi-armed bandits.

The new proposed algorithm ""lil-KLUCB"" is a marriage of two recent algorithms: ""lil-UCB"" and ""KL-LUCB"".
According to the authors, the key improvement from these algorithms is:
- not being based on sub-Gaussian confidence bounds;
- to use Chernoff information for problem-dependent bounds instead of KL-divergence (or square gap);
- to tightly exploit the assumption of bounded rewards;
- to remove a logarithmic term in the number of arms.

The problem-dependent sample complexity bound for d-correct best arm identification is in
K.log(log(1/D)/d)/D for ""lil-KLUCB"" instead of K.log(K/(dD))/D for ""KL-LUCB"".
This is an improvement when the number of arms K is large.

To illustrate the algorithm's analysis, a few simulations are performed based on crowd-sourcing data against KL-LUCB and lil-UCB.

The scientific contribution (improving KL-type bounds on best-arm identification problem) is incremental but the theoretical work seem solid. The ""strong motivation"" toward ""large scale Crowdsourcing"" sounds a bit overplayed.

Remark:
The literature on best-arm identification is already quite rich. To name a few existing algorithms, we have: lil-ucb, kl-lucb, kl-racing, exponential-gap elimination, sequential halving, median elimination, successive elimination, lucb1, and prism.
On that ground it would make sense to add a few more competitors in the experiments.

AFTER REBUTTAL

Reading the rebuttal I was quite surprised to learn that the New-Yorker (through the ""NEXT"" API) chose such a fresh MAB algorithm to run their caption contest. It could be worth adding a reference to their white paper: http://nextml.org/assets/next.pdf
"
Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes,"Jianshu Chen, Chong Wang, Lin Xiao, Ji He, Lihong Li, Li Deng",https://proceedings.neurips.cc/paper/2017/hash/c0e90532fb42ac6de18e25e95db73047-Abstract.html,"This paper proposes new reinforcement learning algorithm which can be applied when both action spaces and state spaces are characterised by natural languages. The proposed model combines the LDA and q-learning approaches for this purpose and performs better than the previously suggested models. 

Overall, this paper is written well and easy to follow. While this paper might be not significantly novel in both topic modelling and reinforcement learning area, the seamless combination of two approaches is still interesting and novel. My biggest concern is the practical use of the proposed model. Although it shows a good performance at the given tasks, making a decision based on topics may not be an ideal idea. For example, what if the possible actions have the same topic distribution? let's say we have two actions: 'go to the coffee shop' and 'go to the restaurant'. The topic of two sentences would be very similar, but the consequence of a different action might be very different. This might be an inherent problem with the suggested method.

+ missing reference at line 76

","* Summary

This paper introduces the Q-LDA model to learn a sequential decision
process for text. The model builds on the LDA topic model to improve
the interpretability of the decisions. More specifically, the models
draws inspiration from supervised LDA and dynamic LDA. The authors
present an algorithm to train the model and apply the model to
text-based games.


* Evaluation

I really liked this paper. It is well written and well explained. I
find the problem to be interesting, and the model is interesting as
well. I went over the proof of Appendix A and B and I feel like this
gave me a good intuition about the proposed training method. The
experimental evaluation is a bit weak, but it is not flawed and is
interesting, and the model is original enough that it shouldn't be an
issue.

The proofs are conceptually simple, and the key idea to enable the
training is the typical Monte-Carlo integration with 1 sample. 

* Discussion

One thing I missed is why did you choose Q to be of this specific
form? It would be great to explain this choice more. I guess we
definitely want the theta from the action and the theta from the
observation to be involved. What about U? Did you just add such a
multiplication to have enough learning potential? Why isn't this going
through a non-linearity?

* Typos

106: anexploration
113: \alpha_1 or \alpha_t
172: the the a variant

Appendix B. You could break down step 29 to 30 a bit more. ","This paper targets on two text games and propose a new reinforcement learning framework Q-LDA to discover latent patterns in sequential decision process. The proposed model uses LDA to convert action space into a continuous representation and subsequently use Q-learning algorithm to iteratively make decision in a sequential manner.
      
Authors apply the proposed model to two different text games, and achieve better performance than previous proposed baseline models.
      
The paper is a little bit hard to follow with some missing or inconsistent information. The paper is not self-contained, for a reader that is not familiar with the problem domain, one may need to refer to the Appendix or prior works almost all the time.

Some detailed comments:
      
- I would suggest authors to include a detailed section highlighting the contribution of the paper.

- Authors provide some screenshots on the text game interface in the appendix material, but the information of the text games is still short. The reference [11] also doesn't provide much useful context neither. I would recommend authors to include some example text flow (at least in the Appendix) from these games to better illustrate the target scenario. What are and how many is the possible conversation flow of each text game? 
      
- In the context of the game, the agent only receives a reward at the end of game. This is consistent with the text in line 74-75. However, in the graphically model shown in Figure 1. It seems like there is a reward after each turn. I assume this graphical illustration is for general process, but it should be nice to include an explanation in the text.
      
- In the graphically illustration, it is unclear to me which variables are observable and which are not. For example, all \beta_A and \beta_S are not observable. The rewards r_t's are not observable in my understanding.
      
- In the generative process, it occurs to me that the observation text W are generated following LDA process as well, but in the graphically model illustration, there is a missing plate in the figure.
      
- I would encourage authors to include a section of model complexity analysis, for example, what is the complexity of parameter space. Given the size of the dataset and the complexity of the proposed model, it is hard to judge if the learned policy is generalizable.
      
- In the experiments, what are the vocabulary size of the dataset, in terms of observed text and action text? In Table 2 and Figure 2, authors demonstrate some interesting outcome and observation from the generated topics. I am wondering are these topics being used in other episode, since these topics look very fine-grained and hence may not be applicable to other scenario.
      
- I would also recommend to add in a list of all topics (maybe in Appendix), rather than the cherry-picked ones, to display.
      
- In the experiments, authors mention that the upper bound for reward is 20 for ""Saving John"" and 30 for ""Machine of Death"". Are these reward numbers objectively specified in the game flow or assigned in a post-hoc manner? If latter, how do you justify this measure (especially if the reward are assigned upon game termination).
      
- Missing reference in line 76. Missing space in line 106. {D}irichlet in line 291.

%%%%%%%%%%%%%%%%%%%%%%%%

The authors' response have clarified some of my questions and they also agree to improve the paper to make it more self contained. I have adjusted the score accordingly. "
Streaming Weak Submodularity: Interpreting Neural Networks on the Fly,"Ethan Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi",https://proceedings.neurips.cc/paper/2017/hash/c182f930a06317057d31c73bb2fedd4f-Abstract.html,"This paper proposes a new approach STREAK for maximizing weakly submodular functions. The idea is to collect several outputs of the Threshold Greedy algorithm, where the selection is based on a given threshold. The theoretical results of the Threshold Greedy algorithm and STREAK are verified sequentially. STREAK is also used to provide interpretable explanations for neural-networks and the empirical studies are given.

This is an interesting work. The streaming algorithm is novel and the analyses are elaborate. The problem constructed to prove the ineffectiveness of randomized streaming algorithms is ingenious. The experiments also show the superiority of STREAK. However, how to choose the proper \epsilon in STEAK? Figure 2(a) shows that by varying \epsilon, the algorithm can achieve a gradual tradeoff between speed and performance, but the result is trivial because with the increase of \epsilon, the time and space complexity will both increase and lead to better performance. 

I have checked all the proofs, and believe that most of them are correct. However, there are also some places which are not clear.

1. In the proof of Lemma A.1, the choice of appropriate arrival order is unclear and how to derive the inequality at the end of the proof?

2. In the proof of Lemma A.9, what is the meaning of the first paragraph? It seems to have nothing to do with the proof in the second paragraph.

3. In the proof of Theorem 5.6, why m=0 implies I=\ emptyset?","This paper studies the problem of maximizing functions that are approximately submodular in a streaming setting. The authors develop the STREAK algorithm, which achieves a constant factor approximation for weakly submodular functions with O(k log k) memory when the elements arrive in a random order in the stream. This result is complemented with an impossibility result for streams with adversarial ordering: no algorithm can obtain a constant factor approximation for 1/2-weakly submodular functions. Weakly submodular functions arise in nonlinear sparse regression applications where the function is not submodular. Experimental evaluations show how this algorithm can be used to interpret neural networks in a streaming setting, obtaining explanations of similar quality compared to previous work, but much faster.

The problem of extending known theoretical guarantees for submodular optimization problems to more general classes of functions is an important direction and this paper achieves a nice constant factor approximation for maximizing the well-motivated class of weakly submodular functions in a streaming setting, which has been heavily studied for submodular functions. This constant approximation is especially strengthened by the strong impossibility result in the worst case order. The application of this problem to interpreting neural networks is also very interesting and the paper is overall very well written.","This paper considers the streaming weakly submodular maximization problem and propose a novel algorithm, STREAK which obtains constant factor guarantees.

1.	The authors propose that no randomized streaming algorithm that uses o(N) memory to solve max_{|S| < 2k} f_k(S) has a constant approximation ratio for a worst-case stream order. Does this apply to offline algorithms? If you have o(N) memories, it is possible to store all elements first.

2.	 As for the bounds in theorem 5.1 and 5.5, I would suggest the author make some plots to show them. Also, when gamma goes to 1 (i.e. the function is submodular), the bounds are 0.18 and 0.016, which is relative low compared to the existing algorithms on submodular online max. Is it possible for STREAK to meeting the performance of THRESHOLD GREEDY on boundary cases (gamma = 1)?

3.	What is the theoretical running time complexity of STREAK?

4.	For most submodular function in the paper, It should be mentioned that f(\emptyset) \geq 0; otherwise, f(OPT) might be less than 0.
"
Decomposable Submodular Function Minimization: Discrete and Continuous,"Alina Ene, Huy Nguyen, László A. Végh",https://proceedings.neurips.cc/paper/2017/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html,"This paper studies the problem of minimizing a decomposable submodular function. Submodular minimization is a well studied and important problem in machine learning for which there exist algorithms to solve the problem exactly. However, the running time of these algorithms is a high polynomial and they are thus oftentimes not practical. To get around this issue, submodular functions that can be decomposed and written as a sum of submodular functions over a much smaller support (DSFM) are often considered as they often appear in practice. 

This paper improves the analysis of the fastest algorithms for DSFM by a factor equal to the number of functions in the decomposition. It also provides an experimental framework based on a distinction between “level 0” algorithms, which are subroutines for quadratic minimization, and “level 1” algorithms which minimize the function using level 0 as a black box. These allow a more meaningful comparison where the same level 0 algorithms are used to compare different algorithms. These experiments show a tradeoff between the discrete algorithms that require more calls to the level 0 subroutines and gradient methods with weaker requirements for level 0 but more computation for level 1.

The analysis is complex and relies on both discrete and continuous optimization techniques to meaningfully improve the running time of an important problem where the computational complexity is expensive. The experiments also highlight an interesting tradeoff which suggests that different algorithms should be used in different contexts for the running time of DSFM.

A weakness of the paper is that the writing is very dense and sometimes hard to follow. It would have been nice to have more discussion on the parameters kappa* and l* and the precise bounds in terms of these parameters. It would have also been nice to have some comparison on the theoretical bounds between RCDM, ACDM, and IBFS.","This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. The authors claim better theoretical time complexity bounds and experimental results on image segmentation. 

1.In line 73 of page 2, instead of saying “improves the worst-case complexity by a factor of r”, it is better to clarify what the current state-of-art result is, what is the paper’s contribution; otherwise, readers won’t know the significant of your results.

2. In line 140 of page, the bounds contain the maximum value of the function. But this only makes sense when the function is integer-valued. The author should be clearer on this. Also, are these algorithms weakly polynomial or strongly polynomial? 

3. In line 144, it is better to notify the reader that when C_i is small ~O(1), r has to be no smaller than ~O(n). Also, how do you compare your results with the state-of-art submodular minimization running time?

4. In Corollary 3.4, what is the definition of epsilon-approximate? 

5 When introducing the algorithms, it is better to use \algorithm environments than using plain text.
"
Learning Affinity via Spatial Propagation Networks,"Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong, Ming-Hsuan Yang, Jan Kautz",https://proceedings.neurips.cc/paper/2017/hash/c22abfa379f38b5b0411bc11fa9bf92f-Abstract.html,"The authors incorporate ideas from  image processing into CNNs and show how
nonlinear diffusion can be combined with deep learning. This allows to train more
accurate post-processing modules for semantic segmentation, that are shown to outperform
denseCRF-based post-processing, or recurrent alternatives that rely on more straightforward
interpretations of recursive signal filtering, as introduced in [3,16].

The main practical contribution lies in extending the techniques of [3,16]: when these
techniques apply recursive filtering, say in the horizontal direction of an image,
they pass information along  rows in isolation. Instead the method of the authors allows one to
propagate information across rows, by rephrasing the originally scalar recursion in
terms of vector-matrix products. This is shown to be much more effective than the baseline.


On the one hand this is an interesting, and technically non-trivial development -it was therefore
intriguing to read the paper. The results also seem  positive.

On the other hand the paper is written in a confusing manner.
Even though I am quite familiar with
recursive filtering and nonlinear diffusion I had to read the paper more than 4 times until I could
understand what the notation is supposed to mean.
Below is a sample of problems I have had with  notation when reading this the first time (by now I do understand - but it is the authors responsibility
to make the reading easy rather than have the reader guess):

Eq. 1: w_t is used but not defined - is it a scalar or a matrix; d_t is used and defined in terms of the (undefined) w_t;
Eq. 2: we get an expression for d_t(i,i) - but the range of i is not specified, and the values of d_t(i,j) i \neq j are not defined.
Eq. 3:  lambda_t appears (one more symbol), but is only defined after 5 lines.
l. 102: we only now learn the dimensionality of w_t

Theorem 1: even after reading 4 times I still do not know how this was useful (but I agree that it is correct).
Even if I could understand it, I think it would be better to first show why this is needed, before moving on to prove it.
And even better to put it in some appendix.

Theorem 2: not directly clear what you mean by the ""degree matrix composed of d_t""- it would be good to specify the dimensions of d_t and D, and how
D is formed by d_1, d_N (I do understand, but I need to imagine)

Results: even though there seem to be substantial improvements, I am not too sure about some things:

- Table 3: if one-way connection is the same (or almost the same) as [3], how can it be that in your case
you get worse results than the baseline, while they get better?

-Table 1: the numbers are quite different from those reported in [3,4]
-Table 2: It looks like even your baseline is better than [16] (due to a stronger network maybe?)
But the most interesting thing is the added value that your method brings to the problem, not the exact number itself.
It would be good if possible to run an experiment with the same starting point as [16]
","The authors propose a Spatial Propagation Network to learn the affinity matrix with applications to semantic segmentation on HELEN and PASCAL VOC 2012 benchmarks. The proposed method with three way connections improves strong baselines on both datasets. Furthermore, the authors have provided detailed experimental results and analysis of the proposed model. The reviewer finds this work very interesting (very impressive visualization results on PASCAL VOC), and has only a few concerns:

1. Two propagation units are employed for experiments. Will the performance further improve with more units?

2. Is it possible to elaborate more on how Theorem 3 is enforced in the implementation, especially for the tree way connection case?

3. Is it possible to visualize the learned propagation strength, p (maybe for each connection way)?

4. Thanks for providing the inference time in the supplementary material. It may be interesting to also list the inference time consumed for the other components so that the readers have a better understanding of whole model inference time.

5. The proposed one-way SPN fails to refine the segmentation results on PASCAL VOC 2012. Is it possible to analyse the failure modes visually? Maybe it is because of the line-style artifacts, which can not even be removed with multiple propagation units?

6. Typos:
  line 173: Figure. 1(b)
  line 184: Figure 1(a) and 1(b)
  line 186: Figure 1(b)
  line 268: previous
","The paper describes a method for learning pairwise affinities for recurrent label refinement in deep networks. A typical application is as follows: a feature map is produced by a convolutional network and is then refined by additional layers that in effect pass messages between pixels. The weights for such message passing are often set using hand-defined feature spaces (although prior work on learning such weights exists, see below). The submission describes a formulation for learning such weights.

The paper has a number of issues that lead me to recommend rejection:

1. The general problem tackled in this paper -- refining poorly localized segmentation boundaries -- has been tackled in many publications. Two representative approaches are: (a) add layers that model mean field inference in a dense CRF and train them jointly with the initial segmentation network (as in [1,13,30]); (b) add a convolutional refinement module, such as the context module in [Multi-Scale Context Aggregation by Dilated Convolutions, ICLR 2016], also trained jointly with the segmentation network. The submission should provide controlled experiments that compare the presented approach to this prior work, but it doesn't. An attempt is made in Table 1, but it is deeply flawed. As far as I can tell, the dense CRF is not trained end-to-end with the segmentation network, as commonly done in the literature, such as [1,13,30]. And the context module (the ICLR 2016 work referred to above) is not compared to at all, even though it was developed for this specific purpose and is known to yield good results. (In fact, the ICLR 2016 paper reports refinement results on the VOC 2012 dataset with the VGG network that are better than the SPN results in Table 1 (IoU of 73.9 in the ""Front + Large + RNN"" condition in Table 3 of the ICLR 2016 paper). And that's a comparatively old work by now.)

2. There is other related work in the literature that specifically addresses learning affinities for label refinement. This work is closely related but is not cited, discussed, or compared to:
   -Semantic Segmentation with Boundary Neural Fields. Gedas Bertasius, Jianbo Shi and Lorenzo Torresani. CVPR 2016
   - Convolutional Random Walk Networks for Semantic Image Segmentation. Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi. CVPR 2017
   - Learning Dense Convolutional Embeddings for Semantic Segmentation. Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos. ICLR Workshop 2016

3. The results on VOC 2012 are well below the current state of the art, which stands at 86% IoU (compared to 79.8% in the submission). One could argue that the authors are more interested in evaluating the contribution of their refinement approach when added to some baseline segmentation networks, but (a) such controlled evaluation was not done properly (see point (1) above); and (b) the authors' combined approach is quite elaborate, so it would be hard to claim that it is somehow much simpler than state-of-the-art networks that dominate the leaderboard. With this level of complexity, it is reasonable to ask for state-of-the-art performance.


Minor comment:

- Since the dense CRF seems to play an important role in the submission, as a baseline that is repeatedly compared against, the submission should cite the dense CRF paper: [Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials, NIPS 2011].
"
Gated Recurrent Convolution Neural Network for OCR,"Jianfeng Wang, Xiaolin Hu",https://proceedings.neurips.cc/paper/2017/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,"This paper introduces a modification to an existing network architecture, adding in gating to recurrent convolutional networks and applies this to scene text recognition. The method shows modest improvements over previous state of the art results.

This is a difficult problem and to achieve across the board improvements on all datasets is a very hard task. The architecture adds some small novelties and trained cleanly end-to-end, with nice ablation studies in Table 2 so I think this is an important method for this area, and these ideas could transfer to other image analysis tasks. I am going to raise my rating to a 7.","This paper introduces gates to the RCNN layer and shows positive improvements over vanilla RCNN architectures on a wide range of pre-detected text line recognition benchmarks. This paper doesn’t try to address the end-to-end problem of text extraction from natural images, where detection needs to be combined with recognition.

The authors did a very rigorous analysis on different challenging benchmarks compared to their baseline and show SOTA results in many of them.

But the paper has too many grammatical and orthographic errors for a NIPS paper. I encourage the authors to give a more thorough review. Also, the idea proposed by the paper which is to introduce a gating mechanism for recurrent output of the layer makes sense but is not sufficiently novel on its own. The combination of this and just decent improvement over previous SOTA makes it just a small positive contribution.

The author could potentially modify the scope of the paper to include the detection piece of the problem of extracting text from natural images. If they show more improvements on this end-to-end system, it would be more interesting as the community has not yet figured out very good systems on such end-to-end tasks [1]. Another approach is to show that this gating idea is more general, meaning that it can be used for non-OCR problems. I would encourage to try this on the ImageNet benchmark and potentially another problem (detection, depth estimation, etc.).

[1] Uber-Text: A Large-Scale Dataset for Optical Character Recognition from Street-Level Imagery","Authors added gating mechanism in Recurrent Convolutional NN that allow controlling the amount of context / recursion. In other words, it is similar to improvement from RNN to GRU but for Recurrent CNNs. Authors benchmark their model on text reading task. 

Positives aspects:
- authors improved upon the state of the art results on very well research benchmark
- the proposed model combines few proved ideas into a single model and made it work. 

Areas to improve:
- Lines 195-196: you state that choose 10 best models per architecture that perform well on each dataset and state the results as the median of them. It is ok when you compare different iterations of you model to make sure you found good hyperparameters. But this cannot be at the final table with the comparison to other works. You should clearly state what is your pool of models (is it coming from different hyperparameters? what parameters and what values did you choose). How many models did you try during this search? You should provide single hyper parameter + model setup with the results that works the best across all the problems in the final table. Additionally, you may provide variance of the results with the clear explanation of hyper parameter search you performed. 
- did authors try to use attention not bidirectional RNN + CTC loss? In few recent works (https://arxiv.org/pdf/1603.03101.pdf, https://arxiv.org/pdf/1704.03549.pdf, https://arxiv.org/pdf/1603.03915.pdf). This should still allow for improving the results, or if didn't work, this is worth stating. 

I vote for top 50% of papers as improving upon the previous state of the art results on such a benchmark is a difficult job. Authors proposed a new model for extracting image features that is potentially beneficial to other visual tasks. This is conditional on fixing the results number (to present numbers from a single model, not median) and open-sourcing the code and models to the community for reproducibility. 



"
Multi-view Matrix Factorization for Linear Dynamical System Estimation,"Mahdi Karami, Martha White, Dale Schuurmans, Csaba Szepesvari",https://proceedings.neurips.cc/paper/2017/hash/c2964caac096f26db222cb325aa267cb-Abstract.html,"The authors introduce a novel method to do MAP estimation of an linear dynamical system. This model is derived through a reformulation of maximum likelihood estimation which introduces two possible parametrizations of the LDS. Experimentally, this methods provides better parameter estimates and predictions than previous methods, and also runs faster on average.

Clarity: This paper is clear, but suffers somewhat from its condensed format. In particular, Algorithm 1 should ideally be provided with a larger font size

Quality: This paper is of good quality. 

Originality: This paper introduces a novel approach to estimating the parameters of a linear dynamical system using MLE; previous methods based on EM suffer from poor local minima.

Significance: This paper seems somewhat significant.

Detailed comments:
- Some mathematical results would be clearer with a line or two more that explain the computations, e.g. lines 106-107.

- Line 102: would there be cases where it might be valuable to consider another convex combination of the two log-likelihoods?

- Instead of using the L2,1 norm to induce sparsity, how would inducing a direct rank constraint by considering matrices of the form VV^T with V rectangular of rank k < n impact your approach?

- Lemma 1: could you please define Z and not only \tilde Z? It also seems that at the second line of the equation, you have inverted L_2 and L_1 compared to Eq. (4), is this correct? 

- What is the complexity of Algorithm 1?

Minor comments:
- line 100: ""that ignoreS""
- line 159: ""a nonconvex optimization problem""
- Please increase the font size for Algorithm 1 and for the legend/axis labels in Figure 2.
- Please fix the citations from arXiv so that they have the same format; in particular, lines 336 and 339.","This paper proposes an efficient maximum likelihood algorithm for parameter estimation in linear dynamical systems. The problem is reformulated as a two-view generative model with a shared latent factor, and approximated as a matrix factorization problem. The paper then proposes a novel proximal update. Experiments validate the effectiveness of the proposed method. 

The paper realizes that maximum likelihood style algorithms have some merit over classical moment-matching algorithms in LDS, and wants to solve the efficiency problem of existing maximum likelihood algorithms. Then the paper proposes a theoretical guaranteed proximal update to solve the optimization problem. 

However, I do not understand why the paper tell the story from a two-view aspect. In LDS, we can construct x_t from two ways. One is from \phi_t and the other is from \phi_{t-1}. Eq.4 is minimizing the reconstruction error for x_t constructed in both ways, with regularization on \Phi, C and A. This is a general framework, which is a reconstruction loss plus regularization, widely used in many classical machine learning algorithms. I do not see much novelty in the proposed learning objective Eq.4, and I cannot tell the merit of stating the old story from a two-view direction.

Lemma 1 proposes a method to transform the nonconvex problem into a convex one. However, I cannot see any benefit of transforming Eq.4 into a convex problem. The new learning objective Z in Lemma 1 is just a de-noised version of x_t. The original low-dimensional latent space estimation problem now becomes the problem directly estimating the de-noised high-rank observed state. Thus all merits from low-dimensional assumption are not available now. Since the proposed model is still non-convex, what is its merit compared to classical non-convex matrix factorization style algorithms in LDS?
 
","This paper derives a new system identification method that avoids the global optimum method of EM and is more efficient from being MLE based unlike N4SID. The approach is well motivated.

The two-view setup in eqn 2 seems to effectively double the observations, i.e. each data point is treated as two data points. In a Bayesian posterior update this would definitely make things go wacky. However, since this method only aims to approximate an MLE point estimate, it may be reasonable. However, what biases this introduces should be explored further.

What concerns me more is the replacement of the latent space likelihood P(phi_t|phi_t-1) with the Frobenious norm of the latent states \phi. It is unclear to me why these two penalties should behave alike. What effect this has should be explored.

It seems the objective in eqn 4 is more like method that find a joint MAP estimate on the parameters and latent states than method that find an MLE estimate from the marginal likelihood of the data P(X|theta) as EM attempts to do.

I would also like to see the regular MSE as a loss function in the table since the predictive mean is not necessarily the Bayes optimal prediction with the NMSE loss function.

It is somewhat surprising that EM does much poorer than N4SID in the experiments. How was EM initialized? What happens if one uses EM initialized to the N4SID solution? What about EM initialized to the LDS-MV solution?

Comments:
The comment on exponential families being well understood on L34 is somewhat vague. Understood in what sense?

The \sigma in L248 seems to be the eigenvalues, but this should be explicitly stated.

The fonts in Figure 2 are way too small

latex:
see align package and &="
Stochastic Submodular Maximization: The Case of Coverage Functions,"Mohammad Karimi, Mario Lucic, Hamed Hassani, Andreas Krause",https://proceedings.neurips.cc/paper/2017/hash/c2f32522a84d5e6357e6abac087f1b0b-Abstract.html,"The papers deals with the problem of submodular maximization; specifically, it proposes a stochastic optimization algorithm for maximizing a specific family of submodular functions, i.e. weighted coverage, under matroid constraints. The algorithm operates on the multilinear extension of the weighted coverage function. This way, the authors are sacrificing accuracy by optimizing a concave function which is a bounded approximation of the target function. However, they gain theoretically supported bounds on the convergence rate and the running time, which they also showcase in the experiments.

In general, the paper is well written, and the set of ideas that it uses are well put together.  The experimental section, although brief, drives the point that the authors want to make. From the application point of view, the influence maximization is quite strong and interesting, with potential impact to the practitioners as well. This is not the case for the facility location though, which feels very forced and lacks in presentation. In total, the techinical novelty of the paper is very limited, as it mostly brings together ideas from the literature.  Although it is an interesting paper to read, it does not have  neither a groundbreaking idea, a groundbreaking application nor a groundbreaking theory.

More detailed comments:
line 8: rewrite the sentence around ""formalization""
line 146: the notation B_v is a little strange. Maybe introduce V as {B_1, ... B_|V|} ?
line 190: \bar{x}^* = \argmax and not \in
line 199: F^* -> \barF^*
line 208: denote -> denotes
line 211: form -> from
line 241: how is the facility location problem related to the examplar-based clustering that is being used before and after this section? Why not use one name instead?
line 241+: This section comes out as over-complicated. It is hard to parse and it requires going back and forth many times. Could be either because of notation or because it lacks an intuitive explanation.
line 260: two two problem classes
line 264+ : The use of apostrophe as the thousands sign is a bit strange to get used to. Maybe leave a small space between the numbers instead?
line 267: Not sure what the last sentence means.
line 270: In the Table and the Figure, the term LAZYGREEDY is being used (together with a number in the Figure). What is the difference between GREEDY and LAZYGREEDY? What does the number represent?
caption of Table 1: There is an orphan footnote.","This paper considers the problem of maximizing a weighted coverage function subject to matroid constraints in the stochastic setting. For weighted coverage functions, the multilinear extension, which is typically optimized instead of the discrete objective, is known to admit a concave upper bound which is at most a factor of (1 - 1/e) away from it. The authors propose to optimize the concave upper bound via stochastic project gradient descent, thus obtaining a stochastic algorithm which returns a (1-1/e)-approximation to the original problem, after applying pipage rounding.

I couldn't identify any reasonable contribution of this work. By restricting the problem considered to the case of weighted coverage function, the solution presented is quite straightforward: For this particular class of functions, it is known that the multilinear extension can be approximated by a concave function which is at most a factor of (1 - 1/e) away from it. Thus stochastic projected gradient descent can be of course used to maximize the concave approximation, since now it's a concave maximization problem.

Besides this nice but straightforward observation, the only other novel result is Proposition 3 which bounds the number of samples needed by the greedy algorithm, when ran over the emipirical objective function subject to cardinality constraint. As noted in the proof, this proposition follows directly from Hoeffding's inequality and the union bound.

The paper is well written but does not present any substantial contribution to warrant a nips publication.
","The paper discusses the development of stochastic optimization for submodular coverage functions. The results and proofs are a mish-mash of existing results and techniques; nevertheless the results are novel and should spark interest for discussions and further research directions. 
The central contribution of the paper is providing with a framework to exploit the SGD machinery. To this effect, coverage functions can be harnessed, because the concave upperbound is obtained easily. While this seems restrictive, the authors present applications with good empirical performance vs classic greedy. 

I have a couple of points:- 
(a)	Is there to use projected SGD over Stochastic Frank-Wolfe [A] ? The latter will get rid of messy projection issues for general matroids, albeit the convergence might be slower. 
(b)	An interesting aspect of the paper is connection between discrete optimization and convex optimization. I would suggest including more related work that discuss these connections [B,C,D,E]. [B] also talks about approx. form of submodularity, [C,D] both follow the notion of “lifting” atomic optimization to continuous domain. 

Minor:
The paper is well-written overall. There are minor typos for which I suggest the authors spend some time cleaning up the paper. E.g. Line 22,24 “trough”, line 373 “mosst”. An equation reference is missing in the appendix.
Numbering of Proof Proposition 2 is messed up. 
The proof of Theorem 2 seems to be missing, I understand it follows from pipage rounding, but it might be a good idea to give a Lemma or something with reference from which approx. guarantee for rounding follows. In general, adding more information about pipage rounding in the appendix for completeness might be a good idea.
The main text is missing a page of references. 

[A] Linear Convergence of Stochastic Frank Wolfe Variants. Donald Goldfarb, Garud Iyengar, Chaoxu Zhou 
[B] Restricted Strong Convexity Implies Weak Submodularity. Ethan R. Elenberg , Rajiv Khanna , Alexandros G. Dimakis , and Sahand Negahban
[C] On Approximation Guarantees for Greedy Low Rank Optimization Rajiv Khanna , Ethan R. Elenberg1 , Alexandros G. Dimakis , and Sahand Negahban
[D] Lifted coordinate descent for learning with trace-norm regularization. Miro Dudik  Zaid Harchaoui , Jérôme Malick 
[E] Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains. Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, Andreas Krause"
Stochastic Approximation for Canonical Correlation Analysis,"Raman Arora, Teodor Vanislavov Marinov, Poorya Mianjy, Nati Srebro",https://proceedings.neurips.cc/paper/2017/hash/c30fb4dc55d801fc7473840b5b161dfa-Abstract.html,"The authors propose a stochastic approximation of CCA which sound reasonnalbe and technically correct.

They start with a description of CCA and regularized CCA and conclude with a least squares formulation of rCCA. 
They proposed a convex relaxation of rCCA easier to extend to the stochastic context. 
The correponding algorithm (MSG-CCA) is then derived and relies on the estimation of the gradient 
of the objective function. A simple formulation for the approximation of the gradient is proposed.

Then two bounds are given. The first one bounds the size of the expected noise in the estimate of the inexact gradient
The second one bounds the suboptimality gap in the CCA objective between the
true optimal rank-k subspace and the rank-k subspace returned by MSG-CCA after a number of iterations


Motivated by its efficiency for stochastic PCA and stochastic PLS, 
a formulation of the algorithm based on Matrix Exponentiated Gradient (MEG-SCCA)
is proposed. 


These two algorithm are compared in the Experiments section on 
the real dataset Mediamill (a two-view 
dataset consisting of 10000 videos and text annotations) and compared to 
competitive approaches in term of CCA objective, CPU time and number of iterations.

We mention that for MSG-CCA and MEG-CCA, the regularization parameters and the step 
size that has to be tuned are fixed with no justification.","I acknowledge having read the rebuttal. I thank for the clarifications regarding the experiment, but I did not find the answer particularly convincing -- I simply do not understand why you would use a medium-scale data set that is too small to show the difference, and the rebuttal did not really clarify this. Since you did not even hint at what kind of results the omitted artificial data experiments would provide, we cannot evaluate whether they would strengthen the paper or not.


	The paper proposes a stochastic approximation algorithm for CCA, and provide theoretical analysis of the algorithm in terms of \epsilon-suboptimality. It is empirically demonstrated to outperform earlier stochastic approximation algorithms that optimize the empirical estimate.

	This is overall a solid theoretical work. While we have seen a few stochastic algorithms for CCA in recent years, the previous work has completely ignored the generalization ability that is in focus here. The proposed solution and its theoretical analysis seem to be sound, and I can foresee other potential applications for the matrix stochastic gradient approach.

	The practical impact of the actual computational method is, however, somewhat limited. Figure 1 illustrates a CPU gain of roughly one order of magnitude over CCA-Lin, but given that the experiment is on a single medium-scale data set we cannot draw very strong conclusions -- there is no evidence that some practical problem would be solved by this method while being infeasible for the existing ones. The poor empirical section clearly degrades the value of the paper.

	Issues:
	1. Introduction is missing some references; even though this is a theoretical paper it would make sense to cite some works when motivating the multi-view setup -- combining ""tweets + friends network"" sounds like it refers to a particular work, yet it is not cited. Also, ""It can be shown that CCA is non-learnable"" would need a justification.
	2. The experiments are too hurried. You never explain what ""batch"" refers to (batch gradient descent for Eq.(4)?). More importantly, you do not comment in any way the fact that especially for k=4 it is actually clearly the fastest algorithm in run-time and competitive for other k as well. This partly defeats the purpose of the paper, but I wonder whether this is simply because of poor choice of data. Perhaps you could consider running a somewhat larger (say, n=10^6 or more) example instead to demonstrate the practical importance? Some comment on limiting to only very small values of k would also be crucial, since all examples used to motivate the work require values of k in the order of tens or hundreds. Without evidence on efficient computation for large-scale problems the Introduction that motivates the work in part by ""scaling to very large datasets"" is misleading, leaving online learning as the only advantage.
	3. Section 1.1 has Frobenius and nuclear norm in the wrong order.
      ","I read the rebuttal and thank the authors for the clarifications about differences between the proposed method and previous work in [3,4]. I agreed with Reviewer 2's concerns about the limits of the evaluation section. Giving experiments on only one medium scale data set, it is insufficient to show the potential computational advantages of the proposed method (scale easily to very large datasets) compared to the alternative methods.

Summary
The paper proposes two novel first-order stochastic optimization algorithms to solve the online CCA problem. Inspired from previous work, the proposed algorithms are instances of noisy matrix stochastic / exponentiated gradient (MSG / MEG):  at each iteration, for a new sample (x_t , y_t), the algorithms work by first updating the empirical estimates of the whitening transformations which define the inexact gradient partial_t, followed by a projection step over the constrain set of the problem to get a rank-k projection matrix (M_t). The theoretical analysis and experimental results show the superiority of the algorithms compared to state-of-the-art methods for CCA.

Qualitative Assessment

Significance - Justification:

The main contributions of the paper are (1) the derivation of a new noisy stochastic mirror descent method to solve a convex relaxation for the online version of CCA and (2) the provision of 
generalization bounds (or convergence guarantees) for the proposed algorithms. These contributions appear to be incremental advances considered that similar solutions have been proposed previously  for PCA [3] and PLS [4].

Detailed comments:

The paper on the whole is well written, the algorithms are explained clearly, the theorems for the algorithms are solid and sound, and the performance of the algorithms outperforms that of state-of-the-art methods for CCA.

Some questions/comments:

What differentiates your algorithm from that of the previous work in references [3, 4]? Except solving different problems (CCA vs PCA and PLS), is your algorithm fundamentally different in some ways (e.g. add whitening transformations to alleviate the gradient updates for CCA problems or provide convergence analysis)? 

The reviewer read references [3, 4] and noticed that the algorithms in your papers are similar to those proposed in [3, 4] in terms of similar convex relaxation and stochastic optimization algorithms. The reviewer would have expected the authors to provide a comparison/summary of the differences / similarities between previous work [3, 4, 21] and current paper in the related work. This would also highlight explicitly the main contributions of the paper.

There are a few typos:

Line 90: build -> built; study -> studied
Line 91: in the batch setting ->  in a batch setting
Line 96: pose -> posed;  consider -> considered
Line 101-102: rewrite this sentence “none of the papers above give generalization
bounds and focus is entirely on analyzing the quality of the solution to that of the empirical minimizer”. I guess you intend to express the following: “all the papers mentioned above focus entirely / mainly on analyzing the quality of the solution to that of the empirical minimizer, but provide no generalization bounds”
remove line 150"
Linear regression without correspondence,"Daniel J. Hsu, Kevin Shi, Xiaorui Sun",https://proceedings.neurips.cc/paper/2017/hash/c32d9bf27a3da7ec8163957080c8628e-Abstract.html,"The topic of linear regression with unknown correspondence is interesting and should be studied intensively. The main difficulty is the high complexity of the set of permutations when the number n of measurement vectors from R^d is large, and the design problem is to overcome this difficulty by efficient algorithms.

When the response is noiseless with \sigma =0, the exact recovery problem is pretty similar to that in compressive sensing, and nice statistical analysis is carried out for the algorithm reduced to the Subset Sum problem. 

The approximate recovery problem with \sigma >0 is much more involved, and only lower dimensional cases with d=1, 2 were considered in the literature. Here the authors present a polynomial time method by means of a Row Sampling algorithm and provide some approximation analysis. Lower bounds for the approximate recovery are also provided to illustrate that the signal-to-noise ratio needs to be large for efficient recoveries. This part is not fully developed and the used generalized Fano method is rather standard. 

In general, the topic is interesting and the obtained results are excellent. ","The article ""Linear regression without correspondence"" considers the problem of estimation in linear regression model in specific situation where the correspondence between the covariates and the responses is unknown. The authors propose the fully polynomial algorithms for the solution of least squares problem and also study the statistical lower bounds. 

The main emphasis of the article is on the construction of fully polynomial algorithms for least squares problem in noisy and noiseless case, while previously only the algorithms with exponential complexity were known for the cases with dimension d > 1. For the noisy case the authors propose the algorithm which gives a solution of least squares problem with any prespecified accuracy. For noiseless case another algorithm is proposed, which gives the exact solution of the least squares problem. Finally, the authors prove the upper bound for the range of signal to noise ratio values for which the consistent parameter recovery is impossible.

In general, the proposed algorithms, though being not practical, help to make an important step in understanding of computational limits in the linear regression without correspondence. The statistical analysis is limited to lower bound while for the upper bound the authors refer to the paper by Pananjady et al. (2016). What puzzles me a lot is that provided lower bound for parameters recovery is d / log log n, while in Pananjady et al. (2016) the lower bound for permutation recovery is proved to be n^c for some c > 0. Moreover, in another paper Pananjady et al. (2017) the constant upper bound on prediction risk is proved. While all these results consider the different quantities to be analyzed, it is seems that the fundamental statistical limits in this problem is far from being well understood.


To sum up, I think that the paper presents a strong effort in the interesting research direction and the results are sound. However, I believe that the main impact of the paper is on computational side, while some additional statistical analysis is highly desired for this problem. Also I believe that such type of paper, which includes involved theoretical analysis as well as algorithms which are more technical than intuitive, is much more suitable for full journal publication than for short conference paper.","summary: 
In this paper, the authors studied theoretical properties of linear regression problems without correspondence between the covariate and response variables. Since the least squares problem including permutation operations is computationally intractable, an approximation algorithm was proposed. The degree of the approximation was theoretically clarified. Furthermore, under the noiseless setting, the authors proposed a learning algorithm that achieves the exact recovery with high probability. Also, a lower bound of the estimation accuracy for noisy data was presented. 

review:
In this paper, the linear regression problems without correspondence are intensively studied. I checked all proofs except the supplementary appendix and I believe that proofs in the main part of the paper are mathematically sound. The main results are heavily depends on some previous works such as Theorems 1 and 4, and the algorithm 2 is fragile to the noise as the authors pointed out. However, I think that the contribution of this paper is still significant. Especially, the lower bound shown in Section 4 is interesting and it will be worthwhile for the theorist in NIPS community. This paper focus only on theoretical aspects of the present problem and totally lacks applications and numerical experiments. This may be a small flaw of this paper. 

other comments: 
- l.90:  I believe that w in ""satisfies \|Aw-b\|"" should be w'.
- Running time analysis was provided in l.117 and l.220 and it showed that the computation cost is of the order poly(n,d) or n^O(k) or so. This is still high and computationally depending in practice. Probably, there may be a gap between theoretical runtime and the practical computation cost. Some comments on it would be nice. 

"
Structured Generative Adversarial Networks,"Zhijie Deng, Hao Zhang, Xiaodan Liang, Luona Yang, Shizhen Xu, Jun Zhu, Eric P. Xing",https://proceedings.neurips.cc/paper/2017/hash/c3535febaff29fcb7c0d20cbe94391c7-Abstract.html,"Summary:

This paper proposes a novel GAN structure for semi-supervised learning, a setting in which there exist a small dataset with class labels along with a larger unlabeled dataset. The main idea of this paper is to disentangle the labels (y) from the hidden states (z) using two GAN problems that represent p(x,y) and p(x,z). The generator is shared between both GAN problems, but each problem is trained simultaneously using ALI[4]. There are two adversarial games defined for training the joints p(x, y) and p(x, z). Two ""collaborative games"" are also defined in order to better disentangle y from z and enforce structure on y.

Pros:
1) Overall, the paper is written well. All the steps are motivated intuitively. The presented ideas look rational.
2) The presented results on the semi-supervised learning task (Table 2) are very impressive. The proposed method outperforms previous GAN and VAE works. The qualitative results in terms of generative samples look very interesting.  

Cons:
1) I found the mutual predictability measure very ad-hoc. There exist other measures such as mutual information that have been used in past (e.g. infoGAN) for measuring disentanglability of DGMs.

2) The collaborative games presented in Eq.4 and Eq.5 is equivalent to the mutual information term introduced in infoGAN[2] (with \lambda=1). 
In this case, the paper can be considered as the combination of infoGAN and ALI applied to semi-supervised learning. This slightly limits the technical novelty of the paper.

Questions:
1) I am wondering why the adversarial game L_{xy} is only formed on the labeled set X_l. A similar game could be created on the unlabeled set. The only difference would be that instead of labels, one would use y=C(x) for the first term in Eq.3. However in this paper, instead, a collaborative game R_y is introduced.

2) Theorem 3.3 does not state whether minimizing R_z or R_y w.r.t G will change the equilibrium of the adversarial game or not. In other words, with minimization over G in both Eq.4 and Eq.5, can we still guarantee that G will match the data distribution. 

Minor comments:
1) line 110: Goodfellow et al is missing the reference.
2) line 197: Is l-2 distance resulted from assuming a normal prior on z?
3) line 213: p_g(z|y)  --> p_g(x|y)
4) With all the different components (p, p_g, p_c, etc) it is a bit challenging to follow the structure of adversarial and collaborative games. A figure would help readers to follow the logic of the paper easier.","This paper proposes the SGAN model which can learn an inference network for GAN architectures. There are two sets of latent variables, y for the label information and z for all other variations in the image (style). The generator p_g conditions on y and z and generate an image x. The adversarial cost L_xz, uses an ALI-like framework to infer z, and the adversarial cost L_xy uses another discriminator to train a standard conditional GAN on the few supervised labeled data by concatenating the labels to both the input of the generator and the input of the discriminator. The SGAN network also uses R_y and R_z to auto-encode both y and z latent variables. R_y has an additional term that minimizes the cross-entropy cost of the inference network of y on the labeled data.

To me, the core idea of this algorithm is very similar to the line of works such as Info-GAN [1] which learns a conditional GAN by auto-encoding the latent variable using an additional decoder network. The cost function R_z and R_y of SGAN are auto-encoding the latent variables in a similar fashion.
Unlike what the paper mentions ""However, the semantic of each disentangled dimension [in info-gan] is uncontrollable because it is discovered after training rather than designated by user modeling."", it is straightforward to incorporate label information in Info-GAN by additionally training the decoder to minimize the reconstruction over the supervised label pairs of (x,y). This is actually what the Improved-GAN paper [2] does and is similar to the first term of R_y in the SGAN model in equation 4.
However, both of the Info-Gan and Improved-GAN models do not bother to learn the style variable z. The SGAN model, however, infers it by using an ALI-like technique to match p_i(x,z) to p_g(x,z) and thus making sure both the marginals match.

In short, this paper does a good job in putting the architectures of the recent GAN works such as Info-GAN, Improved-GAN and ALI together to infer both the label and style, and use the resulting model in semi-supervised learning and style transfer. However, I didn't find the ideas of the paper significantly original.

[1] Chen, Xi, et al. ""Infogan: Interpretable representation learning by information maximizing generative adversarial nets.""
[2] Salimans, Tim, et al. ""Improved techniques for training gans.""","The paper presents a new flavor of generative adversarial nets that is more robust to perform conditional generation when limited labeled data is available. The paper is well written and I could follow the explanation of the method and experiments without issues. The experimental results are quite interesting both in terms of semi-supervised accuracy for image classification and in terms of “disentanglability” of y and z.

Some questions/comments:

-	The mutual predictability (MP) metric proposed by the authors seems sound, although I don’t think that the comparison (in Fig. 1) with VAE trained without labeled information is fair. I think it would be better to compare with a semi-supervised VAE.

-	Are the images presented in Fig. 3 (b) and (c) randomly selected? Visually is hard to tell if SGAN is better than TripleGAN. Although the authors have shown that SGAN produces better inception score.

-	The authors should have included more details about the neural network architectures used. It could have been included as supplemental material. They mentioned that they “mostly follow those used in baseline works”, but which baseline work are they referring to? TripleGAN? Improved GAN?

-	SGAN seems to be quite expensive to train when compared to Improved GAN or even TripleGAN. The authors should have mentioned some statistics about the training time / convergence speed of the proposed method. Moreover, the authors should be more specific about the training process. For instance, how exactly are they increasing the proportion of synthetic samples throughout the training? 
"
Dynamic-Depth Context Tree Weighting,"Joao V. Messias, Shimon Whiteson",t,"The paper develops a variation on Context Tree Weighting (CTW) which keeps memory costs low by adapting the depth of each branch to the extent that it aids prediction accuracy. The new algorithm, called Utile Context Tree Weighting (UCTW), is shown empirically in some illustrative examples to use less memory than fixed-depth CTW (since it can keep some branches short) and to be more effective under a memory bound (in which it must prune a node every time it expands a node). 

---Quality---

As far as I can tell the technical claims and formalization of the algorithm are sensible. The experiments are, for the most part well designed to answer the questions being asked. 

One experiment that felt less well-posed was the T-Maze. The text says ""We consider a maze of length 4. Thus we set K = 3."" What does that ""thus"" mean? Is the implication that K = 3 should be deep enough to represent the environment? Later it says that ""during the initial stages of learning the agent may need more than 3 steps to reach the goal."" I assume that means that the agent might move up and down the ""stem"" of the T for a while, before reaching the goal, thus forgetting the initial observation if the suffix is limited to depth 3. If that's the case, then K = 3 is only sufficient to make predictions under the *optimal* policy, so it's no surprise that CTW+UCT can't perform well (UCT does random rollouts!). In fact, under those dynamics no finite suffix is enough to represent the environment (for arbitrary action sequences), so even the depth 4 model that UCTW learns is incorrect -- it just happens to be deep enough to be sufficiently robust to suboptimal behavior to allow the planner to work. I guess I'm just not entirely sure what to conclude from these results. We see that CTW does poorly when given inadequate depth (no surprise) and that UCTW adapts its depth, so that's fine. But UCTW doesn't learn a good model either, and it's basically a coincidence of the domain that it happens to work out for planning purposes. The other experiments, which are more focused on performance under memory bounds, make a lot more sense to me.

---Clarity---

I think the paper was pretty clearly written. The theoretical framework of CTW is always a challenge to present, and I think the authors have done pretty well. The main idea of the algorithm is described well at an intuitive level as well as at a formal level.

I will say this: the name of the algorithm is confusing. The ""utile"" in utile suffix memory refers to the fact that the tree is expanded based on *utility* (i.e. value). The main point of that work was that the tree should only be as complicated as it needs to be in order to solve the control task. Here the tree is being split based on prediction error of the next observation, not utility, so it is strange to call it Utile CTW. I saw the footnote acknowledging and clarifying this mismatch...but the fact that you had to write that footnote is a pretty good sign that the name is confusing! How about ""Incremental Expansion CTW"", ""Dynamic Depth CTW"", or ""Memory Bounded CTW""? UCTW is just not descriptive of what the algorithm does.... 

---Originality---

Clearly the work is directly built upon existing results. However, I would say that it combines the ideas in a novel way. It re-purposes an alternative formulation of CTW in a clever way and develops the necessary updates to repair the tree after an expansion or pruning. 

---Significance---

I think UCTW is interesting and may have a significant practical impact. CTW is an important algorithm in the compression literature and gaining interest in the AI/ML literature. I agree with the authors that memory is a major bottleneck when applying CTW to interesting problems, so a memory-bounded version is definitely of interest. Empirically UCTW shows promise -- though the experiments were performed on basic benchmarks they do demonstrate the UCTW uses less memory than fixed-depth CTW and can cope  with a memory bound.

UCTW is a little bit of a strange beast, though. One of the appeals of CTW is that it has this very clear Bayesian interpretation of representing a distribution over all prunings of a tree. It's not at all clear what happens to that interpretation under UCTW. UCTW is *explicitly* expanding and pruning the tree using a statistical test rather than the posterior beliefs. The claim that UCTW will eventually do as well as fixed-depth CTW makes sense, and is comforting -- it eventually finds its way to the original Bayesian formulation and can overcome any funkiness in the initialization due to the process up until that point. Furthermore it's not clear what happens to the regret bounds that CTW enjoys once this expansion/pruning scheme is introduced. This is not really an objection -- sometimes some philosophical/mathematical purity must be sacrificed for the sake of practicality. But it does make things muddier and it is harder to interpret the relationship of this algorithm to other CTW variants.

Similarly, once we discard the clean interpretation of CTW, it does raise the question for me of why use CTW at all at this point? The authors raise the comparison to USM, but don't really compellingly answer the question ""Why not just use USM?"" The point is made that USM uses the K-L test, which is expensive, and doesn't have a memory bound. However, the main ideas used here (use likelihood ratio test instead and require a trade-off between expanding and pruning once the limit is hit) seem like they could just as easily be used in USM. I do no intend to suggest that the authors must invent a memory-bounded version of USM to compare to. However, if the authors do have a clear idea of why that's not a good idea, I think it would be valuable to discuss it. Otherwise I feel like the motivation of the work is a little bit incomplete. 

***After Author Response***

I do think the name of the algorithm is misleading, and that leads to confusing comparisons too. For instance, in the author response the authors say ""Also, the expansion tests in USM are performed over nonparametric representations of distributions over future reward, so the complexity of each test is a function of the sample size for each distribution."" But, and I cannot stress this enough, *that is because USM is trying to predict value and UCTW is not.* They use different expansion tests because they address fundamentally different prediction problems. If one were to use a USM-like algorithm for predicting the next symbol from a finite alphabet, it would make perfect sense to represent the distribution using a histogram and use likelihood ratio tests instead of K-S; the complexity would be linear in the size of the alphabet, not the number of examples. USM uses K-S *because it is predicting a continuous value*. (In this case, I do nevertheless acknowledge that the CTW calculations have a nice side effect of making likelihood calculations efficient and thank the authors for that clarification).

I think this paper should be accepted, so, if that happens, obviously it will be up to the authors what they do with the title and the algorithm name and so on. My point is that the direct link between USM and UCTW is not sound -- USM and UCTW are solving different problems. Pretty much the *only* thing UCTW takes from USM is the fact that it incrementally grows its depth. So it's fine to draw this connection between two algorithms that incrementally expand a suffix tree, and its good to acknowledge inspiration, but they can't be directly compared. At best you can compare UCTW to a USM-like algorithm that predicts future symbols rather than utility, but then, because it has a different prediction problem, the design choices of USM might not make sense anymore. I think the name UCTW reinforces this flawed direct comparison because at first glance it implies that UCTW is solving the same problem as USM, and it is not. None of this is fatal; a motivated reader can untangle all of this. I just hope the authors will get really clear about the distinctions between these algorithms and then make sure the paper is as clear as it can possibly be.

I can see where the authors are coming from with the T-maze. I still think it's a bit of a wonky experiment, but adding a bit of the analysis given in the response to the paper would help a reader understand what the authors mean to extract from the results."
"Fast, Sample-Efficient Algorithms for Structured Phase Retrieval","Gauri Jagatap, Chinmay Hegde",https://proceedings.neurips.cc/paper/2017/hash/c3a690be93aa602ee2dc0ccab5b7b67e-Abstract.html,"The authors' rebuttal contains some points which should be explicitly included in a revised version if the paper is accepted.

-----------

The authors study the problem of compressive (or sparse) phase retrieval, in
which a sparse signal x\in\R^n is to be recovered from measurements abs(Ax),
where A is a design (measurement) matrix, and abs() takes the entry-wise
absolute value. The authors propose an iterative algorithm that applies CoSaMP
in each iteration, and prove convergence to the correct, unknown sparse vector x
with high probability, assuming that the number of measurements is at least
C*s*log(n/s), where s is the sparsity of x and n is the ambient dimension. 

The paper is well written and clear, and the result is novel as far as I can
tell. However, the problem under study is not adequately motivated. The authors
only consider real-valued signals throughout the paper, yet bring motivation
from real phase retrieval problems in science in which the signals are, without
exception, complex. The authors aim to continue the work in refs. [21,22] which
also considered only real-valued signals. Looking at those papers, I could find
no satisfying motivation to study the problem as formulated. 

The algorithm proposed is compared with those proposed in refs. [21,22]. 
According to Table 1, it enjoys the same sample complexity requirement and the
same running time (up to constants) as SPARTA [22] and TWF [21]. (While the
sample complexity requirement is proved in the paper - Theorem 4.2 - I could not
find a proof of the running time claimed in Table 1). As far as I could see, the only
improvements of the proposed algorithm over prior art is the empirical
performance (Section 6). 

In addition, the authors study a closely related problem, where the signal is
assumed to be block sparse, and develop a version of the algorithm adapted to
this problem.

As for the sparse (non-block) version, in my opinion the limited simulation
study offered by the authors is not sufficient to establish that the proposed
algorithm improves in any way over those of [21,22]. Without a careful empirical
study, or alternatively a formal results showing that the proposed algorithm
improves over state of the art, the merit of the proposed algorithm is not
sufficiently established. 

As for the block version, it seems that here the algorithm, which is
specifically tailored to the block version, improves over state of the art.
However, in my opinion the problem of recovery of *real* block-sparse signals
from absolute value of linear measurements is not sufficiently motivated. 


Other comments: 
The authors assume a Gaussian measurement matrix. The main results (Theorem 4.1,
Theorem 4.2, Theorem 5.2) must explicitly specify this assumption. As written
now, the reader may incorrectly infer that the Theorem makes no assumptions on
the measurement matrix.","This paper considers the phase retrieval problem under Gaussian random sampling with additional structural assumptions (e.g. sparsity and block sparsity of signals of interest).  The authors propose a parameter-free nonconvex algorithm (by combining alternating minimization and CoSAMP), which allows to achieve appealing sample complexity and runtime all at once.  

Perhaps the most interesting message of this paper is the sample complexity for block-sparse signals.  In particular,  the sample complexity is O(s^2 / b) with b denoting block size and s the total sparsity.  This seems to imply that the performance of sparse phase retrieval improves as block size increases (so that there is less uncertainty in support location). This is in stark contrast to sparse signal recovery case as changing the block size only affects the sample complexity by no more than a logarithmic factor.     

The paper is interesting and well-written.  Below are some additional comments. 

1.  The thresholded WF has runtime O(n^2 log n), while CoPRAM has runtime O(s^2 n log n).  So CoPRAM does not necessarily outperform threshold-WF, especially when s > sqrt{n}, right?  This requires more discussion.  

2.  In Section 1.3,  the authors mentioned that this work outperforms [22] as it allows to consider signals with power-law decay.  If we use convex relaxation approaches (L1 + trace norm), the sample complexity for recovering power-law signals can be much better than O(s^2 log n);  see [54] and [19].  Will CoPRAM also exhibit improved performance when restricted to signals with power-law decays?  

3.  Given that Section 1.3 entails detailed discussions of / comparisons with prior works, the authors may consider merging Section 1.3 and Section 2. 

4.  Section 4:  the authors might need to be a bit careful when saying the problem ""has multiple local minima"".  Given that the authors only consider i.i.d. Gaussian measurements and the sample size are quite large (much larger than information-theoretic limits),  it's possible (although I am not sure) that we will have benign geometric structures with no spurious local critical points.  

5. After the authors describe the second stage of their algorithm,  it would be helpful to compare the computational bottleneck with threshold WF (in particular,  why Alt-min + CoSAMP achieves better computational complexity than WF when s = o(\sqrt{n})).  This would be helpful messages for the readers. 


Typos:
--- abstract:  arises from to structured sparsity models --> arises from structured sparsity models
--- Page 1:  ... of the light waves but not its phase -->  ... of the light waves but not their phase
--- Page 4:  there has been no rigorous results --> there has been no rigorous result"
Hierarchical Methods of Moments,"Matteo Ruffini, Guillaume Rabusseau, Borja Balle",https://proceedings.neurips.cc/paper/2017/hash/c44e503833b64e9f27197a484f4257c0-Abstract.html,"This paper studies the following problem: Can we make learning algorithms based on tensor decomposition robust to model misspecification? The authors propose to replace the tensor decomposition step by approximate simultaneous diagonalization. 

The problem studied here is clearly important. This paper does not really solve the problem, but makes reasonable progress, in particular at the experimental level for certain settings. I believe this is a reasonable paper for NIPS.
","Spectral methods for learning latent variable models reduce the learning problem to the problem of performing some low-rank factorization over matrices collecting some moments of the target distribution. There are many variants of this in the literature, most approaches use SVD or other forms of tensor decompositions. This paper highlights an important limitation of many of these approaches. The main limitation is that most theoretical guarantees apply when the goal is to recover a model of k states assuming that the mixture distribution from which the moment matrix is computed also has k components. However, in many cases one is interested in learning models for l lower than k, what the authors call the misspecified setting.

This paper proposes a method that performs joint diagonalization by combining a
truncated SVD step with an optimisation. The theoretical analysis shows that this
algorithm can work in the misspecified setting. The paper presents an application
of the proposed algorithm to topic modelling.

I believe this is a nice contribution, it is true that there are already many
variants of the spectral method, each with its own claimed advantages. This been
said, the one proposed in this paper is technically sound and addresses a true
limitation of previous work.

In terms of references the paper is quite complete, there are however a couple of
relevant missing references: 

- B.Balle et al. ""Local Loss Optimisation in operator models: A new insight into spectral learning""; since it is one of the first papers to frame spectral learning directly as an optimization. 

- More closely related
to this submission, B. Balle et al ""A Spectral Learning of Finite State
Transducers"" is to the best of my knowledge the first paper that proposes a
spectral method based on joint diagonalization."
Near Optimal Sketching of Low-Rank Tensor Regression,"Xingguo Li, Jarvis Haupt, David Woodruff",https://proceedings.neurips.cc/paper/2017/hash/c4de8ced6214345614d33fb0b16a8acd-Abstract.html,"This paper studies the problem of sketching the tensor version of the least-squares regression problem.
Assuming that the problem is defined by a low-rank tensor it gives a sketching algorithm that reduces the dimension of the problem to rank * (sum of the dimension of factors) instead of the naive bound of the product of dimensions of factors (up to polylog-factors and for constant \epsilon) which roughly matches the number of parameters of the problem.
On a technical level this paper introduces new ideas that go far beyond simple generalizations of the work of [3] for OLS regression.
Overall, I think this paper is strong enough to be accepted to NIPS and should be of interest to the audience. However, there are some questions I have for the authors (see detailed comments below).


Strengths:
-- Introduction to the paper is fairly well-written and does a good job introducing technical ideas.
-- Interesting problem and a convincing set of theoretical results. The results are clearly stated and the writing is formal throughout the paper.

Weaknesses:
-- Some of the assumptions made in the main theorem (Theorem 1) were not particularly convincing to me. Also these assumptions come in after the introduction that explicitly stresses that you do not assume anything about the tensor (page 2, line 66). How so? In particular, you justify your first assumption that R <= \sum p_d / 2 by the fact that you are interested in the case R <= p_d. This is the first time the latter assumption seems to appear in the paper and I wasn't sure why is it reasonable. Similar question arises for the second assumption on the leverage scores that you are using.
-- When it comes to the technical sections the writing gets patchy. Sometimes the notation gets too heavy and I couldn't follow some of the notation in places (see specific examples below).
-- I couldn't quite follow experimental evaluation, starting from the description of the setup. E.g. how was parameter \eps set? Given abundance of log-factors and 1/eps^2 dependence in the sketch dimension I am very surprised to see this approach turns out to be practical. How is this explained?

Typos and notation issues:
-- line 5: \sum p_D -> \sum p_d
-- line 196: in more in -> in more detail in
-- line 196: from -> for
-- line 198: A^{v_1} -- what does this notation mean
-- line 204: \mathcal N -- I assume this is a covering number of some kind (probably L2?), but couldn't find it being defined anywhere","*Summary*

This paper studies the tensor L2 regression problem and proposed to use matrix sketching to reduce computation. 1+epsilon bound is established to guarantee the error incurred by sketching.

I don't like the writing. First, there is little discussion of and comparison with related work. Second, the proof in the main body of the paper is messy, and it takes much space which could have been used for discussion and experiments. Third, the use of notation is confusing. 

Some claims in the paper seems false or suspicious. I'd like to see the authors' response. I'll adjust my rating accordingly. 


*Details*

I'd like to see the authors' response to Questions 2, 4, 9, 10.

1. I don't find the description of your algorithm anywhere. I don't find the time complexity analysis anywhere. I want to see a comparison of the computational complexities w/ and w/o sketching.

2. How is this paper related to and different from previous work, e.g., [32], Drineas's work on sketched OLS, and tensor sketch? For example, if A is matrix, which is a special case of tensor, then how does your bounds compare to Drineas's and Clarkson&Woodruff's?

3. The notation is not well described. I look back and forth for the definition of D, p, r, R, m, s, etc.

4. In Line 67, it is claimed that no assumption on incoherence is made. However, in Line 169 and 231, there are requirements on the max leverage scores. How do you explain this?

5. There are incompleteness and errors in the citations.
* There are numerous works on sketched OLS. These works should be cited telow Eqn (6).
* In Definition 5: SJLT is well known as count sketch. Count sketch has been studied in numerous papers, e.g,

Charikar et al: Finding frequent items in data streams. 
Clarkson & Woodruff: Low rank approximation and regression in input sparsity time 
Meng and Mahoney: Low-distortion subspace embeddings in input- sparsity time and applications to robust linear regression 
Nelson & Nguyen: Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. 
Pham and Pagh. Fast and scalable polynomial kernels via explicit feature maps 
Thorup and Zhang. Tabulation-based 5-independent hashing with appli- cations to linear probing and second moment estimation

I don't think [11] is relevant.

6. How does this work extend to regularized regression?

7. Line 182: How is the assumption mild? The denominator is actually big. Matrix completion is not a good reference; it's a different problem.

8. The synthetic data in the experiments are not interesting. Sampling from normal distribution ensures incoherence. You'd better generate data from t-distribution according to the paper
* Ma et al: A statistical perspective on algorithmic leveraging.

I'd like to see real data  experiments to demonstrate the usefulness of this work.

9. The title claims ""near optimal"". Why is this true? Where's your lower bound?

10. Tensors are rarely low-rank; but sometimes they are approximately low-rank. How does your theory and algorithm apply in this case?


=== after feedback ===
I appreciate the authors' patient reply. I think my evaluation of the technical quality is fair. I won't change my rating.



","This paper discusses how sparse JL transform can be constructed and used for tensor regression, where the tensors are vectorized. 
For notation simplicity,  I will assume the tensors are D-way and each mode is of size p, so the vectorized tensors are of dimension {Dp}. My understanding of the author's argument is that using sparse JL, the problem size is reduced where there are only O(R * Dp polylog_factor ) equations in the approximated linear inverse problem, and this matches the problem's intrinsic degrees of freedom. 

I'm not familiar enough with the sketching literature, to help me understand the paper more in limited time, I'd like to ask a few questions:

1)  I think the structure of low rank tensor is considered when proving the bound, but not utilized in the construction of linear system. The objective is squared loss only. I'm wondering if one can sketch a system of smaller size, by imposing some regularizers or constraints for the structure? For example, say D is an even number, using the squared norm [1], we only need n = O(R * p^{D/2}) linear measurement of the tensor. If we sketch this system, by JL theorem we will have m = O(D log (Rp) ) -- which is much smaller.  Is this doable? 

2) Related to 1), I would appreciate the author discussed the relationships among m (sketched size), n(#measurement) and tensor parameters (R, p, D) more clearly in the paper. Because the JL transform is a relationship between m & n, and the relationship between n & (R, p, D) is more or less the main barrier for tensor recovery problem. It seems these two relationships are mixed in this paper, the authors connects m and (R, p, D) directly.

3) I'm interested in the experimental results for Fig 1(b). I think the Y-axis plots MSE, X-axis plots iteration number. The number of measurement is of order n3 > n2 > n1, but the convergence speed you get is n1 (fastest) > n3 > n2. Could you please explain why the error decreases slower with more data?

Thanks!

[1] Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery"
Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models,Sergey Ioffe,https://proceedings.neurips.cc/paper/2017/hash/c54e7837e0cd0ced286cb5995327d1ab-Abstract.html,"In this paper, the authors propose Batch Renormalization technique to alleviate the problem of batchnorm when dealing with small or non-i.i.d minibatches. To reduce the dependence of large minibatch size is very important in many applications especially when training large neural network models with limited GPU memory. The proposed method is vey simple to understand and implement. And experiments show that Batch Renormalization performs well with non-i.i.d minibatches, and improves the results of small minibatches compared with batchnorm.


Firstly, the authors give a clear review of batchnorm, and conclude that the key drawbacks of batchnorm are the inconsistency of mean and variance used in training and inference and the instability when dealing with small minibatches. Using moving averages to perform normalization would be the first thought, however this would lead to the model blowing up. So the authors propose a simple batch renormalization method to combine minibatch mean and variance with moving averages. In my opinion, what Batch Renormalization does is to gradually changing from origin batchnorm (normalizing with minibatch mean and variance) to batchnorm with (almost) only moving averages. In this way, the model can adopt part of the advantage of moving averages, and converge successfully.

And I have several questions about this paper. 
(1)	When using large minibatch size (such as 32), why Batch Renormalization has no advantage compared with batchnorm. It seems that the consistency of mean and variance in training and inference does not help much in this case.
(2)	Experiments show that the result of small minibatch size (batchsize=4) is worse that the result of large minibatch size (batchsize=32). So I wonder if using two (multiple) moving averages (mean and variance) with different update rates (such as one with 0.1, one with 0.01) would help. Small update rate helps to solve the inconstancy problem, large update rate helps to solve the small minibatch size problem.
(3)	The results of how r_max and d_max will affect the performace are not provided. There seems to have plenty of parameter tuning work.

This work is good, and I am looking forward to seeing a more elegant solution to this problem.
","This is an interesting paper, nice and neat. Batch normalization has proven to be an effective technique for dealing with internal covariate shift and has been widely used these days in the training of neural networks with a deep structure. It is known that BN has issues with small-size mini-batches. First of all, it gives rise to unreliable estimates of mean and variance of each mini-batch. Second, the mean and variance of the whole population, which is used in classification or inference, is computed by a moving average of mini-batches during training. This is a mismatch.  This paper proposes a simple way to cope with the two issues in the conventional BN. It introduces another affine transform to correct the bias between the local and global normalization and this so-called renormalization makes the normalization in training and inference matched.  The idea is simple but it seems to be fairly effective from the reported experimental results.  

I have no real criticism to lay out regarding the paper.  However, I think the experiments can be more convincing if more results can be reported on mini-batches with a variety of sizes to give a full picture on the behavior of this batch renormalization technique.  I am particularly curious about the case where the size of a mini-batch is down to one. In this case, the conventional batch normalization doesn't work any more but this renormalization can still be applied. ","This work is an important contribution to improving SGD training of the Neural Networks.

One remark I would like to make:
Renormalizing moving-average affine transformation A(r,d) and the output affine transformation A(beta,gamma) form a composition which is also an affine transformation (see Algorithm 1, page 4). Therefore, one can ""hide"" (r,d) transformation inside the redefined (beta, gamma). Thus, the renormalization becomes a modification of the (beta,gamma) training algorithm."
Position-based Multiple-play Bandit Problem with Unknown Position Bias,"Junpei Komiyama, Junya Honda, Akiko Takeda",https://proceedings.neurips.cc/paper/2017/hash/c57168a952f5d46724cf35dfc3d48a7f-Abstract.html,"The paper investigates  a multiple-play multi-armed bandit model with position 
bias i.e., one that involves $K$ Bernoulli bandits (with parameters $\theta_i$ \in (0,1)$ and $L$ slots 
with parameters $\kapa_j$ \in (0,1]$. At each point in time $t$ the system selects  $L$ 
arms $(I_1 (t), . . . , I_L (t))$ and receives a Bernoulli reward from each arm 
$Ber(\theta_{I_j(t)} \kapa_j{I_j(t)})$, $j= I_1 (t), . . . , I_L (t)$.  The paper derives asymptotically 
optimal policies in the sense of Lai & Robbins (1985).

I like the model and the analysis, but I did not have time to check all the mathematics at a satisfactory level. 

Finally, I  would like to bring to the attention of the author(s) the papers below. [1] provides 
the extension of the Lai Robbins (1985) work (that involves  single parameter bandits) to those 
with multiple unknown parameters.  [2] Provides asymptotically optimal polices for unknown MDPs.

[1] Optimal adaptive policies for sequential allocation problems
AN Burnetas, MN Katehakis - Advances in Applied Mathematics, 1996

[2] Optimal adaptive policies for Markov decision processes
AN Burnetas, MN Katehakis - Mathematics of Operations Research, 1997
 ","** Summary **

The paper tackles the learning-to-rank problem under delayed feedback that was proposed by Lagrée et al. 2016 [LVC16]. In the latter NIPS paper, the authors analyze the problem of learning how to sequentially select ordered lists of items when the position-related parameters are known. The present contribution releases this hypothesis and proposes a way to jointly learn the best allocation strategy with respect to unknown item and position parameters. They first prove a problem dependent lower bound that comes as the solution of a constraint optimization problem. Then, they make use of this result to design an algorithm that uses MLE of the parameters as plug-ins to solve the same optimization problem and obtain allocation rules. Experiments show the behavior of their algorithm PMED on simulated and pseudo-real data and an analysis of a slightly modified version of the algorithm is provided in Section 7.

** General comment **

This is a good paper overall and I would definitely recommend accepting it. I have a few questions though (see below) but I think the way the problem is posed is very relevant and interesting. Even though the authors build on the model from [LVC16], they clearly depart from the latter work and propose an original approach. 

** Questions and comments **

- I am first surprised by the way the *asymptotic* results are stated. Lemma 1, Theorem 2 and Lemma 6 are asymptotic bounds but it is not very clear in the way they are stated I think. It would be more explicit by using either a ratio ""\lim_T\to \infty f(T)/log(T) \geq ore \leq ..."" or to add ""There exist a positive M such that for all T\geq M , ..."" in the statement of the result. Or maybe I missed something and I'd be happy to read your answer.

- Your algorithm is based on optimization routines. You comment on the computational complexity of the methods you use but we don't know how it translate on computation time. I know that this is a relative metric that depends on hardware and implementation but I would be curious if you could give some insight on your experiments and on how 'scalable' is the proposed approach. 

** Minor remarks **

- l94-95 in the definition of \Delta and the regret, the parameters should be ^*
- Maybe you could split equation (1) after the ""="" sign in order not to cut the optimization problem itself, for readability
- The title of the paper is long and a bit close to the one of [LVC16], I'd suggest to maybe find another, simpler one. 

%%% After Rebuttal %%

I read the rebuttal and I thank the authors for their answers to my questions and for clarifying many points after Reviewer 2's concerns. I believe this is a strong paper and I would push for acceptance.
","#Summary
This submission studies a multi-armed bandit problem, PBMU.
In this problem, there are L slots (to place ads) and K possible ads.
At each round, the player need to choose L from the K possible ads and place them into the L slots.
The reward of placing the i-th ad into the l-th slot is i.i.d. drawn from a Bernoulli distribution Ber(\theta_i \kappa_l), where \theta_i is the (unknown) quality of the i-th ad and \kappa_l is the (also unknown) position bias of the l-th slot.
The authors derived a regret lower bound and proposed two algorithms for PBMU. 
The second algorithm has a matching regret bound. 
The authors also discussed how to solve the related optimization problems appeared in the algorithms.

The authors give a complete characterization of the hardness of PMMAB in this paper. Their main contributions are:

1.A regret lower bound， which states that the expected regret of a strongly consistent algorithm for PMMAB is lower bounded by ClogT-o(log T). Where C is a parameter determined by every arm's mean profile and slot's position bias.

2.An algorithm for PMMAB. The authors called it Permutation Minimum Empirical Divergence(PMED) which can be compared with the Deterministic Minimum Empirical Divergence (DMED) algorithm for SMAB (see Honda and Takemura COLT2010).  They give an asymptotically upper bound of a modification of PMED  which is Clog T + o(log T).

From the technical point, I think the regret lower bound in this paper is a standard, which follows some techniques from Honda and Takemura COLT2010. In their upper bound analysis, the authors solve some new difficulties. In PMED algorithm, they compute Maximum Likelihood Estimator and use it to compute a K times K matrix which optimizes a corresponding optimization problem in the lower bound analysis. The authors then convert the matrix into a convex combination of permutation matrix (due to  Birkhoff–von Neumann Theorem). Though this idea is natural, the analysis is not straightforward. The authors analyze a modification of PMED in which they add additional terms to modify the optimization problems. They obtain a corresponding upper bound for this modified algorithm.


Experimental results are attached. 

#Treatment of related work
In the introduction part, the authors introduced some other related models and compared them with their new model.
They also gave convincing reasons why this new model deserves study.
To the best of my knowledge, their model is novel. 
However, the author did not make much effort in comparing their techniques with previous (closely related) results.
This made it difficult to evaluate the technical strength of their results.

#Presentation
The presentation of the paper still needs some work.
In Section 3, the authors defined a set \mathcal{Q}. 
However, it is dubious why they need to define this quality when deriving the regret lower bound (the definition of this quality is not furthered used in Section 3 and related parts in the appendix).  
On the other hand, the authors did use several properties of this quantity in Section 4 (to run Algorithm 2).
Thus, I advise the authors to place the definition of \mathcal{Q} into Section 4 in order to eliminate unnecessary confusion and change the claim of the lower bound correspondingly. 

Line 16-20 of Algorithm 1 (PMED and PMED-Hinge) is difficult to follow. 
To my understand, \hat{N}_{i, l} is the number of necessary drawings for the pair (i, l).
The algorithm then decomposes the matrix \hat{N} into a linear combination of permutation matrices and pull the arms according to the permutation matrices and the coefficients. 
When will c_v^{aff} be smaller than c_v^{req}? 
Shouldn't we pull (v_1, v_2, \ldots, v_L) for c_v^{req} times (instead of once)? 
Should c_v^{req} be rounded up to an integer?

There are some other typos/minor issues. See the list below.
1. Line 57-58: Non-convex optimization appears in your algorithm does not necessarily mean that PBMU intrinsically requires non-convex optimization. 
2. Line 63-65: Define K, or remove some details. 
3. Line 86-87: at round t -> of the first t rounds.
4. Line 102-105: According to the definition, (1), (2), \ldots, (L) is a permutation of {1, 2, 3, \ldots, L}, which is not what you want.
5. Line 115-117: The fact that d_{KL} is non-convex does not necessarily make PBMU intrinsically difficult.
6. Line 118, the definition of \mathcal{Q}: What do you mean by \sum (q_{i, l} = q_{i + 1, l})?
7. Line 122: (1) is not am inequality. Do you want to refer to the Inequality in Lemma 1? ""the drawing each pair (i, l)"", remove ""the"".
8. Line 124-127: Please elaborate on the reason why the constraint \theta_i?\kappa_i?= \theta_i \kappa_i is necessary. The argument given here is not easy to follow.
9. Line 138: is based on -> is closed related to
10. Line 141: what do you mean by ""clarify""?
11. Algorithm 1, Line 12: Explicitly define \hat{1}(t), \ldots, \hat{L}(t). Currently it is only implicitly defined in Line 160.
12. Algorithm 1, Line 14: ""where e_v for each v is a permutation matrix"" -> ""where for each v, e_v is a permutation matrix"".
13. Line 172-174: Explicitly define the naive greedy algorithm.
14. Line 343: native->naive.
15. Algorithm 1, Line 17: \max_{c \ge 0} ... \ge 0 -> \max_{c \ge 0} s.t. ... \ge 0.

-------------------

The rebuttal addressed most of my major questions.
Score changed to 7.
"
Deep Voice 2: Multi-Speaker Neural Text-to-Speech,"Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou",https://proceedings.neurips.cc/paper/2017/hash/c59b469d724f7919b7d35514184fdc0f-Abstract.html,"Seems like this paper will attract a lot of interest.  Not doing a detailed review since it looks like it will be a definite accept.
","This paper presents Deep Voice 2, and neural TTS system that is able to generate voices from multiple speakers. This is a solid paper.
      The paper would benefit from addressing a few points:
      - there is no mention of training and, in particular, inference time. The original WaveNet is notably slow, how do these systems perform?
      - why use different sampling frequencies for different systems? I don't believe there is a technical hurdle in using the same sampling frequency for all, and it would be easier to compare MOS scores.
      - the paper mentions that giving the ground truth may influence the MOS for other methods: why not test that by splitting raters into two groups, those that get to listen to the ground truth and those that don't, and comparing the results?","This paper presents a solid piece of work on the speaker-dependent neural TTS system, building on previous works of Deep Voice and Tacotron architecture. The key idea is to learn a speaker-dependent  embedding vector jointly with the neural TTS model. The paper is clearly written, and the experiments are presented well. 

My comments are as follows.

-- the speaker embedding vector approach is very similar to the speaker code approach for speaker adaptation studied in ASR, e.g.,

Ossama Abdel-Hamid, Hui Jiang, ""Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code"", ICASSP 2013

This should be mentioned in the paper. ASR researchers later find that using fixed speaker embeddings such i-vectors can work equally well (or even better). It would be interesting if the authors could present some experimental results on that. The benefit is that you may be able to do speaker adaptation fairly easily, without learning the speaker embedding as the presented approach. 

--in the second of paragraph of Sec. 3, you mentioned that the phoneme duration and frequency models are separated in this work. Can you explain your motivation?

--the CTC paper from A. Graves should be cited in section 3.1. Also, CTC is not a good alignment model for ASR, as it is trained by marginalizing over all the possible alignments. For ASR, the alignments from CTC may be very biased from the ground truth. I am curious how much segmentation error do you have, and have you tried some other segmentation models, e.g., segmental RNN?

--in Table 1, can you present comparisons between Deep Voice and Tocotron with the same sample frequency?

--is Deep Voice 2 trained end-to-end, or the modules are trained separately? It would be helpful to readers to clarify that.

--the speaker embedding are used as input to several different modules, some ablation experiments in the appendix would be helpful to see how much difference it would make for each module. 
"
Eigen-Distortions of Hierarchical Representations,"Alexander Berardino, Valero Laparra, Johannes Ballé, Eero Simoncelli",https://proceedings.neurips.cc/paper/2017/hash/c5a4e7e6882845ea7bb4d9462868219b-Abstract.html,"This is a great paper!  And long overdue.  The authors show that image distortions easily perceptible by humans are not well discriminated by neurons at higher levels of a deep net.  This is done by a Fisher information analysis which computes how perturbations in pixel space translate into changes in the representation space at a given level of a deep net.  Along with earlier findings showing that small image perturbations that are imperceptible to humans cause large changes in categorization, this study demonstrates that deep nets are not a good model of human visual perception, despite their impressive performance at trained object categorization tasks.  It is an important neural network ""psychophysics"" study that should be of interest to NIPS.

One question that arises from this study is whether the top layer of a neural net is the best or only level to tap into for obtaining information about the image.  There is a common assumption that the higher levels of visual cortex correspond to the ""top box"" that is more closely tied to our perception, but increasing evidence points to the idea that *all* levels contribute information for solving a task, and which level is most important depends on the nature of the task.  So in that sense the fact that lower levels are more closely tied to human subjects judgements of low level perturbations makes sense.  But the fact that even a simple neurophysiolgoically informed front-end model outperforms the CNN networks seems to indicate that they are missing some fundamental components, or that the specific categorization task is driving them away from achieving a more generically useful image representation.



","The submission presents a method to generate image distortions that are maximally/minimally discriminable in a certain image representation. The maximally/minimally distortion directions are defined as the eigenvectors of the Fisher Information Matrix with largest/smallest eigenvalue. Distortions are generated for image representations in the VGG-16 as well as for representations in models that were trained to predict human sensitivity to image distortions. Human discrimination thresholds for those distortions are measured. It is found that the difference in human discrimination threshold between max and min distortions of the model is largest for a biologically inspired 'early vision' model that was trained to predict human sensitivity, compared to a CNN trained to predict human sensitivity or the VGG-16 representations. For the VGG representations it is found that the difference in detection threshold for humans is larger for min/max distortions of earlier layers than for later layers. 
Thus it is concluded that the simple 'early vision' representation is better aligned with human perceptual representations than those of the VGG network trained on object recognition.

Pro
	- The idea of generating extremal distortions of a certain representation seems appealing and a useful tool for visualising important aspects of complex image representations.
        
	- The finding that the max/min distortions of the early vision model are better aligned with human discrimination thresholds than those of the VGG is interesting and could be useful for the design of 'perceptual loss functions' in tasks like super-resolution.
        
Contra
        - The extremal distortions are an informative but limited aspect of the image representations. I am not sure to what extend one can conclude from the results that one representation is in general closer to human perception than another.
    
Comments
        - I would be very interested if the VGG representation improves if it is fine-tuned to predict human sensitivity to image distortions.
","PAPER SUMMARY
The paper attempts to measure the degree to which learned image representations from convolutional networks can explain human perceptual sensitivity to visual stimuli. This question is well-motivated on the neuroscience side by recent studies that have used learned image representations to predict brain activity in primates, and on the computer vision side by recent methods for image synthesis which have used image representations learned for object recognition as a surrogate for human perception.

The paper develops a clever experimental paradigm to test the extent to which a (differentiable) image representation agrees with human perception by experimentally measuring the human discrimination threshold for image distortions along eigendirections of the Fisher information matrix corresponding to extremal eigenvalues.

Using this experimental technique, the paper demonstrates that for a VGG-16 model pretrained for ImageNet classification, image representations at earlier layers correlate with human perception more strongly than those at later layers.

The paper next explores image representations from models trained explicitly to match human judgements of perceived image distortion, training both a four-layer CNN and an On-Off model whose structure reflects the structure of the human LGN. Both models are able to fit the training data well, with the CNN slightly outperforming the On-Off model. However using the experimental paradigm from above, the paper shows that image representations from the On-Off model correlate much more strongly with human perception than either the shallow CNN model or representations from any layer of the VGG-16 model trained on ImageNet.

FEATURES FROM IQA CNN
For the four-layer IQA CNN from Section 3.1, it seems that only the image representation from the final layer of the network were compared with human perceptual judgements; however from Figure 3 we know that earlier layers of VGG-16 explain human perceptual judgements much better than later layers. I’m therefore curious about whether features from earlier layers of the IQA CNN would do a better job of explaining human perception than final layer features.

RANDOM NETWORKS AND GANS
Some very recent work has shown that features from shallow CNNs with no pooling and random filters can be used for texture synthesis nearly as well as features from deep networks pretrained on ImageNet (Ustyuzhaninov et al, ICLR 2017), suggesting that the network structure itself and not the task on which it was trained may be a key factor in determining the types of perceptual information its features capture. I wonder whether the experimental paradigm of this paper could be used to probe the perceptual effects of trained vs random networks?

In a similar vein, I wonder whether features from the discriminator of a GAN would better match human perception than ImageNet models, since such a discriminator is explicitly trained to detect synthetic images?

ON-OFF MODEL FOR IMAGE SYNTHESIS
Given the relative success of the On-Off model at explaining human perceptual judgements, I wonder whether it could be used in place of VGG features for image synthesis tasks such as super-resolution or texture generation?

OVERALL
Overall I believe this to be a high-quality, well-written paper with an interesting technical approach and exciting experimental results which open up many interesting questions to be explored in future work.

POST AUTHOR RESPONSE
I did not have any major issues with the paper as submitted, and I thank the authors for entertaining some of my suggestions about potential future work. To me this paper is a clear accept."
Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon,"Xin Dong, Shangyu Chen, Sinno Pan",https://proceedings.neurips.cc/paper/2017/hash/c5dc3e08849bec07e33ca353de62ea04-Abstract.html,"The authors consider neural network pruning problem. They approach the problem by pruning the network layer by layer. The paper has contributions in multiple directions both algorithmic and theoretical.

Theoretically they obtain the pruning error between the pruned network and the original (trained baseline) network. Since the approach is layer by layer, the output error is also in terms of the accumulation of the individual layers. While theoretical results are intuitive and clean, I am not sure how novel they are. It appears to me that [12, 13] already did similar analysis for shallow networks and the observation of this work is propagating the layerwise errors for deep networks. Authors should address this (in particular Lemma 3.1 and Theorem 3.3) as it is important to determine the novelty of the work.

Algorithmically, they propose a fast algorithm for inverse Hessian calculation and essentially implement a careful deep version of the optimal brain surgeon. The proposed algorithm outperforms competing methods (closest one is DNS) and requires less training which are both plus.

I recommend the authors additionally compare their methods to iterative-hard thresholding methods (on top of l1 regularization (nettrim) such as
Jin et al. ""Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods"".

Secondly, is there any argument for DNS and the proposed methods are performing the best? Can authors provide an argument?","Summary:
This paper adapts Optimal Brain Surgeon (OBS) method to a local version, and modified the objective function to be the target activation per each layer. Similar to OBS, it uses an approximation to compute Hessian inverse by running through the dataset once. Compare to prior methods, it finishes compression with much less retraining iterations. A theoretical bound on the total error based on local reconstruction error is provided.

Pros:
- The paper explores a local version of OBS and shows effectiveness of proposed method in terms of less time cost for retraining the pruned network.
- Leveraging local second order information is a much clever way than solving/retraining a network through brute force.
- The proposed method achieves good compression ratio without losing performance, with much less retraining iterations.

Comments:
- I recommend the authors to include the basics of Optimal Brain Surgeon (OBS) as a background section. This will make the flow of the paper much better. Many of the techniques used later (e.g. Lagrange multiplier of the MSE error function, iterative algorithm for computing the Hessian inverse) should be part of the background.
- The formulation of layer-wise objective is difficult to understand. Z^l is also a function of Theta^l, and since Z^l = Y^(l-1) W^l, then E^l is actually not even a function of Theta^l, since both terms cancel out in Z^l and hat(Z)^l. This is another reason why the - paper should have included OBS in the background, since here it uses Z^l as a local target activation value, and no longer treated as a function of Theta^l. I had read section 3.2 over many times to realize this point.
- Line 129: “For a well-trained network, gradient w.r.t. Parameters is small enough to be ignored as well”. This is not true. For the original OBS, it is true since the error is the total loss function of the network. For a local MSE function, the gradient of E^l wrt. Theta^l is literally zero (not just small enough).
- I recommend the authors move the proof Theorem 3.2 to the Appendix. This bound is good to know (that the total error is bounded by each local errors) but not very useful and hurts the flow of the paper.
- I recommend the authors edit section 3.4 into algorithmic format. It is hard to read an algorithm in a paragraph. Same for section “prune convolutional layer”.
- Line 191: Again, I don’t understand why “well-trained” is part of the clause here. For any function, if you perturb the input a little bit, the output will be quite close. In the original OBS, it is the final output vs. the target, so “well-trained” is necessary. But here, the target is itself.
- The paper gives some asymptotic bound on time complexities, but I am still curious of the actual wall clock timing compared to related work (DNS, Net-Trim), especially the time cost to compute the Hessian inverse.
- The paper claims that Net-Trim has memory limitation. But maybe it is worth trying a mini-batch version of Net-Trim?
- The paper puts ResNet-101 as future work. It would have been better if the authors can give preliminary results to give insights on how expensive it is to compute Hessian inverse on a deeper network like this. Based on the time complexity, I don’t see why running ResNet-101 with L-OBS is an issue.
- The model achieves good results with much less number of retraining iterations. However, in the AlexNet experiment, it is still losing to DNS in terms of compression ratio. Although, the paper claims to have faster compression time, one major use case for network compression/sparsification deployment on embedded systems, in which the compression ratio is more of a concern than time cost to retrain the network. For completeness, I also recommend the authors to implement DNS and report numbers on VGG-16.

Overall, I feel the paper has some good contribution, but the paper clarity needs to be improved by a lot (Background section, local error function formulation). Some experiments need to be clarified (e.g. wall clock time comparison), and completed (DNS numbers on VGG). Based on these points, I recommend marginal accept, conditioned on a major revision.

After rebuttal note: I am glad that DNS numbers on VGG and ResNet is provided. After seeing the rebuttal I decided to upvote my rating to 7. I hope the authors will reorganize the paper as promised in the rebuttal.","The paper extends the optimal brain surgeon approach to pruning parameters by defining a layerwise loss to determine which parameters to prune. The loss computes the change in the function of the layer and a 2nd order is used to approximate it. This objective is minimized with a Lagrangian constraint to find the the best parameters to prune. The algorithm is well-reasoned but the claim in the abstract that ""there is a guarantee that one only needs to perform a light retraining process"" is overstated. The theoretical results only bound the error at each layer and say nothing of the end-to-end classification error. Though experimentally, it is observed that the algorithm requires less retraining - but without the proper bounds that does not constitute a guarantee.

The method is evaluated on an array of datasets inclusing MNIST, CIFAR and Imagenet. It achieves significant compression rates with comparatively smaller drops in performance.

Typos:
L147 ""By far"" -> ""So far"""
Deliberation Networks: Sequence Generation Beyond One-Pass Decoding,"Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, Tie-Yan Liu",https://proceedings.neurips.cc/paper/2017/hash/c6036a69be21cb660499b75718a3ef24-Abstract.html,"This paper introduces a two steps decoding model applied to NMT and summarization. The idea is well explained and the experimental results show significant improvements over realistic baseline. 

The authors chose to approximate the marginalization of the intermediate hypothesis with monte-carlo. This is indeed a solution, but it is not clear how many samples are used? Does this hyperparameter have an impact on the performance ? Do you use beam search instead of simple sampling? These technical details should be more precisely described. 

At the end, the work described in this paper is interesting and the experimental part is meaningful. However, I found the introduction very pretentious. Two paragraphs are dedicated to a ""cognitive"" justification as the authors have reinvented the wheel. Multi-steps decoding exists for many years and this is really a great idea to propose a solution for end to end neural model, but you don't need to discard previous work or motivate it with pop cognitive consideration (note that in ""cognitive science"" there is ""science"").


Page 2: The sentence in line 73 is useless.

Line 78: this sentence is correct for end to end neural models, but there is a long history in speech recognition with multiple pass decoding: in the first step, a word lattice is generated with ""weak"" acoustic and language models, then more sophisticated and powerful models are used to refine iteratively this word lattice.  So you cannot say that for sequence prediction it wasn't explored. It is not mandatory to discard previous work because you find a nice name. 


Page 8:
Line 277: ""In this work, inspired by human cognitive process,"" You could avoid this kind of pretentious phrase.
","-----
After response: thank you for the thorough response. Two of my major concerns: the weakness of the baseline and the lack of comparison with automatic post-editing have been resolved by the response. I've raised my evaluation with the expectation that these results will be added to the final camera ready version.

With regards to the examples, the reason why I said ""cherry-picked?"" (with a question mark) was because there was no mention of how the examples were chosen. If they were chosen randomly or some other unbiased method that could be noted in the paper. It's OK to cherry-pick representative examples, of course, and it'd be more clear if this was mentioned as well. Also, if there was some quantitative analysis of which parts of the sentence were getting better this would be useful, as I noted before.

Finally, the explanation that beam search has trouble recovering from search errors at the beginning of the sentence is definitely true. However, the fact that this isn't taken into account at training and test time is less clear. NMT is maximizing the full-sentence likelihood, which treats mistakes at any part of the sentence equally. I think it's important to be accurate here, as this is a major premise of this work.
---------

This paper presents a method to train a network to revise results generated by a first-pass decoder in neural sequence-to-sequence models. The second-level model is trained using a sampling-based method where outputs are sampled from the first-pass model. I think idea in the paper is potentially interesting, but there are a number of concerns that I had about the validity and analysis of the results. It would also be better if the content could be put in better context of prior work.

First, it wasn't entirely clear to me that the reasoning behind the proposed method is sound. There is a nice example in the intro where a poet used the final word of the sentence to determine the first word, but at least according to probabilities, despite the fact that standard neural sequence-to-sequence models are *decoding* from left to right, they are still calculating the joint probability over words in the entire sentence, and indirectly consider the words in the end of the sentence in the earlier decisions because the conditional probability of latter words will be lower if the model makes bad decisions for earlier words. Of course this will not work if greedy decoding is performed, but with beam search it is possible to recover. I think this should be discussed carefully.

Second, regarding validity of the results: it seems that the baselines used in the paper are not very representative of the state of the art for the respective tasks. I don't expect that the every paper has to be improving SOTA numbers, but for MT if ""RNNSearch"" is the original Bahdanau et al. attentional model, it is likely missing most of the improvements that have been made to neural MT over the past several years, and thus it is hard to tell how these results will transfer to a stronger model. For example, the below paper achieves single-model BLEU scores on the NIST04-06 data sets that are 4-7 BLEU points higher than the proposed model:

> Deep Neural Machine Translation with Linear Associative Unit. Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu (ACL 2017)

Third, the paper has a nominal amount of qualitative evaluation of (cherry-picked?) examples, but lacks any analysis of why the proposed method is doing better. It would be quite useful if analysis could be done to show whether the improvements are indeed due to the fact that the network has access to future target words in a quantitative fashion. For example, is the proposed method better at generating beginnings of sentences, where standard models are worse at generating words in this more information-impoverished setting.

Finally, there is quite a bit of previous work that either is similar methodologically or attempts to solve similar problems. The closest is automatic post-editing or pre-translation such as the following papers:

> Junczys-Dowmunt, Marcin, and Roman Grundkiewicz. ""Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing."" WMT2016.
> Niehues, Jan, et al. ""Pre-Translation for Neural Machine Translation."" COLING2016

This is not trained jointly with the first-past decoder, but even without joint training is quite useful. Also, the following paper attempts to perform globally consistent decoding, and introduces bilingual or bidirectional models that can solve a similar problem:

> Hoang, Cong Duy Vu, Gholamreza Haffari, and Trevor Cohn. ""Decoding as Continuous Optimization in Neural Machine Translation."" arXiv 2017.","This paper proposes a two stage decoding for Seq2Seq models: first, an output sequence is generated, and then, a second improved target sequence is produced using attention on the source and this initially created target sequence. The motivation behind is that this procedure would help to see the whole target sequence when making decision about word choice, etc.

Although the authors show some improvements in the BLEU score by this method, I find the overall approach somehow awkward. Seq2Seq models have replaced traditional SMT, eg. phrase-based models (PBSMT) because it's a nice end-to-end framework with a global training objective, instead of many individually models which were first trained independently and then combined in a log-linear framework (PBSMT).

You basically train a second NMT model which translates from a 1st target hypothesis to an improved one. This is similar in spirit to ""statistical post-editing (SPE)"". This is computationally expensive (+50% training time, decode 2 times slower).

Do you use beam search for both decoder networks ?

I would also say that your baseline is quite old: RNNSearch was the first model for NMT. In the meantime, better architectures are known and it would be good to show that the proposed two-stage approach improves on top of those systems (you mention yourself deeper encoder/decoder architectures).

An alternative, easy to implement baseline, would be to perform a forward and backward decode (last to 1st word) and combine both.
"
Do Deep Neural Networks Suffer from Crowding?,"Anna Volokitin, Gemma Roig, Tomaso A. Poggio",https://proceedings.neurips.cc/paper/2017/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html,"The paper study the effect of crowding, under particular conditions,
on the object recognition performance of deep neural networks.

The authors considered different DNN architectures, in particular
Convnets and eccentricity-dependent networks, both with different
pooling strategies.
For the convnets they tried: (i) no pooling (60-54-48-42), (ii)
progressive pooling (60-27-11-1), and (iii) at the end pooling
(60-54-48-1) and trained with minibatches of 128 images and 32 feature
channels. 
The eccentricity-dependent network constructs a multiscale
representation where smaller sections are sample densely at a high
resolution and larger sections are sampled with at a lower
resolution. The configurations for this type of network were: (i) at
the beginning (11-1-1-1-1), (ii) progressively (11-7-5-3-1), and (iii)
at the end (11-11-11-11-1).

Models were trained on even MNIST digits with/without flankers of
different datasets and embedded in a dataset of places.

From the results it can be seen that when the models are trained with
isolated objects, the recognition rate falls when flankers are added
to the image. This is specially true with flankers that are similar to
the objects to be recognized. The eccentricity dependent network has
better results when the object to recognize is in the center of the
image.

This is an interesting paper that is testing the robustness of DNNs
and helps to explain some degradation in performance when different
background objects are shown in testing images. 
","​Crowding is an effect in human vision, in which objects that can be recognized in isolation can no longer be recognized in the presence of nearby objects, even though there is no occlusion. This paper tries to answer the question whether deep neural network suffers from the crowding effect as well as the relationship of crowding to various types of configurations such as pooling, eccentricity, target-flanker similarity, flanker configurations etc. I think this paper is a good attempt of understand crowding of neural networks, but the execution of this work needs major improvements.

​
1. Most of the result shown in this paper are well expected.
For example, neural network could suffer from crowding, and training the network with the same type of flankers can improve the performance.

2. Some of the statements are not well supported by the experiments.
For example, line 168 ""thus in order for a model to be truly robust to all kinds of clutter, it needs to be trained with all possible target-flanker configurations"". This is not well justified, since the experiments are conducted based on 120x and 240x spacing only.

3. The study is purely empirical and the settings studied are quite limited in their scope.
For example, it is stated that ""we investigate pooling in particular, because some computational models of crowding have suggested that feature integration may partly be the cause of this phenomenon in humans"". However, neural networks are very different from human vision. The effect of crowding may be well based on other aspects of the network rather than the pooling operation.

4. I do not see utilities of the proposed method. 
Towards the end of this paper, ""our results suggest that the eccentricity -dependent model coupled with a system for selecting eye fixation location would give benefit of low sample complexity in training"". This is a conjecture rather than conclusion.","This paper studies if crowding, a visual effect suffered by human visual systems, happens to deep neural network as well. The paper systematically analyzes the performance difference when (1) clutter/flankers is present; (2) the similarity and proximity to the target; (3) when different architectures of the network is used. 

Pros:
There are very few papers to study if various visual perceptual phenomenon exists in deep neural nets, or in vision algorithms in general. This paper studies the effect of crowding in DNN/DCNN image classification problem, and presents some interesting results which seems to suggest similar effect exists in DNN because of pooling layers merges nearby responses. And this is related to the theories of crowding in humans, which is also interesting. The paper also suggests what we should not do prematurely pooling if when designing architectures. In my opinion such papers should be encouraged.

Cons:
My main criticism to the paper is that it solely studies crowding in the context of image classification. However, if crowding is studied in the context of object detection, where the task is localize the object and recognize its category, the effect may be significantly lessened. For example, R-CNN proposes high saliency regions where the object might be and perform classification on that masked region. Because targets are usually centered in such proposed region and background clutters are excluded from the proposed region, the accuracy can potentially be much higher. 

After all, the extent to which crowding is present in DNN depends a lot on the chosen architecture. And the architecture in this paper is very primitive compare to what researchers consider state-of-the-art these days, and the accuracy of the MNIST tasks reported by the paper are way lower than what most researchers would expect from a practical system. For example, [1] performs digit OCR which has much more clutters but with very high accuracy. It is not obvious architectures like that also suffer from crowding.

Suggestion:
The paper is overall easy to follow. I feel the experimental setup can be more clear if some more example images (non-cropped, like the ones in the Fig1 of supplementary material).

Overall, this paper has an interesting topic and is a nice read. The conclusions are not too strong because it uses simplistic architecture/datasets. But I think it is nonetheless this is a helpful paper to (re-)generate some interest on drawing relation between theories of human visual recognition and neural nets.

[1] Goodfellow, Ian J., et al. ""Multi-digit number recognition from street view imagery using deep convolutional neural networks."" arXiv preprint arXiv:1312.6082 (2013)."
Non-Stationary Spectral Kernels,"Sami Remes, Markus Heinonen, Samuel Kaski",https://proceedings.neurips.cc/paper/2017/hash/c65d7bd70fe3e5e3a2f3de681edc193d-Abstract.html,"This paper proposes an interesting and new approach to learning flexible non-stationary (and I believe also non-monotonic, though this could be emphasized more) kernels. I have some concerns about the development and flow of the paper but overall I think it makes a useful contribution.

Section 2.1 presents a bivariate version of the Spectral Mixture kernel, by inducing a dependence between s and s'. It would be helpful if a comparison could be made here to the work referenced later, by Kom Samo and Roberts. The remark at the end of 2.1 that the BSM vanishes presumably justifies your abandonment of this parameterization, but it would be helpful to have more exposition here; why doesn't this issue apply to the SM kernel (answer: because the SM kernel is stationary (?)). Are there no settings in which the kernel shown in Figure 1b would be a useful one? The kernel learned in Garnett, Ho, Schneider [ICML 2015] might be a setting, for example.

Section 2.2 is the crux of the development but it very breezily goes from what seemed well motivated in Section 2.1 to a parameterization involving the Gibbs kernel and GPs giving smooth input dependent parameters for the BSM kernel. I would definitely appreciate more time spent here explaining these choices, especially in light of the remark that only comes at the end, which is that the solution to the spectral surface is unknown. Whether or not you thought it up this way, to me it seems that the motivation for the parameterization is that of making the parameters of the SM kernel input dependent rather than using the generalized Fourier transform. And ultimately, isn't the Gibbs kernel doing much of the work of non-stationary anyway? That needs more motivation.

In 2.1 and 2.2 you remark that the extension to multivariate inputs is straightforward. If so, this should go in the Appendix. 

Eq (10) has an error: it should be log(w_ip), log(\mu_ip), and logit(l_ip), not w_ip, \mu_ip, and l_ip.

Figure 5 is interesting. A numerical comparison would be useful as well---what is the MSE/log-likelihood for prediction (in-sample) and also forecasting (out of sample) using: BSM, GSM, SM, and SS? (A comparison to Kom-Samo would also be helpful, but I would very much like to see the others and I imagine they are more straightforward since they're all already implemented in GPML.) 

The SM kernel suffers from issues with initialization and local optima, especially with a large number of mixture components. It would be very useful to know whether your approach suffers from this issue as well.","--- Updated Comments Since Rebuttal --- 

This seems like very nice work and I hope you do keep exploring in your experiments.  I think your approach could be used to solve some quite exciting problems.

Some of the experiments are conflating aspects of the model (non-stationarity in particular) with what are likely engineering and implementation details, training differences (e.g. MAP), and initialization.  The treadplate pattern, for instance, is clearly mostly stationary. Some of these experiments should be modified or replaced with experiments more enlightening about your model, and less misleading about the alternatives.  It would also be valuable, in other experiments, to look at the the learned GP functions, to see what meaningful non-stationarity is being discovered.

Your paper is also very closely related to ""Fast Kernel Learning for Multidimensional Pattern Extrapolation"" (Wilson et. al, NIPS 2014), which has all the same real experiments, same approach to scalability, developed GPs specifically for this type of extrapolation, and generally laid the foundation for a lot of this work. This paper should be properly discussed in the text, in addition to [28] and [11], beyond swapping references.  Note also that the computational complexity results and extension to incomplete grids is introduced in this same Wilson et. al (2014) paper and not Flaxman et. al (2015), which simply makes use of these results.  Wilson & Nickisch (2015) generalize these methods from incomplete grids to arbitrary inputs.  Note also that a related proposal for non-stationarity is in Wilson (2014), PhD thesis, sections 4.4.2 - 4.5, involving input dependent mixtures of spectral mixture kernels.

Nonetheless, this is overall very nice work and I look forward to seeing continued experimentation.  I think it would make a good addition to NIPS.  I am happy to trust that the questions raised will be properly addressed in the final version.

---


The authors propose non-stationary spectral mixture kernels, by (i) modelling the spectral density in the generalized Bochner's theorem with a Gaussian mixture, (ii) replacing the resulting RBF component with a Gibbs kernel, and (iii) modelling the weights, length-scales and frequencies with Gaussian processes.

Overall, the paper is nice. It is quite clearly written, well motivated, and technically solid. While interesting, the weakest part of the paper is the experiments and comparisons for texture extrapolation. A stronger experimental validation, clearly showing the benefit of non-stationarity, would greatly improve the paper; hopefully such changes could appear in a camera ready version.  The paper also should make explicit connections to references [28] and [11] clear earlier in the paper, and should cite and discuss ""Fast Kernel Learning for Multidimensional Pattern Extrapolation"", which has all of the same real experiments, and the same approach to scalability.  Detailed comments for improvement:

- I didn't find there was much of a visual improvement between the SM kernel and the GSM kernel results on the wood texture synthesis, and similar performance from the SM kernel as the GSM kernel could be achieved for extrapolating the treadplate pattern, with a better initialization. It looks like the SM result on treadplate got trapped in a bad local optima, which could happen as easily with the GSM kernel. E.g., in this particular treadplate example, the results seem more likely to be a fluke of initialization than due to profound methodological differences. The comparison on treadplate should be re-evaluated. The paper should also look at some harder non-stationary inpainting problems -- it looks like there is promise to solve some really challenging problems here. Also the advantage of a non-stationary approach for land surface temperature forecasting is not made too clear here -- how does the spectral mixture kernel compare?  

- While the methodology can be developed for non-grid data, it appears the implementation developed for this paper has a grid restriction, based on the descriptions, experiments, and the imputation used in the land-surface temperature data. Incidentally, from an implementation perspective it is not hard to alleviate the need for imputation with virtual grid points. The grid restriction of the approach implemented here should be made clear in the text of the paper.

- The logit transform for the frequency functions is very nice, but it should be clarified that the Nyquist approach here only applies to regularly sampled data. It would be easy to generalize this approach to non-grid data. It would strengthen the method overall to generalize the implementation to non-grid data and to consider a wider variety of problems. Then the rather general benefits of the approach could be realized much more easily, and the impact would be greater.

- I suspect it is quite hard to learn Gaussian processes for the length-scale functions, and also somewhat for the frequency functions. Do you have any comments on this?  It would be enlightening to visualize the Gaussian process posteriors on the mixture components so we can see what is learned for each problem and how the solutions differ from stationarity. One of the really promising aspects of your formulation is that the non-stationarity can be made very interpretable through these functions.

- Why not perform inference over the Gaussian processes?

- There should be more discussion of reference [11] on ""Generalized spectral kernels"", earlier in the paper; Kom Samo and Roberts also developed a non-stationary extension to the spectral mixture kernel leveraging the generalized Bochner's theorem. The difference here appears to be in using Gaussian processes for mixture components, the Gibbs kernel for the RBF part, and Kronecker for scalability. ""Fast kernel learning for multidimensional pattern extrapolation"" [*] should also be referenced and discussed, since it proposes a multidimensional spectral mixture product kernel in combination with Kronecker inference, and develops the approach for texture extrapolation and land surface temperature forecasting, using the same datasets in this paper. Generally, the connection to [28] should also be made clear much earlier in the paper, and section 4 should appear earlier.  Moreover, the results in [*] ""Fast Kernel Learning for Multidimensional Pattern Extrapolation"", which have exactly the same real experiments, indicate that some of the results presented in your paper are from a bad initialization rather than a key methodological difference.  This needs to be remedied and there need to be more new experiments.

- Minor: ""the non-masked regions in Figure 3a and 3f"" in 5.2 is meant to refer to Figure 4.","This paper extends the spectral mixture kernel [28] to the nonstationary case, where the constant weights, length scales, and frequencies are replaced by nonconstant functions. The paper is well written and enjoyable to read. Here are some technical comments.

1. I do not quite understand the computational cost of evaluating the posterior and the gradient (stated to be O(PN^{(P+1)/P})), even after reading the appendix. The computation at least needs to perform P eigendecompositions on kernel matrices (size N*N for each data dimension). Therefore there should be at least a factor O(PN^3) in the cost?

2. Figures 2 and 3 show that the learned kernel is quite smooth, which must be because the learned functions w, ell, and mu are smooth. This is good but sounds a little mysterious to me. How does this happen? How would optimization end up with a smooth solution?

3. I suspect that the periodic pattern in the texture image 4(a) should be better modeled by using a periodic mean function rather than manipulating the covariance function. In such a case whether the kernel is stationary or not matters less."
Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations,"Marcel Nonnenmacher, Srinivas C. Turaga, Jakob H. Macke",https://proceedings.neurips.cc/paper/2017/hash/c7558e9d1f956b016d1fdba7ea132378-Abstract.html," The paper proposes a novel method for extracting a low-dimensional latent subspace to describe high-dimensional measurements when the measurements are of disjoint subsets with limited overlap in time. The author’s propose a variant of subspace identification (SSI) that works by matching empirical covariances to a parametrized model of the covariance.  They ultimately propose a combination of their SSI method and EM for application and demonstrate the efficiency and accuracy of their method using both simulation experiments and data-based experiments.

The paper appears to be well written and contains appropriate references to prior work. I found the prose and basic technical ideas of the paper surprisingly easy to follow considering the complexity of the material.

The work does not propose any big new ideas but it does propose a novel solution to a problem of growing importance in neuroscience.  Furthermore, the authors demonstrate the improvement of their method on existing approaches. I can see the method being potentially widely used in neural data analysis.

Two points stand out to me as open questions not addressed by the authors.  1) How should one choose the latent dimensionality, and 2) what happens in their simulation experiments when the latent dimensionality is wrong? 
","The authors suggest a method to estimate latent low dimensional dynamics from sequential partial observations of a population. Using estimates of lagged covariances, the method is able to reconstruct unobserved covariances quite well. The extra constraints introduced by looking at several lags allow for very small overlaps between the observed subsets.

This is an important work, as many imaging techniques have an inherent tradeoff between their sampling rate and the size of the population. A recent relevant work considered this from the perspective of inferring connectivity in a recurrent neural network (parameters of a specific nonlinear model with observations of the entire subset) [1].
The results seem impressive, with an ability to infer the dynamics even for very low overlaps, and the spiking network and zebrafish data indicate that the method works for certain nonlinear dynamics as well.
A few comments:
1.	In Figure 3b the error increases eventually, but this is not mentioned.
2.	Spiking simulation – the paper assumes a uniform sampling across clusters. How sensitive is it to under-sampling of some clusters?
3.	The zebrafish reconstruction of covariance. What is the estimated dimensionality of the data? The overlap is small in terms of percentage (1/41), but large in number of voxels (200K). Is this a case where the overlap is much larger than the dimensionality, and the constraints from lagged covariances do not add much? This example clearly shows the computational scaling of the algorithm, but I’m not sure how challenging it is in terms of the actual data.


[1] Soudry, Daniel, et al. ""Efficient"" shotgun"" inference of neural connectivity from highly sub-sampled activity data."" PLoS computational biology 11.10 (2015): e1004464.
","Summary
---------
The authors propose a method for stitching together multiple datasets by estimating a low-dimensional latent dynamical system underlying the partially observed neural data. They apply their method to simulated data and a light-sheet imaging dataset from zebrafish.

Questions
----------
For predicting unobserved covariances, how well would a matrix completion algorithm work for, say, the zebrafish dataset in Fig 6? It is hard to interpret the correlation of ~0.9 for the partially observed technique without comparison to other baselines.

How does the technique compare with the approach of Turaga et. al. on smaller datasets?

Minor comments
-----------------
- line 35: typo- ""Our methods allow *us* to capture the ...""
- Font sizes in all figures are too small, making them hard to read
- Line markers (triangles and squares) in Fig4a are small and make it hard to compare the different methods
- line 263: s/remaining/last
- line 271: change ""We here provide"" to ""Here, we provide"""
Minimizing a Submodular Function from Samples,"Eric Balkanski, Yaron Singer",https://proceedings.neurips.cc/paper/2017/hash/c75b6f114c23a4d7ea11331e7c00e73c-Abstract.html,"In this paper, the authors discuss the problem of minimizing a submodular function `f' from samples (S_i,f(S_i)), where the hardness of this problem is described despite its polynomial-tractability in optimization. I think this paper is a natural and necessary expansion of this type of problem from submodular function maximization in (Balkanski et al.,17). Although the results of this paper is somehow negative (a submodular function is hard to be minimized from samples) and thus its practical usefulness is a bit questionable, this paper could be a clue for further discussions of this type of problem, which would be important in this community.
","Informally stated, the paper states the following.
There exists a family F of submodular functions valued in [0, 1], so that if after polynomially many samples any algorithm estimates some set S as the minimizer, there exists some f in F that is consistent with the samples and on which S is suboptimal optimum by at least 1/2.
The argument is based on the probabilistic method, i.e., by showing that such functions exist with non-vanishing probability.
Moreover, the class that is considered is shown to be PAC-learnable, effectively showing that for submodular functions PAC-learnability does not imply ""minimizability"" from samples.
However, one has to be careful about this argument.
Namely, PAC learnability guarantees low error only on samples coming from the training distribution, while to minimize the function it is necessary to approximate it well near the optimum (where we might lack samples) up to an additive factor.
Thus, it is not very surprising to me that the former does not imply the latter and that one can construct function classes to fool any distribution.
This is a purely theoretical result that both answers an interesting question, and also provides with a construction of intuitive class of submodular functions which are (as proven) hard to minimize from samples.
I only read the (sketched) proofs in the main text, which seem to be very rigorously executed.
The paper is very dense, which I do not think can be easily improved on, but it is nicely written.
Based on this grounds, I would recommend this paper for acceptance.

I think it would be more interesting if the authors present some natural settings where the learning setting they consider is used in practice.
In other words, when does learn a submodular functions from training data of the form (S_i, F(S_i))?
This setting is different from how submodular functions are learned in the vision domain (where one might argue is the primary application field of submodular minimization).
There, one is given a set of instances, which we want to have a low energy, either using a min-margin approach, or via maximum likelihood.


Questions / Remarks:

1. The argument l129-132 is unclear to me. Do you want to say that as n -> oo both P_{f\in F}(f(S) >= 1/2) and |F'|/|F| go to one, there has to exist an f in |F'| satisfying f(S) >= 1/2 - o(1)?

2. In prop. 2 you do not define F_A(B), which I guess it means F(A \cup B) - F(B), which is confusing as it is typically defined the other way round. You could also use F(A | B), which is I guess the more commonly used notation.

3. I think the discussion after prop 2 and before section 3 should be moved somewhere else, as it does not following from the proposition itself.

4. Maybe add a note that U(A) means drawn uniformly at random from the set A.

5. l212: It is not motivated why you need one predictor for each possible masking element.

6. When you minimize a symmetric submodular function it is to minimize it over all sets different from the empty set and the ground set. This is exactly what is done using Queyranne's algorithm in cubic time.

7. Shouldn't the 1 be 1/2 in the last equation of the statement of lemma 4?

8. I find the notation in the two inequalities in Lemma 5 (after ""we get"") confusing. Does it mean that it holds for any h(n) = o(1) function with the given probability? Also, maybe add an ""and"" after the comma there.

9. l98: a word is missing after ""desirable optimization""","The authors study hardness results on minimizing submodular functions form samples. The results presented in the paper are interesting and non trivial and I think that the paper can be of interest for the NIPS audience.

The authors start by presenting a hardness results in the setting where the samples are drawn from a uniform distribution. The main intuition behind this result is that there exists submodular functions that achieve their minimum only when enough elements from a specific subset are sampled and this event happen only with low probability.

Then the authors show how to extend this idea to prove that their construction can be extended to handle non uniform samples. Interestingly the function that they construct is learnable but not optimizable. I find this particularly interesting and surprising.

Overall, I think that the paper is interesting and I suggest to accept it.

Minor comment:
- line 140: remove ')'"
A graph-theoretic approach to multitasking,"Noga Alon, Daniel Reichman, Igor Shinkar, Tal Wagner, Sebastian Musslick, Jonathan D. Cohen, Tom Griffiths, Biswadip dey, Kayhan Ozcimder",https://proceedings.neurips.cc/paper/2017/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html,"This paper solves an excellent theoretical problem: what are the limits of inducable matchings in bipartite graphs? This is a novel formulation in graph theory and the paper provides near tight scaling laws for the problem. Indeed, this problem is directly applicable in communication theory where interference and multi-user communication are accurately modeled by inducable matchings. 

1. Why inducable matchings is a good model for multi-tasking is a bit unclear: in a multi-tasking problem what are the inputs and what are the outputs. It seems in a human or humanoid setting, the inputs are input sense streams and the outputs are action streams. In such a case, a task should use a subset of inputs to obtain a given output (for example, vision, hearing and tactile feedback to decide how to hold a paper cup). However, in the model, a task simply connects one input to one output. 

2. The mathematical model of inducable matchings is quite interesting, and shows why the multi-tasking parameter could be of interest. The paper points out the pitfalls of using matchings as a proxy for multi-tasking and proposes this alternative formulation.  Indeed, a future algorithmic problem is the calculation of some inducable matching coefficient given a graph. 

3. The scaling laws connecting the degree of the graph to its multi-tasking ability is interesting, and it can potentially be used to design networks with appropriate overlaps for multi-tasking. While this model is not immediately applicable, the extremal graphs for inducable matchings can indeed inspire multitasking network architectures in future research. 

4. Overall multi-tasking is an important problem and while the paper presents a fairly simplified model, the mathematical modeling has novelty and the results are quite interesting. 

","The paper considers an interesting ability of neural networks, named concurrent multitasking, and reveals its relation to the efficiency of shared representations, which is presented by the average degree d.
This paper first defines the multitasking capacity as the maximal induced matching of every matching in the original graph of size from 1 to n.
Then it proves that the multitasking capacity is upper bounded by f(d)n with f(d) approaches 0 as d grows.
The paper shows upper bounds in both regular and irregular graphs, but with different orders of d.

The topic studied in this paper is of interest.
The result is meaningful and its proofs look ok.
The result can give us some intuition about how to construct the neural network especially in the view of representation efficiency and multitasking capacity.
This work will encourage more people to move on to the research about neural network in theory.

There should be some intuitions or examples to explain why choosing d as the efficiency of representations and induced matching as the capacity of the multitasking.
The proofs are somewhat technical and most of them is presented in the supplementary material, so I just check some of them.","The authors lay an ambitious groundwork for studying the multitasking capability of general neural network architectures. They prove a variety of theorems concerning behavior of their defined criteria--the multitasking capacity \alpha--and how it relates to graph connectivity and size via graph matchings and degree.

Pros:  The problem being studied is quite clearly motivated, and the authors present a genuinely novel new measure for attempting to understand the multitasking capability of networks.  The authors fairly systematically calculate bounds on their parameter in a variety of different graph-theoretic scenarios.

Cons:  It's unclear how practically useful the parameter \alpha will be for real networks currently in use.  Admittedly, this work is preliminary, and the authors point out this shortcoming in their conclusion.  While it is likely the topic of an upcoming study, it would benefit the thrust of the paper considerably if some \alpha's were calculated for networks that are actually in use, or *some* sort of numerical comparison were demonstrated (ex: it's recently become popular to train neural networks to perform multiple tasks simultaneously--what does \alpha have to say about the topologies of these networks?)

Plainly, it's not obvious from a first read of the paper that \alpha *means* anything short of being an abstract quantity one can calculate for graphs.  Additional reads clarify the connection, but making some sort of empirical contact with networks used in practice (beyond the throwaway lines in the conclusion about pseudorandom graphs) would greatly clarify the practical use of (or even intuition for) the \alpha parameter.

That said, this reviewer thinks the article should still be accepted in the absence of such an empirical study, because performing such a study would likely (at least) double the length of the paper (which would be silly).  At the minimum, though, this reviewer would appreciate a bit more discussion of the practical use of the parameter."
Adversarial Surrogate Losses for Ordinal Regression,"Rizal Fathony, Mohammad Ali Bashiri, Brian Ziebart",https://proceedings.neurips.cc/paper/2017/hash/c86a7ee3d8ef0b551ed58e354a836f2b-Abstract.html,"The paper proposes an adversarial approach to ordinal regression, building upon recent works along these lines for cost-sensitive losses. The proposed method is shown to be consistent, and to have favourable empirical performance compared to existing methods.

The basic idea of the paper is simple yet interesting: since ordinal regression can be viewed as a type of multiclass classification, and the latter has recently been attacked by adversarial learning approaches with some success, one can combine the two to derive adversarial ordinal regression approaches. By itself this would make the contribution a little narrow, but it is further shown that the adversarial loss in this particular problem admits a tractable form (Thm 1), which allows for efficient optimisation. Fisher-consistency of the approach also follows as a consequence of existing results for the cost-sensitive case, which is a salient feature of the approach.

The idea proves effective as seen in the good empirical results of the proposed method. Strictly, the performance isn't significantly better than existing methods, but rather is competitive; it would be ideal if taking the adversarial route led to improvements, but they at least illustrate that the idea is conceptually sound.

Overall, I thus think the paper makes a good contribution. My only two suggestions are:
	o It seems prudent to give some more intuition for why the proposed adversarial approach is expected to result in a surrogate that is more appealing than standard ones (beyond automatically having consistency) -- this is mentioned in the Introduction, but could be reiterated in Sec 2.5.

	o As the basis of the paper is that ordinal regression with the absolute error loss can be viewed as a cost-sensitive loss, it might help to spell out concretely this connection, i.e. specify what exactly the cost matrix is. This would make transparent e.g. Eqn 3 in terms of their connection to the work of (Asif et al., 2015).

Other comments:
- notation E_{P(x, y) \hat{P}(\hat{y} | x)}[ L_{\hat{Y}, Y} ] is a bit confusing -- Y, \hat{Y} are presumably random variables with specified distribution? And the apparent lack of x on the RHS may confuse.
- the sentence ""using training samples distributed according to P̃(x, y), which are drawn from P(x, y)"" is hard to understand
- Sec 3.2 and elsewhere, may as well define \hat{f} = w \cdot x, not w \cdot x_i; dependence on i doesn't add much here
- Certainly consistency compared to Crammer-Singer SVM is nice, but there are other multi-class formulations which are consistent, so I'm not sure this is the strongest argument for the proposed approach?
- what is the relative runtime of the proposed methods in the experiments?
- it wasn't immediately clear why one couldn't use the existing baselines, e.g. RED^th, in the case of a Gaussian kernel (possibly by learning with the empirical kernel map).
","The paper concerns the problem of ordinal classification. The authors derive a unique surrogate ordinal regression loss by seeking the predictor that is robust to a worst case constrained approximation of the training data. The theoretical part is built upon the recent results published in ""Adversarial Multiclass Classification: A Risk Minimization Perspective"" (NIPS 2016).

The authors could notice that the simplest approach to ordinal classification relies on estimating the conditional probabilities and then applying the Bayes rule on them (see, for example, ""Ordinal Classification with Decision Rules"", MCD 2007). Let me also add that ordinal classification is closely related to monotone classification or isotonic regression. Some theoretical results in this regards have been obtained in ""Rule Learning with Monotonicity Constraints"" (ICML, 2009) and ""On Nonparametric Ordinal Classification with Monotonicity Constraints"" (IEEE Transactions on Knowledge and Data Engineering, 2013). 

In general it is interesting paper that is worth publishing at NIPS.

Minor comments:
Eq. (13) => Eq. (5) (several times)

After rebuttal:

I thank the reviewers for their response.","This paper propose to adapt a method build surrogate losses based on adversarial formulation from binary classification to ordinal regression. A surrogate loss is proposed and is claimed to be consistent with the absolute error. Then, a set of experiments proves it to be competitive with the state-of-art algorithms of the task.

- Presenting the 3 main theorems of the paper without any sketch of proof or simplified proof or intuition of proof in the paper bothered me. Make sure to add sketches of proofs in the final version.

Content:
- Despite the form, I'm convinced by the approach as it is an adaptation of the binary case. Which limits the impact of the paper in my opinion.
- The paper proved the approach benefits from similar theoretical guarantees as state-of-art (such as Fisher consistency).
- The experimental results show a competitive performance, without showing a significant improvement over the RED baseline."
Self-Supervised Intrinsic Image Decomposition,"Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Josh Tenenbaum",https://proceedings.neurips.cc/paper/2017/hash/c8862fc1a32725712838863fb1a260b9-Abstract.html,"The paper proposes a novel neural network-based method to carry out intrinsic image decomposition, that allows it to benefit from training on both labeled images (i.e., with ground truth shape / lighting / reflectance maps available) and un-labeled images (where only the image itself is provided). Specifically, it trains a network that takes an image as input and produces shape / lighting / reflectance maps, using a loss on these output maps when ground truth is available, and also with an ""autoencoder"" loss by rendering / recombining the predicted maps to produce the an image and comparing it to the input.

While the auto-encoder loss by itself would clearly be insufficient (a network that learned to produce flat shading and reflectance = input image would minimize this loss perfectly), the paper shows when used to augment a supervised training set with a broader set of unlabeled images, it can lead to improved performance and the ability to deal with more diverse scene content.

Overall, the paper demonstrates a new and interesting learning-based approach to intrinsic image decomposition. My only comment about the paper is that it seems to lack a more standard evaluation/comparison to existing algorithms. While the comparisons to SIRFS in Table 1 / Fig 5 are useful, results on a more standard dataset (like the MIT intrinsic image set) would allow the reader to gauge how the method performs in comparison to the larger set of intrinsic image algorithms out there.
","Paper describes a method to recover intrinsic images (albedo, shading) using a clever autoencoder formulation.
One autoencoder produces an albedo, a shape model, and a lighting representation.  Another combines the shape model
and lighting representation to produce a shaded shape (it's essentially a learned renderer).  This shaded shape is then combined
with the albedo to produce a predicted image.  This collection of autoencoders is trained with a set of synthetic
example albedos and shading representations, together with their renderings, and with images.  The synthetic data
encourages the intermediate representations to take the right form; the real data causes the autoencoders to behave
sensibly.

The idea is novel, and the approach seems to work well and to generalize.  Comparison to SIRFS is successful, but
likely somewhat unfair.  SIRFS isn't really trained (there's some adjustment of prior parameters), and doesn't have
much access to a shape prior.  This process has a strong and important shape prior, though transfer to new
shapes is quite successful.

The final system is somewhat conditioned on category, and must be updated when the category changes (l222 et seq.).
This is a problem, but likely opens room for new ideas: for example, one might set recognition before intrinsic image
recovery rather than after.

In summary, paper contains a novel method for an established problem; offers results that suggest the method is
competitive with the best; and has lots of room for expansion.  It should be accepted.","The paper presents an interesting approach on the intrinsic image decomposition problem: given an input rgb image, it decomposes it first into shape (normals), reflectance (albedo) and illumination (point light) using an encoder-decoder deep architecture with 3 outputs. Then there is another encoder-decoder that takes the predicted normals and light and outputs the shading of the shape. Finally, the result comes from a multiplication between the estimated reflectance (from the 1st encoder-decoder) with the estimated shading. 

The idea of having a reconstruction loss to recover the input image is interesting, but I believe that is only partially employed in the paper. The network architecture still needs labeled data for the initial training. Also, in lines 164-166 the authors mention that the unlabeled data are used together with labeled so the ""representations do not shift too far from those learnt"". Does this mean that the unlabeled data corrupts the network in a way that the reconstruction is valid but the intermediate results are not correct? That implies that such data driven inference without taking into account the physical properties of light/material/geometry is not optimal.

I found difficult to parse the experiments and how the network is being deployed. How are the train/validation/test sets being created? For example, in 4.1 the motorbike numbers correspond to a test set of motorbikes that were not seen during the initial training (it is mentioned that the held-out dataset is airplanes)? And for decomposing a new shape into its intrinsic properties, do you directly take the output of the 3 decoders, or you have another round of training, but with including the test data. 

Another important limitation is the illumination model being just a single point light source. This simple illumination model (even a spotlight) can not be applied in real images. 

Regarding that, the experiments are only on the synthetic dataset proposed in the paper and there is no comparison with [13, 15] (or a discussion why it is missing). Without testing on real images or having a benchmark such as MIT intrinsics, its difficult to argue about the effectiveness of the method outside of synthetic images.

Also, Figure 3 is the output of the network or is it an illustration of what is not captured by simple lambertian shading?

There is some related work that is missing both for radiometric and learned decomposition into intrinsic properties.
Decomposition into shape/illumination/reflectance:
Single Image Multimaterial Estimation, Lombardi et al, CVPR 12
Shape and Reflectance Estimation in the Wild, Oxholm et al, PAMI 2016, ECCV12
Reflectance and Illumination Recovery in the Wild, Lombardi et al, PAMI 2016, ECCV 12
Deep Reflectance Maps, Rematas et al, CVPR 2016
Decomposing Single Images for Layered Photo Retouching, Innamorati et al, CGF 2017 (arxiv 16)
Deep Outdoor Illumination Estimation, Hold-Geoffroy et al, CVPR 2017 (arxiv 16)

Learning shading using deep learning:
Deep Shading: Convolutional Neural Networks for Screen-Space Shading, Nalbach et al CGF 2017 (arxiv 16)


"
On-the-fly Operation Batching in Dynamic Computation Graphs,"Graham Neubig, Yoav Goldberg, Chris Dyer",https://proceedings.neurips.cc/paper/2017/hash/c902b497eb972281fb5b4e206db38ee6-Abstract.html,"
The paper presents an automatic mechanism for converting unbatched computation
graphs into batched computation graphs for efficient execution.  The method
works on dynamically specified graphs, and is therefore applicable to neural
networks on strangely shaped data such as trees and graphs of varying topology.
Since finding an ideal batching structure is NP hard, the paper proposes a few
simple heuristics: these are not optimal but capture the bulk of the speedup
due to batching.

I am extremely happy with this paper.  As someone who has worked with neural
networks on tree shaped data, I can attest that batching is the source of most
of the complexity.  Static graph-based techniques such as TensorFlow Fold work
but require significant refactoring of the code.  This refactoring then
complicates mundane issues, such as running out of memory when a batch of trees
contains many large trees.

My main request is a discussion of how best to deal with memory limits.  The
issue is that a batch of 32 trees requires a varying amount of memory depending
on how large the trees are (the same applies to sequences, graphs, etc.).  The
easy approach is to try with a large batch and shrink if one runs out of memory,
though the easiest way to do this changes the behavior of the estimator.  Thus,
different problems may warrant different ways of handling memory limits, and I
am curious if the flexibility of the proposed scheme would hold up.

Finally, a quantitative discussion of the cost of copying data into contiguous
form would be interesting.  Two specific statistics include (1) the fraction of
elements that need copying and (2) the proportion of total time spent copying.
In the worst case this copying could double the memory bandwidth; presumably the
common case is much better, and I am curious how much.
","Summary:
The authors of this paper extend neural network toolkit DyNet with automatic operation batching. Batching enables efficient utilization of CPUs and GPUs by turning matrix-vector products into matrix-matrix products and reducing kernel launch overhead (for GPUs) but it is commonly done manually. Manual batching is manageable for simple feed-forward-networks but it becomes increasingly a headache as we explore more flexible models that take variable-length input, tree-structured input, or networks that perform dynamic control decisions.

Chainer, DyNet, and PyTorch are recently proposed neural network toolkits that allow user to dynamically define the computation graph using the syntax of the host language (if, while, etc in python). This is desirable as it avoids tookit specific constructions (e.g., cond in TensorFlow) and make the network definition intuitive but it tends to limit performance because the network construction and computation happens at the same time. Thus although it would be straightforward to program a control flow that supports variable length or tree-structured input in these toolkits, in practice it would be too inefficient to process single instance at a time.

The  key contribution of this paper is to delay and automatically batch the computation so that a user can still define an arbitrary complex control flow using the host language as in original DyNet without worrying about operation batching. The approach is similar to TensorFlow Fold (TFF) but differs in two places: first, the computation graph is defined in a dynamic manner using the control flow of the host language and the user does not need to learn any new language (note that TFF introduces many new operations that are not in TensorFlow); second, it employs an agenda-based approach for operation batching in contrast to the depth-based approach employed in TFF. The authors empirically show that agenda-based batching is slightly more efficient than depth-based batching.

Detailed comments:
It wasn't clear to me if there is a stronger argument for dynamic definition of the computation graph other than the simplicity of coding compared to static definition of the graph in TFF, for example.

Would the lazy evaluation still work when the control decision depends on the computed value (this is the case for the training with exploration scenario [2, 4, 9]. In this case, static graph definition may have an advantage.","This paper presents a solution to automatically do the operation batching in dynamic computation graphs. Their procedure contains three steps: graph definition, operation batching and determining execution order. They propose a ByAgenda batching algorithm and shows its superiority than the ByDepth algorithm in TensorFlow Fold. The problem is important and the solution sounds. 

One thing I am not sure about is the contribution other than the ByAgenda batching algorithm. Does TF Fold also utilizes the techniques in this paper about graph definition and determining execution order? I suspect the contribution of this paper is a little thin if it is all about a ByAgenda batching algorithm. "
Fitting Low-Rank Tensors in Constant Time,"Kohei Hayashi, Yuichi Yoshida",https://proceedings.neurips.cc/paper/2017/hash/c930eecd01935feef55942cc445f708f-Abstract.html,"Motivated by the problem of selecting an appropriate Tucker rank for low-rank tensor approximation, the authors provide an algorithm for estimating the RMSE of a tensor at a given Tucker rank. The algorithm is simple: randomly sample a subset of the indices in each mode, restrict to the resulting tensor, and compute the RMSE for the given Tucker rank of this smaller tensor. 

Their theory argues that the number of samples needed to get a good additive error approximation to the RMSE at a given Tucker rank depends on intrinsic parameters of the problem rather than the dimensions of the tensor: in particular, on the order of the tensor, the largest magnitude entry in the tensor (its spikiness), the spikiness of the Tucker core tensor of the original tensor, and the spikiness of the Tucker core tensor of the sampled tensor. 

The experimental results are impressive, and show that not only does this algorithm provide good estimates of the RMSE at given Tucker ranks (compared to the HOOI baseline), it also preserves the ordering of the Tucker ranks by the RMSEs this achieve. The latter point is crucial for model selection. 

This work is interesting and worth publishing in NIPS because it tackles the problem of rank selection for low-rank tensor approximation in a novel way, provides theoretical guarantees that seem reasonable, and the resulting algorithm is both fast and does a good job of rank selection. The main drawback of this paper is that the theory is presented in a non-intuitive manner that makes it hard to understand the connection that the authors draw between tensors and dikernels and how it is useful in establishing the main result (I did not read the supplementary material).

- I would like to see the authors address the dependence on the spikiness of the Tucker core tensor of the subsampled tensor: is there a reason this should be here, or can it be removed? can this spikiness be bounded in terms of properties of the original tensor, or can it be arbitrarily poorly behaved?
- It would be useful to state your theorem 3.4 for a matrix first, so the readers can use their experience in that domain to understand the quantities involved in your bounds 
- The argument in line 162 that Lemma 3.3 implies the following equation is not clear to me, since it seems to ignore the effect of the measure-preserving bijection. I did not check the supplementary material to see if this argument is made clearer, but if so, please state this in the paper.

Another point:
- I would like to see bold and italicization in Tables 2--4 to indicate the best and second best performing methods, as is standard


","The paper proposes to approximate the minimum value of the Tucker tensor decomposition optimization problem (as this is directly related to Tucker rank). The algorithm the paper proposed is very efficient. The problem is indeed interesting and useful. The theoretical contribution is indeed novel and is based on the weak version of Szemerédi’s regularity lemma. Experiments demonstrate the practical usefulness of the algorithm.

Summary. I believe that the algorithm is novel and the theoretical expositions interesting, useful and important. I believe that this a very good paper to be presented in NIPS.

I believe that the paper should be accepted in the main programme. 
","Summary:
This paper introduces an approximation algorithm 'sample_s' to approximate the original tensor X using an a constant small-sized sub-tensor X_tilt, which is 'extracted' from the original via the random sampling of sub-indices, such that the residual error of Tucker decomposition of X_tilt can stay close to that of X. And the algorithm depends only on the sampling size 's', which is free of the original tensor size 'N'.

Strength:
1. By adapting the concept of 'dikernels' (and the associated theoretical result) to tensor representation, this paper provides theoretical analysis that characterizes the bounds of the residual error of tensor for the proposed algorithm.

2. The paper is clearly written, both the motivation and proposed solution can be easily grasped from the beginning of the text.


Weakness:
1. The real applications that the proposed method can be applied to seem to be rather restricted. It seems the proposed algorithm can only be used as a fast evaluation of residual error for 'guessing' or 'predetermining' the range of Tucker ranks, not the real ranks.

2. Since the sampling size 's' depends on the exponential term 2^[1/(e^2K-2)] (in Theorem 3.4), it could be very large if one requires the error tolerance 'e' to be relatively small and the order of tensor 'K' to be high. In that situation, there won't be much benefits to use this algorithm.

Question:
1. In the Fig.1, why blue curve with large sample size 's=80' achieves the worst error compared with that of red curve with small sample size 's=20'?

Overall, although proposed algorithm is theoretically sound, but appears be limited in applications for practical propose.
"
Random Projection Filter Bank for Time Series Data,"Amir-massoud Farahmand, Sepideh Pourazarm, Daniel Nikovski",https://proceedings.neurips.cc/paper/2017/hash/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html,"This paper proposes a new algorithm for learning on time series data. The basic idea is to introduce random projection filter bank for extracting features by convolution with the time series data. Learning on time series data can then be performed by applying conventional machine learning algorithms to these features. Theoretical analysis is performed by studying both the approximation power and statistical guarantees of the introduced model. Experimental results are also presented to show its effectiveness in extracting features from time series.

Major comments:
  (1) It seems that the algorithm requires the information on both $M$ and $\epsilon_0$, which depend on the unknown target function $h^*$ to be estimated. Does this requirement on an apriori information restrict its practical application? Since $Z_i'$ are complex numbers, the hypothesis function in (9) would take complex outputs. It is not clear to me how this complex output is used for prediction.
  (2) It is claimed that the discrepancy would be zero if the stochastic process is stationary. However, this seems not to be true as far as I can see. Indeed, the discrepancy in Definition 1 is a random variable even if the stochastic process is i.i.d.. So, it depends on the realization of $X$. If this is true, Theorem 1 is not clear to understand since it involves a discrepancy which would not be zero in the i.i.d. learning case.
  (3) There exists some inconsistences in Algorithm 1 and materials in Section 4. Algorithm 1 is a regularization scheme while ERM is considered in Section 4. There are $m$ time series in Algorithm 1, while $m=1$ is considered in Section 4. The notation is not consistent, i.e., $f$ is used in Algorithm 1 and $h$ is used in Section 4.
  (4) The motivation in defining features in Section 3 is not clear to me. It would be better if the authors can explain it more clearly so that readers without a background in complex analysis can understand it well.

Minor comments:
  (1) line 28: RKSH
  (2) line 65, 66: the range of $f\in\mathcal{F}$ should be the same as the first domain of $l$
  (3) eq (11), the function space in the brace seems not correct
  (4) the notation * in Algorithm 1 is not defined as far as I can see
  (5) $\delta_2$ should be $\delta$ in Theorem 1
  (6) line 254: perhaps a better notation for feature size is $n$?
  (7) Appendix, second line in equation below line 444, a factor of $2$ is missed there?
  (8) Appendix, eq (28), the identity should be an inequality? I suggest the authors to pay more attention to (28) since the expectation minus its empirical average is not the same as the empirical average minus expectation.
","The paper develops a technique for extracting features from sequential time series data analogous to (in a sense, generalizing) ideas from random projections for dimensionality reduction and random features for kernel space approximation. At a high level view, the algorithm produces a set of (randomized) feature maps by projecting the input sequence onto randomly generated dynamical filters, which in turn are argued to approximate a certain class of dynamical filters.

I unfortunately lack the fluency in dynamical systems needed to evaluate the mathematical novelty/significance and the correctness of the approach. I had to take several of the paper's claims at face value. From the point of view of statistical learning, and as a possible user, I find the overall idea intriguing and I found the paper's high-level organization clear.

A few suggestions for improvement, or possible concerns, primarily bearing on novelty/significance for a machine learning audience:

1. The experiments are described without background. There is a missing mention of scale, and whether these tasks are considered challenging (or are closer to being small-scale benchmarks), and this background seems more crucial for the wider NIPS audience. The introducing presents the low-computational-power setting as motivation for this work, so to what extent do these datasets represent tasks relevant to a computationally constrained setting? Also, an interpretation of the results is missing, which takes away from the significance signal.

2. It seems worth discussing (or evaluating?) alternatives in the wider context of machine learning today. This paper argues that it provides a good data-independent representation when RNNs are too compute-intensive (line 39). Concretely, what justifies this claim? And when computational resources are not a tight concern, are the two approaches at all in the same task performance ballpark? One could optimistically hope for a positive result that provides a decent baseline for consideration when training RNNs.

Minor/specific:
- Line 21: ""convoluted"" -> ""convolved""?
- Line 39: ""power methods"" -> ""powerful methods""?
- Line 40: why might RNNs be computationally infeasible? Any example?
- Line 62: why have $Y_{i,T_{i}} = +1$?
- Line 114: ""prune"" -> ""prone""?","*** Following authors feedback that addressed two of my main concerns about experiments and theory I am updating my rating ***

This paper proposes a feature extraction mechanism that applies an ensemble of order one autoregressive filters whose poles are randomly sampled from the unit circle. The authors show that such filters span a higher order filter. These features can be plugged into a conventional machine learning algorithm to construct a predictive model from a time series to a target variable. The authors provide a finite sample error bound.

The paper is well written. The idea to adapt random projection methods to time series by using random filters is interesting and the method outperforms fixed window based features. However, to demonstrate the usefulness of this approach, the proposed method should be compared to other methods that can automatically learn how to integrate the history of signal. A natural family of methods to compare, are those based on state space representations, such as simple RNNs or the Statistical Recurrent Unit [Oliva etal. 2017]. 

The suggested method applies to univariate signals because the generated filter bank is univariate. It would be interesting to pursue how does this approach scale up to multi-dimensional signals, to increase the significance of this paper.

The error bound requires some clarifications: 
(1) Does it apply to case 2 as well as case 1? 
(2) n appears in the numerator of the first term as well as in the denominator of the second one, which may reflect a bias-variance tradeoff in the choice of number of filters. This trade-off implies that an optimal choice of n depends on T^(1/3). If so, how does the method handle multiple time series with varying lengths? 
(3) Following (2), is the number of filters a hyperparameter that should be tuned by cross validation? 

In the experimental section, is the task is to classify entire sequences or to monitor the rotating system at every time point? In case it’s the entire sequence, how is it done in practice, do you consider the label of the last observation Y_t as the label of the entire series, or do you exploit feature associated with earlier time points? 
Does the dataset include three time series or three sets of time series? In each case, how was it split to train and test? 

To summarize, this is an interesting manuscript, but more work is needed to mature the proposed methodology."
Dynamic Revenue Sharing,"Santiago Balseiro, Max Lin, Vahab Mirrokni, Renato Leme, IIIS Song Zuo",https://proceedings.neurips.cc/paper/2017/hash/cb8acb1dc9821bf74e6ca9068032d623-Abstract.html,"In online display advertising, companies such as Google, Facebook organize a marketplace to connect publishers (sellers) and advertisers (buyers). In this type of auction, the publishers can specify a cost which he/she wants to be paid for showing an ad on his/her page. The auction designer often ads value to the transaction through e.g. targeting etc and hence also wants to benefit from the transaction. Often, this is done through a revenue sharing scheme where the auction designer takes a fraction of the revenue paid by the buyer as profit and passes the rest on to the seller.

In this paper, the authors introduce a ""dynamic revenue sharing"" scheme. The idea here is to adjust the revenue fraction which the auction designer keeps, in a dynamic way, so that his/her revenue is increased and possibly fill rates are increased.

The authors propose two different schemes. In the single period revenue sharing scheme, the minimum revenue sharing fraction needs to be met for every transaction - although the revenue share can be increased to optimize long term profits. In the multi-period revenue sharing scheme, the revenue sharing fraction only needs to be met in expectation across a large number of transactions.

The authors derive the optimization solution in the single period revenue sharing scheme and a heuristic (but asymptotically optimal) solution in the multi period revenue sharing setting. Finally, the authors describe a comparative analysis and empirical evaluation.

Main comments:
- The paper is well written and provides a lot of good intuition.
- I believe dynamic revenue sharing is a very interesting idea with practical applications.
- Line 213: it is not clear to me why in the formula for r^*, we are using \Pi(r,0) rather than \Pi(r,c)?
- The paper which is presented as such feel a bit incomplete. Section 6 and section 7 feel a bit rushed and don't provide a lot of insight.

Minor comments:
- Line 13: missing reference numbers.
- Line 49: declared -> declare
- Line 141: maybe clarify what the Myerson reserve price is here?","This paper studies the problem of sharing revenue between the ad exchange platform and the web publishers in ad exchange auctions. In practice, any revenue sharing mechanism has to satisfy certain properties: The payment to the web publisher should cover the publisher’s cost and the exchange can keep at most an alpha fraction of the revenue. The authors consider the single-stage setting and the multiple stage setting. For the single stage setting they show how to compute the optimal payments to the publisher. For the multiple stage setting, they introduce a dynamic revenue sharing scheme that balances the constraints over multiple periods of time and guarantees them in expectation or high probability. The paper also examine the implications of these results via experiments.


I find the paper to be difficult to read at some points. One problem is that the paper has quite a lot of jargon e.g., elastic demand, thick/thin markets, opportunity cost, etc. These make the paper difficult to read for the NIPS audience. Another problem seems to be that the authors have tried to include more results that they can nicely discuss in the paper. This has lead to some sections losing their clarity. For example, section 6.1 or 7 need more details.

Overall I like the direction the paper is taking. It is quite natural to allow the constraints to be met with high probability (less so in expectation) over multiple periods of time. So, it is interesting but not surprising to see that in the multi-stage setting optimal policy achieves better utility. The authors formalize the difference between these utilities via upper and lower bounds in proposition 5.2. Let me emphasize that in the multi-stage setting, the policy still appears to be static, it is the constraints that are met to varying degrees dynamically. If this is indeed the case, it would be helpful for the authors to clarify early in the paper and make additional comparisons to when the policy does not have to be static. It appears to me that by allowing a dynamic policy one can get strong concentration bounds for satisfying the publisher’s constraints. This might be the insight behind section 6.1, but there is not enough information provided in the section to verify this point. 

The paper also includes empirical evaluation of the proposed revenue-sharing policies. However, very little information regarding these experiments are included in the main body of the paper, so it’s hard to get a sense of the significance of the empirical results. Given that the authors have some space left in the paper, I suggest that they include more information about these experiments. This is the main section that connects the results of the paper with data science and machine learning, which is a major interest of the NIPS community, whereas the rest of the results mentioned in the paper do not have a learning flavor. 

After the rebuttal:
It would be great if the authors add more details to the experiments section in the NIPS version of the paper. It would be good to also improve the readability. See the 2nd paragraph of my review for specific points that can be improved.","This paper studies marketplaces where there is a single seller, multiple buyers, and multiple units of a single good that are sold sequentially. Moreover, there is an intermediary that sells the item for the seller. The intermediary is allowed to keep some of the revenue, subject to various constraints. The underlying auction is the second-price auction with reserve prices. The intermediary must determine how to set the reserve price in order to maximize its profit. Since this auction format is strategy-proof for the buyers, they assume the buyers bid truthfully. They also make the assumption that the seller will report its costs truthfully.

First, the authors study “single period revenue sharing,” where at every time step, the intermediary can keep at most an alpha fraction of the revenue and it must cover the seller’s reported cost if the item is sold. Next, they study “multi period revenue sharing,” where the aggregate profit of the intermediary is at most an alpha fraction of the buyers’ aggregate payment and the costs must be covered in aggregate as well. For both of these settings, the authors determine the optimal reserve price. Theoretically, the authors show that multi period revenue sharing has the potential to outperform single period revenue sharing when there are many bidders (the second-highest bid is high), and the seller’s cost is “neither too high nor too low.” They analyze several other model variants as well.

The authors experimentally compare the optimal pricing policies they come up with for the various models with the “naïve policy” which sets the reserve price at c/(1-alpha). Their experiments are on real-world data from an ad exchange. They show that the single period and multi period policies improve over the naïve policy, and the multi period policy outperforms the single period policy.

This paper studies what I think is an interesting and seemingly very applicable auction format. It’s well-written and the results are not obvious. I’d like to better understand the authors’ assumption that the seller costs are reported truthfully. The authors write, “Note that the revenue sharing contract guarantees, at least partially, when the constraint binds (which always happens in practice), the goals of the seller and the platform are completely aligned: maximizing profit is the same as maximizing revenue. Thus, sellers have little incentive to misreport their costs.” I think this means that in practice, the cost is generally so low in contrast to the revenue that the revenue sharing constraint dominates the minimum cost constraint. This doesn’t really convince me that the seller has little incentive to misreport her cost as very high to extract more money from the exchange. Also, as far as I understand, this statement detracts from the overall goal of the paper since it says that in practice, it’s always sufficient to maximize revenue, so the intermediary can ignore the sellers and can just run the Myerson auction. The application to Uber’s platform is also a bit confusing to me because Uber sets the price for the passenger and the pay for the driver. So is Uber the intermediary? The drivers seem to have no say in the matter; they do not report their costs.

=====After author feedback=====
That makes more sense, thanks."
Prototypical Networks for Few-shot Learning,"Jake Snell, Kevin Swersky, Richard Zemel",https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html,"Strengths:
- The paper presents a simple, elegant and intuitive approach to few-shot learning.
- The performance is strong compared to much more complex systems that have been proposed in the past.
- I happen to have tried prototype networks on a low-shot learning setup of my own. I can attest that (a), it took me all of half an hour to implement it, and (b) with little tuning it worked quite well albeit a bit worse than a tuned matching networks implementation.
Weaknesses:
- I am curious how the performance varies quantitatively if the training ""shot"" is not the same as ""test"" shot: In realistic applications, knowing the ""shot"" before-hand is a fairly strong and impractical assumption.
- I find the zero-shot version and the connection to density estimation a bit distracting to the main point of the paper, which is that one can learn to produce good prototypes that are effective for few-shot learning. However, this is more an aesthetic argument than a technical one.","This paper proposes a simple extension of Matching networks for few-shot learning. For 1-shot learning, the proposed method and Matching networks coincide. An insightful interpretation as a mixture of density estimation is also presented.
For 5-shot learning, prototypical networks seem to work better while being more efficient. Also, they can deal with zero-shot classification. Overall, the paper is well written, and the material is well presented. Nothing earth-shattering, but an interesting contribution. ","Summary: This paper addresses he recently re-popularised few-shot classification task. The idea is to represent each class as its mean/prototype within a learned embedding space, and then recognising new classes via softmax over distances to the prototypes. The model is trained by randomly randomly sampling classes and instances per episode. This is appealing due to its simplicity and speed compared to other influential few-shot methodologies [21,28]. Some insights are given about the connection to mixture density estimation in the case of Bregman divergence-based distances, linear models, and matching networks. The same framework extends relatively straightforwardly to zero-shot learning by making the class prototype be the result of a learned mapping from meta-data like attributes to the prototype vector. The model performs comparable or better than contemporary few/zero-shot methods on Omniglot, miniImageNet, and CUB. 

Strengths/Weaknesses:
+ Intuitive and appealingly elegant method, that is simple and fast.
+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.
+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.
+ Good results
- Some other design decisions (normalisation; number of training classes per episode,  etc) less well explained. How much of good results is the proposed method per-se, and how much of it is tuning this stuff?
- Why the zero-shot part specifically works so well should be better explained.

Details:
- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPR’16, Zhang & Salgrama ECCV’16)
- This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The “center” in the above paper matches “prototype”. Probably this connection should be cited.
- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are “attribute”. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in “Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016”. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings.


Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.
"
Unsupervised learning of object frames by dense equivariant image labelling,"James Thewlis, Hakan Bilen, Andrea Vedaldi",https://proceedings.neurips.cc/paper/2017/hash/cbcb58ac2e496207586df2854b17995f-Abstract.html,"This paper extends the work in [30] to learn a dense object coordinate frame given pairs of images and dense correspondences between the pairs.  The key insight is to include a distinctiveness constraint that encourages the correspondence error to be small when label vectors in the object coordinate frame are close.  The approach is evaluated on a synthetic toy video sequence of a 2D robotic arm and a dataset of faces, and compared against [30] for sparse correspondences.  

Strengths:

The approach is interesting and novel, as far as I’m aware.  The writing and references look good.  There aren’t major glaring weaknesses in my view.

Potential weaknesses (if there’s space, please comment on the following in the rebuttal):

W1. Loss (5) seems to have a degeneracy when v==gu, i.e., p(v|u) term is ignored.  While this doesn’t seem to affect the results much, I’m wondering if the formulation could be slightly improved by adding a constant epsilon so that it’s (||v-gu||^\gamma + epsilon) * p(v|u).

W2. L170-172 mentions that the learning formulation can be modified so that knowledge of g is not required.  I find having g in the formulation somewhat unsatisfying, so I’m wondering if you can say a bit more about this possibility.

W3. The shown examples are approximately planar scenes.  It might be nice to include another toy example of a 3D object, e.g., textured sphere or other closed 3D object.

Minor comments/suggested edits:

+ There are a number of small typos throughout; please pass through a spell+grammar checker.

+ Equation after L123 - It isn't clear what alpha is on first read, and is described later in the paper. Maybe introduce alpha here.

+ L259 mentions that the approach simply observes a large dataset of unlabelled images.  However, flow is also required.  Perhaps rephrase.","+ intersting idea

- confusing exposition
- insufficient experiments

I like the idea of embedding all points in an image onto a sphere in order to do image alignment and keypoint tracking. I have not seen this before and the preliminary results seem to indicate that there is some promise. It's actually quite surprising that the idea works. All the authors do is say that the embedding should distort with the image. Why doesn't the network learn to embed different images as rotations of each other? Why does a consistent reference frame emerge? Would it work without optical flow examples, or does the optical flow do most of the work?

The exposition of the paper is a bit confusing. Starting with the title: What is ""object learning""? The authors might want to just call it embedding learning. What is a ""dense equivariant image labeling""?  Why is it ""Unsupervised"" learning when a ""labeling"" is involved? The authors might want to use the term weakly supervised.
The language issues continue the the abstract.
In the technical section some details are missing. How is (2) optimized? Or is it optimized at all? If not what is the connection to (4) and (5)? 

the main issue in the paper is the evaluation. The toy example is of little use, unless it's used to compare to prior work. The same is true for the main evaluation. There the authors compare to [30] (which works better), but they do not compare to other alignment baselines. How would the presented approach compare to [18] or [35] or vanilla optical flow? How sensitive is the presented approach to the training data? Does it heavily rely on optical flow, or would it work without it?

Post rebuttal:
The rebuttal clarified most of the concerns above. I like the new experiments, which should definitely find their way into the final paper.","Blue565
Unsupervised object learning from dense equivariant image labelling

An impressive paper, marred by flaws in exposition, all fixable.

The aim is to construct an object representation from multiple images, with dense labelling functions (from image to object), without supervision. Experiments seem to be very successful, though the paper would be improved by citing (somewhat) comparable numerical results on the MAFL dataset. The method is conceptually simple, which is a plus. The review of related methods seems good, though I admit to not knowing the field well enough to know what has been missed.

Unfortunately, insufficient details of the method are given. In particular, there is no discussion of the relationships between the loss functions used in practice (in (4) and (5)) and those in the theoretical discussion (in page 4, first unnumbered equation and (2)).

Also, I have some concerns about the authors’ use of the terms “equivariant” and “embedding” and their definition of “distinctive”. The standard definition of “equivariant” is: A function f is equivariant with respect to group of transformations if f(gx) = gf(x) for all x and all transformations g.
(We note that the usage of “equivariant” in [30] is correct.)
In contrast, f is invariant if f(gx) = f(x) for all x and all transformations g. 
Your definition in (1) says that \Phi is invariant with respect to image deformations. So please change the manuscript title!
Note that both equivariance and invariance are a kind of “compatibility”.

The word “embedding” usually means an embedding of an object into a larger space (with technical conditions). This doesn’t seem to be how the word is used here (though, OK, the sphere is embedded in R^3). The manuscript describes the map \Phi as an embedding. Perhaps “labelling” would be better, as in the title.

The definition of “distinctive” on p4, line 124, is equivalent to: invariance of \Phi and injectivity of \Phi(x, \cdot) for every x.

On p4, line 27:  for a constant labelling, argmax is undefined, which is worrying. 

Overall the quality of the English is good, however there are several typos that should be fixed, a few of which are noted below.

The paper is similar to [30], which is arXiv 1705.02193 (May 2017, unpublished as far as I know). They have significant overlap, however both the method and the datasets are different.

—Minor comments.
p1, line 17: typo: “form”->”from”
p2, Figure 1 caption: 
- should be “Left: Given a large number of images”
- typos: “coordiante”, “waprs”
p4, first displayed equation: \alpha undefined. should be (x,g)
p5, line 142: \Phi(x,u) has become \Phi_u x without comment.
Fig 3 caption doesn’t explain clearly what the 2nd and 3rd rows are.
"
Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays,"Cesar F. Caiafa, Olaf Sporns, Andrew Saykin, Franco Pestilli",https://proceedings.neurips.cc/paper/2017/hash/ccbd8ca962b80445df1f7f38c57759f0-Abstract.html,"The paper propose an efficient approximation of the ""Linear Fascicle Evaluation model"" (LiFE), which is used for tractography. The key issue with the LiFE model is that it computationally scales poorly such that it is impractical to analyze MRI data at full resolution. The authors propose a sparse approximation based on a Tucker model (tensor model). The authors prove that the approximation is both good and achieves a high compression; this is further evaluated experimentally. The model appears solid and likewise for the analysis. My key objection to the paper is that the actual evaluation of the model for tractography is treated very superficially (just a couple of lines of text). I understand that within the tight boundaries of a NIPS paper, the authors does not have much space for such an evaluation, so I'm willing to forgive this limitation of the paper.

Other than that, the paper is very well written with intuitive figures guiding the reader along the way. As a non-expert, I found the paper a please to read. I do stress that I'm not an expert in neither tractography or tensor models.

A few minor comments (not important in the rebuttal):

*) In line 36, the authors point out that tractography is poorly understood and that competing methods can give wildly different results. The authors then argue that this motivates convex models. I don't buy this argument -- the reason different models give different results is more likely due to different modeling assumptions, rather than whether a global optimum has been found or not.

*) After the propositions: the authors take the time to explain in plain words the key aspects of their propositions. Most authors neglect this, and I just want to say that such efforts make the paper much easier to read. So: thank you!

*) In line 195, you write: ""...dataset Fig. 4.a."" There's something wrong with that sentence. Perhaps the figure reference should be in a parenthesis?

*) In line 202: typo: ""aproach"" --> ""approach"".

== Post rebuttal ==

I have read the other reviews and the rebuttal, and stand by my evaluation that this paper should be accepted.","The authors present a memory-efficient, approximate implementation of the LiFE method for track filtering/reweighing in diffusion MRI. The LiFE method takes any input set of white matter fibre tracks and assigns each of them a weight such that their overall distribution matches the apparent fibre density in the diffusion MRI data. In the original work, this was described as a large non-negative least squares (NNLS) problem. In this work, the design matrix is approximated using discrete atoms associated with fibre orientation, enabling to cast the NNLS problem as a memory-efficient, sparse tensor representation. Performance of the approximation, as well as memory efficiency of the tensor encoding, are demonstrated in human brain data.

The paper is well-written and clear. The conclusions are supported by the data. The work is an important contribution, of interest to the diffusion MRI tractography community. I have only 2 points of criticism:

1.  I disagree with the terminology ""decomposition"" and ""factorization"" in the title and throughout the text. This vocabulary is commonly associated with methods such as the singular value decomposition and nonnegative matrix factorization, in which a single matrix or tensor is represented as a linear combination of _unknown_ matrices or tensors, computed _simultaneously_ under certain constraints. In this work, matrix M and its sparse tensor representation D and Phi are precomputed as inputs to the method. At its core, LiFE(sd) is computing the remaining nonnegative weights, only one factor in the tensor representation.

2.  The stick model only accounts for restricted, intra-axonal diffusion, ignoring hindered extra-cellular diffusion or free water compartments. Furthermore, the axial diffusivity seems to be fixed at 1 ms/um^2 in this work. How appropriate is this model and its fixed parameter value for these data? At what b-value was the data acquired and is this sufficiently high to assume that CSF and extra-cellular diffusion are sufficiently attenuated?
 ","Summary:
This paper extends the matrix-based LiFE model, which can be used to represent the measured diffusion signal (dRMI) by the trajectories of fascicles contained in brain connectomes generated via tractography algorithm, to a tensor-based LiFE_sd model by factorizing the 3rd-order encoded candidate connectome tensor into coefficient core tensor Phi along with factor dictionary D and weight vector w. 
In this way, the LiFE_sd with compact multi-linear representation can significantly reduce the size of LiFE model while maintaining the approximation accuracy.


Strength:
1. The paper applies tensor decomposition methods on a novel application of estimating the brain contectome to improve the potentiality on the big data, which could be significantly important in neuroscience. 

2. Some analysis results such as accuracy and size compression are availbe when comparing with matrix-based model.

3. The paper is clearly written.


Weakness:
1. From the methodology aspect, the novelty of paper appears to be rather limited. The ENCODE part is already proposed in [10] and the incremental contribution lies in the decomposition part which just factorizes the M_v into factor D and slices Phi_v. 

2. For the experiment, I'd like to the effect of optimized connectome in comparison with that of LiFE model, so we can see the performance differences and the effectiveness of the tensor-based LiFE_sd model. This part of experiment is missing.
"
Random Permutation Online Isotonic Regression,"Wojciech Kotlowski, Wouter M. Koolen, Alan Malek",https://proceedings.neurips.cc/paper/2017/hash/cd3afef9b8b89558cd56638c3631868a-Abstract.html,"In this paper, the authors study isotonic regression on linear orders for online settings. The authors mention that it was shown that online isotonic regression can not be learned in a fully adversarial model, and this motivates them to tackle the problem
by proposing a more practical random permutation model. 

Some typos and grammatical inconsistencies aside, the paper is well written, clear and addresses an interesting topic. However, it is difficult to confirm the validity and the veracity of the claims formulated in the paper due to the absence of experiments.

To support the claims formulated in the paper it would be good to simulate certain examples and to demonstrate that the isotonic regression on linear orders operate in online settings. ","Summary: The authors study the online Isotonic regression problem under the random permutation model. The adversary picks the set of instances x_1, ..., x_n beforehand, and reveals it according to a random permutation. At round t, the instance x_i_t is revealed, and the algorithm predicts y-hat_i_t. Next, the adversary reveals y_i_t, and the algorithms incurs the squared loss (y-hat_i_t - y_i_t)^2. The goal is to minimize regret.

From a previous work, it is known that if the sequence of instances is arbitrary, the regret can be linear in the number of rounds. And the same work also studied the regret under the fixed design model where the sequence of instances is fixed and revealed to the algorithm beforehand, and gave an algorithm to achieve an optimal ~T^(1/3) regret.

In this work the authors present results in the significantly more general random permutation model, and present a (computationally inefficient) algorithm achieving optimal ~T^(1/3) regret, and show that a large family of efficient algorithms (Forward Algorithms) achieve ~T^(1/2) regret in the general case, but an optimal ~T^(1/3) regret in the case that revealed labels are isotonic.

Opinion: I think the problem of online Isotonic regression is interesting, and the authors present good results it in a very natural and compelling model (Esp given that the completely adversarial model is hopeless). The efficient algorithms presented do not achieve optimal bounds, but I think the authors make a solid contribution in understanding the behavior of a large class of algorithms, with good regret bounds; including conjecturing an efficient optimal algorithm. I think the techniques in this paper will be interesting and necessary for making further progress.
I recommend acceptance.

One concern: I would like the authors to spend some time explaining why this notion of regret is natural since the algorithm/learner is not constrained to isotonic outputs, it seems to me that the current notion of regret could be negative? 

Edit: I have read the authors' feedback, and the review stands unchanged","I am not an expert in online learning, and did not read the proofs in the appendix. My
overall impression of the paper is positive, but I am not able to judge the importance of
the results or novelty of the analysis techniques. My somewhat indifferent score is more a
reflection of this than the quality of the paper.

Summary: The authors study isotonic regression in an online setting, where an adversary
initially chooses the dataset but the examples are shown in random order. Regret is
measured against the best isotonic function for the data set. 

The main contribution of the paper seems to be in Section 4, i.e a class of ""forward
algorithms"" which encompass several well known methods, achieve sqrt(T) regret. The authors
also prove a bunch of complementary results such as lower bounds and results for different
settings and loss functions.

Can the authors provide additional motivation for studying the random permutation model?
I don't find the practical motivation in lines 46-47 particularly convincing. It might
help to elaborate on what is difficult/interesting for the learner this setting and what
makes the analysis different from existing work (e.g. [14]). 

Section 3: How do Theorems 3.1 and 3.2 fit into the story of the paper? Are they simply
some complementary results or are they integral to the results in Section 4?
- The estimator in (3) doesn't seem computationally feasible. Footnote 2 states that the
  result holds in expectation if you sample a single data and permutation but this is
  likely to have high variance. Can you comment on how the variance decreases when you
  sample multiple data points and multiple permutations? 

Clarity: Despite my relative inexperience in the field, I was able to follow most of the
details in the paper. That said, I felt the presentation was math driven and could be
toned down in certain areas, e.g. lines 149-157, 242-250 
 
While there seem to a some gaps in the results, the authors have been fairly thorough in
exploring several avenues (e.g. sections 4.3-4.6). The paper makes several interesting
contributions that could be useful for this line of research. 

----------------------------------------------------
Post rebuttal: I have read the authors' rebuttal and am convinced by their case for the setting. I have upgraded my score."
PRUNE: Preserving Proximity and Global Ranking for Network Embedding,"Yi-An Lai, Chin-Chi Hsu, Wen Hao Chen, Mi-Yen Yeh, Shou-De Lin",https://proceedings.neurips.cc/paper/2017/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html,"This paper proposes an unsupervised network embedding learning framework. The contributions include: a) designing an objective function based on matrix tri-factorization which can preserve the proximity and the global node ranking of the graph simultaneously; b) providing sufficient theoretical derivations to the objective function they designed; c) testing the performance of node embeddings, which are learned by optimizing the objective function through a neural network, on three real world datasets and several data mining tasks.
Pros:
+: This paper has good theoretical foundations, and the loss function can be well explained from the proximity preserving and global ranking preserving view.
+: The results are fairly strong on all the tasks and datasets.
Cons:
-: The paper puts too much space on theoretical derivations but little on experiments, and some lemmas are trivial thus should be omitted or concisely written, e.g., Lemma 3.5 and Lemma 3.6.
-: Part in Line 169 ~ Line 184 is not directly related to the final objective function. Maximizing the modularity is the special case of (6) when alpha = 1, and they don’t use this function as a part of the final loss function, thus I think this part is relatively irrelevant to the main framework.
-: Also it seems quite straightforward to incorporate node ranking information in their frameworks. The simplest way is to add a second objective, like the way the authors did in this paper, although the experiment results show that adding this part boosts the performance.
-: Some writing details should be improved. In the experiment part, results under different evaluation metrics should be listed separately. And I suggest to add one more baseline, i.e., GCN (Kipf and Welling (ICLR 2017)), which is a strong node embedding learning method based on graph convolution network.
","The paper presents a NN model for learning graph embeddings that preserves the local graph structure and a global node ranking similar to PageRank. The model is based on a Siamese network, which takes as inputs two node embeddings and compute a new (output) representation for each node using the Siamese architecture. Learning is unsupervised in the sense that it makes use only of the graph structure. The loss criterion optimizes a weighted inner product of the output representations (local loss) plus a node ranking criterion (global loss). Some links with a community detection criterion are also discussed. The model is evaluated on a series of tasks: node ranking, classification and regression, link prediction, and compared to other families of unsupervised embedding learning methods.
The paper introduces new loss criteria and an original method for learning graph embeddings. The introduction of a node ranking term based on the graph structure is original. The authors find that this term brings interesting benefits to the learned embeddings and allows their method to outperform the other unsupervised baselines for a series of tasks. The links with the community criteria is also interesting. On the other hand, the organization and the form of the paper makes it somewhat difficult to follow at some places. The motivation for using a Siamese architecture is not explained. Two representations are learned for each node (notations u and z in the paper). Since the z representation is a deterministic function of the u representation, why are these two different representations needed, and why not directly optimizing the initial u representation? The explanation for the specific proximity preserving loss term is also unclear. For example, it is shown that this criterion optimizes a “second-order” proximity, but this term is never properly defined. Even if the result looks interesting, lemma 3.4 which expresses links with a community detection criterion is confusing.

In the experimental section, a more detailed description of the baselines is needed in order to understand what makes them different from the proposed method. The latter outperforms all the baselines on all the tests, but there is no analysis of the reasons for this good behavior.

Overall the paper reveals intriguing and interesting findings. The proposed method has several original aspects, but the paper requires a better organization/presentation in order to fully appreciate the proposed ideas.
","This paper studied the network embedding problem, with focuses on proximity and global ranking preserve. The authors proposed an unsupervised Siamese neural network structure to learn node embedding following a generative fashion. They claimed that their model satisfies four characteristics, which are scalability, asymmetry, unity and simplicity.  
The Siamese neural network is designed as a multi-task learning framework, with shared hidden layers for embedding vectors of each pair of connected nodes.
The node ranking hidden layer is designed to encode the global ranking information, and the proximity hidden layer is designed for preserving local proximity information. As a result, this generative framework updates the two input embedding vectors using gradients propagated back from the output layers on an objective function consists of two parts of rewards. one for each task. The network design looks simple and robust with only one hidden layer, that leads to much fewer hyper-parameters to tune. And the authors provide detailed proof for second-order proximity preservation, and proof for global ranking preservation with a upper bound of PageRank. 

This paper is well-written and easy to follow. Figure 1 is nice and clean, which provides a short cut for reader to understand their model.  The experimental part is well-designed, the results demonstrated that the proposed embedding model outperforms its competitors on task like rank prediction, classification/regression and link prediction. And the authors also perform extra experiments to show that the proposed method is more robust to noisy data compared to the state-of-art. 

I found no obvious flaw in this paper during review, good presentation and technical quality. 


"
"Online to Offline Conversions, Universality and Adaptive Minibatch Sizes",Kfir Levy,https://proceedings.neurips.cc/paper/2017/hash/ce5140df15d046a66883807d18d0264b-Abstract.html,"The paper introduces methods for convex optimization which is based on converting adaptive online algorithms into offline methods. Concretely, it presents extensions and generalizaitons of AdaGrad [1] in the case of convex and strongly-convex functions in both offline and stochastic settings. The introduced methods guarantee favorable convergence results in all settings.

The paper is well-written and nicely organaized. In particular, the intuitive proof sketches makes it easy to follow/read. 

The paper provides supportive theoretical analysis and the proofs seem technically sound. The desirable convergence results of the generalizations of AdaGrad (AdaNGD, SC-AdaNGD, LazySGD) look theoretically signficant.


TYPOS:
LINE 192: well-known

","This paper considers two striking problems in optimizing empirical loss in learning problems: adaptively choosing step size in offline optimization, and mini-batch size in stochastic optimizations. 

With a black-box reduction to into AdaGrad algorithm and utilizing gradient normalization and importance weighting in averaging the intermediate solutions, this paper presents a variant of AdaGrad algorithm that attains same convergence rates as GD in convex and smooth optimization without the knowledge of smoothness parameter in setting the learning rate or any linear search (except strongly convex setting, where the algorithm still needs to know the strong convexity parameter H). In stochastic setting, the paper presents a LazySGD algorithm that adaptively chooses the mini-batch size according to the norm of sampled gradients.  The paper is reasonably well written, and the structure and language are good. The paper is technically sound and the proofs seem to be correct as far as I checked. The authors also included some preliminary experiments on a toy example to demonstrates some their theoretical insights. Overall, the paper is well written and has some nice contributions.
		

"
Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network,"Wengong Jin, Connor Coley, Regina Barzilay, Tommi Jaakkola",https://proceedings.neurips.cc/paper/2017/hash/ced556cd9f9c0c8315cfbe0744a3baf0-Abstract.html,"The paper proposes to model molecular reactions using a Weisfeihler-Lehman graph neural network, an architecture that was previously introduced as a neural network counterpart of the Weisfeihler-Lehman graph kernel.

The novelty of the paper resides mainly in the careful application of this neural network framework to chemistry, for predicting reaction centers and ranking chemical reactions. The paper is well written, and most of the neural networks architectural choices for each problem look sound.

In WLN+sumpooling, the sum-pooling of differences reduces to the difference of sums, which looses all spatial information. There seems to be an intermediate step of complexity which is therefore missing between the WLN+sumpooling and the WDLN. Applying at least one nonlinear transformation between the difference and pooling operations could have been considered.

The authors cite a number of relevant papers both in machine learning and chemistry. To those, one could have also added the original Weisfeiler-Lehman kernel paper as well as some recent papers that use similar graph neural networks (DTNN, MPNN) to predict molecular properties without the bond structure.","A deep learning approach is proposed for the application of predicting organic chemical reactions. Given a set of reactants, the algorithm first predicts the likely reaction centers, and then ranks the possible reactions involving those atoms.
 
This paper is well-organized, mostly clear, and makes a significant contribution. Chemical reaction prediction is an important application, and from a machine learning perspective this is a very interesting use of deep learning because of the unique structure of the data --- this paper nicely builds off recent work that uses neural networks to encode graph-structured data.
 
My main comment is that this work is very similar to that of Kayala et. al. (presented at NIPS 2011), who propose a very similar two-step process in which neural networks first predict reaction centers and then rank the predicted products. A number of the details are different in this paper, including the prediction of reaction centers as pairs of atoms rather than separate predictions of electron sources and sinks, the encoded representation of the reaction centers, and the use of a difference graph for the ranking where Kayala et. al. use a Siamese neural network. It would be interesting if the authors could comment on the advantages or disadvantages of these differences.
 
A number of details about the neural network training have been left out, presumably due to space restrictions, but should be included in the appendix at least. These include the network architectures, activation function, initialization, optimization hyperparameters, etc.

Learning to Predict Chemical Reactions
Matthew A. Kayala, Chloé-Agathe Azencott, Jonathan H. Chen, and Pierre Baldi
Journal of Chemical Information and Modeling 2011 51 (9), 2209-2222","Summary:
This work provides a novel approach to predict the outcome of organic chemical reactions. A reaction can be computationally regarded as graph-prediction problem: given the input of several connected graphs (molecules), the model aims to predict a fully-connected graph (reaction product) that can be obtained by performing several graph edits (reaction) on some edges and nodes (reaction center) in the input graphs. 

Past reaction predictions involving exhaustively enumeration of reaction centers and fitting them to a large number of existing reaction templates, which is very inefficient and hard to scale. In this work, the author proposed a template-free method to predict the outcome. It is a 3 step pipeline: 1) identify the reaction center given the input graphs using a Weisfeiler-Lehman Network. 2) generate candidate products based their reactivity score and chemical constraints. 3) rank the candidate products using a Weisfeiler-Lehman Difference Network.
The proposed method outperformed an existing state-of-art method on a benchmark chemistry dataset in both accuracy (10% rise) and efficiency (140 times faster), and also outperformed human chemist experts.

Qualitative Evaluation:
Quality:
The work is technically sound. The proposed method is well supported by experiments in both real world dataset and human expert comparisons.

Clarity:
This work describes their work and methods clearly. The experiments are introduced in details.

Originality:
This work provides a novel solution for reaction outcome prediction, which does not need prior knowledge of reaction templates.  
The author may want to relate some past NIPS work on computational chemistry to their work:
Kayala, Matthew A., and Pierre F. Baldi. ""A machine learning approach to predict chemical reactions."" Advances in Neural Information Processing Systems. 2011.

Significance:
The work outperforms the state-of-art of reaction product prediction in both accuracy and efficiency. The user study experiment shows that it also outperforms human experts.
"
Inferring Generative Model Structure with Static Analysis,"Paroma Varma, Bryan D. He, Payal Bajaj, Nishith Khandwala, Imon Banerjee, Daniel Rubin, Christopher Ré",https://proceedings.neurips.cc/paper/2017/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html,"The authors consider the setting where we wish to train a
discriminative model using labels that generated using so-called
heuristic functions, which in turn make use of primitive features. In
order to generate a single label that combines multiple heuristics,
the authors learn a probabilistic model (represented as a factor
graph). This probabilistic model incorporates two types of structure.
The method performs static analysis on the heuristic functions to
identify cases where multiple functions make use of the same
primitives. In order to capture correlations between primitives, the
model learns pairwise similarities. The paper is very well written.
The experiments which consider 4 datasets, demonstrate that knowing
dependencies between primitive features does indeed significantly
accuracy of the trained discriminative model in certain cases.

## Methodology 

In all the proposed methodology sounds reasonable to me. I am perhaps
a little bit skeptical about the significance of the static analysis
part of this work. Based on the fact that we are doing static
analysis in the first place, it would appear that the authors assume
a setting in which the user writes the heuristic functions, but makes
use of black box primitives in doing so. Given that experiments
consider settings in which there are < 10 primitives and < 10
heuristic functions, it seems to me that the user could easily
specify interdependencies by hand, so walking the AST seems like a
heavyweight solution for a relatively trivial problem.

Given the above, I would appreciate a clearer articulation of the
contributions made in this work (which is not in my area). If I
understand correctly, learning binary dependencies between primitives
is not novel here, so is the real contribution the static analysis
part?

If so, then I would like the authors to be a bit more explicit about
what the limitations of their static analysis techniques are. If I
understand correctly, the authors assume that all dependencies
between heuristic functions arise in definitions of primitives that
derive from other primitives and that each lambda accepts only one
(possibly derived) primitive as input?

## Experiments

The bone tumor result is a nice validation that generated labels
increase classifier accuracy on the test set. Space allowing, could
the authors perform the same experiment with other datasets (i.e.
train the FS case on half of the data, and train the HF / HF+DSP
models using the full dataset).

## Minor Comments 

- Figure 5: It would be helpful to explain in the main text HF means
  ""only static analysis"" whereas HF+DSP means ""static analysis +
  pairwise similarities""

- Figure 5: Could the authors indicate in the caption that the
  HF/HF+DSP are trained on additional data for which no labels are
  available.

- Figure 4 is not called out in the text.

- Bone Tumor: Could the authors explain a bit more clearly why
  knowing the number of primitives that each HF employs yields an
  improvement over the independent case?

- when we looked only used   
  -> when we only used","The authors propose using static analysis to improve the synthesis of generative models designed to combine multiple heuristic functions. They leverage the fact that expressing heuristics as programs often encodes information about their relationship to other heuristics. In particular, this occurs when multiple heuristics depend (directly or indirectly) on the same value, such as the perimeter of a bounding box in a vision application. The proposed system, Coral, infers and explicitly encodes these relationships in a factor graph model. The authors show empirically how Coral leads to improvements over other related techniques.

I believe the idea is interesting and worth considering for acceptance. However, I have various queries, and points that might strengthen the paper:

* I found it difficult at first to see how Coral relates to other work in this area. It might be useful to position the ""Related Work"" section earlier in the paper. It might also help to more explicitly describe how Coral relates to [2] (Bach et al.), which seems closest to this work -- in particular, there is quite a lot of similarity between the factor graphs considered by the two models. (Coral seems to be obtained by inserting the phi_i^HF and p_i nodes between the lambda_i and phi^DSP nodes, and by restricting only to pairwise terms in the phi^DSP factor.)

* It isn't stated anywhere I could see that lambda_i \in {-1, 1}, but based on the definition of phi^Acc_i (line 147), this must be the case, right? If so, the ""True"" and ""False"" values in Figure 3 (a) should potentially be replaced with 1 and -1 for clarity.

* Why does (i) (line 156) constitute a problem for the model? Surely over-specification is OK? (I can definitely see how under-specification in (ii) is an issue, on the other hand.)

* What is the justification for using the indicator function I[p_i = p_j] for phi_ij^Sim? I would expect this hardly ever to have the value 1 for most primitives of interest. It is mentioned that, for instance, we might expect the area and perimeter of a tumour to be related, but surely we do not expect these values to be identical? (I note that similar correlation dependencies occur in [2] (Bach et al.), but these compare the heuristic functions rather than the primitives as here. Since the heuristic functions return an estimate of the label Y, it seems much more reasonable to expect their values might be equal.)

* It is stated in line 176 that the supplementary materials contains a proof that ""if any additional factor would have improved the accuracy of the model, then the provided primitive dependencies would have to be incorrect"". However, I couldn't find this -- where exactly can it be found?

Finally, some minor (possible) typos:

* Line 133: ""if an edge point"" should be ""if an edge points""
* Figure 3 (a): The first ""return False"" has an extra indent
* The double summation in the definition of P between lines 170 and 171 seems to double count certain terms, and is slightly different from the definition of phi^DSP between lines 169 and 170
* Line 227: ""structure learning structure learning""
* Line 269: ""structure learning performed better than Coral when we looked only used static analysis to infer dependencies""","This paper builds on the work of Ratner et al. (NIPS 2016) and Bach et
al. (ICML 2017) of using generative models and heuristics to cheaply
obtain labeled data for problems where data is scarce.
The contribution of this paper is use static analysis on these
heuristic functions (HFs) to guide the structure learning. The insight
is that many HFs will share primitives. These primitives are specified
manually by the user and are called Domain-specific primitives
(DSP). Modeling the dependencies between these HFs in the generative
model leads to an improvement on F1 score for several datasets when
the labels from that generative model are later used as training data in a
discriminative model.

The approach of using static analysis for this problem is a novel
contribution. Static analysis here means the Heuristic Functions (HF)
meaning the source code was analyzed to figure out which
primitives they share.


The work is complete with experimental results which support the
claims of the paper. The experiments seem to use a small number of
DSPs and HFs. This makes the experiments less than ideally support
what we getting from use static analysis to extract the higher order
dependencies.  The paper suggests pairwise dependencies between HFs
are modeled as a hedge if the static analysis fails. What is nice is
that in all the datasets used the DSP factor always seemed to make no
difference to the F1-score. It would have been nice to see a dataset
where that factor really makes a difference.

The experimental results are also interesting in that for the Bone
Tumor dataset the heuristics outperform on F1-score the
fully-supervised case. I'd be curious if there are other datasets
that exhibit this behavior or is this just an artifact of the Bone
Tumor dataset being smaller.

The paper is clearly written, though it does build significantly on
recent work. That makes the paper challenging to understand by itself.

Minor details:

Is F1 the best metric to use here? Also why are some of the values in
Table 1 in terms of F1 score and others use accuracy?

DSP is really overloaded for me as Digital Signals Processing.
Is there any other abbreviation that could have been chosen?

The Primitive Set looks like a record type. Are there any
restrictions on what constitutes a Primitive Set?
"
Influence Maximization with $\varepsilon$-Almost Submodular Threshold Functions,"Qiang Li, Wei Chen, Institute of Computing Xiaoming Sun, Institute of Computing Jialin Zhang",https://proceedings.neurips.cc/paper/2017/hash/cf2226ddd41b1a2d0ae51dab54d32c36-Abstract.html,"The paper studies the influence maximization problem under the non-submodular model. The work differs from the traditional sub-modularity assumption. The problem is new and challenging. However, the paper has the following weakness: First, the \varepsilon-AS assumption is not convincing. I'd like to see data analysis to support the assumption. Second, the proofs are difficult to read. Intuitive explanations with examples are necessary to support the proofs. 
","
        This paper studies influence maximization in the general threshold diffusion model, specifically when node threshold functions are almost submodular (or epsilon-submodular). The key results are: 1) a proof of inapproximability for the influence maximization problem under this model when there are sufficiently many epsilon-submodular threshold nodes; 2) an efficient, simple and principled approximation algorithm for this problem when there is some fixed number of epsilon-submodular threshold nodes.

        This is a strong paper for the following reasons:
        1- Influence maximization under the General Threshold model is a challenging and poorly studied problem;
        2- The epsilon-submodularity condition is very reasonable because it is consistent with empirical evidence in real diffusion processes;
        3- The hardness result and the approximate algorithms are intuitive and non-trivial.

        Barring minor typos and the like (see below for details), the paper is very well-written. In my understanding, the technical proofs are correct. The experimental evaluation is thorough and convincing, and the results are consistent with the theoretical guarantees.

        Minor comments (by line):
        102: shouldn't the equation right-hand side have 1/(1-\epsilon) instead of (1-\epsilon)?
        237: ""linear"" instead of ""liner""
        241: ""satisfying"" instead of ""satisfies""
        262: ""PageRank"" instead of ""PagrRank""
        297: ""compared to 2.05"" instead of ""while 2.05""
        298: ""spread"" instead of ""spreads""
        299: ""The more $\epsilon$-AS nodes there are in the network, the more improvement is obtained""

      ","Influence maximization is a #P hard problem to solve in general. A greedy algorithm is successful in giving a constant fraction approximate solution when the local influence functions (and by Mossel, Roche [8], also the global influence function) is submodular. This paper investigates the question of how well can one approximate influence if the local influence functions are not exactly submodular, but approximately submodular. Somewhat surprisingly, even when a only a sublinear fraction of the nodes are approximately submodular, the influence function cannot be approximated well in polynomial time. The authors establish this result by constructing a special example which ""amplifies"" the non-submodularity and gives rise to a non-approximable influence function. Furthermore the authors show that if only a constant number of nodes are approximately submodular, it is possible to obtain a constant fraction approximation, and the authors propose an algorithm to do the same.

The paper considers a very interesting problem and has interesting theoretical results. The effectiveness of the algorithm in latter half of the paper is backed by simulations. I recommend this paper be accepted.

Comments:
-Line 24, please refer to authors by last names
-Line 29, typo in LiveJournal
"
Improved Dynamic Regret for Non-degenerate Functions,"Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, Zhi-Hua Zhou",https://proceedings.neurips.cc/paper/2017/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html,"Summary:
This paper studies dynamic regret of online convex optimization and propose a new notion of regularity of the comparator sequence, i.e. the sum of the *square* distances of consecutive optimal points, which could potentially be much smaller than the sum of distances studied previously. Bounds that are essentially the minimum of these two regularities are shown to be achieved by a simple idea of doing multiple steps of gradient descent/damped Newton method at each time for (semi-)strongly convex functions and self-concordant functions. 

Major Comments:
The main contribution of the paper is this little cute idea of doing multiple steps of gradient descent, which turns out to lead to nontrivial and better dynamic regret bounds. Despite its simplicity, I found the results interesting and I like the fact that the achieved bounds are actually the minimum of the new regularity and the one used before.

Overall the paper is well written, except that the intuition on why this idea works seems to be missing. It's not clear to me why it leads to this specific kind of bounds. All proof are in the appendix and I did not verify them.

Typos:
1. L164, even -> even if
2. L187, blow -> below","In this paper, the authors study the problem of minimizing the dynamic regret in online learning. First, they introduce squared path-length to measure the complexity of the comparator sequence. Then, they demonstrate that if multiple gradients are accessible to the learner, the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. Finally, they extend their theoretical guarantees to functions that are semi-strongly convex or self-concordant.

This is a theoretical paper for analyzing the dynamic regret. The main difference from previous work is that the learner is able to query the gradient multiple times. The authors prove that under this feedback model, dynamic regret could be upper bounded by the minimum of the path-length and the squared path-length, which is a significant improvement when the squared path-length is small.

Strong Points:
1. A new performance measure is introduced to bound the dynamic regret.
2. When functions are strongly convex, the authors develop a new optimization algorithm and prove its dynamic regret is upper bounded by the minimum of the path-length and the squared path-length.
3. This is the first time that semi-strong convexity and self-concordance are utilized to tighten the dynamic regret.

Suggestions/Questions:
1. It is better to provide some empirical studies to support the theoretical results.
2. For self-concordant functions, why do we need an additional condition in (11)?
3. Due to the matrix inverse, the complexity of online multiple Newton update (OMNU) is much higher than online multiple gradient descent (OMGD), which should be mentioned explicitly.
","The authors study online convex optimization with dynamic regret. While typical bounds in this setting are in terms of the path length of actions, the authors show that by performing multiple gradient descent updates at each round, one can bound the regret by the minimum of the path length and the squared path length. This is demonstrated in the setting of smooth and strongly convex functions
and for semi-strongly convex functions. The authors also present a similar result for self-concordant functions by designing an algorithm that performs multiple Newton updates at each step.

This paper is relatively clear and well-written. However, there are a couple of issues that lead to my relatively low score. The first and most obvious issue is that the theoretical bounds in this paper do not improve upon existing results. For these types of algorithms, the authors usually either present compelling examples of experiments demonstrating that their method is meaningful and useful. However, this is not done.

Another issue is that unlike most work in this setting, and in fact most of online convex optimization, the bounds presented in this paper seem to only apply against the sequence of per-step minimizers and not arbitrary sequences of actions. For instance, if the learner were to compare against the sequence of static comparators in the original Zinkevich algorithm, the bound would automatically reflect this. This is not possible with the OMGD algorithm. I view this as a severe limitation of this work, and I believe it is a point that should at the very least be discussed in the paper.   "
AdaGAN: Boosting Generative Models,"Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann SIMON-GABRIEL, Bernhard Schölkopf",https://proceedings.neurips.cc/paper/2017/hash/d0010a6f34908640a4a6da2389772a78-Abstract.html,"This paper builds a mega-algorithm that can incorporate various sub-models to tackle with missing mode problem. It tactfully applied AdaBoost and other mixture model spirits in the context of GAN. The paper theoretically analyzed the optimal and suboptimal distributions that are added as mixture components, and get the corresponding convergence results. The paper is well organized and the details are clearly elaborated.

There are some places that are better to be explained more clearly: 

1.	Whether the mixture model weight matches well the corresponding mode height? Many of the results(median) in table 1 is larger than 0.95 and it may suggest some mode drop. In the Toy Dataset, it can possibly be shown by changing 0.95 in calculating C to be 0.1 or even 0.05 and see the results.

2.	In the calculation of data point weights, the algorithm requires an optimum discriminator D_M between the original data and the current mixture. Can this optimum discriminator be obtained during training in practice? If not, how does this influence the data points weight and the new model component?

In general, the paper is theoretically sound and the results are supportive. 
","The paper proposes a new method inspired by AdaBoost to address the missing mode problem often occurs in GAN training.
In general, the paper is quite well written.  The studied problem is interesting and important, and the theoretical analyses seem solid. 
The proposed algorithm is novel and shows good empirical results on synthetic dataset.
Below are my minor comments:

1. I know it is probably due to space limit, but it would be good if the authors can provide more explanation of the intuition on the theoretical results such as Theorem 1 and 2. This will make the paper much easier to understand.

2. The proposed method does not seem to have significant advantage over the standard GAN on real datasets such as MNIST and MNIST3. It would be good if the authors can try more datasets such as ImageNet. Otherwise the practical applicability is still in question.
","AdaGAN is a meta-algorithm proposed for GAN. The key idea of AdaGAN is: at each step reweight the samples and fits a generative model on the reweighted samples. The final model is a weighted addition of the learned generative models. The main motivation is to reduce the mode-missing problem of GAN by reweighting samples at each step.

It is claimed in the Introduction that AdaGAN can also use WGAN or mode-regularized GAN as base generators (line 55). This claim seems to be an overstatement. The key assumption of AdaGAN is that the base generator aims at minimizing the f-divergence (as mentioned in Line 136-137). This assumption does not hold for WGAN or mode-regularized GAN: WGAN minimizes the Wasserstein distance, and the mode-regularized GAN has an additional mode regularizer in the objective.

WGAN and mode-regularized GAN are state-of-the-art generative models targeting the mode-missing problem. It is more convincing if the author can demonstrate that AdaGAN outperforms the two algorithms.

Corollary 1 and 2 assume that the support of learned distribution P_g and the true distribution P_d should overlap for at least 1-beta. This is usually not true in practice. A common situation when training GAN is that the data manifold and the generation manifold are disjoint (see e.g., the WGAN paper)."
Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences,"Kinjal Basu, Ankan Saha, Shaunak Chatterjee",https://proceedings.neurips.cc/paper/2017/hash/d10ec7c16cbe9de8fbb1c42787c3ec26-Abstract.html,"In this paper, authors develop a method that transforms the quadratic constraint into a linear form by sampling a set of low-discrepancy points. This approach can be used to solve a large-scale quadratically constrained quadratic program by applying any state-of-the-art large-scale quadratic solvers to the transformed problem. 

The approximation of a quadratic constraint uses a set of linear constraints as the tangent planes through these sampled points. Algorithm 1 presents the approximation algorithm. And also, in section 3, authors show that algorithm 1 generates the approximate problem and it converge to the original problem as N goes to infinity. 

From the results reported in Table 1, the proposed algorithm can reduce the computation on the size of data larger than 1000. However, it is hard to see the objective difference between the proposed algorithm and the exact solution obtained by SDP. As discussed in the introduction, the targeted optimization problem has been used in many real world applications. In order to evaluate the performance, it might be interesting to see how the approximated solution affects the application results.

Another concern is from the results in Table 2. It seems that the relative error is increasing greatly when n increases. In other words, the approximated solution will be very different if n becomes large.  The question is: can we trust this algorithm when n is large? 
","Contributions:
The paper proposes a sampling based method to approximate QCQPs. The method starts from a sampling of the hypercube, the mapping it to the sphere, and finally to the ellipsoid of interest. Unfortunately the authors don't provide enough evidence why this particular sampling is more efficient than simpler ones. How is the complexity dependence on the dimension? (Net seems to be exponential in dimension). The theorems seem to hide most dimensionality constants. My main question: How does the approach compare experimentally and theoretically to naive sampling? 

While the main advantage of the contribution is the scalability, the authors do unfortunately not in detail compare (theoretically and practically) the resulting method and its scalability to existing methods for QCQP.

The paper is considering convex QCQP only, and does not comment if this problem class is NP hard or not. For the list of applications following this statement, it should be mentioned if those are convex or not, that is if they fall into the scope of the paper or not. This is not the only part of this paper where it occurred to me that the writing lacked a certain amount of rigor and referenced wikipedia style content instead of rigorous definitions.

Experiments: No comparison to state-of-the-art second order cone solvers or commercial solvers (e.g. CPLEX and others) is included. Furthermore, as stated now, unfortunately the claimed comparison is not reproducible, as details on the SDP and RLT methods are not included. For example no stopping criteria and implementation details are included. No supplementary material is provided.

How was x^* computed in the experimental results? How about adding one real dataset as well from the applications mentioned at the beginning?

Riesz energy: ""normalized surface measure"" either formally define it in the paper, or precisely define it in the appendix, or at least provide a scientific reference for it (not wikipedia). The sentence ""We omit details due to lack of space"" probably takes as much space as to define it? (Current definition on the sphere is not related to the actual boundary of S).

Style: At several points in the paper you mention ""For sake of compactness, we do not go deeper into this"" and similar. This only makes sense if you at least more precisely state then why you discuss it, or at least make precise what elements you require for your results. In general, if such elements do add to the value of the paper, then they would be provided in an appendix, which the authors here did not.

line 105: missing 'the'
line 115: in base \eta: Definition makes no sense yet at this point because you didn't specify the roles of m and eta yet (only in the next sentence, for a special case).
line 149: space after 'approaches'. remove 'for sake of compactness'


== update after rebuttal ==
I thank the authors for partially clarifying the comparison to other sampling schemes, and on x^*.

However, I'm still disappointed by the answer ""We are presently working on comparing with large scale commercial solvers such as CPLEX and ADMM based techniques for SDP in several different problems as a future applications paper."". This is not to be left for the future, this comparison to very standard competing methods would be essential to see the merits of the proposed algorithm now, and significantly weakens the current contribution.","Review of ""Large-Scale Quadratically Constrained Quadratic Program via
Low-Discrepancy Sequences.""

The paper presents a method for solving large-scale QCQPs. The main
idea is to approximate a single quadratic constraint by a set of N
linear constraints. Section 1 explains the QCQP problem setting with n
optimization variables. The two main related methods involve
increasing the number of variables to O(n^2), which is intractable for
large n. Section 2 explains the linear approximation with equations
and Figure 1, which is very helpful for understanding the idea.
Section 2.1 introduces the concept of the optimal bounded cover, which
is essentially a set of N points/constraints which is the best linear
approximation. Section2 2.1.1-2.1.3 provide details about how the N
points/constraints are constructed, and Section 2.2 gives
pseudocode. Section 3 provides a proof that the linear approximation
converges to the solution of the original (quadratically constrained)
problem, as the number of points/constraints N is increased. Section 4
provides experimental results on simulated data sets.

The paper provides a very clear explanation of an interesting idea
with a very elegant geometric interpretation. A strong point of the
paper is the figures, which are really do help the reader understand
the idea. Another strong point is that there seems to be a few avenues
for future work (e.g. other point/constraint sampling strategies). The
main weak point of the paper is the experimental section, and the lack
of a discussion/analysis about how many constraints N is necessary to
achieve a good approximation -- I would hope that it is less than
O(n^2).

In Table 1 it seems that Our Method gets a lower objective value than
the Exact method -- does that mean Our Method is not feasible for the
original problem? Is that an issue? Please add some discussion about
feasibility.

I was a bit disppointed to see the numbers in Table 1 rather than a
Figure, which I think would probably be easier for the reader to
interpret (and take up less space).


"
Graph Matching via Multiplicative Update Algorithm,"Bo Jiang, Jin Tang, Chris Ding, Yihong Gong, Bin Luo",https://proceedings.neurips.cc/paper/2017/hash/d1e946f4e67db4b362ad23818a6fb78a-Abstract.html,"The paper presents a new method called Multiplicative Update Graph Matching (MPGM) to provides a multiplicative update technique for the general Quadratic Programming problem. Compared to existing methods, the update steps have a closed form solution, the convergence is guaranteed, and the doubly stochastic constraints are directly integrated in the optimization process. 

The  paper is technically sound, and the claims seem theoretically and experimentally well supported.

The paper writing is clear and the organization is neat. 

Without knowing too much about graph matching and its related literature, the approach seems novel, and given the description and the qualitative and quantitative results, the advance over state-of-the-art seems sufficient.

Graph Matching is a commonly known problem. Having an algorithm with guaranteed convergence
","This paper presents a novel method (MPMG) to solve QP graph matching problem. The graph matching problem is formulated as argmax_x(x^TWx) where W encodes the pair-to-pair and node-to-node affinities and x is the desired permutation matrix (in vector form). The desired solution is a permutation encoding the optimal matching, also expressed as doubly stochastic (entries >= 0, rows/columns sum to 1) and discrete (entries in {0,1}). The standard approaches to solving the QP is to relax the NP-hard problem (relaxation can be for either the doubly-stochastic constraint or the discrete constraint). This proposed MPMG solution follows the Lagrange multiplier technique which moves the doubly-stochastic constraint into the error term. This proposed approach, along with proofs for convergence and KKT-optimality are a a novel contribution for this graph matching formulation. Experimentally, evidence is provided that shows the approach converges near a sparse/discrete solution even though the constraint is not explicitly modeled in the solution, which is the most promising feature of the method. However, there are some concerns regarding the experimental details which might jeopardize the strength of the empirical claims.

Other state of the art techniques do rely on iterative solutions, so the paper should mention the convergence criterion especially for experiments like figure 1 where the MPMG is initialized with RRWM, it would be useful to make sure that the convergence criterion for RRWM is appropriately selected for these problems.

Following up on the previous comment, it seems initialization with RRWM is used for FIgure 1 but initialization with SM for the subsequent experiments. Furthermore, regarding the experiments, how is performance time computed? Is this including time for the initialization procedure, or just the subsequent MPGM iterations after initialization?

Also, RRWM also requires an initialization. In [3] the basic outline of the algorithm uses uniform, so is it a fair comparison at all for MPMG that uses a established algorithm as initialization vs RRWM which uses uniform? If the details have been handled properly perhaps more explanation should be given in the text to clarify these points. In light of these points it is hard to take anything meaningful away from the experiments provided.

In the experiments, how are the node-to-node affinities determined (diagonals of W)? I think this is not described in the paper.

Image feature point matching would not actually be solved as a graph matching problem in any practical scenario (this is true for a number of reasons not limited to number of features, pair-to-pair affinities, robustness to large # of outliers). As the paper suggests, there are many other practical labeling problems which are typically solved using this QP formulation and perhaps the paper would have more value stronger ","The purpose of the paper is to find a way to iteratively update a solution that will converge to the KKT optimum solution. Starting with the KKT solution (line 141), the authors find a way to write it in a iterative recurrence (by splitting the square term into x(t) * x(t+1)).

Then, they construct an auxiliary lower bound function (in 2 variables: the old and the new solution) for the Lagrangian such that its solution gives the update rule. The lower bound is a concave function which, except for the variable in the Lagrangian, brings in a new variable (with the role of the old solution), having as intention the recurrence rule between new and old solution.

Moreover, the authors force it by construction to converge to a sparse solution (empirically proven only). This part should be analyzed more. Did you construct more update rules, but with non-sparse results? You should find the particularities in your update rule that make the solution sparse."
Neural Expectation Maximization,"Klaus Greff, Sjoerd van Steenkiste, Jürgen Schmidhuber",https://proceedings.neurips.cc/paper/2017/hash/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Abstract.html,In this work author propose a new framework that combines neural networks and the EM algorithm to derive an approach to grouping and learning individual entities.  The proposed approach is evaluated on a perceptual grouping task for generated images and videos. The described empirical study on the three data types shows the potential of the approach. My only concern is there isn’t really a solid baseline to compare against. In a summary I think that this is an interesting work. I should also note that It seems that a shorter version of this paper was published on the ICLR 2017 workshop track. Figure 6 should be properly labeled.,"  This paper presents some though-provoking experiments in unsupervised entity recognition from time-series data. For me the impact of the paper came in Figs 3 and 5, which showed a very human-like decomposition.
  I'm not convinced that analyzing a few static shapes is an important problem these days.  To me, it seems like a ""first step"" toward a more significant problem of recognizing concurrent actions (In this case, they have actions like ""flying triangle"" and ""flying 9"", with occasional occlusions muddying the picture). For example, RNN-EM running on non-pixel input features (output from a static object detector output (YOLO?)) seems one reasonable comparison point.

- line 109: B[space]presents
- 135: ""stopping the gamma gradients"" (and something similar in Supplemental). Not exactly sure what this means. Is it setting those gradients to zero? or simply not backpropagating at all through those layers?
- 215: Figure 4.2 should be Figure 4?
- 209 and 220: Table 4.2 should be Table 1?
  If training with higher K is favorable, why use low K for flying MNIST?
  In  Table 1, what happens at test time if you fail in ""correctly setting the number of components at test time? Do you get empty clusters if K > # objects? I guess I'd like some perspective about the perennial problem of choosing the ""correct"" number of clusters at test time.

  For flying MNIST, carefully stepwise training seems complicated and not transferable to more general cases. For me, this seems like a disadvantage.
  My hope would be that the non-digit [line segment, line 239] ""unsupervised"" statistically relevant groupings could still be used as input features to train a net whose output matches the human concept of complete number shapes.
  What happens if you see new objects at test time, or if the number of object classes gets very large? Recognizing line segments or other moving subfeatures again no longer be such a bad idea or disadvantage.  I would have enjoyed seeing them tackle approaches with grouping together ""flying [this collection of statistical features]"".
","
	The paper presents two related deep neural networks for clustering entities together into roughly independant components.  In their implementations the entities are always pixels in an image.  The first technique they brand as neural expectation maximization, and show how the network is performing an unrolled form of generalized EM.  The E-step predicts the probability that a given pixel was generated by each of the components.  The M-step, then updates the parameters of each component using these probabilities.  This is neural, because each of these steps relies on a neural network which maps from the parameters of a component to the pixel values. By unrolling this process they can train this network in an end-to-end basis.  They also proposed a related model which modifies this process by updating the parameters of the components at each step in the unrolling using an RNN-style update.  This allows them to extend the model to sequential/video data where the image is changing at each step.

	The show results only on toy datasets consisting of a small number of shapes or MNIST images (3-5) superimposed on top of each other.  Their neural EM approach performs significantly worse than recent work on Tagger for static data, however the RNN version of their model slightly out performs Tagger.  However Tagger significantly outperforms even the RNN version of their work if Tagger is trained with batch normalization.  In the video setting, their model performs significantly better, which is not surprising given the additional information available in video.  They show both higher scores for predicting the pixel to component mapping, but also much better reconstruction MSE than a standard  (fully entangled) autoencoder model (which is the same a their model with 1 component).

	Overall these seems like a solid paper with solid results on a toy dataset.

	My main concern is that the technique may not work on messier real world data, since they only show results on relatively toy datasets where the components are completely independant, and the component boundaries are relatively clear when they are not overlapping.  Results in a more realistic domain would make the paper much strong, but even in the current state I believe both the technique and the results are interesting.
      "
"Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs","Saurabh Verma, Zhi-Li Zhang",https://proceedings.neurips.cc/paper/2017/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html,"The authors propose a kernel for unlabeled graphs based on the histogram of the pairwise node distances where the notion of distance is defined via the Eigen decomposition of the laplacian matrix. They show that such a notion enjoys useful uniqueness and stability properties, and that when viewed as an embedding it is a generalization of known graph embedding and dimensional reduction approaches. 
The work would benefit from a clearer exposition (briefly anticipating the notion of ""distance"" or of invariance, uniqueness, stability and sparsity albeit in an informal way). 
Figure 3 needs to be rethought as it is not informative in the current form (perhaps showing the histogram of the number of non zero elements in the two cases?)
The authors should be careful not to oversell the results of Theorem 1, as in practice it is not known how does the number of non unique harmonic spectra increases with the increase in graph size (after all graphs in real world cases have sizes well exceeding 10 nodes). Also it is not clear whether having near identical spectra could be compatible with large structural changes as measured by other sensible graph metrics.
Theorem 5 in the current form is not informative as it encapsulates in \theta a variability that is hard for the reader to gauge.
The stability analysis seems to be based on the condition that the variation of weights be positive; perhaps the authors want to add a brief comment on what could be expected if the variation is unrestricted? 
An empirical analysis of the effect of the histogram bin size should probably be included. 
The work of Costa and De Grave. ""Fast neighborhood subgraph pairwise distance kernel."" ICML 2010, seems to be related as they use the histogram of (shortest path) distances between (subgraphs centered around) each pair of nodes, and should probably be referenced and/or compared with. 
There are a few typos and badly formatted references.  
However the work is of interest and of potential impact.","SUMMARY

This paper studies the properties of features extracted from graph spectral decomposition. 
The procedure to compute the proposed method: FGSD (family of graph spectral distances) is as follows. Given some function f and a graph, distance between nodes (x and y) in a graph is given as S_f as in equation (1). Then the graph representation R_f of g is obtained as the histogram of distribution of edges in feature space. S_f (or FGSD) essentially marginalizes distance between x and y in graph spectrum with a weight function f. Depending on f, S_f is shown to be able encode either global or local structure  of a given graph. 

FGSD is also shown to possess uniqueness (up to permutation of vertex labels), stability with respect to eigendecomposition. Sparsity in resulting graph feature matrix is presented for f(x)=1/x, and an efficient computation of S_f using a fixed point iteration is presented.

Finally, FGSD is demonstrated to show superior performance to existing graph kernels in classifying graphs.


COMMENTS

This paper studies graph similarity based on a weighted reconstruction of spectral decomposition. Interesting properties are derived, however, more experiments would show the detailed properties of FGSD, for example, by using synthesized graphs.

In computational experiments classifying labeled graphs, FGSD is claimed to outperform graph kernels that utilize node labels. This point should be investigated more into details. For what type of graphs do FGSD perform well ? Is the node label information not essential for those tasks? 
","This paper proposes a family of graph spectral distances and characterizes properties of uniqueness, stability, and sparsity. The paper proposes fast approximate algorithms for computing these distances and identifies harmonic and biharmonic distances as a suitable family. Empirical evaluations support the utility of these distances. 

These distances will form a useful tool in featurizing and classifying graphs. The only reason for not providing a stronger accept is that I am not familiar with the relevant literature in this area."
Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems,"Le Fang, Fan Yang, Wen Dong, Tong Guan, Chunming Qiao",https://proceedings.neurips.cc/paper/2017/hash/d38901788c533e8286cb6400b40b386d-Abstract.html,"This paper considers the variational approach to the inference problem on certain type of temporal graphical models. It defines a convex formulation of the Bethe free energy, resulting in a method with optimal convergence guarantees. The authors derive Expectation-Propagation fixed point equations and gradient-based updates for solving the optimization problem. A toy example is used to illustrate the approach in the context of transportation dynamics and it is shown that the proposed method outperforms sampling, extended Kalman Filtering and a neural network method.

In the variational formulation, the optimization problem is generally non-convex, due to the entropy terms. Typical variational methods bound the concave part linearly and employ an EM-like algorithm. The key contribution of this paper is to work out a dual formulation on the natural parameters so that the dual problem becomes convex. The derivations seem correct and this looks like an important contribution, if I have not missed some important detail. Authors should elaborate on the generality and applicability of their approach to other models.

I think it would be useful to compare their method with the standard double-loop approach [11] both in terms of describing the differences and in numerical comparison (in terms of error and cpu-time).

The authors state that they are minimizing the Bethe free energy, in found a bit confusing that they call this a mean field forward and backward in the algorithm.

Regarding the experiments, the state representation goes from a factorized state with the state of an agent being one component to location states in which the identities of the agents are lost and the components become strongly coupled. This is OK, but to me it contradicts a bit the motivation in the introduction, in which the authors state ""we are not satisfied with only a macroscopic description of the system (...) the goal is (..) to track down the actual underlying dynamics"".

Some details about the other methods would be good to know. For example, how many particles N where used for PF? How sensitive are the results (accuracy and time) w.r.t N? Also it is not clear how the NNs are trained. Is the output provided the actual hidden state?

The paper looks too dense. I think substantial part in the Introduction is not necessary and more space could be gain so that formulas do not need to be in reduced font and figure 1 can be made larger.

Other minor comments:

It should be stated in the beginning that x is continuous variable unless otherwise stated
Eq (3) no need to define g_v, since it is not used in the rest of the paper
line 130: better $h(x_{t-1},c_v)$ than $h(x_{t},c_v)$
Eq (4) I think $1-\tau$ should be $(1-\tau)$
Equations between 139-140. \psi(|x_{t-1,1}) -> \psi(x_{t-1,1})
Equations between 139-140. ""F_dual"" -> ""F_Dual""
line 104: ""dynamics"" -> ""dynamic""
line 126: remove ""we have that""
line 193: -> ""by strong duality both solutions exist and are unique""
line 194: ""KKT conditions gives"" -> ""KKT conditions give""
Use \eqref{} instead of \ref{} for equations
Algorithm 1: the name can be improved
Algorithm 1: Gradient ascent: missing eqrefs in Eqs. ...
Algorithm 1: Gradient ascent: missing parenthesis in all $\langle f(x_t \rangle$
Figure 1
line 301: ""outperforms against"" -> ""outperform""
line 302: ""scales"" -> ""scale""","Overall a well-written and interesting paper in the domain of nonlinear state-space models. The nonlinear state-space dynamics follow a particular prior determined by the ""stochastic kinetic model"". Exact Inference in the model is intractable due to nonlinearities so Expectation Propagation is used to infer p(x|y) and the predict future observations. Excellent results are obtained for the problem at hand. I'm particularly impressed with how much better EP-based algorithms do w.r.t. RNNs for this problem. The analysis for runtime is weak however, the scaling analysis is missing. It would be nice to have an explanation as to why PF and EKF scale so badly and show it in a separate plot. Also, why are runtime values missing for RNNs? This is a big issue. You cannot claim your method scales better than RNNs and then not give runtime values for RNNs ;) 

Another overall issue I have with this paper is that the inductive bias implicit in the choice of prior for the dynamics may cause this method to not work well at all with time series from other domains. A few more experiments showing performance on datasets from other domains is essential for this paper to be considered as an accept. 

Overall I do like the approach and love that EP is beating neural nets. I just want some more extensive experiments. ","This paper claims to make a new setup for doing EP that guarantees the local optima of the energy function is also a global optima. The introduction reads as if they have solve this for the general case. However, it appears this is only done for the stochastic kinetic models case. In any case, this is a big claim.

The derivations to back up this claim on page 5 are dense. I did not have time check them entirely.

The introduction reads seems long and a bit disorganized. A shorter introduction with a more clear train of thought would be better.

Clarity would also be increased by stating the probability of everything for the SKM as was done for the state space model in Equation 1.

Questions
Equation 1 seems to be basically the definition of a state space model. Wouldn't it be simpler to state it as such?

Typos: L28, L137

Latex issues: L127, L161"
Welfare Guarantees from Data,"Darrell Hoy, Denis Nekipelov, Vasilis Syrgkanis",https://proceedings.neurips.cc/paper/2017/hash/d3a7f48c12e697d50c8a7ae7684644ef-Abstract.html,"The paper introduces a way to provide data dependent bounds on the price of anarchy (PoA). The main idea behind the paper is to provide a finer notion of the PoA restricted to a particular bid distribution. The authors show that we can upper bound this quantity by a ratio of two simple expectations which can in turn be estimated from samples under some reasonable assumptions. 

The ideas behind the paper are simple, well explained and are really well structured. There are only a few things that would improve the paper:

1) When the authors compare their approach with that of inverting the equilibrium function, they neglect to mention that the inversion algorithm would estimate the true PoA while the proposed algorithm here only estimates an upper bound. Nevertheless, I agree with the authors that those estimates only hold under restrictive assumptions on the density function of the values. 

2) Is assumption 6 absolutely necessary ? I understand that it is needed in your proof technique. However it seems that since the denominator defining \mu(D) is always larger than the numerator, a close to 0 denominator should be unlikely unless the numerator is also close to 0. 

3) Line 140  I think in the upper bound it should be T_i / \mu 
   Line 141  This should also  be (1 - e^-\mu)/ \mu 
   Ditto for eq (9) and (10)","This paper is about game theory which falls out of my expertise. I can only give the following points confidently.

1. The title does not contain much information since it ambitiously relates itself to too many branches in statistics.
2. The equation number and theorem number in the appendix should be carefully edited instead of left as question marks.","This paper studies a non-worst-case notion of the price of anarchy (PoA). An auction A’s PoA is the worst-case ratio between the welfare of the optimal auction and the welfare in a Bayes-Nash equilibrium of A, taken over all value distributions and equilibria. The authors propose a non-worst case relaxation which they call the distributional PoA (DPoA), which is defined by a distribution D over bids. It measures the same ratio as the original PoA, but only over all value distributions and equilibria that could generate the bid distribution D. They bound the DPoA for single-dimensional settings and show that it can be estimated using samples from D.

In one example, the authors provide an upper bound on the DPoA for the single item first price auction (Lemma 1). There are a few things I couldn’t reproduce in the proof. In line 140, the authors write that 1/\mu*\int_0^{v_i(1 – e^{-\mu})} (1 – G_{-i}(z)) dz <= T_i. Isn’t it less than or equal to T_i/\mu? Also, it seems as though if I add 1/\mu*\int_0^{v_i(1 – e^{-\mu})} (1 – G_{-i}(z)) dz to the right-hand-side of Equation (8), I’ll get 1/\mu*\int_0^{v_i(1 – e^{-\mu})} (1 – G_{-i}(z) + G_{-i}(z)) dz = 1/\mu*\int_0^{v_i(1 – e^{-\mu})} dz = v_i(1 – e^{-\mu})/\mu. (The authors write that it’s just v_i(1 – e^{-\mu}).) Lastly, in line 145, the authors write that \sum_i T_i*x_i >= max_i T_i. Since this is a single-item auction, there’s exactly one x_i that equals 1 and the rest equal 0. Should the inequality be flipped? (\sum_i T_i*x_i <= max_i T_i.)

I would really like to see the proof of Lemma 5 written out. It’s not clear to me how it follows immediately from Lemma 9.9 in [5]. Rademacher complexity does have nice compositional properties. However, I’m not convinced, for example, that “multiplication of bid vectors with constants” is covered by Lemma 9.9 in [5]. Lemma 9.9 in [5] applies to function classes mapping some domain X to R. The function class defined by “multiplication of bid vectors with constants” maps from R^{n-1} to R^{n-1}. How exactly does Lemma 9.9 apply? Also, it seems like multiplication would have to be involved in the form of many payment functions. For example, in the first price auction, the payment function is b_i*x_i, where x_i can be written as {b_i > b_1}*…*{b_i > b_{i-1}}*{b_i > b_{i+1}}*…*{b_i > b_n}. Is there a way to write this without multiplication? The authors write that the conditions in Lemma 5 hold for sponsored search auctions. Is there no multiplication in that setting either?

I think this paper is currently better written for an economics community as opposed to a learning community. For example, it’s not clear to me what a pay-your-bid auction is. On line 187, the authors write that “For any auction, we can re-write the expected utility of a bid b: u_i(b, v_i) = x_i(b)(v_i – p_i(b)/x_i(b)).” What if x_i(b) = 0?

I think that overall, this is a really interesting contribution to beyond-worst-case analysis and the experiments seem to validate its practical relevance. However, I think the presentation could be improved to make it more understandable for an ICML audience. I’m also hesitant to recommend it for acceptance without a proof of Lemma 5.

======after rebuttal======
I'm willing to believe Lemma 5 if the authors are assuming that number of bidders is constant. I didn't realize that they were making this assumption. The authors should mention this explicitly in the preliminaries and/or the lemma statement."
Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference,"Abhishek Kumar, Prasanna Sattigeri, Tom Fletcher",https://proceedings.neurips.cc/paper/2017/hash/d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html,"The author(s) extend the idea of regularizing classifiers to be invariant to the tangent space of the learned manifold of the data to use GAN based architectures. This is a worthwhile idea to revisit as significant advances have been made in generative modeling in the intervening time since the last major paper in the area, the CAE was published.

Crucial to the idea is the existence of an encoder learning an inverse mapping of the standard generator of GAN training. This is still an area of active research in the GAN literature that as of yet has no completely satisfactory approach.

As current inference techniques for GANs are still quite poor, the authors propose two improvements to one technique, BiGAN, which are worthwhile contributions. 1) They adopt the feature matching loss proposed in ""Improved techniques for training gans"" and 2) they augment the BiGAN objective with another term that evaluates how the generator maps the inferred latent code for a given real example. Figure 2 provides good evidence of the usefulness of these modifications.

However, even with the demonstrated improvements, the label switching problem is still very significant. The reconstructed CIFAR10 examples using their approach are only classified correctly ~35% of the time which causes me to wonder whether the approximation errors of current approaches are still too high to realize the potential benefits of the approach. In addition, the visualized tangents are qualitatively quite different for the encoder and generator which is also suggestive of significant approximation error.

Nonetheless, the empirical results suggests benefits can be realized even with current approximation issues. The results are much stronger for the case of the simpler data manifold of SVHN (improving the FM-GAN baseline from 18.44% to 6.6% error in the case of 500 labeled examples) compared to CIFAR-10 where consistent, but much smaller benefits, are seen. Could the author(s) speculate or comment on why even with all the approximations of the current approach why their system still performs reasonably?

The paper appears to have struggled with the 8 page manuscript limit and abruptly cuts off after the empirical results with no concluding section.","Chiefly theoretical work with some empirical results on SVHN and CIFAR10. This paper proposes using a trained GAN to estimate mappings from and to the true data distribution around a data point, and use a kind of neural PCA to estimate tangents to those estimates, then used for training a manifold-invariant classifier. Some additional work investigating regular GAN vs feature matching GAN is briefly presented, and an augmentation to BiGAN.

It feels a bit like this is two works squeezed into one. Maybe three.

There is an exploration of FM-GAN vs a regular GAN training objective, with some nice results shown where the classification entropy (confidence of the classification, as I read it) is much better for an FM-GAN than for a regular GAN.

There is an Augmented BiGAN which achieves nicely lower classification error than BiGAN on infer-latents-then-generate g(h(x)).

The most substantial work presented here is the manifold-invariance. The first thing I wrestle with is that the method is a bit complex, making it probably tricky for others to get right, and complex/fiddly to implement. In particular, 2.1.2 proposes to freeze f, g, and h, and introduce p and pbar as a nonlinear approximation to SVD. This introduces the second thing I wrestle with: numerous layered approximations. The method requires g and h to be good approximations to generate and infer to and from the data manifold. The results (e.g. figure 2) do not indicate that these approximations are always very good. The method requires that p and pbar reasonably capture singular directions in the latent space, but figure 3 shows this approximation only sort-of holds. This has me wondering about transferability, and wondering how to measure each of the approximation errors to evaluate whether the method is useful for a dataset. The CIFAR-10 results reinforce my concern.

The MNIST result in line 264 (0.86) is quite good for semi-supervised. What is the difference between the results in 264-265 vs the results in Table 1? Different numbers are given for SVHN in each location, yet line 251 suggests the results in both locations are semi-sup. Work would feel more complete with comparisons on semi-sup MNIST in Table 1, then you could show ladder, CAE, MTC, etc. I'm guessing you're up against space constraints here...

Table 1 missing some competitive results. A quick Google search for svhn semi supervised gives https://arxiv.org/abs/1610.02242 showing 7.05% for SVHN with 500 labels, 5.43% with 1000 labels; 16.55% CIFAR10@4k. https://papers.nips.cc/paper/6333-regularization-with-stochastic-transformations-and-perturbations-for-deep-semi-supervised-learning.pdf reports 6.03% on SVHN with 1% of labels (~700).

Missing conclusion/future directions.

Minor nitpicks:
grammar of lines 8-9 needs work
grammar of lines 42-43
lines 72/73 have two citations in a row
lines 88/89 unnecessary line break
line 152 wrong R symbol
line 204 comma before however
lne 295 'do not get as better' grammar needs work

All in all, the results are OK-to-good but not universally winning. The topic is of interest to the community, including a few novel ideas. (The paper perhaps should be multiple works.) It looks like a new SOTA is presented for SVHN semisup, which tips me toward accept.","This paper describes a method for semi-supervised learning which is both adversarial and promotes the classifier's robustness to input variations. The GAN-based semi-supervised framework adopted in the paper is standard, treating the generated samples as an additional class to the regular classes that the classifier aims to label. What is new is the Jacobian-based regularizations that are introduced to encourage the classifier to be robust to local variations in the tangent space of the input manifold.

The paper proposes an efficient method for estimating the tangents space at each training sample, avoiding the expensive SVD-based method used in contractive autoencoders. The paper also proposed an improved version of BiGAN, called Augmented-BiGAN, for training the encoder used in calculating the Jacobians. 

The technical development culminates in a semi-supervised objective that simultaneously incorporates classification of labeled samples, adversarial generative learning of unlabeled/labeled samples, and variation penalties that encourages smoothness of the classifier in the input manifold.   

The experiments are based on CIFAR10 and SVHN, showing the benefits of Augmented-BiGAN over the original BiGAN and the performance improvements on semi-supervised learning due to the Jacobian-based regularizations. The discussions on the experiments could be expanded. A detailed comparison to ALI could be added to make the results more comprehensive.  



 "
Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples,"Moustapha M. Cisse, Yossi Adi, Natalia Neverova, Joseph Keshet",https://proceedings.neurips.cc/paper/2017/hash/d494020ff8ec181ef98ed97ac3f25453-Abstract.html,"In the adversarial machine learning literature, most of research has been focusing on methods for fooling and defending image classifiers.
This the paper instead proposes to generate adversarial examples for machine learning models for structured prediction problems.

The white-box attacks to these image classifiers generate adversarial examples by backgpropagating the gradient from a differentiable classification loss function.
The authors instead propose a loss function (called Houdini) that is surrogate to the actual task loss of the model that we are interested in breaking.

The paper is interesting to the community in that it shows empirical lessons in fooling a set of real-world problems (ASR, human pose estimation, semantic segmentation) different than image classification that has been well studied. However, the overall finding that fooling these models is possible is not totally surprising.
The demonstration of a common surrogate loss (Houdini) that could be applied to multiple applications is important.

My questions are:
- What are the key differences between Houdini surrogate loss and the probit surrogate loss in [18,19] (Keshet et al)? Is the main difference in the gradient approximation where the authors instead use Taylor approximation? (which basically means that the main advantage of Houdini over probit is computation time?)
- What is the main conclusion between direct maximizing task loss vs maximizing Houdini loss (which is the task loss times the stochastic component) in the context of adversarial ML? My takeaways from Table 1 and Table 2 is the two losses perform similarly. Could you please clarify?

At the moment, I vote for borderline ""Marginally above acceptance threshold"". But I'm willing to change my decision after hearing from the authors.","This paper presents a way to create adversarial examples based on a task loss (e.g. word error rate) rather than the loss being optimized for training.  The approach is tested on a few different domains (pose estimation, semantic segmentation, speech recognition).

Overall the approach is nice and the results are impressive.  My main issues with the paper (prompting my ""marginal accept"" decision) are:
- The math and notation is confusing and contradictory in places, e.g. involving many re-definitions of terms.  It needs to be cleaned up.
- The paper does discuss enough whether optimizing the task loss with Houdini is advantageous against optimizing the surrogate loss used for training.  It seems in some cases it is clearly better (e.g. in speech recognition) but the scores given in the image tasks do not really show a huge difference.  There also aren't any qualitative examples which demonstrate this.
Overall I would be happy to see this paper accepted if it resolves these issues.

Specific comments:

- There are various typos/grammatical errors (incl. a broken reference at line 172); the paper needs a proofread before publication.

- ""we make the network hallucinate a minion""  What is a minion?

- ""While this algorithm performs well in practice, it is extremely sensitive to its hyper-parameter""  This is a strong statement to make - can you qualify this with a citation to work that shows this is true?

- ""\eps \in R^d is a d-dimensional...""  I don't see you defining that \theta \in R^d, but I assume that's what you mean; you should say this somewhere.

- Some of your terminology involves redefinitions and ambiguity which is confusing to me.  For example, you define g_\theta(x) as a network (which I presume outputs a prediction of y given x), but then you later define g_\theta(x, y) as the ""score output by the network for an example"".  What do you mean by the ""score output by the network"" here?  Typically the network outputs a prediction which, when compared to the target, gives a score; it is not a function of both the input and the target output (you have defined y as the target output previous).  You seem to be defining the network to include the target output and some ""score"".  And what score do you mean exactly?  The task loss? Surrogate loss? Some other score?  It seems like maybe you mean that for each possible output y (not a target output) given the intput x, the network outputs a score, but this involves a redefinition of y from ""target output"" to ""possible output"".  You also define y as the target, but y_\theta(x) as a function which produces the predicted target.

- ""To compute the RHS of the above quantity..."" Don't you mean the LHS?  The RHS does not involve the derivative of l_H.

- The arguments of l_H have changed, from \theta, x, y to \hat{y}, y in equation (9).  Why?

- ""Convergence dynamics illustrated in Fig. 4 "" - Figure 4 does not show convergence dynamics, it shows ""most challenging examples for the pose re-targeting class...""

- Suggestion: In image examples you show zoomed-in regions of the image to emphasize the perturbations applied.  Can you also show the same zoomed-in region from the original image?  This will make it easier to compare.

- ""Notice that Houdini causes a bigger decrease in terms of both CER and WER than CTC for all tested distortions values.""  I think you mean bigger increase.

- In Table 5 it's unclear if ""manual transcription"" is of the original speech or of the adversarially-modified speech.

- In listening to your attached audio files, it sounds like you are not using a great spectrogram inversion routine (there are frame artefacts, sounds like a buzzing sound).  Maybe this is a phase estimation issue?  Maybe use griffin-lim?

- Very high-level: I don't see how you are ""democratizing adversarial examples"".  Democratize means ""make (something) accessible to everyone"".  How is Houdini relevant to making adversarial examples accessible to everyone?  Houdini seems more like a way to optimize task losses for adversarial example generation, not a way of making examples avaiable to people."
Clustering Stable Instances of Euclidean k-means.,"Aravindan Vijayaraghavan, Abhratanu Dutta, Alex Wang",https://proceedings.neurips.cc/paper/2017/hash/d54ce9de9df77c579775a7b6b1a4bdc0-Abstract.html,"The authors give provably efficient algorithms to recover optimal k-means clusters on input instances that are stable with respect to additive perturbations. This notion is incomparable to scale invariant multiplicative perturbations that were studied earlier [5, 12]. The main technical contribution of this paper is to remove the diam(X) dependency in the exponent from the running time of previous algorithms by Ben David and others for additive perturbation stable instances. As a bonus, this also makes their result robust to a constant fraction of outliers. Overall, the paper is well-written and easy to follow.

In the experimental section, the authors show that their algorithm gives a better initialization than k-means++. I guess this is because their algorithm is robust to outliers whereas k-means++ is extremely sensitive to outliers. However, strangely the last 2 columns of Table 1 suggest that when this initialization is followed by Lloyd's iterations, then k-means++ gets comparable (or sometimes better). ","The authors consider stability of the Euclidean k-means problem, which is well used method in various data analysis. As the measure for the stability, they consider epsilon-APS. 
I have questions for your methods: 
- The various clusterability measures have been proposed as surveyed by M. Ackerman et al (M. Ackerman et al., PMLR. 2009). Comparing these measures, what is an advantage of the measure you used? 
- Can't we use other measures as an initialization method of Lloyd’s method? If we can, how about the performance of them comparing to epsilon-APS.
- The stability of clustering result depends on the parameters rho, delta, and epsilon. It is unclear for me how to find a set of appropriate parameters before we perform clustering. 
- I guess that there are datasets that cannot satisfy epsilon-APS. Can I know the existence of epsilon-APS condition in the dataset before clustering?","The authors propose a notion of additive perturbation stability (APS) for Euclidean distances that maintain the optimal k-means clustering solution when each point in the data is moved by a sufficiently small Euclidean distance. I think the paper is rather interesting; however, the results of the paper are not very surprising.

Here are my comments regarding the paper:

(1) To my understanding, the results of Theorem 1.2 are only under the condition of APS. They only hold for the case of k=2 components and may lead to exponential dependence on $k$ components for large $k$. However, under the additional margin condition between any two pairs of cluster, we will able to guarantee the existence of polynomial algorithm on $k$. Can you provide a high level idea of how this additional assumption actually helps? Is it possible to have situation without that margin condition that there exists no algorithm that is polynomial in terms of $k$?

(2) I find the term $\Delta = (1/2-\epsilon)D$, where $D$ is the maximum distance between any pairs of means, is interesting. It also has a nice geometric meaning, which is the distance between any center to the apex of cone. I wonder whether this term $\Delta$ is intrinsic to the setting of APS? May it still be available under other perturbation notions?

(3) In the result of Theorem 1.3 and later Theorem 3.2, we both require the condition that $\rho$, which is a threshold for the margin between any two pairs of cluster, to be at least the oder of $\Delta/\epsilon^{2}$. As $\epsilon$ is sufficiently small, which is also the challenging theme of APS, the term $\Delta/\epsilon^{2}$ becomes very big, i.e., the distances between any two pairs of cluster become very large.  The fact that there is an algorithm running in time polynomial of $k$ components, sample size $n$, and dimension $d$ under that setting is not surprising. Is it possible to improve the lower bound of the margin between any two pairs of cluster? 

(4) In all the results with running time of the paper, the authors only provide the bound in terms of $n$, $d$, and $k$. I wonder about the constants that are along with these terms. How do they change with margin $\rho$ and $\epsilon$-APS?

(5) As the authors indicate in the paper, data in practice may have very small value of $\epsilon$. It seems that the conditions of the results with polynomial algorithms in the paper, e.g. Theorem 3.2, will not be satisfied by real data.

(6) It seems to me that the results in the paper can be extended to other distances that are different from Euclidean distance. The geometric explanation of additive perturbation stability may be different, which also leads to the modifications of other notions and results in the paper. Such extensions will be very useful in case that we want to work with kernel K-means to deal with sophisticated structures of data."
Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin,"Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi",https://proceedings.neurips.cc/paper/2017/hash/d594b1a945b5d645e59e21f88bd2d83b-Abstract.html,"Aim
The work aim was to classify gene expression over 56 different type of cells based on histones and histones modification marks considered on the whole genome.
They used an attention-based deep learning with a Long Short-Term Memory (LSTM) module to model the dependencies.
The attention-base deep learning is needed to enlight the more significant features in the input matrix, while LSTM is used for the encoding.

Comments
The proposed method provides the possibility to interpret the results and to detect significant features over all the genome considering also the position.
The method works better than previously proposed method in terms of classification score, even though the authors only use the accuracy as performance score. It may be worth commenting also on other metrics. 

In section 3 a direct reference to the figures in supplementary materials would have allowed a better understanding.
It would be interesting to see the behaviour in case of regression since setting the expression as ON and OFF is an over-semplification of the problem.
","The paper presents a novel method for predicting gene regulation by LSTM with an attention mechanism. The model consists of two levels, where the first level is applied on bins for each histone modifications (HM) and the second level is applied to multiple HMs. Attention mechanism is used in each level to focus on the important parts of the bins and HMs. In the experiments, the proposed method improves AUC scores over baseline models including CNN, LSTM, and CNN with an attention mechanism. This is an interesting paper which shows that LSTM with an attention mechanism can predict gene regulation. 

1. It is unclear to me why the second level is modeled using LSTM because there is no ordering between HMs. Would it be reasonable to use a fully connected layer to model dependencies between HMs? Based on Table 1 and 2, the one level model (LSTM-\alpha) outperforms the two level model ((LSTM-\alpha,\beta) in many cases. It would be interesting to investigate how HMs are coupled with each other in the learned model.

2. In Section 2, an algorithm box that includes the equations for the entire model would be very helpful. 

3. It is unclear to me if DeepChrome was compared with the proposed method in the experiments. It would be helpful to indicate which model corresponds to DeepChrome.

4. As baselines, it would be helpful to include non-neural network models such as SVM or logistic regression. 

5. To allow researchers in biology to use this method, it would be very helpful to have the source code publicly available with URLs in the paper. 






 "
Deep Reinforcement Learning from Human Preferences,"Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei",https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html,"The paper introduces a technique for reinforcement learning in scenarios in which the reward function is unclear by asking human experts for preferences between two trajectories.   These preferences are used to inform a learned reward function, which in turn is used to improve the policy producing the trajectories.  The paper produces little theory and no guarantees, but the approach is shown to be competitive with learning from the true reward function, with relatively little human interaction.

The paper should be accepted because it is an important problem, an interesting approach, and the experiments are well-conducted and the results promising; I believe it will get a lot of attention from the community.  However, there are many unanswered questions that I am curious about.

First, how closely does the learned reward \hat r match the true reward function?  I suspect human preferences are informed not strictly by reward, but rather by value, or something like it - so a kind of shaping reward might be appearing in cases where queries outperformed learning from the true reward function.  The question of how good is your reward model seems important for extensions to other domains.

Second, I'm unclear how the compared trajectories are created.  Are the policies stochastic, so that trajectories diverge even as the policy stays the same, or are the trajectories generated from different starting states?","
This paper implements previously existing systems for learning RL policies from human preferences in much higher dimensional problems.

Learning from human preferences has be proposed before, in this special case the authors use comparisons between trajectories has the information about preferences.

The method is a trivially extension without any major difficulties to apply previous preference elicitation methods using policy optimization methods.","Overall I find this paper is generally interesting, clearly presented, and technically sound. My concerns are that the contributions of this paper seems rather incremental when compared to previous work. Also, some of the experiments would benefit from further analysis. Let me elaborate below:

Summary: This paper advocates a preference-based approach for teaching an RL agent to perform a task. A human observes two trajectories generated by different policies and indicates which one is better performing the desired task. Using these indications, the agent infers a reward signal that is hopefully consistent with the preferences of the human, and then uses RL to optimize a policy that maximizes the inferred rewards.

My main hesitation for accepting this paper is that the method presented is not sufficiently different from previous work on preference-based RL (such as Wirth '16). The authors acknowledge that the main contribution of this work is to scale up preference-based RL to more complex domains by using deep reinforcement learning. I am less enthusiastic about this paper if the main algorithmic contribution is just using a NN to approximate the policy and extending to Mujoco/Atari.

Finally, I am concerned with the fact that synthetic queries are outperforming RL using the true reward function. It seems to me that the synthetic query examples should do strictly worse than having the actual reward function as they are just eliciting preferences based on sub-trajectories of the real rewards. The fact that they are in some cases solidly outperforming standard RL is highly unexpected and, in my opinion, deserves a more rigorous analysis. I don't find the authors explanation of the learned reward function being smoother a satisfying one."
Subset Selection under Noise,"Chao Qian, Jing-Cheng Shi, Yang Yu, Ke Tang, Zhi-Hua Zhou",https://proceedings.neurips.cc/paper/2017/hash/d7a84628c025d30f7b2c52c958767e76-Abstract.html,"This paper considers the problem of maximizing a monotone set function subject to a cardinality constraint. The authors consider a novel combination of functions with both bounded submodularity ratio and additive noise. These setting have been considered separately before, but a joint analysis leads to a novel algorithm PONSS. This has improved theoretical guarantees and experimental performance when compared to previous noise-agnostic greedy algorithms. The paper flows well and is generally a pleasure to read.

Clarity
Small typo: ""better on subclass problems"" on line 26

The paper could benefit from a more thorough literature survey, including more recent results on the tightness of the submodularity ratio and applications to sparse regression [1] [2].

One comment on the beginning of Section 2: in this setting minimizing f subject to a cardinality constraint is not the same as maximizing -f subject to a cardinality constraint. Generally if -f is (weakly) submodular then f is (weakly) supermodular, and -f will not have the same submodularity ratio as f.

I would suggest including ""i.i.d. noise distribution"" when the assumptions for PONSS instead are first mentioned on page 2 (""fixed noise distribution""), instead of waiting until Section 5.

Impact
Influence maximization is an interesting application of noisy submodular optimization, since the objective function is often hard to compute exactly.

The authors should address that the PONSS guarantees hold with probability < 1/2 while the other guarantees are deterministic, and explain whether this affects the claim that the PONSS approximation ratio is ""better"".

O(Bnk^2) iterations can be prohibitive for many applications, and approximate/accelerated versions of the standard greedy algorithm can run in sublinear time.

Experiments
Most of my concerns are with the experiments section. In the sparse regression experiment, the authors report final performance on the entire dataset. They should explain why the did not optimize on a training set and then report final performance on a test set. Typically the training set is used to check how algorithms perform on the optimization, and the test set is used to check whether the solution generalizes without overfitting. Additionally, it appears that all of the R^2 values continue to increase at the largest value k. The authors should continue running for larger k or state that they were limited by large running times of the algorithms.

It would be more interesting to compare PONSS with other greedy algorithms (double greedy, random greedy, forward backward greedy, etc.), which are known to perform better than standard greedy on general matroid constraints

Questions
-What is the meaning of multiplicative domination in PONSS experiments if the noise estimate parameter \theta is set to 1? It would be interesting to quantify the drop in performance when 1 > \theta^' > \epsilon is used instead of \epsilon.
-For what range of memory B do the claims on line 170 hold?
-Could PONSS be parallellized or otherwise accelerated to reduce running time?

[1] A. A. Bian, J. M. Buhmann, A. Krause, and S. Tschiatschek. Guarantees for greedy maximization of non-submodular functions with applications, ICML 2017.

[2] E. R. Elenberg, R. Khanna, A. G. Dimakis, and S. Negahban. Restricted strong convexity implies weak submodularity, https://arxiv.org/abs/1612.00804
","The paper analyzed the performance of the greedy algorithm and POSS algorithm under the noise. Moreover, the paper proposed PONSS, a modification of POSS, that has better performance guarantee than POSS. 

Theorem 5 (performance analysis of PONSS) seems very interesting. However, I did not understand the correctness. Thus I tentatively set my decision ""wrong"". Please clarify this point.
I checked other proofs and did not find serious flaw.

[Concern]
First, I would like to clarify the problem setting. Can we obtain sufficiently accurate estimation of f(x) by evaluating F(x) many times?
If NO, the proof of the correctness of Algorithm 3 (PONSS) will be incorrect because the events ""F(\hat x) \le F(x)"" are dependent, so ""Pr(\hat x \in argmin F) \le (1/2)^B"" does not hold.

---
[After rebuttal]

I believe the paper is wrong.
The most serious point is line 210: ""We know from Eq. (5) that Pr(F(\hat x) \le F(x)) \le 0.5.""

To derive Eq.(5), we must ensure that the comparisons are independent.
However, if we have values of F(x), this is wrong.
For example, if F(x) = f(x) + (0 with prob 1/2, M with prob 1/2) for large M, we have ""Pr(F(\hat x) \le F(x)) \le 0.5"" but Eq.(5) is not hold.

I asked this point in the review, and the authors' answered that we can evaluate the function multiple times.

However, this causes another serious issue: ""argmin"" in line 9 of PONSS algorithm does not make sense, because the comparisons are inconsistent.
Moreover, if this issue is solved, the comparison with other algorithms does not make sense because the only PONSS follows the different computational model."
PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,"Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas J. Guibas",https://proceedings.neurips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html,"This paper is improving the PointNet [19] with a recursive approach to introduce local context learning with gradually increasing scales. 

+ The exposition is clear and the math is easy to follow. 
+ Experiments show that PointNet++ achieves the best results on pointset benchmarks. 
- Even though the motivation builds upon local detail recovery ability of PointNet++, this is not strongly supported by the results.

I am leaning towards acceptance due to the fact that clear experiments are provided on benchmarks with state of the art results. ","This paper is a follow-up on the very recent PointNet paper. Its main contribution is to make PointNet more similar to CNNs by introducing the spatial relation between points (in the form of grouping), and thus a form of locality and shift invariance for the low level descriptors. 

While the contribution is straightforward to explain, and at the end not a huge step from PointNet, it is well presented, evaluated, , it was probably some work to make it work efficiently (and it would be good to add numbers about speed in the paper, I don't think I saw them and sampling, centring etc. may improve the time quite a lot) and the results are satisfying (even if they are not amazing: for classification,  I would consider the most meaningful comparison pointnet vs. pointnet++ without normal, so 89.2/90.7 . For segmentation, the comparison without normals should be added )

As a sidenote, I appreciated the results on SHREC non rigid shape classification, I don't know much paper evaluating both for it and ModelNet.","As clearly indicated in the title, this paper submission is an extension of the PointNet work of [19], to appear at CVPR 2017. The goal is to classify and segment (3D) point clouds. Novel contributions over [19] are the use of a hierarchical network, leveraging neighbourhoods at different scales, and a mechanism to deal with varying sampling densities, effectively generating receptive fields that vary in a data dependent manner. All this leads to state-of-the-art results.

PointNet++ seems an important extension over PointNet, in that it allows to properly exploit local spatial information. Yet the impact on the overall performance is just 1-2 percent.

Some more experimental or theoretical analysis would have been appreciated. For instance: 
- A number of alternative sampling options, apart from most distant point, are mentioned in the text, but not compared against in the experiments. 
- In the conclusions, it is mentioned that MSG is more robust than MRG but worse in terms of computational efficiency. I'd like to see such a claim validated by a more in-depth analysis of the computational load, or at least some indicative numbers. "
"Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search","Benjamin Moseley, Joshua Wang",https://proceedings.neurips.cc/paper/2017/hash/d8d31bd778da8bdd536187c36e48892b-Abstract.html,"The authors analyze the lower bounds on some well known clustering algorithms by deriving their approximation ratios. 

I enjoyed reading the paper. It is well written, and has flow and clarity.

A few minor things:

- I don't quite understand the reason for the term 'applied' in the title.

- The popularity of the algorithms or their practical usage is determined not just by their performance in terms of time, but also their ability to produce better results (both according to intrinsic properties of the clusters as well as comparison with some external 'gold' standard). So I am not sure whether we can say that k-means is not popular in practice because it has lower performance in terms of a poor bound on its approximation ratio. A comparison of algorithms on the basis of time is more appropriate for this claim. 

- The paper needs to be proof read for some minor language errors e.g. 'except' instead of expect and 'form' instead of from.

- The paper ends rather abruptly. It needs a proper Conclusions section.","The paper extends the work of Dasgupta towards defining a theoretical framework for evaluating hierarchical clustering. The paper defines an objective function that is equivalent to that of Dasgupta in the sense that an optimal solution for the cost function defined by Dasgupta will also be optimal for the one defined in this paper and vice versa. The behaviour of approximate solution will differ though because the cost function is of the form D(G) - C(T), where D is a constant (depending on the input G) and C(T) is the cost (defined by Dasgupta) and T is the hierarchical solution. The authors show a constant approximation guarantee w.r.t. their objective function for the popular average linkage algorithm for hierarchical clustering. They complement this approximation guarantee by showing that this approximation guarantee is almost tight. They also show that the decisive algorithm for hierarchical clustering gives poor results w.r.t. their objective function. They also claim to show approximation guarantee for a local search based algorithm.

Significance:
The results discussed in the paper are non-trivial extensions to the work of Dasgupta. Even though the model is similar the the previous work, it is interesting in the sense that the paper is able to give theoretical guarantees for the average linkage algorithm that is a popular technique for hierarchical clustering.

Originality:
The paper uses basic set of techniques for analysis and is simple to read. The analysis is original as per my knowledge of the related literature.

Clarity: 
The paper uses simple set of techniques and is simple to read. The authors do a fair job in explaining the results and putting things in context with respect to previous work. The explanations are to the point and simple. There are a few places where more clarity will be appreciated. These are included in the specific comments below.

Quality:
There is a technical error in line 261. It is not clear how the very first inequality is obtained. Furthermore, the last inequality is not justified. The authors should note that as per the analysis, in case (iii) you assume that |B|<=|A| and not the other way around.
Given this, I do not know how to fix these errors in a simple way.


Specific comments:
1. Please use proper references [C] can mean [CC17] or [CKMM17]

2. Line 69: “Let leaves …”. I think you mean leaves denote the set of leaves and not number.

3.Line 91: Please correct the equation. The factor of n is missing from the right hand side.

4. Line 219: Figure 1 is missing from the main submission. Please fix this, as the proof is incomplete without the figure which is given in the appendix."
Thinking Fast and Slow with Deep Learning and Tree Search,"Thomas Anthony, Zheng Tian, David Barber",https://proceedings.neurips.cc/paper/2017/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html,"SUMMARY:

The paper proposes an algorithm that combines imitation learning with tree search, which results in an apprentice learning from an ever-improving expert.
A DQN is trained to learn a policy derived from an MCTS agent, with the DQN providing generalisation to unseen states. It is also then used as feedback to improve the expert, which can then be used to retrain the DQN, and so on.

The paper makes two contributions: (1) a new target for imitation learning, which is empirically shown to outperform a previously-suggested target and which results in the apprentice learning a policy of equal strength to the expert. (2) using the apprentice to improve the expert after each imitation learning step is shown to result in strong play against a policy gradient algorithm for the game Hex. 


COMMENTS:

I found the paper generally well-written, clear and easy to follow, barring Section 6. The idea of combining reactive-style approaches with explicit planning techniques is an interesting one, and makes sense intuitively (I particularly liked the link to the different systems of human thinking), although the downside here is that we then require a model of the environment, which negates the benefits of using model-free approaches (like DQN) in the first place. 

A few questions that I had about the paper:

1. On line 69, some of the drawbacks of RL in general are stated. What guarantees, though, are there that these issues don't apply to ExIt as well?
2. On line 212, why is it advantageous or desirable for the positions in the training set to be similar to those in the test set?
3. I found section 6 lacking in detail, especially for a purely empirical paper. In particular, was the version of ExIt presented here the online version? On lines 122-125, some reasons are provided as to why an online variant may be superior, but there is no experiment validating this. I would therefore expect to see maybe an online version, and a version where the data is discarded after each iteration in the results graph, which could then show that ExIt is better than policy gradients, as well as the advantage (or not) of the online version. 
4. I do not follow the results in section 6.2. What were the ""reference agents"" mentioned in the caption? Which network is being referred to on line 267?  How many games were played? What were the standard errors on those results? What is ""NN evaluations""?


Some remarks on the actual writing:

1. For an arbitrary policy, the definition of the advantage function appears incorrect on line 50. It should be defined as A^\pi(s, a) = Q^\pi(s,a ) - V^\pi(s), where V\pi(s) = Q^\pi(s,\pi(s)) which is not necessarily equal to the max of one-step lookahead as is currently written.
2. In the pseudocode on line 80, there is a function called ""Targets"". Since it is only made clear two pages later what ""Targets"" actually refers to, it may be better to briefly mention its meaning here.
3. The exploration constant c_b in the UCB1 formula on line 162 is not explained. Also, was this set to zero because RAVE was used?
4. On line 232, should it not read ""When a node is TRAVERSED""? The word ""expanded"" gives the impression that it is in the expansion phase of MCTS where a child is randomly added to the tree, as opposed to the tree phase where UCB1 selects the next child. 

There were also a few minor typographical errors:

1. There are missing full stops on lines 101 and 171
2. The footnote on line 167 should appear after the full stop
3. The semicolon on line 187 should be a comma
4. Line 254: ""by network trained"" => ""by a network trained""
5. In the footnote on line 176, ""ADAM"" has been capitalised, but in the supplementary material it is referred to as ""adam""
6. The reference on line 340 has the first names of the authors listed, instead of their surnames.","It's not clear to me what the major distinction is between the proposed approach and AlphaGo. It seems to me that ""expert iteration"" is fundamentally also a variation of using Monte Carlo Tree Search as was in the case of AlphaGo ","In general, I think it is a good paper. I like the idea, and find the paper well written. It would be nice to see how well it does in practice. Why not share a few games played by raw nets and N-MCTS ? What is SOTA in computer Hex ? How is this doing compared to SOTA ? Why Hex ? Why not Go, as this arguably an improvement of AlphaGo N-MCTS ? Also would like to see winrates for final N-MCTS + networks rather than just raw networks winrate. With more solid experiments / analysis it would have been an excellent paper and I would love to give it a higher rating.

Also, can you explain how you convert the policy / softmax of TPT net into an advantage ? Are they just equal ?

l 241. Ho I see, you mean the advantage is the logits of your TPT net ? Would be nice to make that explicit before. Also, this is a bit abusive. Why should the logits of your policy match the advantage of a Q value of that policy ? Would be cleaner to just not use that advantage terminology here and just stick to policy IMHO.

l 245. Do you mean N-MCTS wins 97% against MCTS with the same number of simulations ? But presumably, N-MCTS is much slower to do one iteration, because of neural network evaluation. A more interesting datapoint here would be your N-MCTS winrate against your MCTS with fixed time per move.

l 267. Same comment here. Are these head to head network games played with softmax or argmax ? Softmax is not really fair, it could be the two learning rules results in different temperatures.

Figure 2. Why does your ExIt win curves consist in plateaus ? Would be nice to see the learning dynamics between each dataset iterations. What are the 'reference agents' ?

Figure 2 shows that ExIt imrpove raw networks playning strength. Does this also actually improves the expert N-MCTS ? Would be interesting to see the winning rate curve for the N-MCTS experts. How much stronger is N-MCTS + ExIt network versus N-MCTS + IL network versus N-MCTS + IL->REINFORCE network ?


"
Learning Combinatorial Optimization Algorithms over Graphs,"Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, Le Song",https://proceedings.neurips.cc/paper/2017/hash/d9896106ca98d3d05b8cbdf4fd8b13a1-Abstract.html,"The authors propose a reinforcement learning strategy to learn new heuristic (specifically, greedy) strategies for solving graph-based combinatorial problems. An RL framework is combined with a graph embedding approach. The RL approach effectively learns a greedy policy for selecting constructing an approximate solution. The approach is innovative and the empirical results appear promising. An important advantage of the work is that the learned policy is not restricted to a fixed problem size, in contrast to earlier work.

One drawback of the paper is that I found the writing unnecessarily dense and unclear at various places. In particular, it would be good to include more details on the intuitive description of the approach. I believe the ideas can be stated clearly in words because the concept of learning a greedy policy is not that different from learning any policy as done in RL (Learning the “next move to make” in a game is quite analogous to learning what is the next node in the graph to select. So, the authors can focus more on what makes this work different from learning a game strategy.) The over reliance on formal notation does not help. 

I did not fully check the formal details but some points were unclear. For example, a state is defined as a sequence of action nodes on the graph. Each node in the tagged graph is represented by its p-dimensional embedding (a p-dimensional vector). A state is then defined as the sum of the vectors corresponding to the set of action nodes so far. Why is this choice of state definition the “right one”?

The empirical results are promising but some further insights would be helpful. For most combinatorial problems there is a known basic greedy strategy that already performs quite well. Does RL rediscover those strategies for the domains under consideration? (If the framework has the potential to discover new algorithmic strategy, it would be good to know that the approach can also re-discover known strategies.)
","The authors aim to solve problems in combinatorial optimization by exploiting large datasets of solved problem instances to learn a solution algorithm. They focus on problems that can be expressed as graphs, which is a very general class. Their approach is to 
train a greedy algorithm to build up solutions by reinforcement learning (RL). 

The algorithm uses a learned evaluation function, Q, which maps [a partial solution] and [a candidate change to the solution] to [a value]. To embed graphs of different shapes and sizes in a fixed-length format, they use a learned ""structure2vec"" function (introduced in previous work). The structure2vec function is applied to a partial solution, which is comprised of the input graph where each node is augmented with a per-node feature that indicates a possible solution to the problem. For instance, in minimum vertex cover, the extra feature indicates whether the node is in the cover set. 

The structure2vec and Q functions are implemented as neural networks, and several variants of Q learning are applied to train them. They test on 3 tasks: minimum vertex cover, maximum cut, and traveling salesman problem. They compare their learned model's performance to Pointer Networks, as well as a variety of non-learned algorithms. 

They show that their S2V-DQN algorithm has much better performance than all competitors in most cases, and also generalizes well to problems that are up to 10x larger than those experienced in training. They also show that their approach is often faster than competing algorithms, and has very favorable performance/time trade-offs.

Overall this work is very impressive and should be published. There's not much more to say.","This paper proposes a reinforcement learning framework to learn greedy algorithms which can solve several graph problems, like minimum vertex cover, maximum cut and traveling salesman problem. The underlying contributions of this paper boil down to two points: (1) it provides representations of both graphs and algorithms (2) it provides a way of learning algorithms via reinforcement learning.

Strength:

1, The problem this paper aims at, i.e., “how to represent and learn graph algorithms”, is very interesting and important. I think the framework proposed by this paper is still novel given the fact that there are several existing RL based approaches solving similar problems.

2, Authors did extensive experiments to verify that the proposed method can learn a greedy type of algorithm which generalizes well from small sized graphs to large ones under different tasks.

3, The paper is clearly written.

Weakness:

1, I feel the title is a bit over-claimed as the paper only focuses on one type of combinatorial algorithm, i.e., greedy algorithm and does not discuss how to generalize beyond it. It would be great to have a title which is more specific.

2, In the comparison experiments, cutting off the time of solvers to 1 hour seems to be problematic as it may introduce noise to the approximation ratios. Given the current graph size (~1000), I would recommend authors use better solvers and let them run to the end. Especially, for MAXCUT problem, some advanced SDP solvers can handle this sized graph in a reasonable amount of time.

3, It would be great to show how the propagation step in the struct2vec model affects the performance on graphs which is not fully connected. 

Summary:

Based on above comments, I would like to recommend for acceptance.
"
Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice,"Jeffrey Pennington, Samuel Schoenholz, Surya Ganguli",https://proceedings.neurips.cc/paper/2017/hash/d9fc0cdb67638d50f411432d0d41d0ba-Abstract.html,"This work extends prior work examining linear networks, showing the conditions under which orthogonal initialization makes deep networks well-conditioned, for the case of ReLU and hard-Tanh networks. They find that for ReLU nets, orthogonal initialization should (does) not improve conditioning, but for hard-Tanh it does improve conditioning.

The question of orthogonal initialization has been of particular interest for RNNs, which suffer most from ill-conditioning (this work should perhaps be mentioned e.g. https://arxiv.org/pdf/1511.06464.pdf, https://arxiv.org/pdf/1504.00941.pdf ) and indeed anecdotal results seem to be mixed.

I do not have sufficient background to follow the proofs, but I think the results would be of interest to the NIPS community.

However, I'm disappointed that the authors chose to use such an unrealistic model for their experiments (L219). The model is not specified clearly in the paper or supplement, but it sounds like a 200-layer fully-connected network (judging from a comment in the supplement). Certainly this is not a reasonable network for MNIST or CIFAR. I would like to see comparisons on reasonable models for the dataset being presented. Final accuracy is not reported (hard to tell from the plots in the supplement).

Detailed comments

L99: lighting/lightning

2.3 ""It appears that sigma ~ 4\epsilon L^-1"" where is this shown? Figures plot distributions for particular values of L.

L166 I think you're saying that \sigma^CRE / \sigma^(linear+Gaussian) -> 1 . Make that clear because for CRE linear networks it's well-conditioned. Also, what about the O(1)? I hope it's O(eps) or something because you are looking at a function with a range of [0, 1] so O(1) kills you.

","The article is focused on the problem of understanding the learning dynamics of deep neural networks depending on both the activation functions used at the different layers and on the way the weights are initialized. It is mainly a theoretical paper with some experiments that confirm the theoretical study. The core of the contribution is made based on the random matrix theory. In the first Section, the paper describes the setup -- a deep neural network as a sequence of layers -- and also the tools that will be used to study their dynamics. The analysis mainly relies on the study of the singular values density of the jacobian matrix, this density being computed by a 4 step methods proposed in the article. Then this methodology is applied for 3 different architectures: linear networks, RELU networks and tanh networks, and different initialization of the weights -- Ginibre ensemble and Circular Real Ensemble. The theoretical results mainly show that tanh and linear networks may be trained efficiently -- particularly using orthogonal matrix initialization -- while RELU networks are poorly conditioned for the two initialization methods. The experiments on MNIST and CIFAR-10 confirm these results. 

First, I must say that the paper is a little bit out of my main research domain. Particularly, I am not familiar with random matrix theory. But, considering that I am not aware of this type of article, I find the paper well written: the focus is clear, the methodology is well described, references include many relevant prior works helping to understand the contribution, and the authors have also included an experimental section. Concerning the theoretical part of the paper, the proposed results seem correct, but would certainly need a deeper checking by an expert of the domain. Concerning the experimental section, the experiments confirm the theoretical results. But I must say that I am a little bit skeptical concerning the topic of the paper since all the demonstrations are focused on the simple deep neural network architectures with linear+bias transformations between the layers. It is a little bit surprising to see results on CIFAR-10 which is typically a computer vision benchmark where convolutional models are used, which is not the case here. So I have some doubt concerning the relevance of the paper w.r.t existing architectures that are now much more complex than sequences of layers. On another side, this paper is certainly a step toward the analysis of more complex architectures, and the author could argue on how difficult it could/will be to use the same tools (random matrix theory) to target for example convolutional networks. 

","The authors of this manuscript investigate the learning dynamics of supervised deep learning models with nonlinear activation functions. It is shown that the singular value distribution of the input-output Jacobian matrix is essential to the initialization of model parameters.  This claim performs well on tanh networks but not ReLU networks. 

Majors:
(1) The authors may investigate with their theory why the layer-wise pretraining can provide a good warm start in the model learning. 
(2) I also wonder whether this random matrix theory is applicable to generative models such as deep Boltzmann machine and deep belief net. 
(3) The presentation of the method section should be revised to improve its readability to general readers.

Minors:
The authors tend to use lowercase when uppercase is needed, such as fig, eqnt, relu, and terms in the references. 
"
Adaptive Classification for Prediction Under a Budget,"Feng Nan, Venkatesh Saligrama",https://proceedings.neurips.cc/paper/2017/hash/d9ff90f4000eacd3a6c9cb27f78994cf-Abstract.html,"The paper proposes a framework to train a composite classifier under a test-time budget for feature construction. The classifier consists of a high-performance model, built with no constraints on cost, a low-prediction cost model and a gating function which selects one of the two models to be used for a test sample. 

The setup of the problem is fairly specific: there are no constraints on complexity or feature computation cost during training time, however, at test time the prediction should be made as well as possible using as few features as possible (with a tradeoff). While several domain where this setup is relevant are mentioned in the intro, no details are given on the intended application until the experiment section. Also, only for one of the datasets on which this was tested (the Yahoo one) is runtime prediction even a consideration. The other datasets seem to have been arbitrarily selected. All these factors create the impression that the setup is somewhat artificial.

The authors did a thorough job of reviewing budget-constrained classifiers, though they might also consider “Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive Models” IJCAI ’16, by Daniel P. Robinson and Suchi Saria.

Sections 3 and 4 (problem setup and algorithms) are well presented, the optimization is well designed and the I-projection solution for the linear version is clever.
The one part that’s not entirely clear is Remark 1. “train a new pair of gating and prediction models to approximate the composite system” - why wouldn’t this result in the same solution? what changes in the re-training? wouldn’t you want to just approximate the expensive component in the prediction using its assigned samples?

The experiments on synthetic data help in explaining why complex predictors can be summarized cost-efficiently with the current model and show some of the advantages of the method. However, maybe this part can be shortened and more room be allocated to a discussion on why this method outperforms GreedyMiser. 
What I find a surprising is that, in Figure 4, the performance of GreedyMiser (gradient-boosted trees) is nowhere near that of the other two models even when the cost is high. This might happen because the algorithm is improperly used or the base classifier is either badly tuned or in general not as well-performing as random forests.

The classification cost differs per sample; do does the graph show the average cost for a point or the total cost across all test samples averaged over several runs?

Typo:
line 168: “g can effective” should be “g can effectively”.

Update following the rebuttal: the authors have answered the questions and concerns about Figure 4, so the decision is updated accordingly.","1. Summary of paper

This paper introduced an adaptive method for learning two functions. First, a `cheap' classifier that incurs a small resource cost by using a small set of features. Second, a gating function that decides whether an input is easy enough to be sent to the cheap classifier or is too difficult and must be sent to a complex, resource-intensive classifier. In effect, this method bears a lot of similarity to a 2-stage cascade, where the first stage and the gating function are learned jointly.


2. High level subjective

The paper is written clearly for a reader familiar with budgeted learning, although I think a few parts of the paper are confusing (I'll go into this more below). I believe the idea of approximating an expensive, high-accuracy classifier is new (cascades are the most similar, but I believe cascades usually just consider using the same sort of classifiers in every stage). That said, it is very similar to a 2-stage cascade with different classifiers. I think there are certainly sufficient experiments demonstrating this method. Likely this method would be interesting to industry practitioners due to the increased generalization accuracy per cost.


3. High level technical

Why not try to learn a cost-constrained q function directly? If you removed the KL term and used q in the feature cost function instead of g, wouldn't you learn a q that tried as best to be accurate given the cost constraints? Did you learn a g separately in order to avoid some extra non-convexity? Or is it otherwise crucial? I would recommend explaining this choice more clearly.

I would have loved to see this model look less like a cascade. For instance, if there were multiple expensive 'experts' that were learned to be diverse w.r.t each other and then multiple gating functions along with a single cheap classifier were learned jointly, to send hard instances to one of the experts, then this would be a noticeable departure from a cascade. Right now I'm torn between liking this method and thinking it's a special cascade (using techniques from (Chen et al., 2012) and (Xu et al., 2012)).


4. Low level technical

- Line 161/162: I don't buy this quick sentence. I think this is much more complicated. Sure we can constrain F and G to have really simple learners but this won't constrain computation per instance, and the only guarantee you'll have is that your classifier is at most as computationally expensive as the most expensive classifier in F or G. You can do better by introducing computational cost terms into your objective. 
- Line 177: There must be certain conditions required for alternating minimization to converge, maybe convexity and a decaying learning rate? I would just clarify that your problem satisfies these conditions.
- I think Figure 5 is a bit confusing. I would move Adapt_Gbrt RBF to the top of the second column in the legend, and put GreedyMiser in its original place. This way for every row except the last we are comparing the same adaptive/baseline method.


5. Summary of review

While this method achieves state-of-the-art accuracy in budgeted learning, it shares a lot of similarities with previous work (Chen et al., 2012) and (Xu et al., 2012) that ultimately harm its novelty.","The paper proposes to learn under budget constraints (of acquiring a feature) by jointly learning a low-cost prediction and gating model. Specifically, it gives two algorithms - for linear models and non-parametric models (gradient boosted trees). The paper is well presented explaining the motivations and the scope of the work. 

Concerns:
1. It would be interesting to make a further study about the gains achieved for each of the datasets. For example, CIFAR-10 offers little gains as compared MiniBooNE dataset. Is this because the data does not satisfy some of the assumptions made in the paper ? If so, what are the datasets (from a statistical sense) for which this technique will not have significant cost reduction ?
2. Figure 4 is squished. Please correct this add and gridlines to increase readability. 
3. L50 - low-cost gating
4. It would be cleaner to avoid notational overload such as g(x) = g^Tx. "
Online Convex Optimization with Stochastic Constraints,"Hao Yu, Michael Neely, Xiaohan Wei",https://proceedings.neurips.cc/paper/2017/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html,"This paper considers online convex optimization with time varying constraints. The goal is to minimize the regret similar to standard online convex optimization setting, and at the same time keep the average violation of stochastic constraints well-bounded. The authors address the problem in both fixed and stochastic settings and under mild conditions on the constraints, obtain the optimal \sqrt{T} log T bounds on both regret and violation of constraints that improves upon the best known T^{3/4} bound on the violation of constraints. The authors conducted preliminary experiments on a stochastic job scheduler that complements their theoretical results.

The problem being dealt with is interesting and has proper motivation, though previous papers have dealt it with in the past, in one way or another, but this paper has shown a significant improvement over existing bounds on the regret and violation of constraints. The presentation of the paper was mostly clear. The claimed contributions are discussed in the light of existing results and the paper does survey related work appropriately. Regarding the quality of the writing,  the paper is reasonably well written, the structure and language are good. The paper is technically sound and the proofs seem to be correct as far as I checked (I did not have enough time to carefully check the Appendix and just did few sanity checks,  but definitely will take a careful look in rebuttal period). 

Overall, the problem dealt with consists of a new view to the online linear optimization with constraints that requires a new type of analysis. The exact setup of the feedback lacks motivation, but overall the idea of an analysis aimed to have long-term violation of constraints instead of expensive (and sometime impossible) projections in online learning is sufficiently interesting and motivated.







","The paper considers online convex optimization with constraints revealed in an online manner. In an attempt to circumvent a linear regret lower bound by Mannor et al [17] for adaptively chosen constraints, the paper deals with the setting where constraints are themselves generated stochastically. As a side effect, superior results are obtained for related problems such as OCO with long-term constraints.

The paper does a nice job of introducing previous work and putting the contribution in perspective. The main algorithm of the paper is a first order online algorithm that performs an optimization step using the instantaneous penalty and constraint functions. The formulation is similar to the one that leads, for example, to the classical Zinkevich's OGD update rule except for a term that involves the new constraint functions introduced at that time step. The formulation also uses an adaptive time varying regularization parameter (called a ""virtual queue"" in the paper) for the constraint terms in the formulation. Indeed, as Lemma 1 indicates, the solving the formulation is equivalent to a projected gradient descent (PGD)-style step, with the notion of ""gradient"" now including the constraint function subgradients as well. The virtual queue values are updated to remain lower bounds on the total constraint violation incurred till now.

In terms of theoretical analysis, the arguments relating regret to drift etc seem routine. The most novel portion of the analysis seems to be the drift analysis and its adaptation to the ||Q(t)|| terms (Lemma 7). The paper reports one experimental use of the algorithm on a distributed job scheduling problem. The proposed algorithm is able to offer comparable throughput to a previously proposed approach called REACT while lowering costs by around 10%.

- It would be nice if the paper could clearly point out the challenges in extending the PGD framework to incorporate time varying constraints. It seems except for the drift analysis, the rest of the arguments are routine after proper choices have been made for virtual queue values etc.
- Please give some intuition on how was the form of the virtual queue values (equation 3) motivated. It seems to be a lower bound on the cumulative violation (or ""backlog"") but some intuition would much help.
- Do the results presented in the paper extend to online mirrored descent? I do not see any obvious hurdles but some comments would be nice.
- Experimental work is a bit sparse.","***Summary***
The paper proposes an algorithm to address a mixed online/stochastic setting where the objective function is a sequence of arbitrary convex functions (with bounded subgradients) under stochastic convex constraints (also with bounded subgradients).
A static regret analysis (both in expectation and high-probability) is carried out. A real-world experiment about the allocation of jobs across servers (so as to minimize electricity cost) illustrates the benefit of the proposed method.

Assessment:
+ Overall good & clear structure and presentation
+ Technically sound and appears as novel
+ Good regret guarantees
- Confusing connection with ""Deterministic constrained convex optimization""
- Some confusion with respect to unreferenced previous work [A]
- More details would be needed in the experimental section

More details can be found in the next paragraph.

***Further comments***
-In Sec. 2.2, the rationale of the algorithm is described. In particular, x(t+1) is chosen to minimize the ""drift-plus-penalty"" expression. Could some intuition be given to explain how the importance of the drift and penalty are traded off (e.g., why is a simple sum, without a tuning weight, sufficient?). 

-Lemma 5: I do not really have the expertise to assess whether Lemma 5 corresponds to ""a new drift analysis lemma for stochastic process"". In particular, I am not sufficiently versed in the stochastic process literature.

-Could some improvements be obtained by having \alpha and V dependent on t?

-For Lemma 9, it seems to me that the authors could reuse the results from Proposition 34 in [B]

-Comparison with ""OCO with long term constraints"": It appears that [A] (not referenced) already provide O(sqrt(T)) and O(sqrt(T)) guarantees, using similar algorithmic technique. This related work should be discussed.

-Comparison with ""Stochastic constrained convex optimization"": Is [16] the only relevant reference here?

-Confusion comparison with ""Deterministic constrained convex optimization"": In the deterministic constrained setting, we would expected the optimization algorithm to output a feasible solution; why is it acceptable (and why does it make sense) to have a non-feasible solution here? (i.e., constraint violation in O(1/sqrt(T)))

-Experiments:
   - More details about the baselines and the implementation (to make things reproducible, e.g., starting points) should appear in the core article
   - Is the set \mathcal{X_0} = [x_1^min, x_1^max] \times ... \times [x_100^min, x_100^max]? If this is the case, it means that problem (2) has to be solved with box constraints. More details would be in order.
   - Log-scale for unserved jobs (Figure d) may be clearer
   - Bigger figures would improve the clarity as well
   - An assessment of the variability of the results is missing to decide on the significance of the conclusions (e.g., repetitions to display standard errors and means).
   - To gain some space for the experiments, Sec. 4 could be reduced and further relegated to the supplementary material.

-Could the analysis extended to the dynamic regret setting?

***Minor***
-line 52: typo, min -> argmin. I may have missed them in the paper, but under which assumptions does the argmin of the problem reduce to the single x^*? 
-line 204 (a): isn't it an inequality instead of an equality?


[A] Yu, H. & Neely, M. J. A Low Complexity Algorithm with O(sqrt(T)) Regret and Constraint Violations for Online Convex Optimization with Long Term Constraints preprint arXiv:1604.02218, 2016

[B] Tao, T.; Vu, V. Random matrices: universality of local spectral statistics of non-Hermitian matrices The Annals of Probability, 2015, 43, 782-874

==================
post-rebuttal comments
==================

I thank the authors for their rebuttal (discussion about [A] was ignored). I have gone through the other reviews and I maintain my score. I would like to emphasize though, that the authors should clarify the discussion about the stochastic/deterministic constrained convex opt. case (as answered in the rebuttal)."
Structured Bayesian Pruning via Log-Normal Multiplicative Noise,"Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry P. Vetrov",https://proceedings.neurips.cc/paper/2017/hash/dab49080d80c724aad5ebf158d63df41-Abstract.html,"Summary: The paper explores group sparsity in Bayesian neural networks. The authors use truncated log-uniform distributed random variables to turn off uninformative neurons/filters. Experiments demonstrating sparsity without significant generalization loss are presented. 

Clarity and Quality: The paper is sufficiently clear and easy to follow. The proposed group sparsity inducing priors are a natural extension of existing work interpreting Gaussian dropout as variational inference in BNNs. Truncation is a technically reasonable approach for dealing with issues stemming from the use of improper priors necessitated by this interpretation. Overall the paper is technically sound.

Originality and Significance - The primary contribution of the paper is the use of truncated log-uniform priors over scale (noise) variables to induce group structured sparsity in BNNs. Automatic model selection in BNNs is an interesting problem and being able to turn off uninformative neurons and filters is a useful step in that direction. 

Other Comments: 
1) The choice of dealing with improper log-uniform prior seems cumbersome. Alternate, sparsity inducing proper priors such as continuous relaxations of the spike and slab (Horseshoe variants) are more convenient to work with. Under an appropriate parameterization, they replace the hard constraints on the range of theta  [0, 1], with a smooth penalty and hence an easier optimization problem. These alternate priors do weaken the connection to the variational interpretation of dropout, but so does truncation and the connection to variational dropout is not necessary for structured sparsity.

2) While it may be a common computational trick, summarizing the posterior with a point estimate for test predictions is fraught with difficulties when the posterior variance hasn’t collapsed to (nearly) zero. It will be useful to include an analysis of the posterior variances of theta_i, to ascertain the quality of approximation of Eqn. 18.  In any case, the median is perhaps a better summary of the skewed log-normal distribution than the mean and may be worth exploring in place of Equation 18. 

3) The discussion on Page 4 justifying the use of log-Normal family of variational approximations for theta is bordering on unnecessary. While Gaussian variational approximations are commonly used for approximating the distributions of rvs with support over the entire real line no sensible variational inference scheme uses Gaussian approximations for rvs with support on only the positive half of the real line. Log-Normals are a popular and well-known choice for such random variables.  ","Summary:

The paper addresses the actual problem of compression and efficient computation of deep NNs. It presents a new pruning technique for deep NNs, which is an enhancement of the Bayesian dropout method. Contrary to standard Bayesian dropout, the proposed method prunes NNs in a structured way. The NN-model is altered by dropout layers with an advanced probabilistic model (with truncated log-uniform prior distribution and truncated log-uniform posterior distribution). For the altered model, the authors propose a new ""signal-to-ratio"" pruning metric applicable to both individual neurons and more complex structures (e.g., filters). 

The submission and supplementary files contain a theoretical explanation of the method and derivation of the corresponding formulas. Moreover, an experimental evaluation of the approach is presented, with promising results. 

Quality:
The paper has very good technical quality. It contains a thorough theoretical analysis of the proposed model and its properties. It also presents several experimental results, that evaluate the qualities of the method and compare it with two concurrent approaches.

Clarity:
The presentation is comprehensible and well-organized, the method is described in detail including derivation of formulas and implementation details. 

Originality and significance:
The paper addresses the actual problem of compression and efficient computation of deep NNs. Its main contribution is the presentation of a new general technique for structured pruning of NNs that is based on Bayesian dropout. Contrary to the original method, 
1) it uses more sophisticated and proper noise distributions and it involves a novel pruning metric,
2) the subject of pruning are not weights, but more complex structures (e.g., filters, neurons).
3) The method can be applied to several NN architectures (CNN, fully-connected,...), and it seems that it can be easily added to existing NN implementations.

The presented approach proved in the experiments to massively prune the network and to speed up the computation on GPU and CPU, while inducing only small accuracy loss. However, based on the experimental results presented, the benefit over the concurrent ""SSL"" approach is relatively small.

Typo:
...that is has ... (line 117)

Comments:
- Table 1 and Table 2 contain rows with results for concurrent methods called ""SSL"" and ""SparseVD"", without a short description of the main principles of these methods in the text.
- It would be useful to assess also the efficiency of the pruning process itself.
- The method has several parameters that need to be set. An interesting question is, how does the actual setting of these parameters affect the balance between sparsity and accuracy of the final model. 

","The authors proposes a new dropout variant which is demonstrated in a neural network unit-pruning application, carried out by learning the dropout rates under a sparsity-inducing prior. The methodology follows that of recent Bayesian approaches to deep learning, and the application is similar to that of [1] (which has some fundamental limitations stemming from the improper prior and loose KL approximations). The main novelty of the present paper is the use of a truncated log uniform prior instead of [1]'s log uniform prior, and truncated log normal approximating posterior (instead of [1]'s Gaussian approximating distribution). The authors justify why the choice of the prior and approximating distribution is sensible, and give an analytical KL which does not diverge to infinity unlike [1]'s KL. It is worth noting though that the authors do not place a prior distribution over the weights, but rather place a prior over latent ""dropout"" variables. Ie the model the authors perform inference in is not a Bayesian neural network (like in [1,2]) but rather a latent variable model (like [3]).

Major comments to the authors:
* Please clarify in the introduction what the prior is placed over (ie over auxiliary latent variables); it was not clear until page 3.
* The literature survey is lacking many related works such as [2, 3, 4] ([3] being most directly related to the authors' latent variable model).
* I would expect to see a comparison to [1] by removing units with large alpha parameter values as a baseline.

Minor comments to the authors:
* The Random Labels experiment is rather neat
* Placing dropout after each convolution layer to enforce sparsity over kernel-patch pairs has been proposed before by [5]
* Eq. (4) - it is not clear what ""max_phi"" is
* Missing parenthesis on line 147
* Appendix line 8: I assume x = log theta? There also seems to be a mistake in the first line.


References:
[1]
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In NIPS. Curran Associates, Inc., 2015.
[2]
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. ICML, 2016.
[3]
Shin-ichi Maeda. A Bayesian encourages dropout. arXiv preprint arXiv:1412.7003, 2014.
[4]
S Wang and C Manning. Fast dropout training. ICML, 2013.
[5]
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approximate variational inference. ICLR workshop track, 2016.
"
Clustering with Noisy Queries,"Arya Mazumdar, Barna Saha",https://proceedings.neurips.cc/paper/2017/hash/db5cea26ca37aa09e5365f3e7f5dd9eb-Abstract.html,"The noisy query model considered in this paper is as follows. One is allowed to ask queries of the form : do u,v belong to the same cluster? The oracle outputs +1 if they are and -1 otherwise, but it is allowed to make an error with a certain probability. Suppose the set of points to be clustered is denoted by V = V_1 \sqcup ... \sqcup V_k, with k clusters. The goal is minimize the number of queries and recover the exact clustering with a big enough probability. This paper gives a lower bound for the number of queries, also gives a quasi-polynomial time algorithm that achieves the lower bound upto a log(n) factor and finally gives computationally efficient algorithm with a worse query complexity. 

The paper claims to introduce the noisy query model for clustering. I am not aware of the literature, but this seems to be a very natural model to study and capture a lot of interesting real-life problems. The paper also has many interesting results which improves on previous papers in various regimes. In the symmetric case (that is, when the error probability for pairs belong to same cluster is same as the error probability for pairs belonging to different clusters), the paper gives a lower bound of Omega(nk/(1-2p)^2), and an algorithm that runs in O(n log n + k^6) time and makes at most O(nk^2 log n/(1-2p)^4). The experiments show a difference in the number of nonadaptive queries and adaptive queries required. 

Questions to the authors:
1) Could you elaborate on the experiments? What is the meaning of the sentence on line 363-364? Suppose you want to query if u,v belong to the same cluster, how many users do you ask this question and how do you determine this number?","NOTE: I am reviewing two papers that appear to be by the same authors and on the same general topic.  The other one is on noiseless queries with additional side information.

This paper gives upper and lower bounds on the query complexity of clustering based on noisy pairwise comparisons.  While I am familiar with some of the related literature, my knowledge of it is far from complete, so it's a little hard to fully judge the value of the contributions.  One limitation is that the main result would be extremely easy to obtain if the noise were independent when sampling the same pair multiple times.  On the other hand, the case where the same noisy answer is always returned is well-motivated in certain applications.

My overall impression of both papers is that the contributions seem to be good (if correct), but the writing could do with a lot of work.  I acknowledge that the contributions are more important than the writing, but there are so many issues that my recommendation is still borderline.  It would have been very useful to have another round of reviews where the suggestions are checked, but I understand this is not allowed for NIPS.  

Here are some of my more significant comments:
- Many of the central results are not written precisely.  The order of quantification is Lemma 1 is unclear (e.g., Pr(for all v) vs. for all v, Pr(...)).  The last step in its proof only holds for sufficiently large k, whereas the lemma is stated for arbitrary k.  The appendix is full of theorems/lemmas/claims with the imprecise phrase ""with high probability"", and in some cases it seems to be crucial to specify the convergence rate (even if it's done in the proof, it should still appear in the formal statement).
- Near the bottom of p3, the runtime and query complexity are not consistent, with the latter exceeding the former in certain regimes like k = n^(1/6) or p very close to 1/2.  I guess this is because certain dependences were omitted in the runtime (which they shouldn't be without further statement).
- The appendices are often hard to follow.  The strange and somewhat unclear phrases ""Suppose if possible"" and ""Let if possible"" are used throughout.  Inequalities like the one at the end of Claim 2's proof appear questionable at first (e.g., when p is close to half), and the reader has to go back and find the definitions of c, c' (and check their dependence on p).  There are a lot of statements along the lines of ""similarly to [something earlier]"", which I trusted but wasn't certain of.
- The paper is FULL of grammar mistakes, typos, and strange wording.  Things like capitalization, plurals, spacing, and articles (a/an/the) are often wrong. Commas are consistently mis-used (e.g., they usually should not appear after statements like ""let"" / ""we have"" / ""this means"" / ""assume that"" / ""is"", and very rarely need to be used just before an equation).  Sentences shouldn't start with And/Or.  There are typos like ""entrees"", ""once crucial"".  The word ""else"" is used strangely (often ""otherwise"" would be better).  This list of issues is far from complete -- regardless of the decision, I strongly urge the authors to proof-read the paper with as much care as possible.

Here are some comments of a technical nature:
- The paper is so dense that I couldn't check the upper bound proof in as much detail as ideal.  I was reasonably convinced of the proof of the lower bound.
- In Section 2, I don't see why the first ""balanced"" condition is useful.  Even with a given maximum cluster size, the exact same problem as that described in the first few sentences can still occur, making the problem ill-posed.  I think you should only keep the condition on the minimum cluster size.
- The numerical section does not seem to be too valuable, mainly because it does not compare against any existing works or simple baselines.  Are your methods really the only ones that can be considered/implemented?

Some less significant comments:
- The second half of the abstract can be considerably shortened
- The algorithm descriptions are very wordy.  It might have been useful to accompany them with figures and/or an ""Algorithm"" environment.
- Sometimes extra citations are needed (e.g., runtime of heaviest weight subgraph, Turan's theorem, and maybe previous use of the TV->KL technique on p5 like Auer et al 1998 bandit paper)
- I would remove ""initiate [a rigorous theoretical study]"" from the abstract/intro.  Given that there are so many related works, I don't think this is suitable.  It is even less suitable to simultaneously say it in two different papers.
- In the formal problem statement ""find Q subset of V x V"" makes it sound like the algorithm is non-adaptive.
- The sentence ""which might also be the aggregated..."" is confusing and should be removed or reworded
- Many brackets are too small throughout the appendix
- In the Unknown k algorithm, l (which should be \ell?) is never updated, so k=2l always assigns the same value
- I would avoid phrases like ""information theoretically optimal algorithm"", which makes it sound like it is THE algorithm that minimizes the error probability.


[POST-AUTHOR FEEDBACK COMMENTS]

The authors clarified a few things in the responses, and I have updated the recommendation to acceptance.  I still hope that a very careful revision is done for the final version considering the above comments.  I can see that I was the only one of the 3 reviewers to have significant comments about the writing, but I am sure that some of these issues would make a difference to a lot more NIPS readers, particularly those coming from different academic backgrounds to the authors.

Some specific comments:
- The authors have the wrong idea if they feel that ""issues like capitalization and misplaced commas"" were deciding factors in the review.  These were mentioned at the end as something that should be revised, not as a reason for the borderline recommendation.
- I am still confused about the ""balanced"" condition and minimum/maximum cluster size.  My understanding is that if you only assume a bound on the maximum size, then the problem is in general ill-posed, by your example (definitions of C_1 and C_2) on lines 149-150.  If you only assume a bound on the minimum size, everything looks fine.  You could also assume both simultaneously, but this doesn't seem to be what you are doing (since on line 155 you write ""either of"" rather than ""both of"").   Consider re-wording/explaining more as needed.
- Please revise/re-word the runtime and query complexity on p3 -- if the stated runtime only holds for p = constant, then you should add something like ""when p is constant"".  I think cases like k = n^(1/6) (where samples seems to exceed runtime) might still look strange to some readers, but since there is no true contradiction, I don't insist on anything.
- It is important that theorems and lemmas are stated precisely (e.g., all assumptions are included like k being large or certain quantities behaving as constants, and convergence rates of probabilities are specified if they play an important role in the later analysis)
- Although it is not a major point, I will mention that I don't recall seeing ""Suppose if possible"" or ""Let if possible"" despite reading mathematical papers from many domains.  The latter in particular sounds very strange.  Even for the former, at least for me, a re-wording to something like ""Suppose by way of contradiction that..."" would be clearer."
Compression-aware Training of Deep Networks,"Jose M. Alvarez, Mathieu Salzmann",https://proceedings.neurips.cc/paper/2017/hash/db85e2590b6109813dafa101ceb2faeb-Abstract.html,"The authors present a regularization term that encourages weight matrices to be low rank during network training, effectively increasing the compression of the network, and making it possible to explicitly reduce the rank during post-processing, reducing the number of operations required for inference. Overall the paper seems like a good and well written paper on an interesting topic. I have some caveats however: firstly the authors do not mention variational inference, which also explicitly compresses the network (in the literal sense of reducing the number of bits in its description length) and can be used to prune away many weights after training - see 'Practical Variational Inference for Neural Networks' for details. More generally, almost any regularizer provides a form of implicit 'compression-aware training' (that's why they work - simpler models generalize better) and can often be used to prune networks post-hoc. For example a network trained with l1 or l2 regularization will generally end up with many weights very close to 0, which can be removed without greatly altering network performance. I think it's important to clarify this, especially since the authors use an l2 term in addition to their own regularizer during training. They also don't compare seem to compare how well previous low rank post processing works with and without their regulariser, or with other regularisers used in previous work. All of these caveats could be answered by providing more baseline results in the experimental section, demonstrating that training with this particular regulariser does indeed lead to a better accuracy / compression tradeoff than other approaches.
In general I found the results a little hard to interpret, so may be missing something: the graph I wanted to see was a set of curves for accuracy vs compression ratio (either in terms of number of parameters or number of MACs) rather than accuracy against the strength of the regularisation term.  On this graph it should be possible to explicitly compare your approach vs previous regularisers / compressors.","This paper proposes a low-rank regularizer for deep model compression during training. Overall, this paper is well-written, and the motivation is clear. However, here are some comments as follows.
1 The novelty is relatively limited, as the technical parts are strongly relevant to the previous works.
2 The experiments should be further improved. 
(1) Parameter sensitivity: From Fig 1, the performance of the proposed method (\tau is 1,\lambda is not 0) is similar to [2] (\tau is 0,\lambda is not 0). For other settings of \tau, the compression rate is improved while the accuracy is reduced.
(2) Results on larger models: the comparison with [2] should be performed to show the effectiveness. Furthermore, it would be interesting to compare with other state-of-the-art compression approaches, such as [18].
","The paper is interesting.  The propose training neural networks with a cost that explicitly favors networks that are easier to compress by truncated SVD.  They formulate this regularization as a cost on the nuclear norm of the weight matrices, which they enforce with the soft threshold of the singular values as the proximal operator after every epoch.  I found the idea interesting, and the experimental sections I thought gave a nice breakdown of the results of their own experiments and the behavior of their proposed method, but I would have liked to see some more comparative results, i.e. the performance of their own network versus other compression techniques targeting the same number of parameters on the datasets, for instance.  Overall good paper, interesting idea, good execution, but experiments somewhat lacking."
Maxing and Ranking with Few Assumptions,"Moein Falahatgar, Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati, Vaishakh Ravindrakumar",https://proceedings.neurips.cc/paper/2017/hash/db98dc0dbafde48e8f74c0de001d35e4-Abstract.html,"The paper considers a PAC setting with implicit feedback, namely the arms can be only compared in a pairwise manner. Four research questions are addressed in the paper: assuming strong stochastic transitivity, can the best item be found using O(n) samples and the ranking using O(n log n) samples. The same questions are addressed for Borda ranking for general preference matrix. The authors gave positive answers for all question except the second. 

The idea of the proposed algorithm is to distinguish low and high confidence cases (whether \delta < 1/n). The low confidence case is handled by using a sequential elimination strategy, whereas in the high confidence case, an anchor item is chosen which is among the top-k items, and then this is used to eliminate the worst half of the items. This algorithm is nice. 

The paper is well-written and easy to follow. The questions that are addressed are of importance. In fact, the negative result is somewhat surprising to me. I checked the supplementary material as well, it seems ok. And the supplementary material is also easy to follow.

I have some minor comments:

The following paper might be related: Kevin G. Jamieson and Sumeet Katariya and Atul Deshpande and Robert D. Nowak: Sparse Dueling Bandits, AISTAT, 2015. The paper shows that without assumoption on the preference matrix, the lower bound fo finding the Borda winner is \Omega( n^2). That is why it might be worth to cite this paper. The realted work is already comprehensive.

Regarding the experiments: all algorithm tested in the paper are based on some kind of elimination strategy. The PL-PAC algorithm presented in Szorenyi et al. Online rank elicitation for Plackett-Luce: A dueling bandits approach., is an exception to some extend since it implements a a sorting-based sampling. It would be interesting to compare its performance to the elimination based algorithms. In the experiments of that paper, it had been found that the elimination strategies have quite high sample complexity comparing to the sorting-based sampling. But of course, that algorithm assumes BTL about the preference matrix which is a strong assumption, therefore its accuracy might drop if the model assumption is not true.

","Thanks for the rebuttal, which was quite helpful and clarified most of the issues raised in the review. 

The authors study the problem of PAC maximum selection (maxing) and ranking of n elements via random pairwise comparisons. Problems of that kind have recently attracted attention in the ML literature, especially in the setting of dueling bandidts. In this paper, the only assumption made is strong stochastic transitivity of the pairwise winning probabilisties. Under this assumption, the authors show that maxing can be performed with linearly many comparisons while ranking has a quadratic sample complexity. Further, for the so-called Borda ranking, maximum selection and ranking can be performed with O(n) and O(n log n) comparisons, respectively. 

The paper addresses an interesting topic and is well written. That being said, I'm not convinced that is offers significant new results. It's true that I haven't seen the first two results in exactly this form before, but there are very similar ones, and besides, the results are not very surprising. Further, the argument for the result regarding the non-existence of a PAC ranking algorithm with O(n log n) comparisons in the SST setting is not totally clear. 

More importantly, the last two results are already known. Theorem 8 this known as Borda reduction, see e.g., Sparse Dueling Bandits, Generic exploration and k-armed voting bandits, Relative upper confidence bound for the k-armed duelling bandit problem. Likewise, Theorem 9 is already known, see e.g., Crowdsourcing for Participatory Democracies, Efficient elicitation of social choice functions.

Consequently, the only result that remains, being both novel and relevant, is the PAC maxing algorithm and its analysis. Nevertheless, a study of its performance on real data is missing, as well as a comparison with other algorithms.              

Indeed, the experiments are not very convincing, especially because only a simple synthetic setting is considered, and no experiments for (Borda) ranking are included. SEQ-ELIMINATE is a subroutine of OPT-MAXIMIZE, so does it really makes sense to compare the two? It would be rather more meaningful to compare one of the two with other maxing algorithms in the main paper and move their comparison, if necessary, to the appendix. Lines 189-299: models with other parameters should be considered. In all maxing experiments, it should be tested whether the found element is the actual maxima. 
			  
Minor issues:

- 51: an \epsilon-ranking (and not \epsilon-rankings)

- 100: comparisons (and not comparison)

- 117-118: Please reconsider the statement: ""the number of comparisons used by SEQ-ELIMINATE is optimal up to a constant factor when \delta <= 1/n"".  \delta <= 1/n => n <= 1/\delta => O(n/\epsilon^2 \log(n/\delta)) = O(n/\epsilon^2 \log(1/\delta^2)) != \Omega(n/\epsilon^2 \log(1/\delta)) 

- 117-118: Please reconsider the statement: ""but can be \log n times the lower bound for constant \delta"", since n/\epsilon^2 \log(1/\delta) \times \log(n) != n/\epsilon^2 \log(n/\delta).

- 120-122: It is not clear how should such a reduction together with the usage of SEQ-ELIMINATE lead to an optimal sample complexity up to constants. PLease elaborate.

- 130-131: ""replacing or retaining r doesn’t affect the performance of SEQ-ELIMINATE"". The performance in terms of what? And why there is no effect?

- 165: same comment as for 117-118

- 167: in 120 it was stated that S' should have a size of O(n/\log n), and here It's O(\sqrt(n\log n))

- 165-169: The procedure described here (the reduction + usage of SEQ-ELIMINATE) is different from the one described in 120-122. And again, it's not clear how should such a procedure results in an order-wise optimal algorithm.

- 204: Lemma 12 does not exist.

- 207: Lemma 11 does not exist.

- 210: at least (and not atleast)  

- 215: \delta is missing in the arguments list of PRUNE

- 221-222: same comment as for 165 and 117-118

- 238-239: The sample complexity is not apparent in both cases (distinguished in algorithm 2)"
DropoutNet: Addressing Cold Start in Recommender Systems,"Maksims Volkovs, Guangwei Yu, Tomi Poutanen",https://proceedings.neurips.cc/paper/2017/hash/dbd22ba3bd0df8f385bdac3e9f8be207-Abstract.html,"this paper proposed to use dropout for addressing cold-start problems in CF. The model proposed an architecture that uses user / item latent factors (can be learned from MF) and content features as input layer, and uses dropout on latent factors to make the model perform better on cold start scenarios. Overall the proposed method does not seem very novel: 1) dropout is a well known idea in neural networks, 2) take a step back, even before dropout / NN became popular, making sure training data / test data have similar distribution is common sense. Specifically, if we want to train a model to handle cold start scenarios, it is standard to not use user / item preferences. Also, from experiments it may not be necessary to use dropout to train a unified model at all. If we look at Figure 2, the performance on cold-start is highest with dropout = 1, then why not simply train a model without preferences (dropout = 1) to handle cold-start and train the full model to handle warm-start?","This paper presents a neural network based latent model that uses both the user / item responses and features for warm-start and cold-start scenarios. The main novelty claimed by the paper is to use drop-out to help better learn the feature-based latent model for cold-start. Below are some points:
1. Although it looks to me that leveraging drop-out in neural networks to better learn the feature-based model to help cold-start is a great idea, in this paper the methodology is not clearly written to be fully convincing. For example, in Algorithm 1, what is the probability of applying options 1-5 is not clearly stated. Is that a tuning parameter? If so, how does this relate to drop out rate? In Section 4.3, I am not sure if simply using mean(V_v) for user factors U_u is the best idea, some better justification is required there in my opinion. 
2. Although the authors did a lot of experiments to demonstrate that this approach works better than the other baselines, there are a couple of points that I feel should be further improved. First, In Figure 3 (a) DNN-WMF was compared to a bunch of baselines, but in (b) it was only compared to DeepMusic, and in (c) WMF was not there. Was it intentional or there are some other reasons? Second, in Section 5.1 as the CiteULike is binary data, wouldn't it be better if we use a logistic Loss function rather than square d loss? 
3. Some other minor points. (1) U_u and V_v were not properly defined in the paper. This caused a lot of confusion when I was reading it. (2) The paper needs a clear definition of cold-start vs warm-start. Does cold-start means no data at all for item / users? (3) Page 8, line 349, remove the duplicate ""is able to"".","Excellent paper, presenting an good idea with sufficient experiments demonstrating its efficiency. Well written and with a good survey of related work. 
The paper addresses the following problem: when mixing id level information (user id of item id) with coarser user and item features, any model has the tendency to explain most of the training data with the fine grained id features, using coarse features only to learn the rarest ids. As a result, the generated model is not good at inference when only coarser features are available (cold-start cases, of a new user or a new item). The paper proposes a dial to control how much the model balances out the fine grained and coarser information via drop-out of fine grained info while training.

The loss function in Equation (2) has the merit of having a label value for all user-item pairs, by taking the output of the low rank model U_u V_v ^T as labels. My concern with such a loss is that the predictions from the low rank model have very different levels of accuracy: they are very accurate for the most popular users and the most popular items (users and items with most data), but can be quite noisy in the tail, and the loss function in Equation (2) does not account for the confidence in the low rank model prediction. Replacing the sum by a weighted sum using as weights some simple estimates of the confidence in each low rank model prediction could improve the overall model. But this can be done in future work.

Question about experiments on CiteULike: Figure 2 and Table 1 well support the point the author is making about the effect of dropout on recall for both, warm start cases and cold start cases. However, it is surprising that the cold start recall numbers are better than the warm start recall numbers. Can the authors explain this phenomenon?

Minor details:
Line 138: f_\cal{U} -> f_\cal{V}
Line 349: is able is able -> is able
"
Unsupervised Image-to-Image Translation Networks,"Ming-Yu Liu, Thomas Breuel, Jan Kautz",https://proceedings.neurips.cc/paper/2017/hash/dc6a6489640ca02b0d42dabeb8e46bb7-Abstract.html,"The paper provides an elegant model framework to tackle the problem of image-to-image translation (unsupervised). The model framework consists of a VAE connected to a GAN, with weight sharing at the latter layers of the encoder and former layers of decoder. I've read and reviewed hybrid VAE-GAN approaches before, and this one seems a bit nicer than the rest. They also add skip-connections and stuff to cope with the spatial information loss in the code given to the decoder when you make the encoder deeper.

In terms of experiments, they have three sets of experiments. They are sensible experiments, well constructed and all, but the results aren't shocking or much better than what I'd expect.

Overall, a well-written, and elegant paper. Definitely a strong accept from my side.","
This paper investigated the problem of unsupervised image-to-image translation (UNIT) using deep generative models. In contrast to existing supervised methods, the proposed approach is able to learn domain-level translation without paired images from two domains. Instead, the paper proposed to learn a joint distribution (two domains) from a single shared latent variable using generative adversarial networks. Additional encoder networks are used to infer the latent code from each domain. The paper proposed to jointly train the encoder, decoder, and discriminator in an end-to-end style without paired images as supervision. Qualitative results on several image datasets demonstrate good performance of the proposed UNIT pipeline.

== Qualitative Assessment ==
The unsupervised domain transfer is a challenging task in deep representation learning and the proposed UNIT network demonstrated a promising direction based on generative models. In terms of novelty, the UNIT network can be treated as an extension of coGAN paper (reference 17 in the paper) with joint training objectives using additional encoder networks.

Overall, the proposed method is sound with sufficient details including both qualitative and quantitative analysis. I like Table 1 that interpolates and compares the existing work as subnetworks in UNIT. 

Considering the fact that UNIT extends coGAN paper, I would like to see more discussions and explicitly comparisons between coGAN and UNIT in the paper. Performance-wise, there seems to be slight improvement compared to coGAN (Table 3 in the paper). However, the evaluation is done on MNIST dataset which may not be conclusive at all. Also, I wonder whether images in Figure 8 are cherry-picked? Compared to coGAN figures, the examples in Figure 8 have less visual artifacts. I encourage authors to elaborate this a little bit.

* What’s the benefit of having an encoder network for image-to-image translation?
Image-to-image translation can also be applied to coGAN paper: (1) analysis-by-synthesis optimization [1] & [2]; or (2) train a separate inference network [3] & [4]. Can you possibly comment on the visual quality by applying optimization-based method to a pre-trained coGAN? I encourage authors to elaborate this a bit and provide side-by-side comparisons in the final version of the paper.

* What’s the benefit of joint training?
As being said, one can train a separate encoder/inference network. It is not crystal clear to me why VAE type of training is beneficial. Maybe the reconstruction loss can help to stabilize the training a little bit but I don’t see much help when you add KL penalty to the loss function. Please elaborate this a little bit.

Additional comments:
* Consider style-transfer as alternative method in unsupervised image-to-image translation?
Given the proposed weight sharing architecture, I would assume the translated image at least preserves the structure of input image (they are aligned to some extent). If so, style-transfer methods (either optimization based or feed-forward approximation) can serve as baseline in most experiments.

* Just for curiosity, other than visual examination, what is the metric used when selecting models for presentation?
Though the hyperparameters have little impact as described in ablation study, I wonder how did you find the good architecture for presentation? At least quantitative evaluations can be used as metric In MNIST experiment for model selection. Since you obtained good results in most experiments, I wonder if this can be elaborated a bit.

[1] Generative Visual Manipulation on the Natural Image Manifold, Zhu et al. In ECCV 2016;
[2] Reference 29 in the paper;

[3] Picture: A Probabilistic Programming Language for Scene Perception, Kulkarni et al. In CVPR 2015;
[4] Generative Adversarial Text-to-Image Synthesis, Reed et al. In ICML 2016;
","The author(s) propose an architecture for un-paired correspondence learning between two domains by leveraging a combination of VAEs, GANs, and weight sharing constraints. 

The paper is clearly written with a nice suite of both quantitative and qualitative experiments. The improvement in Unsupervised Domain Adaptation on the SVHN->MNIST task from 84.88% to 90.53% is a strong result and much more interesting than the more trivial case of the MNIST->USPS and vice-versa since SVHN and MNIST are significantly different domains. The ablation experiments are an important inclusion as well since there are quite a few design/architecture decisions involved here. It would be a nice extension to see these ablation studies also carried out on the SVHN->MNIST task as the euclidean distance in pixel space metric which the current ablation test uses can be an unreliable and difficult to interpret as it has semi-arbitrary scaling.

There have been quite a few related approaches in this area of research recently. While the authors include a paragraph in the related work section briefly discussing this, a more thorough comparison/discussion of the differences and motivations of their approach would be appreciated. In particular, the approach appears to have design decisions, such as high-level weight sharing, that are similar to ""Coupled generative adversarial networks"" which appeared at NIPS last year. A more clear comparison and discussion of the differences here could help readers better understand the contributions of the paper. Could the author(s) comment on what are the benefits/drawbacks of their approach compared to these several other architectures so as to help practitioners/researchers make better informed decisions in this space?"
SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability,"Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",https://proceedings.neurips.cc/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html,"The paper presents an analysis of deep networks based on a combination of SVD and CCA to test similarity between representations at different layers. The authors use this technique to analyze aspects of the deep neural network training procedure, for example, how the representation builds bottom-up throughout training, and suggest an improved training procedure based on this analysis where lower-layers are frozen for most the training.

The paper is well-written and contributions are clearly stated.

The authors characterize the representation of a neuron by the vector of its activations for each data point, and then represent each layer as a matrix composed of the activation vectors for each neuron. Abstracting each layer as a kernel, and applying kernel SVD / CCA (with the linear kernel as a special case) would seem a more natural way to describe the same problem.

In section 2.1, it is unclear why the authors perform CCA on two networks instead of PCA on a single network, except for the sake of expressing the analysis as a CCA problem. It would be good to have PCA as a baseline.

While the bottom-up emergence of representation is an interesting effect, it is unclear whether the effect is general or is a consequence of the convnet structure of the model, in particular, the smaller number of parameters in the early layers.

In Figure 4, it seems that the representations have relatively low correlation until the very end where they jump to 1. It suggests noise in the training procedure for the top layers. It would be interesting to test whether the frozen layers training strategy lead to higher correlations also for the higher layers.","The authors proposes a very nice and beautiful method. Their interpretation of a single neuron as a function is very influential. I strongly believe that with this interpretation one can understand deep networks and even further design much better ones by using tools from functional analysis. The paper is very well written and presented. Actually, the toy dataset represents the benefits of the experiments very clearly. The theoretical contributions are intuitive and makes their method applicable to CNNs. The experiments are convincing. Being all these said, I would like to see experiments on larger datasets.","This paper presents a novel technique for analyzing the learned representation of neural networks conditioned on the training or the validation data points. There are some shortcomings of the paper:
1)	Although the proposed method is general, the authors only constrained their experiments to convolutional networks on a small task (CIFAR 10). They are encouraged to extend the experiments to fully connected networks, as well as going for larger and more interesting datasets (< 1M data points) using the proposed faster method.
2)	How could the proposed method be of direct benefit for model building? For example: (1) Could the proposed method be used to suggest a freezing schedule during training? (2) Could SVCCA be used to determine how small a network could be to be able to approximate/compress a larger network? 

Minor comments:
-	From figure (2)-a: does lower layers need more directions to achieve better accuracies?
-	Check “Analysis of Deep Neural Networks with the Extended Data Jacobian Matrix” by Wang et al. from ICML 2016. They have a very similar objective and conclusions but a slightly different method.
-	In the model compression section, refer to prior art on using SVD weight compression, and papers on model compression/distillation. 
-	The paper is too long, and some essential details are presented in the supplementary section. I suggest that the authors move the DFT method to the supplementary section to free up some space without impacting the paper’s readability.

"
Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe,"Quentin Berthet, Vianney Perchet",https://proceedings.neurips.cc/paper/2017/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html,"This paper considers stochastic bandit problem where cumulative loss of player is a global function of actions rather the standard additive linear loss. The authors borrows ideas from Frank-Wolf and UCB algorithms and propose a new algorithm to achieve the optimal regret in this setting, which is quite intriguing

Overall, the paper is convincing and well written. The problem is very relevant for machine learning and statistics applications and the proof arguments are quite convincing. I could not find major flaw in the paper, and the theoretical proofs in the paper and appendix look solid to me. However, it would be more convincing if the authors can provide supportive experimental results
","This is a very interesting paper at the intersection of bandit problems and stochastic optimization. The authors consider the setup where at each time step a decision maker takes an action and gets as a feedback a gradient estimate at the chosen point. The gradient estimate is noisy  but the assumption is that we have control over how noisy the gradient estimate is.  The challenge for the decision maker is to make a sequence of moves such that the average of all iterates has low error  compared to the optima. So the goal is similar to the goal in stochastic optimization, but unlike stochastic optimization (but like bandit problems) one gets to see limited information about the loss function.

For the above problem setting the authors introduce an algorithm that combines elements of UCB along with Frank-Wolfe updates. The FW style updates means that we do not play the arm corresponding to the highest UCB, but instead play an arm that is a convex combination of the UCB arm and a probability distribution that comes from historical updates. The authors establish stochastic optimization style bounds for a variety of function classes.  The paper is failry well written and makes novel contributions to both stochastic opt and bandit literature. I have a few comments

1. It is not clear to me why the authors consider only Frank-Wolfe algorithm. Is there a deep motivation here or can the arguments made in this paper be translated to even work with other optimization algorithms such as stochastic gradient descent etc...?

2. In lines 214-219, it is not clear to me why one needs to invoke KKT condition. It was not clear to me what the relevance and importance of this paragraph is. Can the authors shine some light here?

3. Definition of strong convexity in line 230 is incorrect.

4. It would perhaps be useful to moved the lines 246-249 to the start of the paper. In this way it will be more clear why and how the problem considered here is different from standard stochastic optimization algorithms.

5. In line 257 what is \sigma_max?
","The authors consider a problem of sequential optimization over the K-dimensional simplex where, at time t, a learner selects \pi_t \in \{1,...,K\}, and one observes a noisy version of the \pi_t-th component of the gradient of a unknown loss function L evaluated at point p_t = {1 \over t} \sum_{s < t} e_{\pi_s}. The goal is to minimize the final loss E[L(p_T)] after T rounds of such a mechanism.

The authors propose and analyze a varient of the Frank-Wolfe algorithm which includes confidence bounds, to account for the uncertainty due to noisy gradient estimates. The authors show a O( \sqrt{K log T \over T} ) regret upper bound for functions L whose gradient is Lipschitz continuous and a O( {K log T \over T} ) regret upper bound in the case where L is strongly convex. Matching lower bounds are also provided.

Overall the paper is interesting, the algorithm seems novel and the regret lower and upper bounds match in several cases of interest. My only concern is with the setting which does not seem very general nor intuitive: namely the accuracy of gradient estimates typically increases as time goes by: the error is of order O(1 \over \sqrt{T_i}) where T_i is the number of times i \in \{1,...,K\} has been selected. While this makes sense for the classical multi-armed bandit, this does not seem very natural in most problems (except for the examples provided in the paper, all of them being quite specific). Indeed, the most natural setting seems to be the case where the variance of the noise in the gradient estimate is constant (this is the case considered by most work in stochastic optimization and continuous bandits).

Maybe I misunderstood what the authors meant but in the example of sequential allocation shouldnt the variance be 
\sum_{i=1,...,K: T_i > 0} {\sigma_i \over T_i} 
and not:
\sum_{i=1,...,K} {\sigma_i \over T_i}  ?
(since the variance must remain bounded)"
Identifying Outlier Arms in Multi-Armed Bandit,"Honglei Zhuang, Chi Wang, Yifan Wang",https://proceedings.neurips.cc/paper/2017/hash/dcda54e29207294d8e7e1b537338b1c0-Abstract.html,"The paper introduces the problem of finding all the ""outlier arms"" within a multi-armed bandit framework, where an arm is considered to be an outlier if its mean is at least k\sigma more than the average of the set of means (of the arms), where \sigma is the standard deviation within the set of means and k is some input parameter. The authors also provide an algorithm for the PAC setting, and upper bound its sample complexity.

The problem with the paper is that the model seems to be just another variant of top-k arm selection with no real motivation. What is the task where it makes more sense to find the outlier arms instead of finding the best 5 arms, or the ones with mean above some threshold? Additionally, the analysis does not seem to reveal some novel insights either (the estimate of \sigma is straightforward as well); and the bounds are not even attempted to be made tighter than what follows from straightforward application of existing methods.

Some minor issues and additional questions:
- l66-70: what is the difference between (a) and (c)?
- in the definition of \sigma and \hat{\sigma} below lines 116 and 119: there should be a sum over the i index.
- the sample complexity bound seems to be proportional to k^2. Is this not suboptimal?
- line 197: H_2 and H_1 should be bold face.
- line 236: what is a region in this context?
- line 252: how do you construct and update the set \hat{Omega} for this algorithm?
- Why do you present results of different nature in Figure 1 and in Figure 2? Why would it be meaningful to measure the performance differently for the synthetic and for the twitter data?
- How did you obtain Equation (15)? It does not seem to follow from (14).
- What are the variables you applied McDiarmid's inequality on to obtain (17)?

*******

The current emphasis is on the parts that are more straightforward. Highlighting the technical difficulties that the authors had to overcome could significantly help appreciating this work. Additionally, the motivation should be significantly improved upon.","This work is concerned with a variant of the multi-armed bandit problem, and more specifically a pure exploration, fixed confidence problem. Here, the objective is to identify with probability at least 1-\delta the arms that are over a threshold, using as few samples as possible. The twist, compared to a setting introduced in [1], is that this threshold is not known a priori by the learner, and instead depends on the means of the arms (unknown to the learner) and an outlier parameter k (known to the learner). With respect to the existing literature, this game can be seen as a pure exploration bandit problem with a double exploration trade-off: first, pulling arms in a balanced way as to estimate each mean to improve the estimate of the (unknown) threshold \hat \theta and second, pulling arms whose means are close to \hat \theta in order to determine whether they belong to the set of outliers. If \theta were known to the learner, it is well understood that one would require roughly 1/(y_i - \theta)^2 samples to determine to which set arm i belongs. To solve this problem, two algorithms are introduced. The first one is a simple uniform exploration algorithm, that comes down to a test (making use of confidence intervals on the estimated threshold and means) as to discover which arms are outliers with high probability. The second algorithm is a distorted version of this algorithm, which overpulls (with respect to the uniform sampling) arms that are still active (i.e. we have not yet identified in which set they belong). The analysis shows that careful tuning of this distortion parameter \rho leads to improved performances over the uniform sampling algorithm. The tuning of this parameter with respect to the complexity measures H_1 (worst case complexity) and H_2 (which is similar to the usual complexity measure in pure exploration problems) is discussed. Extensive experiments are conducted to validate the theoretical results, on both a synthetic and real dataset.
I believe there is a typo in the definition of \sigma_y (there should be a sum). Some references are missing see e.g. [1] and [2,3]. In [2,3] the objective is to estimate as well as possible all the means. On the other hand [1] introduced the thresholding bandit problem. Though the lower-bound construction therein does not apply trivially (it is harder to construct problems for the lower-bound given all the interdependencies between the means in this setting), in spirit the complexity terms seem very related, as one should not expect to have a complexity for this problem lower than H_2 (as defined in this work). It would be interesting to see an even more adaptive strategy to tackle this problem, maybe with a different definition for the (problem dependent) threshold if this one proves too difficult for natural adaptivity. I don’t understand the 99% claim in the abstract, you should tone it down or explain it better in the experiments section. 

Regarding the experiments, I believe that \delta should be set to a much lower value (0.01) and correspondingly they should be repeated at least 1000 times (even if \delta is not changed, experiments should be averaged over many more than 10 runs). I believe that an interesting baseline could be one where the threshold is given a priori to the learner, and it can simply successively accept or reject arms with confidence parameter \delta/n (to account for the union bound). This strategy should be the hardest baseline (which we don’t expect to beat, but it shows the possible improvement).  Some further: they could be repeated with different values of \rho, and the \rho parameter used here should be discussed. It would be interesting to see how the “worst-case” \rho parameter as described in (6) performs against a perfectly tuned \rho, on different problems. The synthetic experiments could be run instead on carefully crafted instances that correspond to different extreme problems.
Overall, I believe this new problem is well motivated, and the two algorithms introduced are natural strategies to tackle this problem which should serve as strong baselines for future work. The paper is written in a very clear and clean fashion, and it is easy to follow. One weakness is that the algorithms and techniques used are rather basic as the problem itself is hard to tackle. Moreover, a fixed budget transposition of the setting seems like challenging problem to solve in the future.
[1] Locatelli, Andrea, Maurilio Gutzeit, and Alexandra Carpentier. ""An optimal algorithm for the thresholding bandit problem."" Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48. JMLR. org, 2016.
[2] Carpentier, Alexandra, A Lazaric, M Ghavamzadeh, R Munos, Peter Auer
 ""Upper-confidence-bound algorithms for active learning in multi-armed bandits."" International Conference on Algorithmic Learning Theory. Springer, Berlin, Heidelberg, 2011.
[3] Antos, A., Grover, V., Szepesvári, C.: Active learning in heteroscedastic noise. Theoretical Computer Science 411, 2712–2728 (2010)
","The paper proposes a new bandit problem: among n arms, the learner must find those whose expected payoff is above a certain problem-dependent threshold that is unknown. Concretely, she must identify the outliers defined as the arms whose empirical average is above \mu + k\sigma where \mu is the overall average of all the means of the arms of the problem and \sigma is the standard deviation.

The problem by itself is new and interesting as it cannot be trivially solve by existing best-arm identification algorithms. The paper exposes the idea very clearly and is easy to read overall. 

My comments are the following :

- The definition of \sigma is broken, the sum is missing.

- The RR algorithm does not look like a bandit algorithm stricto sensu. There is no real adaptivity in the sampling so it looks ore like sequential statistical tests. Can you comment on the use of the Generalized Likelihood Ratio statistic as in ""Optimal Best Arm identification"" by (Garivier & Kaufmann, 2016) ?  Their case is parametric but as you consider bounded distributions in most of your results, it does not seem more restrictive. 

- One missing result of this paper is a Lower Bound on the expected sample complexity. One would like to have at least an idea of how it compares to lower bounds on other problems. I think this problem is not trivial at all so it might be a very open question but do you have an idea of how to obtain such a result ?

- I am surprised that the chosen 'state-of-the-art' algorithm that is used to compute the IB base line is CLUCB by (Chen et al, 2014). In your case, you only want to identify sequentially the best arm of the remaining ones so it seems to me that a standard (not combinatorial) algorithm would work. Why not use the Track and Stop by (Garivier & Kaufmann, 2016) which has been proven optimal and whose code is available online ? 

typo: l.31 'If'

It is overall a good paper, though I'm curious to read answers to my comments. I could increase my rating after the author's rebuttal. 

%% After rebuttal %%%

I read the rebuttal and I thank the authors for their answers to my concerns and their precise comments to all the reviewers. I maintain my overall good appreciation of the paper which, I believe, presents an interesting new setting.
"
Discovering Potential Correlations via Hypercontractivity,"Hyeji Kim, Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",https://proceedings.neurips.cc/paper/2017/hash/dcf6070a4ab7f3afbfd2809173e0824b-Abstract.html,"This paper suggests a measure of “potential influence” between two random variables, and a method for estimating this measure from data samples. The motivation is that existing measures are ""average"" measures, and cannot capture “rare” but important dependencies. Specifically, the setup considered by the authors is where two random variables X and Y are independent on much of X’s support (or domain),  but there is a small subset of X on which Y and X have strong dependency. This is articulated in the Axiom 6 in the paper, which requires that a proper measure of influence should attain a value 1, if there is a sub-domain of the variable over which Y=f(X) is a deterministic (Borel measurable and non-constant) function.  

This is a well-written paper on an interesting topic. The proposed method for estimating hypercontractivity coefficient looks interesting. The authors conduct experiments with both synthetic and real-world data and show relative advantage of the proposed measure over some alternatives. 

At the same time, it’s not clear to me what are the advantages of the proposed measure over mutual information (MI). In fact, it seems that MI can capture similar type of dependency, and one only have to modify the 6th axiom, so that instead of 1 the measure would be infinity. 

One practical issue for using MI as a measure of dependency is that most existing estimators  fail to capture strong relationships. This was pointed out in [Gao et. el, AISTATS’15, “Efficient Estimation of Mutual Information for Strongly Dependent Variables”, https://arxiv.org/abs/1411.2003], where the authors showed that, for a wide class of estimators, accurate estimation of mutual information, I(X:Y) between two  variables X and Y, requires a sample size that is exponential in I(X:Y), due to strong boundary effects. Thus, when Y is a near-deterministic function of X (so that I(X:Y) diverges), one needs extremely large sample size for accurate estimation. This point was also shown empirically in a recent critique of MIC by Kinney and G. Atwal,  Equitability, mutual information, and the maximal information coefficient, PNAS 111(9):3354–3359, 2014. 

Furthermore, the Gao et. al. paper also proposed a new estimator that corrected for the boundary bias, and which was shown to capture strong relationships with limited samples.  Thus, to claim that the hypercontractivity coefficient is a better measure than MI, the authors should compare their results to Gao et. al.’s estimator. 


Other comments:  

- The authors have to clarify the setup more explicitly, e.g., that they deal with one dimensional variables. For instance, the main intuition stated in the Introduction is that “if X and Y are highly correlated, it is possible to get a very compact summary of X that is very informative about Y”. It’s obvious that this statement do not hold in the most general case [e.g., if Y is a binary variable, X is a p-dimensional real vector where each component is a noisy copy of Y, then X can be strongly correlated with X, yet any compression of X will lead to information loss]

- The connection with IB is not very clear, especially since the authors do not specify the set up. E.g., in typical IB setting, X is a high dimensional variable, Y is a label (from a finite set), and U is a compression of X.  Looking at the objective in Eq. (1), wouldn’t it be maximized if U was an exact copy of Y?  

- It would help if the supplementary material contained mostly novel contributions, and used references to point to existing relevant work. 
","This paper explores the usage of the hypercontractivity (HC) measure to discover the potential influence of one variable on another. One drawback of previous measures, such as the correlation coefficient, is that they measure only the average influence; whereas sometimes one is also interested in discovering whether there exists any dependence in the data, even one that holds only on a subset of values. The paper proposes a set of desirable axioms (inspired by Renyi's axioms for a measure of correlation) for such a measure, and then proceeds to show that HC fulfills all of them. Furthermore, a method to estimate the HC from samples is presented (also shown to be consistent). Finally, the paper applies the estimator to both synthetic and real-world data, showing that it successfully captures dependencies that make intuitively sense and that other measures do not capture.

PROS:

- This is a clean and well-executed paper that conceptually clarifies the HC coefficient and its usage in estimating dependencies between two random variables. In particular, axiom 6 (apparently novel) is IMO an extremely powerful requirement.

CONS:

- None really, apart from the very minor detail that at least 1-2 of the synthetic datasets should have been in the main text for illustrative purposes.","Summary:
   Finding measures of correlation among data covariates is an important problem. There have been several measures that have been proposed in the past - maximum correlation, distance correlation and maximum information coefficient. This paper introduced a new correlation measure by connecting dots among some recent work in information theory literature. The candidate in this paper is called hyper-contractivity coefficient or recently also is known as information bottleneck. 

   Information bottleneck principle is this: Whats the best summary of variable X (most compressed) such that it also retains a lot of information about another variable Y. So for all summaries of X, U the maximum ratio of I(U;Y)/I(U;X) is the hyper contractivity coefficient. 
  
  This quantity has another interpretation due to recent work. Consider the conditional p(y|x) and marginal p(x) associate with joint distribution of (X,Y). Consider a different distribution r(x) and the marginal r(y) due to the joint r(x) p(y|x). The hypercontractivity coefficient is the maximum ratio of Divergence between r(y) (induced distribution on y by the change distribution r(x)) and p(y) to the divergence between r(x) and p(x). In other words ""whats the maximum change at output y that can be caused by a new distribution that looks similar to x in KL distance""

 Renyi has come up with several axioms for characterizing correlation measures. The authors show that if you make one of the axioms strict in a sense, and then drop the need for symmetry, then hypercontractivity satisfies all the axioms.

 To be less abstract, the goal of authors is that they like to have a measure which has high correlation value even in for most values of X there seems to be no influence on Y while in some rare region of the X space, correlation with Y is almost deterministic. This has practical applications as the authors demonstrate in the experiments. It turns out the hypercontractivity coefficient  has this property while all other previous measures do not have this property provably. This is also related to their modification of the axioms of Renyi by a stronger one (axiom 6 instead of 6').

the authors propose an asymptotically consistent estimator using the second definition of hyper contractivity. They define an optimization problem in terms of the ratio of the change distribution r(x) and original distribution p(x) which they solve by gradient descent.To come to this formulation they use existing results in kernel density estimation + clever tricks in importance sampling and the defn of hyper contractivity.

The authors justify the use of the measure by a very exhanustive set of experiments.

Strengths:

 a) The paper connects hyper contractivity ( which has recently acquired attention in information theory community) to a correlation measure that can actually pick out ""hidden strong correlations under rare events"" much better than the existing ones. Theoretically justifying it with appropriate formulation of axioms based on Renyi's axioms and clarifying relationships amongs various other correlation measures is pretty interesting and a strong aspect of this paper.

b) Authors apply this to discover existing known gene pathways from data and also discover potential correlation among WHO indicators which are difficult to detect. There is an exhaustive simulations with synthetic functions too. 


I have the following concerns too:

a) Detecting influence from X to Y is sort of well studied in causality literature. In fact counterfactual scores called probability of sufficiency and probability of necessity have been proposed (Please refer ""Causality"" book by Judea Pearl 2009. Chapter 7 talks about scores for quantifying influence). So authors must refer to and discuss relationships to such counterfactual measures. Since these are counterfactual measures, they also do not measure just factual influence. Further , under some assumptions the above counterfactual measures can be computed from observational data too. Such key concepts must be referred to and discussed if not compared. I however admit that the current works focus is on correlation measures and not on influence in the strictest causal sense. 

b) This comment is related to my previous comment : The authors seem to suggest that hypercontractivity suggest a direction of influence from X to Y ( this suggestion is sort of vague in the paper). I would like to point out two things : a) In experiments in the WHO dataset, some of the influences are not causal (for example case E) and some of them may be due to confounders (like F between arms exports and health expenditure). So this measure is about finding hidden potential correlations or influences if the causal direction if already known b) For gaussians, S(Y,X) and S(X,Y) are same - therefore directional differences depend on the functional relationships between X and Y. So the title of discovering potential influence may not be a suitable title given prior work on counterfactual influence scores? I agree however that it is discovering rare hidden correlations. 

c) Does it make sense to talk about a conditional version ? If so where would those be useful ?

d) Section 4.3 - Authors say ""X_t will cause ... Y_t to change at a later point in time and so on"". However, the authors for the same t_i check for the measure s(X_{t_i},Y_{t_i}). So are we not measure instantaneous influence here. I agree that the instantaneous influences can come one after the other in a sequence of pairs of genes. However, this discrepancy needs to be clarified.

e) Why did the authors not use UMI or CMI estimators for experiments in Section 4.2 and 4.1 It seems like shannon capacity related estimators proposed in ref 26 does loosely have properties like that of axiom 6 ( roughly speaking). Actually I wonder why the authors did not include shannon capacity in the theoretical treatment with other measures (I understand that certain normalization criterions like being between 0 and 1 and 5 are not satisfied)?

f)Slightly serious concern: Does Axiom 6, for a given {\cal X,\cal Y} domain need to hold for ""some"" non constant function f or ""every"" non constant function f ?? Axiom 6' is for any function f ?? The reason I am asking is because the last expression in page 15 in the appendix has H(f(X) in the numerator and denominator. The authors say ""H(f(X)) =infinity for a continuous random variable X and a nonconstant function f"" - So clearly if this is the proof Axiom 6 is not for any f. Because f being a linear function and X being a uniform distribution on bounded support clearly does not satisfy that statement in the proof if it means any f.

g) Follow up of concerns in (f) : In page 14, proof of Proposition 1, point 6' - If Y=f(X) s(X;f(X)) =1 from Theorem 1"" . That seems to rely on the above concern in (f). It seems like axiom 6 is not valid for any f. In that case, how is proposition 1 point 6' correct? I think that needs a separate proof for alpha=1 right ??

h) Regarding directionality: Can you characterize functions f, noise such that Y= f(X, noise) and S (Y,X) << S(X,Y). If this can be done, this could be used potentially for casual inference between pairs from just observational data on which there have been several works in the past few years. Have the authors considered applying this to that case?

i) With respect to the material on page 27 in the appendix: It seems like definition in (5) has an interventional interpretation. You are trying different soft interventions r(x) at x and you are checking for what soft intervention that is not too different from p(x) the channel output changes dramatically. This seems like a measure that sort of simulates an intervention assuming a causal direction to check the strength of causation. In this sense, mildly it is a counterfactual measure. Can this be use as a substitute for interventions in certain structural equation models (under some suitable assumptions on non-linearity noise etc). If authors have answers, I would like to hear it. 
"
A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering,"Hongteng Xu, Hongyuan Zha",https://proceedings.neurips.cc/paper/2017/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html,"This paper considers the problem of clustering event sequences. To this end, the authors introduce a Dirichlet mixture of Hawkes processes and derive a variational-EM algorithm for its inference. The work is sound and may be useful in some contexts but, in my opinion, not enough innovative to reach the acceptance level. In addition, the use of MCMC within a variational framework for updating the number of clusters is surprising and can slow down the inference algorithm. Furthermore, the application on real data is somehow disappointing since the competitors seem unfavored.","In this work, the authors introduce a novel method to cluster sequences of events. This method relies on a Dirichlet mixture model of Hawkes processes to describe the data and assign events to clusters. This differs from previous work using Hawkes Processes for sequential data by focusing directly on clustering the sequences themselves rather than sub-events; in particular, compared to prior work such as by Luo et al., 2015, the clustering is done as part of the modeling itself rather than applied after learning the parameters of the model, which leads to a sizable improvement in experimental results. 

Quality: This paper is of good quality.

Clarity: This paper is overall clear, but contains some grammar and style mistakes - some have been noted below in the detailed comments, but the authors ideally should have their manuscript proof-read for English grammatical and style issues. Figure 2 is too small to be easily readable. 

Originality: This paper introduces a new method to model sequences of events.

Significance: This paper appears to be significant.

Detailed comments:
- In section 2, it would be valuable to provide a formal definition of Hawkes Process; in particular, it would improve readability to put the intensity equation (line 67) in its own equation environment, and then refer to it by number in Section 3 when relevant.
- When comparing line 67 to Eq. 1, it seems that the sum over the event types (c_i) is missing; could you please clarify this?
- Line 156: you state that the runtime of the algorithm is linearly proportional to the number of inner iterations - could you write out the complexity explicitly?
- Along the same lines, could you please state the complexity of MMHP in section 4.4?
- The second paragraph of section 4.2 needs to be made clearer. 

Clarity comments:
- You do not need to define R+ line 101

- line 23: ""the previous two""
- line 39: ""prior of the clusters""
- line 51: ""the problems of overfitting""
- line 61: ""A Hawkes""
- line 73: ""was first proposed""
- line 77: ""was used""
- line 100: ""basis""
- line 109: ""a lot of work"" should be rephrased
- line 125: ""goes to infinity""
- line 232: ""state of the art""
- line 255: spacing","This paper proposes an interesting idea for event sequence clustering. It made some assumptions on the clustering behavior across sequences and applied a mixture of Hawkes processes. Properties of the model including local identifiability, sample complexity and computation complexity are analyzed. Experimental results are provided with some baselines, demonstrating the modeling power of the proposed method.

1. At the end of Sec 3.1, the comment in the last sentence is a bit ambiguous. If I understand correctly, this model is a mixture model rather than an admixture model like LDA. Once a cluster label is selected for a sequence, the events are generated via a single Hawkes process instead of a mixture. Am I misunderstanding something or can you clarify the comment?

2. In the synthetic data experiments, the results indeed show the algorithm can handle certain degree of model misspecification. But that misspecification is rather minor, in terms of how good a function can be fit in a different function space. However, the data is generated with the same clustering assumptions proposed by the authors. If the clustering assumption is consistent with some baseline method, e.g., VAR+DPGMM, how would you expect the DMHP performance will be? 

3. On the inferred cluster number distribution in the synthetic data, two examples are provided in the supplements but the ground truth is not described there. Is it always concentrated around the ground truth as the cluster number varies from 2 to 5?"
Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification,"Muhammad Farhan, Juvaria Tariq, Arif Zaman, Mudassir Shabbir, Imdad Ullah Khan",https://proceedings.neurips.cc/paper/2017/hash/ddcbe25988981920c872c1787382f04d-Abstract.html,"This paper studies fast approximation of ""mismatch string kernel"" proposed by Leslie et al. [17], in that kernel distance between two strings are defined as the number of matches between k-mers while allowing up to m-mismaches. Since the original algorithm has a high computational complexity, the authors propose an approximation algorithm.
It consists of two parts; (1) determine the size of m-mismatch neighbourhoods of two k-mers, and (2) determine the number of k-mer paris within Hamming distance d.

For the former problem (1), the authors propose to precompute all m-mismatch neighbours with an O(m^3) algorithm, and for the latter problem (2), locality sensitive hashing is applied. The proposed algorithm is equipped with a probabilistic guarantees on the quality of the solution, and an analytical bounds on its running time.
In computational experiments using both simulated and synthetic data, the proposed algorithm is shown to approximate the exact computation well, and run much faster especially when m is large. 

The paper is written clearly. The proposed algorithm is presented step by step with essential proofs.

MINOR COMMENTS

As is also mentioned by the authors, but proposed bound (Theorem 3.13) is loose. Isn't it non-trivial to improve it ?

Some sentences in the section 4 are incomplete. ","Paper summary:
The authors propose an algorithm for the efficient estimation of the mismatch string kernel. They show empirically that the resulting estimate is accurate and provide theoretical guarantees to support their work. Moreover, empirical results support that using the kernel estimate does not hinder the accuracy of the SVM. Finally, the authors provide an upper bound on the runtime of their algorithm.

Strengths and weaknesses:
The paper is clear and well written. Proofs and technical parts of the paper are relatively easy to follow thanks to a consistent notation. It is great that the datasets used are from different domains and that the authors will make their source code available.

This work is strongly based on the hypothesis that increasing the number of mismatch in the kernel will improve classification accuracy. Because of this, it would be great if this hypothesis was more strongly supported in the paper.

In Figure 1, datasets are ordered by sequence lengths. We observe that, as sequence lengths increase, the running time benefit of the proposed approach diminish. Can the authors comment on this?

Clarity & Quality:
What are the values used for epsilon, delta and B in the Evaluation section? Additional experiments exploring the impact of these parameters would be a great addition to the paper.

Why all five datasets are not part of Table 2, 3, and 4? If more space is needed, the authors can provide the complete result as supplementary material.

Significance & Originality:
The paper addresses a very specific question that will most likely appeal to narrow audience of NIPS. However, the work is of high quality and does succeed in improving the computational time of the mismatch kernel. Because the authors will make their source code available after publication, I believe that this paper can have a significant impact in its field.

Errors / typos:
Line 95: ""belongs to theset of""
Table 1: Are the evaluations column for the SCOP and Artist20 datasets correct?
Line 193: Figure 4 should point to Figure 1
Line 204: ""Generating these kernels days""
Line 214: ""an efficient for evaluation of""","The authors describe an approximation algorithm for k-mer (with mismatches) based string kernels.
The contribution is centered around a closed form expression of the intersection size of mismatching neighbourhoods.
The algorithm is evaluated in the context of sequence classification using SVM.

I think this is a great paper: clear motivation, good introduction, clear contribution, theoretical back-up, nice experimental results.

I have a few concerns regarding presentation and structuring, as well as doubts on relevance on the presented theory.

Presentation.
I think Section 3 is too technical. It contains a lot of notation, and a lot of clutter that actually hinder understanding the main ideas. On the other hand, intuition on crucial ideas is missing, so I suggest to move a few details into Appendix and rather elaborate high-level ideas.

-A table with all relevant quantities (and explanations) would make it much easier for a non-string-kernel person to follow establishing Theorem 3.3. An intuitive description of the theorem (and proof idea) would also help.

-Algorithm 1 is not really referenced from the main test. It first appears in Thm 3.10. It is also very sparsely annotated/explained. For example, the parameter meanings are not explained. Subroutines also need annotation. I think further deserves to be in the center of Section 3's text, and the text should describe the high level ideas.

-It should be pointed out more that Thm 3.3 is the open combinatoric problem mentioned in the abstract.

-In the interest of readability, I think the authors should comment more on what it is that their algorithm approximates, what the parameters that control the approximation quality are, and what would be possible failure cases.


-Theorem 3.13 should stay in the main text as this concentration inequality is the main theoretical message. All results establishing the Chebbyshev inequality (Thm 3.11, Lemma 3.12, Lemma 3.14 requirements should go to the appendix. Rather, the implications of the Theorem 3.13 should be elaborated on more. I appreciate that the authors note that these bounds are extremely loose.


Theory.

-Theorem 3.13 is a good first step to understanding the approximation quality (and indeed consistency) of the algorithm. It is, however, not useful in the sense that we do not care about the kernel approximation itself, but we care about the generalization error in the downstream algorithm. Kernel method theory has seen a substantial amount of work in that respect (e.g. for Nystrom, Fourier Feature regression/classification). This should be acknowledged, or even better: established. A simple approach would be perturbation analysis using the established kernel approximation error, better would be  directly controlling the error of the estimated decision boundary of the SVM.


Experiments.

-Runtime should be checked for various m
-Where does the approximation fail? An instructive synthetic example would help understanding when the algorithm is appropriate and when it is not.
-The authors mention that their algorithm allows for previously impossible settings of (k,m). In Table 3, however, there is only a single case where they demonstrate an improved performance as opposed to the exact algorithm (and the improvement is marginal). Either the authors need to exemplify the statement that their algorithm allows to solve previously unsolved problems (or allow for better accuracy), or they should remove it.


Minor.
-Figure 1 has no axis units
-Typo line 95 ""theset""
-Typo line 204 ""Generating these kernels days;"""
Multi-output Polynomial Networks and Factorization Machines,"Mathieu Blondel, Vlad Niculae, Takuma Otsuka, Naonori Ueda",https://proceedings.neurips.cc/paper/2017/hash/dea9ddb25cbf2352cf4dec30222a02a5-Abstract.html,"This paper proposes an extension of factorization machines and polynomial networks for enabling them to predict vector valued functions. This is an important extension of an expressive and powerful class of models. The learning algorithm provided is sound and the experiments show it works well enough. The results clearly show a significant performance improvement over the single output version of these models, which, in my opinion, is enough to show the usefulness of the proposed approach.  ","The authors extend factorization machines and polynomial networks to the multi-output setting casting their formulation as a 3-way tensor decomposition. Experiments on classification and recommendation are presented.

I enjoyed reading the paper and although the model itself seems a somewhat incremental extension with respect to [5], the algorithmic approach is interesting and the theoretical results are welcome.

I would like to see runtimes of the proposed model compared to baselines, kernelized in particular, to highlight the benefits of the proposed method. Besides, the classification experiments could use at least one large dataset, the largest used is letter (N=15,000), again it will serve to highlight the benefits of the proposed model.  

I understand that a polynomial SVM with quadratic kernel is a natural baseline for the proposed model but I am curious about the results with more popular kernels choices, i.e, 3rd degree polynomial or RBF.

The recommender system experiment needs baselines to put the results from single-output and ordinal McRank FMs in context. 

Minor comments:
- line 26, classical approaches such as?","The paper proposes a model to learn a multi-output function of two sets of features such as the ones occurring in recommender systems. The idea of the model is to reduce its number of parameters by using a last layer where each unit (corresponding to each output) shares a basis of internal representations.
The idea seems sound and well explained. 
"
Tractability in Structured Probability Spaces,"Arthur Choi, Yujia Shen, Adnan Darwiche",https://proceedings.neurips.cc/paper/2017/hash/deb54ffb41e085fd7f69a75b6359c989-Abstract.html,"This paper looks at the problem of representing simple routes on a graph as a probability distribution using Probabilistic Sentential Decision Diagrams (PSDDs). Representing a complex structure such as a graph is difficult, and the authors transform the problem by turning a graph into a Boolean circuit where it is straightforward to perform inference, and as an experiment, use their method on a route prediction method for San Francisco taxi cabs, where it beats two baselines. 

PSDDs refer to a framework that represents probability distributions over structured objects through Boolean circuits. Once the object is depicted as a Boolean circuit, it becomes straightforward to parameterize it. More formally, PSDD’s are parameterized by including a distribution over each or-gate, and PSDD’s can represent any distribution (and under some conditions, this distribution is unique). The authors focus on the specific problem of learning distributions over simple routes — those that are connected and without cycles. The advantage of SDD circuits is that they have been shown to work on graphs that would be computationally expensive to model with Bayesian nets. With large maps, tractability is still a problem with PSDDs, and the authors do this by considering hierarchical approximations; they break up the map into hierarchies so that representing distributions is polynomial when constraining the size of each region to be a certain size. 

The paper is well-written, and the authors define PSDD's clearly and succinctly. 

To me, the results of the paper seems incremental -- the authors apply an existing representation method to graphs, and after some approximations, apply the existing inference techniques. Additionally, the paper spends a couple of pages listing theorems, which appear to be basic algorithmic results. 

Additionally, I'm not convinced by the baselines chosen for the experiment. For a task of predicting edges given a set of previous edges, the baselines are a naive model which only looks at frequencies and a Markov model that only considers the last edge used. I imagine a deep learning approach or even a simple probabilistic diagram could've yielded more interesting and comparable results. Even if these would require more time to train, it would be valuable to compare model complexity in addition to accuracy. I would need to see more experiments to be convinced by the approach.  
","The authors develop a hierarchical version of a probabilistic sequential decision diagrams (PSDD) in the context of describing distributions over routes in a discrete graph, which are represented by binary random variables on the graph edges.  Because valid paths without loops are represented as functions of all the edges in a path, a naive graphical model representation would have high treewidth.  PSDDs can represent distributions on structured binary spaces more efficiently, but still have difficulty scaling to realistic situations.  The authors propose segmenting the problem in a hierarchy of problems, represented both sub-problems and the overall structure by separate PSDDs, allowing the authors to scale to real-world sized problems (e.g. the financial district of San Francisco).

The authors rely on some special structure of simple path-finding to make their approach tractable (e.g. the simple-route limitation which 2^|B_i| to |B_i|^2).  Thm 4 and Prop 1 provide some guidance to the validity of the underlying assumptions, but in practice they may be hard to evaluate on really difficult problems (as always, I guess).  I feel like the Markov model is not a very fair comparison -- as the author say, it does not incorporate destination information.  A comparison to route-planning heuristics that are used in practice would have been more appropriate, but I'm afraid I'm not familiar enough with the field to make concrete suggestions.

Despite these minor shortcomings, I think the paper is well-written and the idea interesting enough to be of general interest, and I recommend acceptance.","The paper introduced hierarchical Probabilistic Sentential Decision Diagram and studied its performance in route prediction. It shows how introducing the hierarchical idea preserves the properties of SDD and the improvement in route prediction. It also presents why this hierarchical PSDD enjoys more efficient computation compared to probabilistic graphical models -- due to its efficiency in describing feasible combinatorial structures.

The paper is overall quite nice except it only presented an illustration of hierarchical PSDD on one example dataset/task. Would be helpful to see if this advantage persists through other datasets.

Another question is how computation cost of hierarchical PSDD compares of the original PSDD. Given a more flexible model (hierarhical), the computation cost would probably be higher -- but how much higher is interesting to know.

----------------------
After author rebuttal:

Many thanks for the rebuttal from the authors. They do clarify. My scores remain the same though. Thanks!"
Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search,"Luigi Acerbi, Wei Ji Ma",https://proceedings.neurips.cc/paper/2017/hash/df0aab058ce179e4f7ab135ed4e641a9-Abstract.html,"This work proposes a Bayesian optimisation framework implemented by combining search-based optimisation strategies to Gaussian Process-based local approximations of the target function.
The study has in important practical focus, as the authors provide a detailed description of the proposed optimisations scheme along with a clear illustration of parameters and numerical schemes. The experimental results, both on synthetic and real data, show that the proposed framework provides state-of-art performances on a number of optimisation problems, either for noiseless and noisy problems. 

The work seems solid and comes with important implementation details. Although the novelty of the proposed methodology is somehow limited, the reproducibility effort is rather remarkable and increases the scientific merit of the study. 
It remains to evaluate the effectiveness of the proposed framework in the high-dimensional setting, as the optimisation of the proposed GP surrogate function may become very expensive and prone to local minima.  However, in the present version the proposed framework could already find interesting applications in several domains. ","This paper presents a new optimization methods that combines Bayesian optimization applied locally with concepts from MADS to provide nonlocal exploration. The main idea of the paper is to find an algorithm that is suitable for the range of functions that are slightly expensive, but not enough to require the sample efficiency of standard Bayesian optimization. The authors applied this method for maximum likelihood computations within the range of a ~1 second.
      A standard critique to Bayesian optimization methods is that they are very expensive due to the fact that they rely on a surrogate model, like a Gaussian process that has a O(n^3) cost. The method presented in this paper (BADS) also rely on a GP. This paper solves the issue by computing the GP only of a local region, limited to 50+10D points.
      The paper ignores all the work that has been done in Bayesian optimization with much more efficient surrogate models, like random forests [A], Parzen estimators [B] or treed GPs [7], where available software shows that the computational cost is comparable to the one from BADS. It is known that those methods have worse global performance than GP based BO for problems in R^n, but given that this method uses local approximation, I would assume that the performance per iteration is also lower that GP-BO.
      Furthermore, because the main objective of BO is sample efficiency, some of the problems presented here could be solved with 50-100 iterations. Thus, being even more efficient that BADS as they would not require extra steps. In fact, optimized BO software [C] has a computational cost similar to the one reported here for BADS for the first 100 iterations. Note that [C] already have the rank-one updates implemented as suggested in section 3.2.
      The way the results are presented leaves some open questions:
      - Is the error tolerance relative or absolute? Are the problems normalized? What does it mean an error larger than 10? Is it 10%?
      - Given the fact that at the end of the plot, there is still 1/3 of the functions are unsolved. Without seeing the actual behaviour of the optimizers for any function, it is impossible to say if they were close, stuck in a local optimum, or completely lost...
      - Why using different plots for different problems? Why not doing the noisy case versus evaluations?
      - How is defined the ""...effective performance of BADS by accounting for the extra cost..."" from Figure 2?
      - If BO is used as a reference and compared in the text in many parts, why it is not included in the experiments? If the authors think it should not naively included for the extra cost, they could also ""account for the extra cost"", like in Figure 2.
      There is also a large set of optimization algorithms, mainly in the evolutionary computation community, that relies on GPs and similar models for local or global modeling. For example: [D, E] and references therein.

      [A] Frank Hutter, Holger Hoos, and Kevin Leyton-Brown (2011). Sequential model-based optimization for general algorithm configuration, Learning and Intelligent Optimization
      [B] Bergstra, James S., Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. ""Algorithms for hyper-parameter optimization."" In Advances in Neural Information Processing Systems, pp. 2546-2554. 2011.
      [C] Ruben Martinez-Cantin (2014) BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits. Journal of Machine Learning Research, 15:3735-3739.
      [D] Zhou, Zongzhao, Yew Soon Ong, Prasanth B. Nair, Andy J. Keane, and Kai Yew Lum. ""Combining global and local surrogate models to accelerate evolutionary optimization."" IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 37, no. 1 (2007): 66-76.
      [E] Jin, Yaochu. ""Surrogate-assisted evolutionary computation: Recent advances and future challenges."" Swarm and Evolutionary Computation 1, no. 2 (2011): 61-70.
      ","In this paper the authors propose a Bayesian Optimization (BO) algorithm aimed at finding the optimum of a complex function (like non-differentiable, noisy, no closed form). The principle of the algorithm is relatively simple: approximate the objective function using a Gaussian Process and then move in a direction that would optimize an acquisition function (a combination of mean and variance at the potential next point). If this local search doesn’t improve the solution (little or no change in objective function value), the algorithm enters the poll stage that searches opportunistically in a larger region terminating as soon as a better solution is found.

Function optimization is a problem of utmost importance and forms the backbone of many machine learning algorithms, so a novel method to approach the problem could be quite valuable. And even though some of the techniques used by the authors can be construed new applications, the overall contribution appears incremental. This assessment is partly based on the writing which is dense with too many ideas pushed into the supplementary material. I had a difficult time following all the details and at time just went ahead to avoid disrupting the reading flow. I also feel the authors could have utilized the space better and instead of spending almost two page describing previous studies and the parameter choices (pages 7 and 8), use the space to elaborate the ideas. Minor issue: should it be \Delta_k^{poll} in line 70?

Given that the proposed method does outperform several benchmarks without incurring high cost and has potential to be further developed for use in case of high dimensionality, I am slightly inclined. However, I think the paper needs substantial amount of work to make it more readable."
Multi-Information Source Optimization,"Matthias Poloczek, Jialei Wang, Peter Frazier",https://proceedings.neurips.cc/paper/2017/hash/df1f1d20ee86704251795841e6a9405a-Abstract.html,"This paper deals with the important topic of optimization in cases where in addition to costly evaluations of the objective function, it is possible to evaluate cheaper approximations of it. This framework is referred to as MISO (multi-information source optimization) in the paper, where Bayesian optimization strategies relying on Gaussian process models are considered. An original MISO approach is presented, misoKG, that relies on an adaption of the Knowledge Gradient algorithm in multi-information source optimization settings. The method is shown to achieve 
very good results and to outperform considered competitors on three test cases.

Overall, I am fond of the ideas and the research directions tackled by the paper, and I found the contributions principled and practically relevant. Given the potential benefits for the machine learning community and beyond, these contributions should definitely be published in a high-level venue such as NIPS. Yet, the paper could still be improved in several respects. Le me start by general remarks and then add minor comments.

A lot of emphasis is put throughout the paper, and starting in the abstract, on the use of ""a novel Gaussian process covariance kernel"". In the introduction, ""two innovations"" are listed as 1) the kernel and 2) the acquisition function. I think that this is misleading, and to me the main contributions concern by far the acquisition function in the first place, and the model is more a modality of application. By the way, the proposed model is a quite simple particular case 
of the Linear Model of Corregionalization (which the authors mention), corresponding to an M*(M+1) matrix [1,I], i.e. a concatenation of a vector of ones with an identity matrix. Besides this, it also has quite some similarity with the Kennedy and O'Hagan model that was used by Forrester et al. in optimization, without neither the hierarchical aspect nor regression coefficients. In all, I have the feeling that using the model presented by the authors is indeed a clever thing to do, but to me it doesn't constitute a major contribution in itself. Not also that if misoKG is compared to several competitors, I did not see comparisons of the proposed model (in terms of fit) versus others. Also, what would prevent from using other criteria with this model or other models with the misoKG aquisition function? 

Other than that, let me formulate a few comments/questions regarding the main contributions presented in the third section. First, a finite set of points is 
introduced at each iteration for the calculation and the optimization of the acquisition function. I am wondering to what extent such a set is needed on the one 
hand for the calculation, on the other hand for the optimization, and finally if there are good reasons (beyond the argument of simplicity) why the same 
set should be used for both tasks. I conceive that optimizing over l and x at the same time is uneasy, but couldn't one optimize over D for every l and then 
compare the solutions obtained for the different l's? By the way, an approach was proposed in ""Bayesian Optimization with Gradients"" (arXiv:1703.04389) to 
avoid discretization in the calculation of a similar acquisition function; could such approach be used here? If yes but a discretization approach is preferred 
here, maybe a discussion on when considering discrete subsets anyway would make sense? On a different note, I was wondering to what extent the calculation tricks 
used around lines 158-172 differ from the ones presented in the PhD thesis of P. Frazier and related publications. Maybe a note about that would be a nice addition. 
Finally, I found the formulation of Theorem 1 slightly confusing. One speaks of a recommendation x_N^star in A and then take limit over N. So we have a sequence of 
points in A, right? Besides, it all relies on Corollary 1 from the appendix, but there the reader is referred to ""the full version of the paper""...I do not have 
that in my possession, and as of now I found the proof of Corollary 1 somehow sketchy. I can understand that there are good reasons for that, but it is bit strange 
to comment on a result which proof is in appendix, which itself refers to a full version one doesn't have at hand!

A few minor comments follow: 

* About the term ""biased"" (appearing in several places): I understand well that a zero-mean prior on the bias doesn't mean at all that there is no bias, but 
maybe manipulating means of biases without further discussion on the distinction between the bias and its model could induce confusion. Also, around lines 54-55, 
it is written that ""...IS may be noisy, but are assumed to be unbiased"" followed by ""IS are truly unbiased in some applications"" and ""Thus, multi-fidelity methods 
can suffer limited applicability if we are only willing to use them when IS are truly unbiased..."". As far as I understand, several methods including the ones 
based on the Kennedy and O'Hagan settings allow accounting for discrepances in the expectations of the different IS considered. So what is so different between 
multi-fidelity optimization and what is presented here regarding the account of possible ""biases""? From a different perspective, other approaches like the one of 
""Quantile-Based Optimization of Noisy Computer Experiments with Tunable Precision"" are indeed relying on responses with common expectation (at any given x).

* In line 43, the ""entropy"" is the maximizer's entropy, right? 

* Still in line 43: why would the one-step optimality mean that the proposed approach should outperform entropy-based ones over a longer horizon? 

* About ""From this literature, we leverage..."", lines 70-72: as discussed above, it would be wothwhile to point out more explicitly what is new versus what is 
inherited from previous work regarding these computational methods. 

* In line 79 and in the bibliography: the correct last name is ""Le Gratiet"" (not ""Gratiet""). 

* In line 84: without further precision, argmax is a set. So, the mentioned ""best design"" should be one element of this set? 

* In line 87: the ""independence"" is a conditional independence given g, I guess...? 

* In page 3 (several instances): not clear if the bias is g(.)-f(l,.) or delta_l(.)=f(l,.)-g(.)?

* In line 159, the first max in the expectations should have braces or so around a_i + b_i Z. 

* In line 165: using c is a bit slippery as this letter is already used for the evaluation costs...

* In ""The Parallel Algorithm"": strange ordering between 1) and 2) as 2) seems to describe a step within 1)...?   
 

 ","This paper proposes ideas on Bayesian optimization when the function to optimize is expensive to evaluate and a number of cheaper approximations are available. I think that the problem is interesting and worth investigating. 

The proposed model is essentially imposing a Gaussian process prior to the mismatch between the function and the approximations. In this respect, I'm not sure that the work proposes something really novel from the modeling perspective, given that these approaches have been investigated at large in the literature on quantification of uncertainty (see, e.g., [1,2]). However, the paper studies this in the context of optimization and the theorem proposed in the paper is interesting and it adds some solidity to the contribution.

The paper proposes an ad-hoc objective function. I found it hard to read through page 4 where the paper illustrates the main idea of the paper. Perhaps some illustrations would be helpful here to better motivate the intuition behind the proposed objective. The experiments seem to indicate that the proposed objective function achieves better performance than the competitors.  

Overall the experimental section of the paper could be improved. The image classification benchmark is not so impressive - Neural Nets (even not convolutional), SVMs, Gaussian processes etc... achieve 98%+ accuracy on MNIST, and the optimized logistic regression achieves only ~93% accuracy. Maybe optimizing some of those models instead would make the results look better, while still demonstrating that the proposed method is superior to the competitors.

In Sec 4.4 I find it surprising that ""Random"" performs as well as GP-UCB

Sec 4.1 reports results on benchmarks, which I think is necessary and it's ok to have it there as a sanity check.

References:

[1] MC Kennedy and A O'Hagan. Bayesian calibration of computer models. Journal of the Royal Statistical Society Series B, 2001, vol. 63, issue 3, pages 425-464

[2] MC Kennedy, A O'Hagan; Predicting the output from a complex computer code when fast approximations are available. Biometrika 2000; 87 (1): 1-13.","1) You provide a description of how you pick the initial design, but you do not mention (as far as I could tell) how you pick the candidate set A in the experiments. You mention a hypercube design in the initial description, but is this to be understood as there being a fixed set of A being used throughout? What size?

2) How do you handle hyperparameters for the individual IS processes? 

3) In computing the cost, you drop ""dominated points"". Won't that bias using the formula in line 173? (or step c in the algo). Also, is there anything preventing you from applying that formula immediately? (I'm guessing the instantiation of the sets S is done for computational reasons).

4) you say you use a gradient-based optimization algorithm, but isn't l a discrete variable over the sources? Is that handled by the multi-restart functionality and if yes, is it handled adequately?

After rebuttal:
1,2,4) I think it would suit the paper to clarify the above to the extent possible.
3) I realize I was not familiar enough with the original work, and on inspection it appears to work as you say.
"
Differentially private Bayesian learning on distributed data,"Mikko Heikkilä, Eemil Lagerspetz, Samuel Kaski, Kana Shimizu, Sasu Tarkoma, Antti Honkela",https://proceedings.neurips.cc/paper/2017/hash/dfce06801e1a85d6d06f1fdd4475dacd-Abstract.html,"Summary:
This paper combines secure multiparty computation ideas with differential
privacy (DP)  as applied to bayesian learning, in a distributes setting. To expand
a bit: Bayesian Learning with DP has been studied recently and there are a few
orthogonal approaches to that problem. Some of these use as a privacy primitive
the Gaussian noise mechanism. In the centralized setting, or in the presence of
a trusted party, these approaches can be used. This work studies a distributed
setting where one does not have a single trusted party. The authors propose a
mechanism that they call DCA (Distributed compute algorithm) that allows one to
compute a noisy sum securely, under their threat model which assumes that while
there is no single trusted party, there is a set of compute nodes of which at
least one is trusted.

Given this algorithm, one can do other things that use noisy sum as a primitive.
This includes some approaches to bayesian learning, such as Sufficient
Statistics Perturbation and gradient based MCMC methods.

Opinion:
The summing algorithm is simple and closely related to previous work on the
subject such as 23. While the algorithm is definitely different here, as the
threat model is different, I think the novelty in this paper is not sufficient
to justify acceptance to this conference.

I have read the rebuttal. I agree that simplicity is a good thing and do not hold that against the paper. This is an interesting contribution that should be published somewhere, but am unconvinced that it is strong enough for this conference.","Title: Differentially private Bayesian learning on distributed data

Comments:

- This paper develops a method for differential privacy (DP) Bayesian learning in a distributed setting, where data is split up over multiple clients. This differs from the traditional DP Bayesian learning setting, in which a single party has access to the full dataset. The main issue here is that performing DP methods separately on each client would yield too much noise; the goal is then to find a way to add an appropriate amount of noise, without compromising privacy, in this setting. To solve this, the authors introduce a method that combines existing DP Bayesian learning methods with a secure multi-party communication method called the DCA algorithm. Theoretically, this paper shows that the method satisfies differential privacy. It also shows specifically how to combine the method with existing DP methods that run on each client machine. Experiments are shown on Bayesian linear regression, on both synthetic and real data.

- Overall, I feel that this method provides a nice contribution to work on Bayesian DP, and extends where one can use these methods to data-distributed settings, which seems like a practical improvement.

- I feel that this paper is written quite nicely. It is therefore easy to understand previous work, the new method, and how the new method fits in context with previous work.

- One small note: I might’ve missed it, but please make sure the acronym TA (for trusted aggregator, I believe) is defined the paper. I believe “TA” might be used in a few places without definition.
","1. The problem formulation is perhaps not that practical. Even if it is, the experiments are not designed accordingly to establish the relevance of the formulation for solving some real-world problems. 
2. The analysis techniques used herein are also pretty standard in the literature of differential privacy. So I'm not sure how the audience of NIPS would accept such formulation.
3. There is no other criticism to offer for this paper. In fact, the paper could get accepted in some other conference as it is, but perhaps does not meet the high standard that NIPS demands."
MMD GAN: Towards Deeper Understanding of Moment Matching Network,"Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnabas Poczos",https://proceedings.neurips.cc/paper/2017/hash/dfd7468ac613286cdbb40872c8ef3b06-Abstract.html,"This paper presents the MMD-GAN, a generative architecture which combines the benefits of generative adversarial nets (GANs) and generative moment matching nets (GMMNs). The architecture is based on MMD, but with a kernel function learned by an adversary trying to maximize MMD. There's a bit of theory showing non-degeneracy, and experiments demonstrate that the generated samples are competitive with W-GANs.

Overall, I think this is a strong paper which presents a simple idea and executes it well. The proposed method is intuitively appealing. The modeling choices are well motivated, the writing is clear, the theoretical justification is nice, and relationships to other methods are clearly highlighted. The experiments compare against strong baselines on challenging datasets. Being able to make GMMNs competitive with WGANs is impressive, since to the best of my knowledge there was still a large performance gap between them. 

While the results seem significantly better than the baselines, the paper could use more analysis of where this improvement comes from. How does the MMD-GAN eliminate the need for large mini-batches?  (Don't the samples in each mini-batch still need to essentially tile the space in order to achieve low MMD?)  Is there an explanation for why it outperforms the WGAN?


Minor comments:

- Section 3 describes the method in relation to the GAN, but isn't the WGAN a more direct match?  If I understand right, the MMD-GAN is essentially a kernelized W-GAN. 
- ""Also, the gradient has to be bounded, which can be done by clipping phi."" Is this the same as the Lipschitz constraint from WGANs?  If so, why not use the regularization method from the ""improved WGAN"" paper, rather than the original version (which the authors found was less effective)?
- If the claim is that the proposed method prevents mode dropping, estimating log-likelihoods is probably a stronger quantitative way to test it. E.g., see Wu et al., 2017.
- The experiment of Section 5.3 doesn't seem to be measuring stability. (I assume this refers to avoiding the degenerate solutions that regular GANs fall into?)  
","This paper proposes to improve the performance of the generative moment matching network (GMMN) by learning the kernel of the MMD. The optimization will result in a min-max game similar to that of the generative adversarial networks: the adversarial kernel is trained to maximize the MMD distance of the model distribution and the data distribution, and the generator at the same time is trained to fool the MMD. The kernel of MMD needs to be characteristic and is composed of Gaussian kernels with an injective function that is learned using an autoencoder.

I have some questions from the authors:

The decoder of the autoencoder f_dec does not seem to be learned in Algorithm 1. Is it just the transpose of the encoder? What is the architecture of the decoder and what kind of up-sampling does it have?
The encoder of the autoencoder f_\phi is the same as the discriminator of DCGAN, right? Given that the DCGAN discriminator has max-pooling layers, how does f_\phi become injective? How is the reconstruction quality of the autoencoder given that only h~100 dimensional vector is used for the latent space and given that there is max-pooling? 
Is it really necessary to train the autoencoder to get the model working? (this algorithm is very similar to W-GAN and there is no autoencoder there --- although I understand the autoencoder training is necessary to have an injective function.)

The final algorithm can be considered as the generative moment matching in the code space of an autoencoder, where the features of the autoencoder is also trained to maximize the MMD distance. The algorithm is also very similar to WGAN where instead of only matching the means, it is matching high-order moments due to the kernel trick. So in general I wouldn't expect this algorithm to work much better that other GAN variants, however I found the intuition behind the algorithm and its connection to GMMN and W-GAN interesting.","GANs have received a lot of attention lately. They generate sharp samples but this performance comes with the cost of optimization difficulties. An alternative to GANs that relies on two-sample hypothesis testing is the so called Generative Moment Matching Networks (GMMN). Although GMMNs are more stable, they generate worse samples than GANs. They also require a large batch size during training. This paper proposes to improve the efficiency of the GMMN and its architecture for better performance. The approach consists in using feature matching in the kernel definition and learning the parameters of this feature map jointly with the parameters of the generator in a minimax problem. I liked the paper and have highlighted some remarks below:

1. The reason why adversarial learning of the kernel leads to higher test power was not thoroughly discussed. This is important.

2. The experiment section -- including the complexity analysis -- is very well carried through.

3. The paper can benefit from a thorough editing. There were many typos that tend to distract from the message being portrayed in the paper.

REBUTTAL:
I acknowledge the rebuttal"
Convergence of Gradient EM on Multi-component Mixture of Gaussians,"Bowei Yan, Mingzhang Yin, Purnamrita Sarkar",https://proceedings.neurips.cc/paper/2017/hash/dfeb9598fbfb97cc6bbcc0aff2c785d6-Abstract.html,"This paper studies the problem of learning mixtures of (spherical) Gaussians using EM algorithm. The main result is that under reasonable initialization, (population version of) EM algorithm converges efficiently to the optimal solution. The paper also gives corresponding finite sample results. The result relies on the gradient stability condition that is previously discussed in [1], but this result is more general compared to the Gaussian mixture result in [1] (which arguably is just an example instead of the main result). The proof involves a lot of calculations and bounding different terms.

Overall this is an interesting paper. However the reviewer feels the paper completely ignored a long line of work that analyzes the k-means algorithm under very similar separating conditions. The line of work started at the paper by Kannan and Kumar ""Clustering with Spectral Norm and the k-means Algorithm"", and was improved by Awasthi and Or in ""Improved Spectral-Norm Bounds for Clustering"". In particular, the latter work analyzes k-means algorithm under the same (in fact slightly better) separation guarantee as in this paper. With this amount of separation the hard assignment (k-means) algorithm is not super different from the EM algorithm. Given these results this paper does not appear too surprising. Although the reviewer does note that the current paper is using a new proof technique. It's also worth mentioning that there are algorithms for mixture of spherical Gaussians that can work with even smaller separation (e.g. VempalaWang).","Summary: This paper derives statistical guarantees for the gradient EM algorithm applied to clustering in Gaussian mixture models, generalizing the results of [1] to multiple clusters and non-uniform cluster weights.

The first result (Theorem 1) concerns the ""population EM algorithm"", i.e., when the expectation step can be computed exactly rather than being estimated from data. For this case, if the cluster centers are sufficiently separated and all the estimated cluster centers are initialized sufficiently close to the true cluster centers (i.e., roughly if there is already a sufficiently clear 1-1 correspondence between the estimated and true cluster centers), then the estimated cluster centers converge linearly to the true centers. This result relies on first showing (Theorem 4) that, under these conditions, due to properties of the Gaussian distribution, the curvature of the Q function is bounded (i.e., specifically, the gradient is Lipschitz). By a standard proof, the Lipschitz gradient then implies convergence of gradient descent.

The paper then turns to showing that finite-sample gradient EM approximates population EM, based on a generalization [2] of McDiarmid's inequality (to unbounded data), and a novel bound on the Rademacher complexity of the gradient for GMMs. This in turn implies a linear convergence rate for finite-sample gradient EM, assuming sufficiently good initialization.

Finally, the paper concludes with some synthetic experiments verifying the linear convergence rate and the necessity of the assumption that the initial clusters are good.

Main Comments:

This paper proves fairly strong, tight convergence bounds on a classic clustering algorithm, significantly generalizing the results of [1]. The proofs are well-motivated and lucidly sketched. I'm not too familiar with related work in this area, but I believe the results are quite novel. Overall, this paper is a significant contribution. However, the paper needs extensive proof-reading (see some of the typos noted below).

Some thoughts:

1) The results rely on having a sufficiently good initialization of the estimated cluster centers. Although haven't thought in much detail, I suspect that (perhaps assuming the clusters centers are known to lie in a bounded region), it should be quite easy to derive a high-probability bound the number of random initializations needed to obtain such an initialization. Such a result might nicely complement the given results.

2) It is noted (Remark 4) that the altered version (from [2]) of McDiarmid's inequality used results in extra factors of sqrt(d) and log(n). I wonder if the recent variant (see [R1] below) of McDiarmid's inequality for sub-Gaussian data can give tighter results? This bound appears somewhat more similar to the original (bounded-differences) version of McDiarmid's inequality, though perhaps it is equivalent to that in [2].

The paper has a lot of typos and minor language issues:

Line 18: ""loglikelihood"" should be ""log-likelihood""
Line 26 and Line 142: ""convergence guarantees... is"" should be ""convergence guarantees... are""
Line 27: ""converge to arbitrarily"" should be ""converge to an arbitrarily""
Line 30: The sentence ""In [16]... is continuous."" doesn't parse.
Line 52: ""semi definite"" should be ""semi-definite""
Line 69: ""Ledoux Talagrand"" should be ""Ledoux-Talagrand""
Line 122: ""defined in 1"". What is ""1""? Should this be the reference [1]?
Lines 139-144: It looks like part of the related work section was randomly pasted into the middle of a sentence that should read ""... contraction results [9] coupled with arguments that use...""
Line 187: ""equivalent as"" should be ""equivalent to""
Hyphenation (e.g., ""sub-Gaussian"", ""sub-optimal"") is inconsistent throughout the paper.

[R1] Kontorovich, Aryeh. ""Concentration in unbounded metric spaces and algorithmic stability."" Proceedings of the 31st International Conference on Machine Learning (ICML-14). 2014.","The paper studies convergence of gradient EM for isotropic Gaussian mixture models. They start with the traditional approach of first deriving population convergence and subsequently they analyze the finite sample case. However, they face a difficulty as the function class does not possess bounded differences. The authors circumvent this challenge by deploying an extension of McDiarmid's theorem. The authors obtain radius for contraction region and show some experiments on its tightness.

The paper is well written and easy to understand, with a nice and gentle walk through to more complicated settings. Detailed proofs are provided in the supplementary materials, with some neat tricks. Unfortunately, adequate related works have not been reviewed. Especially, since the paper analyses the convergence properties for isotropic Gaussian Mixture models, the need to compare to the k-Means problem is inevitable.

Comments:
1. I could not find the definition of G (caps G) in the paper. I suspect it represents the gradient update operator as defined in equation (2). Further, ""s"" is not defined or is mentioned that s is positive in equation (2), which I believe is the step-size as referenced much later. Please clarify and update in the paper.

2. Line 144 has a few words missing: ""sub-optimal point. uments that use""

3. In the case of the isotropic Gaussian mixture model, full EM update is no costlier than the full gradient update. What is the motivation of applying gradient EM? A stochastic gradient EM would be highly valuable though, like analyzed in [1]. Does the current analysis carry over to stochastic gradient case? 

4. Finally, since the paper focuses wholly on isotropic Gaussian mixture models, it warrants a comparison to the k-Means problem in L2 distance. For example, we know that PTAS is available for k-Means for arbitrary initialization (cf Z. Friggstad, M. Rezapour, and M. R. Salavatipour Local Search Yields a PTAS for k-Means in Doubling Metrics FOCS, 2016. and Jiri Matousek. On approximate geometric k-clustering. Discrete & Computational Geometry, 24(1):61–84, 2000.). However, as shown in the paper the contraction region for the gradient EM is less than R_min/2. This region can be very small and it is hard to find an initialization here. Can local search help in expanding the contraction region?


Overall the paper presents a neat convergence analysis, an interesting derivation, and a near optimal contraction region, albeit with no motivation. As mentioned above providing some motivation or studying the case of stochastic gradients should make the paper strong and a nice contribution to the field."
Bayesian Dyadic Trees and Histograms for  Regression,"Stéphanie van der Pas, Veronika Ročková",https://proceedings.neurips.cc/paper/2017/hash/e00406144c1e7e35240afed70f34166a-Abstract.html,"The paper gives a detailed theory of a very simple case of regression trees, where there is one predictor variable.  The main contribution is probably the methods used.  There is a follow-up manuscript, and I'd say the more extensive results should have made the paper to make it more interesting.

","This paper analyses concentration rates (speed of posterior concentration) for Bayesian regression histograms and demonstrates that under certain conditions and priors, the posterior distribution concentrates around the true step regression function at the minimax rate.

The notation is clear. Different approximating functions are considered, starting from the set of step functions supported on equally sized intervals, up to more flexible functions supported on balanced partitions. The most important part of the paper is building the prior on the space of approximating functions.

The paper is relatively clear and brings up an interesting first theoretical result regarding speed of posterior concentration for Bayesian regression histograms. The authors assume very simple conditions (one predictor, piecewise-constant functions), but this is necessary in order to get a first analysis.

Proof seems correct, although it is out of this reviewer's expertise. This reviewer wonders why the authors did not considered sending this work to the Annals of Statistics instead of NIPS, given the type of analysis and provided results.

Minor:

- You might want to check the reference of Bayesian Hierarchical Clustering (Heller et.al, 2005) and the literature of infinite mixture of experts.

- l. 116: You mention recommendations for the choice of c_K in Section 3.3, but this Section does not exist.

- Assumption 1 is referred to multiple times in page 4, but it is not defined until page 5.

Writing (typos):

- l. 83: we will
- l. 231: mass near zero
- l. 296: (b) not clear (extend to multi-dimensional?)","In this paper, Authors focus on Bayesian regression histograms aiming at regression with one explanatory variable. They develop an approach for constructing regression surface that is piece-wise constant in which no.of jumps are unknown. The approach has some merits and I have some concerns given below, which I think authors should address for better clarity.

1. Line 116: There is no section 3.3 in the paper.
2. In the proof of lemma 2.1 in the supplementary file, it is not clear ( at least to me) how you arrived at the expression above line 24. I followed your explanation but couldn't get it.
3. In line 178: There should be a better explanation for assuming x's are fixed. What if not? 
4. Line 183: Do you refer this xi to be the scaled predictor? 
Line: 296: Minor comment: How flexible to extend this to dimension reduction with many predictors? 
In a practical application, How one can select Ck? and How sensitive the theorem 3.1 to this choice?
"
Efficient and Flexible Inference for Stochastic Systems,"Stefan Bauer, Nico S. Gorbach, Djordje Miladinovic, Joachim M. Buhmann",https://proceedings.neurips.cc/paper/2017/hash/e0126439e08ddfbdf4faa952dc910590-Abstract.html,"***Update following reviewer discussion and author feedback***

I am happy to revise my score for the paper provided the authors add some discussion of the number of OU processes used in the simulations, the \delta term in the RODE system and add the volatility to the Lorenz96 model (in addition to the other changes to the text recommended by myself and the other reviewers).

**************************************************************

The authors propose a method for combined state and parameter estimation for stochastic differential equation (SDE) models.  The SDE is first transformed into a random ordinary differential equation.  Several solution paths are then simulated to generate a large number of ordinary differential equations, and each of these is then solved using an EM algorithm type approach that was introduced in an earlier paper.  The method is tested on two systems, the Lorenz96 and Lorenz63 models, and compared to a competitor method showing that the new approach can be faster and more accurate.

There are some interesting ideas in the paper but I can’t accept it for publication in its current form.  The general approach seems reasonable, but there are some details of it that the authors don’t really mention that I think need to be explored.  There are also lots of things that I found unclear in the manuscript and I think these need to be fixed.  Since there is no major revision option for the conference I must therefore recommend rejection.  I think if the authors revise the paper accordingly it could be accepted at a similar conference in the future.

Detailed comments:

I think is section 3 you need to spend more time discussing two things:
How many OU processes you should be simulating.  You don’t seem to discuss it but it must be crucial to the performance of the approach in practice.
Why the additional \delta term is added to each RODE system, and how its variance should be chosen.
The main reason for these suggestions is that the approach itself borrows innovations from other sources (e.g. Gorbich et al. 2017).  Because of this, the specifics of how these tools are applied in the SDE context is to my mind the main contribution of the present paper. If you don’t discuss these things then it isn’t clear how useful the paper is.

The first line states ‘A dynamical system is represented by a set of K stochastic differential equations…’ This is not true in general, many dynamical systems cannot be represented by stochastic differential equation models (e.g. when the driving noise is not Brownian).

line 56 why is equation (2) a scalar RODE?  I would have thought that it was a d-dimensional system of RODEs.  Or does scalar not mean what I think it means here?

In (2) the vector field is described as f(x(t),w(t)), whereas in (2) it is written f(x(t),\eta(w)).  Shouldn’t it instead be f(x(t),\eta(t)) in (2) and f(x(t),\eta(t,w)) in (3), since \eta(t) is a r.v. and \eta(t,w) will be the fixed outcome for w \in \Omega?

Line 60.  Here you introduce Example 1, but then fail to discuss it for the next paragraph.  Then you introduce Example 2 which is essentially the second part of example 1.  I think it would be better if you combined example 1 and 2, since the purpose of the example is to show how a RODE can be re-written as an SDE when the driving noise is Brownian. And also put it where example 2 currently is (i.e. below the paragraph of text).

Line 87.  ‘Stationary' for the OU process can either mean it is started at equilibrium or that it is positive recurrent (i.e. it will be ergodic).  Which of these do you mean here?  From the definition I think it should be the latter since the SDE could be conditioned to start at any point, but this could certainly cause some confusion I think.

Line 89.  Why an OU process?  Can the authors offer any intuition here, as it seems to come out of nowhere and I think the reader would be very interested to understand the origins.

Line 167.  Is the Lorenz96 system therefore an SDE with unit volatility?  I think it would be good to clarify this for non-experts.


Typos

line 24 diffusions processes -> diffusion processes
line 44 the fact the we -> the fact that we
line 48 ‘both frameworks are highly related’ doesn’t make sense.  If two things are related then the statement clearly applies to both of them.
line 54 is a R^m valued -> be an R^m valued
line 99 an computationally efficient -> a computationally efficient
line 113 an product of experts -> a product of experts
line 119 an lower bound -> a lower bound
line 174 in pur approach -> in our approach (I think?)
line 190 robuts -> robust
line 198 random ordinary differential equation -> random ordinary differential equations","*** Update following Author's feedback ***
As my main concern was clarity and this issue is well addressed by the authors, I am raising the score to reflect my assessment of the potential impact of this paper.
*** 


This paper provides a novel inference algorithm for estimating the parameters of stochastic differential equations (SDEs) with noisy observations, as well as inferring the posterior distribution over states. The proposed algorithm transforms the SDE into a random ordinary differential equation (RODE) and samples a set of ODEs with noisy observations from the RODE (with a minor modification of adding additional noise). Next, it applies a recently developed variational inference procedure for ODEs with noisy observations [Gorbach etal. 2017]. Finally, it transforms the RODE back to SDE. This approach is scalable in the dimension of the hidden state and it is tested on a 3, 40 and 1000 dimensional systems.

The main contribution is the idea to leverage inference algorithms for ODE with noisy observations to solve SDE using RODE. This approach may open the way to develop additional algorithms for SDEs.

The overall structure and the ideas in the paper are clearly written, but the math of the variational gradient matching is hard to follow. Importantly, update equations for Q are missing, though the reader is referred to another paper. Additionally, the modification of the gradient matching algorithm involves adding a noise factor delta (line 146). It would be helpful to specify how is this parameter determined. 

As the algorithm has a sampling component, a discussion about how does the number of OU-processes N_max affect the accuracy of the algorithm will be very informative.

Some details are missing in the experimental section: 
- It is not clear from the text how does hyperparameters such as phi and the state specific error gamma are set in practice.
- In the runtime comparison (Table 1), do the different methods achieve comparable accuracy?
- In Figure 6 we see convergence of rho and beta towards the true parameter. Sigma seems not to be affected by the time interval. 
- To which version of the L96 system does Figure 7 refer to? (what is D)

Minor comments and typos:
- Figure 2 is too small as well as the fonts in other figures
- Line 174: change pur to our
- Line 182: the word ‘Figure’ is missing between system and 4.
- Line 190: change robuts to robust
- Line 192: ‘is equal’ is written twice  
"
Learning ReLUs via Gradient Descent,Mahdi Soltanolkotabi,https://proceedings.neurips.cc/paper/2017/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html,"### Summary
The paper proves that projected gradient descent in a single-layer ReLU network converges to the global minimum linearly: ||w_t-w^*||/||w_0-w^*||=(1/2)^t with high probability. The analysis assumes the input samples are iid Gaussian and the output is realizable, i.e. the target value y_k for input x_k is constructed via y_k=ReLU(w^*.x_k). The paper studies a regression setting and uses least squares as the loss function.

### Pros and Cons
To the best of my knowledge, the result is novel; I am not aware of any proof of ""convergence to global optimum"" for the nonconvex loss resulted by fitting to a ReLu function via least squares. On top of that, the analysis considers a nonconvex regularization term with little assumptions on it. Of course, there still a clear gap between the results of this paper and models used in practice: notably, relaxing the Gaussian iid input assumption, and extending to multilayer setup. However, this result may become a stepping stone toward that goal.

### Proofs
I followed the building blocks of the proof at the coarse level, and the logic makes sense. However, I did not check at a very detailed level. I was wondering if the authors could include a simple simulation to confirm the correctness of the result? Given that the data is very simple (iid Gaussian) and the network is a single layer ReLU, using a very simple regularization scheme, it seems to take a few lines of Matlab to verify the empirical convergence rate and compare that with the result. In fact, I wonder why this is not attempted by the authors already?

### Regularization
The authors state that the result holds true regardless of regularization R being convex or nonconvex. What if R is such that it creates two far apart and disjoint regions, one containing the origin (initialization) and the other containing w^*. Wouldn't this create a problem for projected gradient descent initialized by w=0 to reach w^*?
 
### Minor Comments
1. There are typos, please proof read.
2. What is the meaning of A in L_A (Eq 5.12)?
3. What is the value of the constant gamma in line 180, or otherwise how is it related to other constants or quantities in the paper?
","Review: Learning ReLUs via Gradient Descent

This paper study the problem of learning the parameters of ReLUs by using the gradient descent algorithm. The authors have shown that under certain condition of the total observed samples, performing gradient descent is able to accurately estimate the true parameter w^* for a 1-layer ReLu network, with high probability.

Generally the paper is well written as easy to follow. I have only a few minor comments. 
1)	The random vector g in line 168 of the paper maybe different from  the g defined in the equation about (5.8), please check and comment. 
2)	For the last two relations in page 6 (one line 181 and 182), please cite the related sources. 
3)	Please comment on which lemma is used to derive Eq. (5.5). 
4)	Last line in page 8, the term ""+ \frac{sgn…}{2n}"" should be included in the summation sign.
5)	I am not exactly sure how the first equality of (5.14) is derived. It appears that a term ""< h^T, w^* >/||w^*||^2"" is missing? Please double check.

","This paper analyzes when gradient descent algorithm can be used to learn a single neuron with ReLU activation function. The main result is when the input is generated from Gaussian distribution, and the output is exactly from a ReLU unit, gradient descent algorithm has geometric convergence starting from 0 with fairly tight sample complexity (the sample complexity depend on the structure of the regularizer). The techniques of the result are quite interesting, and different from the traditional approaches for analyzing gradient descent. The approach is most similar to a recent analysis of phase retrieval problem [15]. 

Pros:
- The paper provides a new analysis for learning ReLU unit, and the algorithm is simply gradient descent.
- The sample complexity seems fairly tight in many settings and especially works in high dimensions (when number of samples is smaller)
- Can handle some non-convex regularizers.

Cons:
- The analysis relies fairly heavily on the Gaussian distribution and realizable assumption. One conjecture is whether the objective function is actually convex when taking expectation over Gaussian? It might be useful to discuss a bit more about that.
- To handle non-convex projection, the analysis seems fairly sensitive in the step size. In particular, unlike traditional analysis it seems like the proof will not work with a small step size (because the non-convex projection might push the point back).

Overall this is a well-written paper with an interesting result."
Learning Graph Representations with Embedding Propagation,"Alberto Garcia Duran, Mathias Niepert",https://proceedings.neurips.cc/paper/2017/hash/e0688d13958a19e087e123148555e4b4-Abstract.html,"The authors introduce embedding propagation (EP), a new message-passing method for learning representations of attributed vertices in graphs.   EP computes vector representations of nodes from the 'labels' (sparse features) associated with nodes and their neighborhood.  The learning of these representations is facilitated by two different types of messages sent along edges: a 'forward' message that sends the current representation of the node, and a 'backward' message that passes back the gradients of some differentiable reconstruction loss.  The authors report results that are competitive with or outperform baseline representation learning methods such as deepwalk and node2vec.

Quality:
The quality of the paper is high.  The experimental technique is clearly described, appears sound, and the authors report results that are competitive with strong baseline methods.  In addition, the authors provide clear theoretical analysis of the worst-case complexity of their method.  

Clarity:
The paper is generally clearly written and well-organized.  The proposed model is clearly delineated.

Originality:
The work is moderately original - the authors draw inspiration from message-passing inference algorithms to improve on existing graph embedding methods.

Significance:
On the positive side, the proposed technique is creative and likely to be of interest to the NIPS community. Furthermore, the authors report results that are competitive or better when compared with strong baseline methods, and the computational complexity of the technique is low (linear in both the maximum degree of the graph and the number of nodes).  However, while the results are competitive, it is unclear which method (if any) is offering the best performance.  This is a little troubling because EP is constructing the representations using both node features and graph structure while the baselines only consider graph structure itself, and this raises the question of whether EP is adequately capturing the node features when learning the node representation.

Overall, I believe that the paper is significant enough to warrant publication at NIPS.  

Some questions and suggestions for improvement for the authors:
 - It seems like this method could be implemented in a distributed asynchronous manner like standard belief propagation.  If so, the scalability properties are nice - linear computational complexity and a low memory footprint due to the sparse feature input and low-dimensional dense representations.  This could be a good area to explore.
 - It seems like there are really two representations being learned: a representation for a node's labels, and an overall representation for the node that takes the label representation of the node and the representations of the neighboring nodes into account.  How necessary is it to learn a representation for the node labels as a part of the pipeline?  Would it be possible to just use the raw sparse features (node labels)?  If so, how would that affect performance?
 - Is it possible to learn this representation in a semi-supervised manner?
 - Learning curves would be nice.  If the method is using fewer parameters, perhaps it's less inclined to overfit in a data-poor training regime?
 
Overall impression:
A good paper that proposes an interesting technique that gives solid (although not spectacular) results.  An interesting idea for others to build on.  Accept.","The paper presented a two-step approach for finding node embeddings of graphs, where each node may be associated with multiple labels of different types. The overall idea is to first find label embeddings based on the graph structure, and then aggregate those label embeddings to obtain node embeddings. The proposed method shows consistent improvement over several other graph embedding algorithms when multiple types of node labels are available.

This is a nice addition to existing graph embedding algorithms which are either unsupervised or only consider a single label type for semi-supervised learning tasks. The method seems useful as many real-world graphs are associated with heterogeneous label types. The proposed framework also offers a unified view of several existing works, though the authors chose to use very naive aggregation strategies to implement each of the two steps to demonstrate the concept.

The authors interpret their method from the label propagation perspective throughout the paper, which is corresponding to a single gradient step of some implicit objective function. I figure it would be informative to explicitly derive the underlying learning objective and compare it with other existing works. In fact, almost all graph embedding algorithms can be viewed as some sort of embedding propagation if we look at their gradient update rules.

Some recent strong baselines were mentioned in the literature review but not included in the experiment, e.g. GCN [1].

1. Kipf, Thomas N., and Max Welling. ""Semi-supervised classification with graph convolutional networks."" arXiv preprint arXiv:1609.02907 (2016).","The paper proposes a new framework for graph embeddings by propagating node embeddings along the observed edges in a graph. The main idea of the framework is to learn embeddings of node labels such that the embedding of all labels of a node is close to the embedding of its neighbours. 

The paper is generally written well and the presented concepts are mostly good to understand. The proposed model seems reasonable and technically sound. Furthermore, it shows promising results in the experimental evaluation where it is compared to standard graph embeddings on benchmark datasets.

However, I am concerned with the lack of novelty in the paper (whose main contribution is the embedding propagation framework) as the idea to describe graph embeddings via a message passing/propagation framework has been proposed previously. Specifically, the recently introduced framework of Message Passing Neural Networks (MPNN) [1] seem very closely related to the framework described in Section 3. It seems that the embedding propagation framework can be described as a submodel of MPNN, where MPNN's M_t corresponds to the distance of a node embedding and its neighbor's embeddings.

An interesting novel aspect of the paper is that it considers multiple labels/embeddings for nodes as this is not commonly done in standard models but has interesting applications also beyond graph embeddings. Unfortunately, the paper doesn't discuss the approach that it takes to this problem and its motivation or advantages in further detail. 

In summary, the paper introduces an interesting method to learn graph embeddings which shows promising experimental results. However, since the general ideas are well known and a closely related framework has recently been introduced, the novel contributions seem limited.

Further comments:
- I am not sure why the paper refers to h-tilda as reconstruction? The method seems to maximize the similarity of the embeddings, but doesn't attempt any explicit reconstruction of the node/neighborhood/graph.
- In what sense are gradients ""propagated""? It seems to me the method is doing standard gradient descent?
- The term ""reward"" in Figure 2 is somewhat confusing as it is typically used for reinforcement-type models.
- l143: This propagation property is not particularly surprising as it is common to all graph embeddings that jointly optimize embeddings of nodes such that the global link structure is captured.

[1] Gilmer et al. ""Neural Message Passing for Quantum Chemistry"", ICML, 2017."
Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation,"Matthias Hein, Maksym Andriushchenko",https://proceedings.neurips.cc/paper/2017/hash/e077e1a544eec4f0307cf5c3c721d944-Abstract.html,"This paper proves that under certain assumptions one needs to change an instance substantially so that the value of the classifier changes. The goal is to understand the effects of adversarial manipulation of various inputs. Prior work had shown examples where a tiny adversarial change can change the value of the input, i.e., a total lack of stability of the classifier.  ","This paper fills an important gap in the literature of robustness of classifiers to adversarial examples by proposing the first (to the best of my knowledge) formal guarantee (at an example level) on the robustness of a given classifier to adversarial examples. Unsurprisingly, the bound involves the Lipschitz constant of the Jacobians which the authors exploit to propose a cross-Lipschitz regularization. Overall the paper is well written, and the material is well presented.  The proof of Theorem 2.1 is correct. I did not check the proofs of the propositions 2.1 and 4.1. 

This is an interesting work. The theorem 2.1 is a significant contribution. 
The experimental section is not very insightful as very simple models are used on small/toy-ish datasets. Some claims in the paper are not consistent:

    - ""the current main techniques to generate adversarial samples [3, 6] do not integrate box constraints""

There exist approaches integrating box constraints: see [1,2]

    - ""It is interesting to note that classical regularization functionals which penalize derivatives of the resulting classifier function are not typically used in deep learning.""

Penalizing the derivatives is a well-known strategy to regularize neural networks[3,4]. In fact, it has recently been exploited to improve the robustness of neural networks to adversarial examples[3].

I have the following question for the authors:

Theorem 2.1 says that the classifier decision does not change inside a ball of small radius for a given p-norm. Yet, we can still find adversarial examples even when the norm of the gradient is small (yes, we can. I have tried!). Does this mean the topology induced by the p-norm does not do justice to the perceptual similarity we have of the image and its adversarial version? In other words; Does close in terms of p-norm reflect ""visually close""?
 
Overall I like the paper. Though it leaves me with mixed feelings due to weak experimental section, I lean on the accept side. 

[1]  Intriguing properties of neural networks
[2]  DeepFool: a simple and accurate method to fool deep neural networks
[3]  Parseval Networks: Improving Robustness to Adversarial Examples
[4]  Double back propagation: increasing generalization performance  
"
Collapsed variational Bayes for Markov jump processes,"Boqian Zhang, Jiangwei Pan, Vinayak A. Rao",https://proceedings.neurips.cc/paper/2017/hash/e0a209539d1e74ab9fe46b9e01a19a97-Abstract.html,"This paper proposes performing Bayesian inference for Markov jump processes using a collapsed variational inference algorithm. Specifically, the authors collapses out the transition times and instead optimizes a discretization of time allowing for easy updates of the remaining model parameters. The method is applied to analyzing user check-in data on FourSquare.

The main ideas of the paper and the variational inference algorithm are very interesting and the authors do a good job explaining them. There are two aspects of the paper that I have questions about. First, the paper does not actually show that variational algorithm is necessary. Specifically, the experiments do not clearly indicate where uncertainty quantification from the variational approximation is useful. In other words, why would someone use this algorithm over say just performing MAP inference? Second, the authors propose a greedy method to optimize the time discretization that works by splitting and merging segments of time and using the ELBO to accept the update. This would require that the ELBOs are nested so that their values are comparable. It is not clear to me that this is the case and the authors should definitely include a justification for this. Finally, as stated before, the experiments are focused on reconstructing paths and it isn't clear why having a posterior distribution is useful.

Beyond these major concerns I have some minor comments:
- There are a lot of typos (some of which are noted here)
- Line 82: ""bottom right"" -> ""right""
- In the ""Updating q(U|T)"" section: I think you mean ""exponential family likelihood"", not ""exponential likelihood"".
- \bar{Z}_i was never defined

Overall, I think that this paper could be accepted. I would just like the authors to address my major concerns above.","The authors present a variational inference algorithm for continuous time Markov jump processes. Following previous work, they use ""uniformization"" to produce a discrete time skeleton at which they infer the latent states.  Unlike previous work, however, the authors propose to learn this skeleton (a point estimate, via random search) and to integrate out, or collapse, the transition matrix during latent state inference.  They compare their algorithm to existing MCMC schemes, which also use uniformization, but which do not collapse out the transition matrix.

While this work is well motivated, I found it difficult to tease out which elements of the inference algorithm led to the observed improvement. Specifically, there are at least four dimensions along which the proposed method differs from previous work (e.g. Rao and Teh, 2014):  (i) per-state \Omega_i vs shared \Omega for all states; (ii) learned point estimate of discretization vs sampling of discretization;  (iii) variational approximation to posterior over latent states vs sampling of latent state sequence; and (iv) collapsing out transition matrix vs maintaining sample / variational factor for it.  That the ""Improved MCMC"" method does not perform better than ""MCMC"" suggests that (i) does not explain the performance gap.  It seems difficult to test (ii) explicitly, but one could imagine a similar approach of optimizing the discretization using sample based estimates of the marginal likelihood, though this would clearly be expensive.  

Dimensions (iii) and (iv) suggest two natural alternatives worth exploring. First, an ""uncollapsed"" variational inference algorithm with a factor for the transition matrix and a structured variational factor on the complete set of latent states U. Indeed, given the discretization, the problem essentially reduces to inference in an HMM, and uncollapsed, structured variational approximation have fared well here (e.g. Paisley and Carin, 2009; Johnson and Willsky, 2014).  Second, it seems you could also collapse out the transition matrix in the MCMC scheme of Rao and Teh, 2014, though it would require coordinate-wise Gibbs updates of the latent states u_t | u_{\neg t}, just as your proposed scheme requires q(U) to factorize over time.  These two alternatives would fill in the gaps in the space of inference algorithms and shed some light on what is leading to the observed performance improvements. In general, collapsing introduces additional dependencies in the model and precludes block updates of the latent states, and I am curious as to whether the gains of collapsing truly outweigh these costs. Without the comparisons suggested above, the paper cannot clearly answer this question. 

Minor comments:
- I found the presentation of MCMC a bit unclear.  In some places (e.g. line 252) you say MCMC ""requires a homogeneously dense Poisson distributed trajectory discretization at every iteration.""  but then you introduce the ""Improved MCMC"" algorithm which has an \Omega_i for each state, and presumably has a coarser discretization.   

References:
Paisley, John, and Lawrence Carin. ""Hidden Markov models with stick-breaking priors."" IEEE Transactions on Signal Processing 57.10 (2009): 3905-3917.

Johnson, Matthew, and Alan Willsky. ""Stochastic variational inference for Bayesian time series models."" International Conference on Machine Learning. 2014.
","The paper proposes an algorithm for variational Bayesian inference in continuous-time Markov jump processes. In particular, while other inference methods for these models make separate updates to the inferred transition matrix and the inferred trajectory and hence suffer from the strong coupling between the two, the proposed method aims to marginalize out the transition matrix and only update the trajectory (and a uniformizing variable).

The basic idea here seems like a good one, though there are some significant weaknesses to the current version of the paper. The updates to the inferred transition times (the variational factor q(T), which is a point estimate) are greedy and heuristic. The paper would be made stronger if these heuristic updates were studied more closely and in isolation. Can the procedure get stuck in poor local optima? Are there conditions under which it should work well, or tricks to its initialization? It would be informative to have more detailed experiments on this step alone, though Figure 6 is a start.

Using an unstructured variational factor on q(U|T) is also a weakness, and the update is very similar to Algorithm 1 of Wang and Blunsom (2013). As a result, I think the main algorithmic contribution is in the update to q(T), which could probably be improved.

The plots look a bit rushed. Some text labels are hard to read and some legends are missing. Also, in several plots the MCMC method does not achieve the same error levels as the VB methods; this error gap is explained in terms of slow mixing (lines 268-271), but in that case it would be informative to run the MCMC methods for longer and see the gap close, or explain why that's not possible. In addition, some of the example trajectories look like toy problems.

Given the heuristic nature of the main algorithmic contribution and the weaknesses in the plots and experiments presented, I think this is a borderline paper."
Is the Bellman residual a bad proxy?,"Matthieu Geist, Bilal Piot, Olivier Pietquin",https://proceedings.neurips.cc/paper/2017/hash/e0ab531ec312161511493b002f9be2ee-Abstract.html,"The paper investigates a fundamental question in RL, namely whether we should directly maximize the mean value, which is the primary objective, or if it is suitable to minimize the Bellman residual, as is done by several algorithms.  The paper provides new theoretical analysis, in the form of a bound on residual policy search.  This is complemented by a series of empirical results with random (Garnet-type) MDPs.   The paper has a simple take-home message, which is that minimizing the Bellman residual is more susceptible to mismatch between the sample distribution and the optimal distribution, compared to policy search methods that directly optimize the value.

The papes is well written; it outlines a specific question, and provides both theoretical and empirical support for answering this question.  The new theoretical analysis extends previous known results.  It is a modest extension, but potentially useful, and easy to grasp.  The empirical results are informative, and constructed to shed light on the core question.  I would be interested in seeing analogous results for standard RL benchmarks, to verify that the results carry over to ""real"" MDPs, not just random, but that is a minor point.

One weakness of the work might be its scope: it is focused on a simple setting of policy search, without tackling implication of the work for more complex settings more commonly used, such as TRPO, DDPG.","The paper discusses a very interesting topic: which measure (BR minimization and mean value maximization) is better for policy search?

1. The reviewer is interested in one possible reason besides the discussion in the paper: the Markov assumption. As we know, the Bellman residual assumption is more restrictive to the Markov assumption of the task, whereas the mean value is not so restrictive to that. In real tasks for policy search, many tasks may break the Markov assumption. Does this also account for the reason why mean-value is a better metric in policy search?

2. There is a missing reference for (projected) Bellman minimization search:
Toward Off-Policy Learning Control with Function Approximation, by H Maei et.al. (using projected TD for off-policy control instead of just policy evaluation)

There is also one question： H Maei's method suffers from the latent learning problem:  the optimal policy, though learned, is not manifest in behavior. Until finishing the learning process, the learned policy is not allowed to be expressed and used. 

The reviewer wonders if such problem (latent learning) exists in the policy-based BR minimization approach. Please explain why if it does not exist. 
","The paper sheds light on the question whether policy search by directly maximizing the mean value or as a proxy minimizing the Bellman residual is more effective. Leaving aside implementation and estimation issues, the effectiveness of both objectives is compared theoretically and empirically. The empirical study suggests that maximizing the mean value is superior as its performance does not deteriorate when the concentrability coefficient becomes high.

Overall, the study presented in this paper is well executed and the paper is well written. While the results are certainly not exciting, the authors raise the valid point that both objectives are prevalent in RL and therefore such a study is of interest. GIven that the study reveals a mismatch between the theoretical and empirical performance of mean value maximization, I would have liked to see a discussion of whether performance bounds without a dependency on the concentrability coefficient are possible and what the challenges to prove such a bound are. Are there lower bounds available that have a dependency on related quantities?

The font size in the plots is too small. Axis labels and ticks are not readable at all.
"
Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems,"Celestine Dünner, Thomas Parnell, Martin Jaggi",https://proceedings.neurips.cc/paper/2017/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html,"This paper proposes a primal-dual block coordinate descent with active block selection based on coordinate-wise duality gaps for separable penalty function. The convergence results account for approximate solution of block update (used when particular machine doesn’t have access to complete data) and greedy block selection scheme to reduce the gap by maximum possible at the current iteration. The algorithm is shown to converge linearly, (c^t) in t iterations, for strongly convex penalty g and (c/t) in t iterations for bounded support penalty g. The algorithm is then applied to heterogeneous environment where machines with different memory and computation power are available. The numerical section compares the algorithm to other reference schemes like sequential or importance sampling based block selection and single threaded CPU. 

The paper is well written with clear organization and I enjoyed reading the proofs. I’m not an expert in this specific area and will leave the novelty judgment to other reviewers. 
","The paper presents a specific way of exploiting a system with two heterogeneous compute nodes with complementary capabilities. The idea is to perform block co-ordinate algorithm with the subproblem of optimizing with the block variables solved at the node with high-compute-low-memory and the subproblem of choosing the block at the node with low-compute-high-memory.

Here are some comments:

1. Overall, it is not very clear why the above is the only worthwhile strategy to try. Atleast empirical comparison with popular distributed algorithms (that ignore the resource capabilities) is necessary in order to access the merit of this proposal.

2. More importantly, the proposed split-up of workload in inherently not parallelizable. Hence a time-delay strategy is employed. Though empirically it is shown that it works, it not clear why this is the way to go?

3. In literature of block co-ordinate descent scoring functions (similar to (6) ) are popular. An empirical comparison with strategies like those used in SMO (libsvm etc.; first order and second order criteria for sub-optimality) seems necessary.

4. The results of Theorem1 and Theorem2 are interesting (I have not been able to fully check the correctness of the proofs).

5. Refer eqn (2). The duality is lower-bounded by the expression on LHS in (2). It is need not be equal to it.  

I have read the rebuttal and would like to stay with the score.","This paper considers the use of compute accelerators (e.g., GPUs) to efficiently solve large-scale machine learning problems. The set-up used by the authors is that of a compute unit with a large memory but with small compute power, coupled with a low memory high compute unit. The paper also proposes a new algorithm called DUHL (Duality Gap-Based Heterogeneous Learning Scheme) which is a coordinate descent based algorithm that works well on the computational setup previously described. The theoretical results support the proposed approach, as do the numerical experiments. 
This is an interesting paper. However, there are a lot of ideas crammed into this work that I feel have not been explained fully (probably due to the space limitations). It feels like more could have been done to ensure that the paper is more cohesive, so it is easier for the reader to follow, and also more motivation for what is being proposed."
Noise-Tolerant Interactive Learning Using Pairwise Comparisons,"Yichong Xu, Hongyang Zhang, Kyle Miller, Aarti Singh, Artur Dubrawski",https://proceedings.neurips.cc/paper/2017/hash/e11943a6031a0e6114ae69c257617980-Abstract.html,"This paper studies active learning using two kinds of feedbacks together: label feedback and comparison feedback. Its main insight is that to apply the noise pairwise ranking algorithm in (Ailon and Mohri, 2008) to get an order of examples, and then use binary search to infer their labels. It establishes query complexity bounds on the number of labels and number of comparisons, and bounds on noise tolerance on the two feedbacks. It shows that using pairwise comparison can substantially reduce the label complexity in active learning scenarios.

I vote for acceptance of this paper since this studies a nice novel problem that has practical motivations, and establishes the first algorithm analyzing noisy comparison feedbacks.

Minor comments:
1. It would be better to establish an excess error guarantees instead of closeness to h^* guarantees. (Although assuming reverse Tsybakov noise condition can directly get it, it is still an unnatural condition in my view.)

2. In line 508 it is worth pointing out that |S_i| > k by the assumption of n. This is crucial, since it leads to a requirement of n >= 1/epsilon^{2kappa-1}.

3. In line 462 it is mentioned that (3) is equivalent to Condition 2. In the literature I only see Condition 2 implying (3), but not the other way round. Also, Condition 2 does not require h^* to be Bayes optimal. Could the authors elaborate on this? 

4. It would be interesting to get a full tradeoff on the distribution (Z, X, X') and distribution of (X,Y). (On one extreme, if we have uninformative comparison queries, the model degrades to label-based active learning. On the other extreme, if we have an uninformative label queries, Theorem 12 studies its limitations.)


(I have read the author feedback - thanks to the authors for clarifying point Q2.)
","he authors study the active binary classification setting where

+ The learner can ask the label of an instance. The labelling oracle will provide a noisy answer (with either adversarial noise or Tsybakov)
+ In addition to the labeling oracle, there is a comparison oracle. The leaner then can ask which of two instances is more likely to be positive. This oracle's answers is also noisy.

The authors provide results which show that if the learner uses comparison queries, then it can reduce the label-query complexity. The problem that the authors study seem to be a valid and interesting one. The presentation of the result however should be improved. In the current state, accurately parsing the arguments is hard. Also, it would have been better if a better proof sketch was included in the main paper (to give the reviewer an intuition about the correctness of the results without having to read the supplements).

The core idea is to use the usual active-type algorithm, but then replace the label-request queries with a sub-method that uses mostly comparison queries (along with a few label queries). This is possible by first sorting the instances based on comparison queries and then doing binary search (using label queries) to find a good threshold. 

One thing that that concerning is the assumption on the noise level. On one hand, the authors argue that the major contribution of the paper compared to previous work is is handling noise. However, their positive result is not applicable when the amount of noise is more than some threshold (which depends on epsilon and even delta).

Also, the authors do not discuss if the learner is allowed to perform repeated comparison queries about the same pair of instances. This seems to be helpful. [Furthermore, the authors assume that the outcome of the queries are iid, even when they have been asked about overlapping pairs]
","This paper considers a new active learning setting, where in addition to a labeling oracle, the learner has access to a comparison oracle. It gives lower bounds, and a general algorithm with upper bounds under various noise conditions.

In my opinion, this paper is well-motivated, clear, and contains sufficient contribution for NIPS. It considers a novel interactive learning setting, and gives an interesting result that with comparison oracle, the d and 1/epsilon factors can be separated in the total query complexity. Its reduction from learning with comparison oracle to learning 1-dimensional threshold is also quite nice.

However, one (minor) technical mistake appears many times in the paper: the dependency of epsilon and kappa in the label complexity under Tsybakov noise condition (under their notation) should be (1/epsilon)^(2-2/kappa).

Some other minor issues I have are:
1. The term ""distribution-free"" in line 177 is confusing to me. In my opinion, the algorithm is depends on the distribution, since it needs to know the Tsybakov noise parameter kappa. 
2. In the presentation of the theorems, the SC_comp is the expectation, but SC_label is a high probability bound. It would be better to state SC_comp and SC_label in the same manner.
3. In line 225-231, the parameters M, tau, kappa (here kappa is overloaded), etc. are not defined in the main text. I understand this is due to space limit, but it would be better if the authors could find some alternative way to present their result for learning halfspaces. 

---
After reading author feedback, I agree with the authors that my claim about the label complexity was incorrect. I didn't notice they were using P(h \ne h^*) in the definition of the label complexity."
Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs,"Sanjiban Choudhury, Shervin Javdani, Siddhartha Srinivasa, Sebastian Scherer",https://proceedings.neurips.cc/paper/2017/hash/e139c454239bfde741e893edb46a06cc-Abstract.html,"Overview and Summary
This paper presents a method for motion planning where the cost of evaluating transitions between robot configurations is high.  The problem is formulated as a graph-search algorithm, where the order of graph expansion has a large impact on the performance of the algorithm due to the edge expansion cost.  The paper uses ideas from optimal test selection in order to derive the resulting algorithm.  The algorithm is tested on a number of synthetic datasets, a simulator, and a real-world helicopter planning problem.  


Detailed Comments

This paper extended work within the well-studied domain of robotic motion planning, extending and combining prior work in the area to construct a new algorithm.  The idea of using optimal test selection to address the graph expansion ordering is a natural, and reasonable approach.  The originality of this paper is moderate, and it does address a problem area of interest to many researchers.

There are some issues with clarity in the paper.  Specifically, starting in section 2, the term P(o) is first defined as an outcome vector, but then in equation (1) is used as a set to be drawn from (probabilistically?).  Next, and more significantly, lines 129-130 define the set of subregions, consisting of a single consistent subregion and a number of disjoint subregions that are each invalid.  The graph as a whole consists of nodes as subregions, and edge costs that are the product of the probabilities of each subregion.  I can see two issues with this formulation that are not made clear within the context of the paper:
- Given that the invalid subregions will expand with each test performed, the graph as a whole will have to change as testing progresses.  
- Because every edge contains at least one invalid subregion, every edge cost must contain at least one test where P(S_i) = 0, and therefore w must also equal 0.  
Due to this, either the problem formulation, or the clarity of the description must be changed.  

Also in terms of clarity, the index i is used on equations 3 and 5 on both the left and right hands of equations with different meanings.

Finally, within equation 5, the last term of the top equation does not equate to the last term of the bottom equation.  

Within the results, the text states that there are 13 datasets, but the results table lists 14.  Line 215 states that there are 8 tests with exclusively best performance, but the table lists only 7.  The claim on line 218 of a factor of 20 improvement is not possible to verify due to the normalization of all results.  Finally, and most importantly, the results are not compared against any of the other motion planning algorithms discussed in the related work, only against self-constructed heuristics used within the same general framework.  We have no idea how these algorithms compare to the state of the art.

Post-feedback:

Thanks to the authors for their explanation, additional material, and commitments to update the paper.  In light of this interaction, I have modified my score. 
","The paper presents an active learning algorithm for solving the Decision Region Determination problem (DRD) when the hypothesis space is determined by bernoullii independent outcomes. The motivation comes from robotic planning, but both the DRD problem and the Bernoulli assumption are interesting in their own right. In DRD, as in the classic active learning setting, a hypothesis (i.e., map from a query space to a binary label/outcome space) is sampled from a known prior distribution. In contrast to classic active learning, where the objective is to learn the hypothesis with as few queries as possible, the active learner is given a set of (possibly overlapping) regions, where each region is a subset of the query space. The objective is then to identify a region that in which all queries are labeled true by the correct hypothesis as quickly as possible.

An algorithm with provable guarantees for DRD already exists from [4], it self an extension of the EC2 algorithm from [16].  When the prior is a multinomial (i.e., outcomes of every query are bernoulli with known parameters), the algorithm requires an exponential computation to determine the next query: the authors show that a simple modification along with a memoization trick makes this linear, while still maintaining adaptive submodularity (the property that yields the optimality guarantees). 

Additional modifications are provided at different steps of the algorithm, that lead to further improvements in query complexity. The algorithm is evaluated against a barrage of state-of-the-art (non-strawmen) competitors on both synthetic and real-life datasets.

This paper is overall well written and lucid despite the technical depth, though familiarity with [4] and [16] would help. The contribution is a bit incremental compared to [4], but there are clear differences, and the algorithms constructed go beyond what was possible through [4]. Moreover, multinomial prior is  a reasonable model in the absence of additional structure, so having an algorithm under which this is tractable is important and deserves to be reported at NIPS. The authors clearly explain how their work relates to [4][16], and the experiments are thorough and exhaustive

Minor comments:

Line 61: move P(o) to right after the word ""distribution"" (it is the prior, I am guessing).

The motivation of the problem goes beyond robot planning; that said, if this is to be used in the introduction as the major problem solved, citations should not be limited to [10,11,8]: motion planning is classic, and dynamic programming and MDPs must be mentioned as alternatives (though their objective is not feasibility/validity).","This paper considers the problem planning in binomial graphs in which evaluating the edges is expensive, which can be used for planning collision-free paths for robots. Naive planning takes exponential time. This papers links motion planning with Bayesian active learning to select a feasible path. By building off of prior work and leveraging the binomial edge assumption, they develop an efficient, near-optimal algorithm. Finally, experiments on multiple simulated and real domains show the efficacy of the approach compared to prior, non-trivial baselines.

The contributions are:
(1) Bisect, which computes each subproblem in linear time (as opposed to exponential)
(2) A novel method for selecting candidate tests

Overall, the paper presents the motivation, contributions, and experiments effectively and convincingly, and therefore I think the paper should be accepted. More detailed comments below:

(a) The technical section seems to require knowledge of prior work, making it difficult for me to evaluate the technical details, however due to page limit constraints there is not much that can be done.

(b) The results showing that MaxProbReg candidate test selection usually is responsible for most of the speed improvement compared to using Bisect makes me as a reader question if it’s worth my trouble to implement Bisect. However, the experiments showing the cases where Bisect helps a lot is good; what would be more convincing is showing the reader that these problems do appear in the real-world (which the current real-world experiments do not demonstrate). However, because the authors will be open-sourcing their code, the issue of implementing Bisect is less problematic.
"
Minimal Exploration in Structured Stochastic Bandits,"Richard Combes, Stefan Magureanu, Alexandre Proutiere",t,"Problem Studied: The paper introduces and studies a wide range of stochastic bandit problem with known structural properties. This includes linear, lipschitz, unimodal, combinatorial, dueling bandits. 
Results: The paper derives an instance specific regret lower bound based on the structure and gives an algorithm which matches the lower bound. The algorithm needs to solve a semi-infinite LP in the general case, but they show that it can be efficiently done for special cases (which follows from previous literature).

Comments: The paper introduces a nice general framework for several of existing known results for several bandit problems such as linear, lipschitz, unimodal, combinatorial, dueling bandits. This encapsulates all these in very broad theoretical framework which can be useful for any new bandit problems.  "
Learning Efficient Object Detection Models with Knowledge Distillation,"Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, Manmohan Chandraker",https://proceedings.neurips.cc/paper/2017/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html,"Overall
This paper proposes a framework for training compact and efficient object detection networks using knowledge distillation [20]. It is claimed that this is the first successful application of knowledge distillation to object detection (aside from a concurrent work noted in Section 2 [34]), and to the best of my knowledge I confirm this claim. In my view, the main contributions of this work are the definition of a learning scheme and associated losses necessary to adapt the ideas of [20] and [32] for object detection. This requires some non-trivial problems to be solved - in particular, the definition of the class-weighted cross entropy to handle imbalance between foreground and background classes and the definition of a teacher bounded regression loss to guide the bounding box regression. Another minor contribution is the adaptation layer proposed in 3.4 for hint learning. On a skeptical note, these contributions can be viewed as rather straightforward technical adaptations to apply knowledge distillation to a related but novel task, object detection. In this regard, the goal may be considered a low-hanging fruit and the contributions as incremental. 

I have two main criticisms to the manuscript. First, I am not convinced that knowledge distillation is particularly well suited to the problem of object detection (see argument below). This leads to my second concern: there are no experiments demonstrating advantages of the proposed model compression method to other model compression approaches.

The manuscript is well written and easy to read, with only a few minor typos. The related work is well structured and complete, and the experiments seem fairly rigorous.

Related Works
+ The related works is well structured and seems to be comprehensive. The related work section even identifies a recent concurrent work with on the same topic and clearly points out the difference in the two approaches.

Approach
- It seems to me that knowledge distillation is not particularly well suited to the problem of object detection where there are only a few classes (and quite often, only foreground and background). One of the main features of [20] is the ability to distill knowledge from an ensemble containing generalist and specialist models into a single streamlined model. The specialist models should handle rare classes or classes that are easily confused, usually subsets of a larger class (such as dog breeds). There is no use of ensemble learning in the proposed approach, and there would be little benefit as there are relatively few classes. My intuition is that this will give much less power to the soft targets and knowledge distillation.
- The problem of class balancing for object detection needs to be more clearly motivated. What are the detection rates from the RPN and the ratio of foreground to background?

Experiments
+ Good experimental design clearly demonstrates the advantages of the method.
+ Important commonly used benchmark datasets are used (PASCAL, MS COCO, ILSVRC)
+ Experiments use several teacher and student models in a variety of configurations, allowing the reader to clearly identify trends (deeper teachers = better performance, bigger datasets = smaller gains)
+ Ablation study justifies the choice of weighted cross-entropy and bounded regression loss functions (note: In my opinion ‘ablation’ is a slight misuse of the terminology, although it is used this way in other works) 
-  A comparison against other compressed models is lacking.
- It would have been interesting to see results with a deeper and more modern network than VGG-16 as teacher.

Other comments
L29 - Deeper networks are easier to train - clarify what you mean by this. Problems like vanishing gradients make it difficult to train deeper networks.
L40-41 - This line is unclear. Should you replace ‘degrades more rapidly’ with ‘suffers more degradation’?
L108 - Typos: features, takes
L222 - Typo: Label 1
","Paper Summary: The idea of the paper is to learn compact models for object detection using knowledge distillation and hint learning. In knowledge distillation, a student model (the compact model) learns to predict the probability distribution that the teacher model (the larger more powerful network) predicts (as opposed to the groundtruth labels). In hint learning, the goal is to learn the intermediate layers of the teacher network. The model is a modification of Faster-RCNN that incorporates new losses for knowledge distillation and hint learning.

Strengths:

- The results are quite interesting that compact models are almost as good as the deeper and larger networks.

- The paper provides thorough analysis of different networks and different datasets.

- The paper is the first paper that uses knowledge distillation for object detection.

- The ablation study is very informative and shows how different components of the model help improving the performance.

Weaknesses:

- The paper is a bit incremental. Basically, knowledge distillation is applied to object detection (as opposed to classification as in the original paper).

- Table 4 is incomplete. It should include the results for all four datasets.

- In the related work section, the class of binary networks is missing. These networks are also efficient and compact. Example papers are:

    * XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, ECCV 2016

    *  Binaryconnect: Training deep neural networks with binary weights during propagations, NIPS 2015



Overall assessment: The idea of the paper is interesting. The experiment section is solid. Hence, I recommend acceptance of the paper.

","

        The paper presents a very nice idea introducing knowledge distillation
        to object class detection in images. It has several positive and
        negative aspects.

        Positive aspects:

        1. Application oriented. The paper is concerned with several practical
        aspects related to object detection, the main being keeping decent
        accuracy at lower computational cost. This is very interesting from
        practicioners point of view.
        2. The paper is well written and easy to read.
        3. The paper reffers to relevant object detection and knowledge
        distillation literature fairly and adequately.
        4. The experiments suggest that the proposed method can indeed achieve
        reasonable performance (sligntly bellow the uncompressed models) at a
        cheaper computational cost accross 4 different object detection
        datasets.
        5. The paper provides analysis and insights into the different aspects
        and novelties introduced in this work.

        Negative aspects:

        1. Novelty. The idea of knowledge distilation is not new and has been
        explored before in the literature. The novelty in this work comes via
        several engineering tricks the paper employs to adapt the knowledge
        distilation idea to the Faster-RCNN framework.

        2. Comparison to baselines and state-of-the-art. The paper failed to
        compare to relevant and newer state-of-the-art works on object detection
        outside the Faster-RCNN framework. In addition, comparison to other
        compression and efficiency-oriented works would be good to have, both
        in terms of quality but also speed.

        3. Limited experiments to AlexNet and VGG. It would be great to see if the method scales to bigger models like Inception and ResNet. In addition, it would be interesting to see if using a smaller model as teacher helps the bigger student model.

        4. In the end, it seems distilation is a technique to get better local minimum when training models.

        Detailed comments:
        1. Line 44: The claim that object class detection is more complex than image classification is an overstatement.
        2. Equation 1 should clearly state what is new w.r.t. Faster-RCNN, if any at all.
        3. Line 127. How do you fix the lambda and gamma params?
        4. Eq 2. How do you fix the mu param?
        5. Line 148. How do you fix the weights?
        6. Eq 6. What about m?
        7. Table 3 seems unfair in two dimensions. First, the finetuned model is compressed, but then the distillation model is trained as VGG16 as teacher. It would be good to keep the teacher to be the uncompressed student model (alexnet). This will reduce the degrees of freedom in the experiment. 


      "
Learning Chordal Markov Networks via Branch and Bound,"Kari Rantanen, Antti Hyttinen, Matti Järvisalo",https://proceedings.neurips.cc/paper/2017/hash/e22312179bf43e61576081a2f250f845-Abstract.html,"The authors present a branch and bound algorithm for learning Chordal Markov networks. The prior state of the art algorithm is a dynamic programming approach based on a recursive characterization of clique tress and storing in memory the scores of already-solved subproblems. The proposed algorithm uses a branch and bound algorithm to search for an optimal chordal Markov network. The algorithm first uses a dynamic programming algorithm to enumerate Bayesian network structures, which are later used as pruning bounds. A symmetry breaking technique is introduced to prune the search space. 

The special property of Chordal Markov networks necessitates that no score pruning can be done. Given that this step is exponential in time and space. It is unlikely that learning CMSL can scale to even medium-size datasets (>20-30variables). 

What is the definition of decomposable DAG? This is important for understanding other concepts defined later on, such as ordered decomposable DAG.

The symmetry breaking rule is useful. What is its relation to the symmetry breaking rules introduced in (van Beek&Hoffmann 15) for learning BNs? The authors referenced that paper and should have discussed the relevance.

The paper did not give a clear picture on how much time is spent on computing scores, and how much time is spent on optimization. Also, the total runtime of the proposed algorithm includes computing optimal BNs and segment trees. How much do they take relative to the total runtime?  The aggregated time does not provide a complete picture.

","The authors present a set of techniques for improved learning of chordal Markov networks, by combining and refining existing techniques. The paper is very well presented, and the empirical evaluation shows that the proposed techniques display advantages over existing methods. The contributions are somewhat incremental when the state-of-art is considered, but still they offer improvements that may be useful to other researchers. 

One point about the presentation: the authors do not explain the reasons why ""symmetry breaking"" is important, and I believe this should be discussed in some detail so as to help the reader understand the main points.

It is not clear to me whether the authors use a bound on the number of parents, and how it is used in the experiments. Please add some information about it. 

Concerning the text, I have a few points to make:
- Please order references when they appear together (say at the end of the first paragraph of the Introduction).
- Page 1, line -1: ""coincide"" instead of ""coincidence"".
- Section 2, line 2: $E^u$."
Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding,"Wenbing Huang, Mehrtash Harandi, Tong Zhang, Lijie Fan, Fuchun Sun, Junzhou Huang",https://proceedings.neurips.cc/paper/2017/hash/e3408432c1a48a52fb6c74d926b38886-Abstract.html,"A very interesting paper that solves clustering and sparse approximation problems of LDSs by using a kernel-based formulation (kernel defined over pairs of LDSs defined using extended observability subspaces) and a projected gradient descent method. The central contribution of the paper, summarized in Theorem 4, is the calculation of the gradient terms by solving a discrete Lyapunov equation. Overall, significant improvements in accuracy and speedups are demonstrated for clustering and sparse coding and their application for video classification of dynamic textures. 

Strengths
+ Well-written paper
+ Strong algorithmic core with theoretical justification
+ Limited but sufficient evaluation

Weaknesses
- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.
- Issues of convergence, especially when applying gradient descent over a non-Euclidean space, is not addressed

In all, a rather thorough paper that derives an efficient way to compute gradients for optimization on LDSs modeled using extended subspaces and kernel-based similarity. At one hand, this leads to improvements over some competing methods. Yet, at its core, the paper avoids handling of the harder topics including convergence and any analysis of the proposed optimization scheme. None the less, the derivation of the gradient computations is interesting by itself. Hence, my recommendation. ","
Summary: the paper proposes a clustering method for linear dynamical systems, based on minimizing the kernalized norm between extended observability spaces.  Since the objective function contains terms involving discrete Lypanunov equations (DLEs), the paper  derives how to pass the derivatives through, yielding a gradient descent algorithm.  Experiments are presented on 2 datasets, for LDS clustering and sparse codebook learning.

Originality:
+1) novelty: the paper is moderately novel, but fairly straightforward. The difference with other related works [3,9,10,11,12] in terms of objective functions, assumptions/constraints, canonical forms should be discussed more.

Quality:
There are a few potential problems in the derivations and motivation.
-1) Theorem 1: The canonical form should have an additional ineqaultiy constraint on Lambda, which is:
   lambda_1 > lambda_2 > lambda_3 > ... lambda_n
which comes from Lambda being a matrix of the singular values of A.  Imposing this constraint removes equivalent representations due to permutations of the state vector.  This inequality constraint could also be included in (12) to truly enforce the canonical form.  However, I think it is not necessary (see next point).

-2) It's unclear why we actually need the canonical form to solve (10).  Since gradient descent is used, we only care that we obtain a feasible solution, and the actual solution that we converge to will depend on the initial starting point.  I suppose that using the canonical form allows for an easier way to handle the stability constraint of A, rho(A)<1, but this needs to be discussed more, and compared with the straightforward approach of directly optimizing (A,C).

-3) Theorem 4: I'm not entirely convinced by the proof.  In the supplemental, Line 35 shows an explicit equation for the vectorized R_12.  Then why in (34) does it need to be transformed into a DLE?  There is something not quite correct about the vectorization and differentiation step. Furthermore, it seems that the gradients could be directly computed from (24). For example, taking the derivative wrt A1:
  d/dA1 (A1^T G12 A2 - G12) = d/dA1 (-C1^T C2)
  G12 A2 + A1^T (dG12/dA1) A2 - (dG12/dA1) = 0
Hence dG12/dA1 can be obtained directly as a DLE.

-4) Given that there are previous works on LDS clustering, experiments are somewhat lacking:
 - Since a kernel method is used, it would be interesting to see the results for directly using the linear kernel, or the other kernels in (19,20).
 - In the video classification experiment, there is no comparison to [3,8,11], which also perform LDS clustering.
 - No comparison with probabilistic clustering methods [21,22].
  
Clarity:
- L210 - Theorem 4 cannot be ""directly employed"" on new kernels, since the theorem is specific to the kernel in (5). Some extension, or partial re-derivation is needed.
- Other minor typos:
  - Eq 8: it should be mentioned why the k(S_m,S_m) term can be dropped.
  - Eq 9: the minimization is missing z_i.
  - Eq 10: the text is missing how (9) is solved. I'm assuming there are two sub-problems: sparse code estmiation, and codebook estimation with z_i fixed.  (10) corresponds to the second step.

Significance:      
Overall, the approach is reasonable and offers a solution to the problem of clustering LDS. However, more could have been done in terms of relating the work with previous similar methods, exploring other kernel functions, and more thorough comparison in the expeirments.  Some steps in the derivation need to be clarified.
      
      "
Deep Subspace Clustering Networks,"Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, Ian Reid",https://proceedings.neurips.cc/paper/2017/hash/e369853df766fa44e1ed0ff613f563bd-Abstract.html,"
The paper propose to enforce self-expressiveness within the latent space of an auto-encoder, so as to make the latent representation better suited to spectral clustering.

The underlying intuition, and the experimental validation sound convincing (comparison with key relevant alternative methods is provided). Paper is also clear and well writen. I do encourage its acceptance. 

Some aspects however deserve a deeper discussion (even if they point out relative weaknesses of the approach). These include the practical use of the algorithm: how does the algorithm deal with large datasets (the number of parameters in the model increases with the number N of samples)? how can you assign a new sample to one of the cluster (without running the algorithm on the whole dataset)? what is the sensitivity of the approach to \lambda_2? what does motivate the definition of lambda_2 on page 6 ?

","The paper addresses the problem of subspace clustering, i.e., separating a collection of data points lying in a union of subspaces according to underlying subspaces, using deep neural networks. To do so, the paper builds on the sparse subspace clustering idea: among all possible representations of a data point as a combination of other points in the dataset, the representation that uses the minimum number of points, corresponds to points from the same subspace. In other words, SSC uses the idea that for a data matrix $X$, a sparse solution of $X = X C$ (subject to $diag(C) = 0$) represents each point as a combination of a few other points from the same subspace. The paper proposes a deep neural network to transform the data into a new representation $Z = f_W(X)$ for which one searches for a sparse representation of $Z = Z C$, with the hope to learn more effective representations of data for clustering. To achieve this, the paper uses an auto-encoder scheme, where the middle hidden layer outputs are used as $Z$. To enforce $Z - Z C = 0$, the paper repeats the middle layer, with the hope that these two consecutive layers obtain same activations. If so, the weights  between the two layers would correspond to coefficient matrix $C$ which will be used then, similar to existing methods, to build an affinity matrix on data and to obtain clustering. The paper demonstrates experiments on Extended YaleB  and ORL face datasets as well as COIL20/100.

The reviewer believes that the paper is generally well-written and easy to read. The paper also takes a sufficiently interesting approach to connect subspace clustering with deep learning. On the other hand, there are several issues with the technical approach and the experiments that limit the novelty and correctness of the paper.

1) The reviewer believes that the idea of the paper is similar to the one in [36], which uses an auto-encoder scheme, taking the middle layer as $Z$. While [36] fixes the $C$ and solves for $Z$, the proposed method repeats the middle layer to also solve for $C$. Despite this similarity, these is only a sentence in the paper (line 82) referring to this work and the experiments do not compare against [36]. Thus there is a need to clearly discuss advantages and disadvantages with respect to [36] and compare against it in the experiments.

2) It is not clear to the reviewer how the paper enforces that the activations of the middle and the middle plus one layer must be the same. While, clearly one can initialize $C$ so that the consecutive activations will be the same, one the back propagation is done and the weights of the auto-encoder change, the activations of middle layer and the one after it will not be the same (unless the $\lambda_2$ is very large and the network achieves it global minima, which is generally not the case). As a result, in the next iteration, the network will minimize $Z - Y C$ for different $Y$ and $Z$, which is not desired. 

3) One of the drawbacks of the proposed approach is the fact that the number of weights between the middle layers of the network is $N^2$, given $N$ data points. Thus, it seems that the method will not be scalable to very large datasets.

4) In the experiments (line 243), the paper discusses training using mini-batches. Notice that while using mini-batches makes sense in the context of recognition, where each sample can be applied separately to the network, using mini-batches does not make sense in the context of clustering discussed in the paper. Using a mini-batch of size $M < N$, how does the paper train a neural network between two layers, each with $N$ nodes? In other words, it is not clear how the mini-batch instances and in what order will be input the the network.","This paper proposes a new subspace clustering (SC) method based on neural networks. Specifically, the authors constructed the network by adding a self-expressive layer to the latent space of the traditional auto-encoder (AE) network, and used the coefficients of the self-expression to compute the affinity matrix for the final clustering.

The idea of doing SC in the latent space of AE is reasonable since the features may be more powerful for clustering task than the original data. Besides, the designing of the self-expressive layer is able to further exploit the structure of the data in latent space. The experiments on three image datasets demonstrated the effectiveness of the proposed method.

My concerns are as follows:

First, as indicated by the authors, [36] also used AE type networks for SC, and thus it is better to discuss the relations and differences of network structures between the two in details. It would also be more convincing to directly compare the two methods in experiments, instead of saying the code ""is publicly available"" and only including partial results reported in [36].

Second, since the hyperparameters are generally very important for deep learning, it would be better to discuss how the hyperparameters, e.g., the number of layers, influence the final performance.

Third, it would also be useful to include the computation time for readers who are willing to follow this work."
Robust Estimation of Neural Signals in Calcium Imaging,"Hakan Inan, Murat A. Erdogdu, Mark Schnitzer",https://proceedings.neurips.cc/paper/2017/hash/e449b9317dad920c0dd5ad0a2a2d5e49-Abstract.html,"This paper proposes a robust statistical estimator for analyzing calcium imaging data to extract timecourses and spatial filters of cells. This is an important application for neuroscience and improvements that increase the ability of algorithms to deal with cross-talk would be useful for the field. The framework appears to be closely related to the CNMF method, but with a robust loss function.  I'm not confident in assessing the novelty or validity of the statistical framework, but I will comment on the practical application.  

My main reservation is that in Fig. 2d it appears that the estimation accuracy declines with each iteration. I guess this is because the authors initialized the algorithm using the ground truth cell locations.  This seems to be an unrealistic scenario, and is not justified in the paper.  Given this, it's hard to evaluate the claim that the EXTRACT algorithm improves performance relative to CNMF (or ICA).

Additional points:

- Line 47: The reason that signals in calcium imaging data are non-negative is not because they come from photons.  Rather, it is because calcium is highly regulated in neurons, with near zero baseline concentration of free calcium and several orders of magnitude increase in calcium following opening of voltage gated ion channels.
- Why not simulate the data using a Poisson distribution to model photon shot noise? This is the most realistic noise model for fluorescence imaging data.
","The authors contribute a matrix factorization method based on a one-sided Huber loss within the framework of robust estimation. Asymptotic proofs of optimality are provided. The method is applied to the matrix factorization problem of extracting cells and time courses from optogenetic neural measurements and improvements over the matrix factorization method CNMF, as well as ICA.

As I am not familiar enough with robust regression I cannot comment much on the proofs. I believe the authors that this contribution is novel and that the proofs are correct. In this setting, the contribution is clear a step ahead in that it provides the possibility to perform the matrix factorization needed for the calcium imaging application.

It is by construction more accommodating of otherwise disruptive types of noise and it is conceivable that this gives the method an edge over existing varieties.

In the evaluation on synthetic data, possibly some more systematic variations of overlap/temporal similarity could have been explored and the outcome compared against the other methods in order to understand breaking points of the proposed method as well as the comparisons more systematically.

","This is an important and timely paper in the automated signal detection of calcium imaging of neurons.   The authors have developed a new methodology based on careful noise model and robust statistics.   The prevalence of calcium imaging experimental studies has increased interest in powerful analysis methods for understanding correlation of neuronal firing patterns, and this paper represents a strong advance in this direction.

The use of robust location estimator appears be a good approach, particularly given the magnitude of signal and noise variability and strong presence of ouliers in neuropil.  It is somewhat surprising that previous methods have not considered this.

The author's noise model as a superposition of positive sources and lower amplitude normal component is interesting and gives more statistical power in the analysis.  In equations (2) these components should be more explicitly labelled for readability.

The authors present a rigorously argument for the algorithms convergence rate which is fast. 

The authors present a nice comparison of their EXTRACT algorithm with two other approaches in actual manually labelled microendoscopic single-photon imaging data and show superior performance.  This section of the paper could be developed a little more carefully with more explicitly stated performance statistics,  although Figure 4 is well done.

In summary this is a very strong paper on a timely and important topic.  The availability of a code and how it might be deployed would be helpful.




"
Fast-Slow Recurrent Neural Networks,"Asier Mujika, Florian Meier, Angelika Steger",https://proceedings.neurips.cc/paper/2017/hash/e4a93f0332b2519177ed55741ea4e5e7-Abstract.html,"The paper proposed a new RNN structure called Fast-slow RNN and showed improved performance on a few language modeling data set. 

Strength: 
1. The algorithm combines the advantages of a deeper transition matrix (fast RNN) and a shorter gradient path (slow RNN). 
2. The algorithm is straightforward and can be applied to any RNN cells. 

Weakness: 
1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly. 
Here are some examples: 
(1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is ""trivial"" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)? 
(2) In line 96, I do not understand the sentence ""our lower hierarchical layers zoom in time"" and the sentence following that.

2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN. 

3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper. 

4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results. ","This paper presents an RNN architecture that combines the advantage of stacked multiscale RNN for storing long-term dependencies with deep transition RNN for complex dynamics that allow quick adaptation to changes in the inputs. The architecture consists of typically four fast RNN cells (the paper uses LSTMs) for the lower deep transition layer and of one slow RNN upper cell that receives from faster cell 1 and updates the state of faster cell 2.

The model is evaluated on PTB and enwiki8, where it achieves the lowest character-based perplexity when compared to similar-sized (#parameters or number of cells) architectures.
The analysis of vanishing gradients and of cell change is insightful.

One small note: should fig 1 say h_{t-1}^{F_k} ?
","
    The paper presents a novel architecture Fast Slow Recurrent Neural Network (FS-RNN), which attempts to incorporates strengths of both multiscale RNNs and deep transition RNNs. The authors performed extensive empirical comparisons to different state-of-the-art RNN architectures.
The authors also provided an open source implementation with publicly available datasets, which makes the mentioned experiments reproducible. It would be better if the authors compared their approach with other recent state-of-the-art systems like Tree-LSTM that captures hierarchical long term dependencies. 
      "
PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs,"Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S. Yu",https://proceedings.neurips.cc/paper/2017/hash/e5f6ad6ce374177eef023bf5d0c018b6-Abstract.html,"The work introduces a new architecture for conditional video generation. It is is based heavily on convolutional LSTMs, with the insight that the prior architecture where each layer of hierarchy represents increasingly abstract properties of the scene may be suboptimal for video generation (unlike classification, a generative model needs, even at the output layer, to retain information about the precise position of objects in the scene). The model introduced here, PredRNN extends convolutional LSTMs to contain two memory cells, one which flows through time at the same layer (like the original ConvLSTM), and one which flows up through the layers. The authors test the model on two datasets: moving MNIST digits and KTH action recognition dataset and demonstrate superior MSE in video prediction to prior work. The paper is fairly well-written and cites prior work.

The architecture is novel and interesting. I think the comparison with prior work could be strengthened which would make it easier to understand the empirical strength of these results. For example, the Kalchbrenner, 2016 claims to reach near the lower-bound the MNIST digit task, but is not included as a baseline. Prior work on MNIST (e.g. (Kalchbrenner, 2016) and the original introduction of the dataset, (Srivastava, 2016) report the cross-entropy loss. Here, the authors focus on the maximum likelihood output, but it would be helpful for comparison with prior work to also report the likelihood.

Additionally, the computational complexity is mentioned as an advantage of this model, but no detailed analysis or comparison is performed so its hard to know how this compares computational complexity with prior work.

Minor notational suggestion:

It might be easier for the reader to follow if you use M instead of C for the cell state in equation 3 so that the connection with equation 4 is clearer.

[I've read the author's response. I think this paper is stronger for comparison with prior work (which I assume they'll include in the final version) so I have raised my evaluation. I'm still unclear if they are training with MSE and other approaches are using different losses, doesn't that provide an advantage to this model when evaluating using MSE?]","This paper deals with predictive learning (mainly video prediction), using a RNN type of structure. A new (to my knowledge) variation of LSTM is introduced, called ST-LSTM, with recurrent connections not only in the forward time direction.
The predictive network is composed of ST-LSTM blocks. Each of these block resemble a convolutional LSTM unit, with some differences to include an additional input. This extra input comes from the last layer of the previous time step, and enters at the first layer of the current time step, it is then propagated through the layers at the current time step.
The resulting ST-LSTM is similar to the combination of two independent LSTM units, interacting through concatenation of the memories (figure 2(left)).

I couldn't find an explicit formulation of the loss (the closest I could find is equation 1). I am assuming that it is a MSE loss, but this should be confirmed, since other forms are possible.

Results are presented on the Moving MNIST (synthetic) and KTH (natural) datasets, showing both PSNR/SSIM and generations. The authors compare their PredRNN with other baselines, and show the best results on these datasets. On Moving MNIST, there is a comparison of different LSTM schemes, and ST-LSTM show the best results, which is a good result.

However, the experimental section could be stronger by having more datasets (the two datasets presented have little ambiguity in their future, it could be interesting to see how the model preforms in less contained dataset, such as Sports1m, UCF101 or the Google ""Push"" dataset, for instance). Although this paper present a complex (and, as it seems, good) structure for the predictor, the loss function is almost never mentioned. As (among others) [17] and [19] mention, a simple loss such as the MSE cannot work well when the future distribution is multimodal. The paper would also be stronger if compared with other methods (non LSTM-based), such as the ones presented in section 1.2. In particular, VPNs and GANs seem like strong competitors.

Notes:
- Figure 1: Although the figure is clear, I do not quite understand what the orange arrows mean (compared to the black arrows).
- Figure 2(right): As I understand it, each W box should also have an Xt input (not just W1)","
This paper introduces a novel convolutional LSTM based architecture for next frame video prediction. The difference with previous attempts is that spatial and temporal variations are gathered in a single memory pool.
 

Comments

The paper is mostly well written, proposes a new approach that appears to be fruitful on two relatively simple datasets.

Providing generated videos for such paper submission would be appreciated.

The results of the paper seem good but the evaluation of the proposed approach to real natural images would be more convincing.

The KTH dataset is described as a dataset of ""natural images sequences"" but remain very simple to analyse: very low resolution, uniform foreground and background... As the proposed approach is claimed to be memory efficient, it shouldn't be a problem.

Could you provide an idea of the training time?

l.84-86 the authors describe the applications of video prediction as a fact in numerous domains, without citing any references. The reader is therefore curious if these applications are already happening in this case, the authors should provide references, or might happen later, in this case the authors should rephrase (finds -> could find)

l. 105-106 I don't understand the introduction of the deconvolution operator, since it seems unused in the equations and the rest of the paper.



Minor:

l.47  no comma
l 57-58 they ... always -> they always
l 57 the next one -> the next
l. 135 no comma
l 141 in in -> in
l 135-137 I don't understand this sentence
l 154 : We -> we
l 242: ignores -> ignore
[9] {LSTM} ICML 15
[15] ICLR workshop 16
[19]: ICLR 16
[23] {SVM}
[24] ICLR 15
please check other references"
Dual Discriminator Generative Adversarial Nets,"Tu Nguyen, Trung Le, Hung Vu, Dinh Phung",https://proceedings.neurips.cc/paper/2017/hash/e60e81c4cbe5171cd654662d9887aec2-Abstract.html,"The paper proposes to train GANs by minimizing lower bounds of KL and reverse KL.
The KL and the reverse KL costs are weighted by two hyperparameters.

The KL and reverse KL estimators were previously mentioned in f-GAN.
It seems that this paper rediscovers the estimators. The theoretical analysis is consistent with the f-GAN properties.

Pros:
- It is good to know that the minimization of the symmetric KL works well.
The experiments show good results on 2D toy data, MNIST-1K and natural images.

Cons:
- It is not clear how the KL and reverse KL estimators were derived. f-GAN is not mentioned in that context.
- It would be nice to see a discussion of the limitations. What will happen if the discriminators are trained till convergence? The loss would go to infinity if measuring KL for distributions with disjoint supports.
What will happen if the discriminators are not updated frequently? The generator can put all mass to a single point.
- It would be good to see whether D2GAN works OK without batch normalization in the generator.


Questions:
1) Have you tried using just one discriminator network with 2 outputs to produce D1(x) and D2(x)?
Was it working worse?

2) Were two discriminator networks used in the experiments?
That would be a bit unfair to the baselines with just one discriminator net.

Minor typos:
- The $p_\mathrm{data}$ style is not consistent in the document.
- Sometimes (x) is missing next to p_G(x) or p_data(x). Maybe it is intended.
- The optimal discriminators in Theorem 2 should be: D1 = alpha, D2 = beta.

Update:
I have read the rebuttal and I thank the authors for the extra experiments.
The authors should mention that the estimated KL and reverse KL
are just lower bounds, if the discriminators are not optimal.","This paper presents a variant of generative adversarial networks (GANs) that utilizes two discriminators, one tries to assign high scores for data, and the other tries to assign high scores for the samples, both discriminating data from samples, and the generator tries to fool both discriminators.  It has been shown in section 3 that the proposed approach effectively optimizes the sum of KL and reverse KL between generator distribution and data distribution in the idealized non-parametric setup, therefore encouraging more mode coverage than other GAN variants.

The paper is quite well written and the formulation and analysis seems sound and straightforward.  The proposed approach is evaluated on toy 2D points dataset as well as more realistic MNIST, CIFAR-10, STL and ImageNet datasets.

I have one concern about the new formulation, as shown in Proposition 1, the optimal discriminators have the form of density ratios.  Would this cause instability issues?  The density ratios can vary wildly from 0 to infinity when the modes of the model and the data do not match well.  On the other hand, it is very hard for neural nets to learn to predict extremely large values with finite weights.  On the toy dataset, however, the proposed D2GAN seems to be more stable than standard GAN and unrolled GAN, is this in general true though?

I appreciate the authors’ effort in doing an extensive hyperparameter search for both the proposed method and the baselines on the MNIST dataset.  However, for a better comparison, it feels we should train the same generator network using all the methods compared, and tune the other hyperparameters for that network architecture (or a few networks).  Also, learning rate seems to be the most important hyperparameter but it is not searched extensively enough.

The paper claims they can scale up their proposed method to ImageNet, but I feel this is a little bit over-claiming, as they only tried on a subset of ImageNet and downsampled all images to 32x32, while there are already existing methods that can generate ImageNet samples for much higher resolution.

Overall the paper seems like a valid addition to the GAN literature, but doesn’t seem to change state-of-the-art too much compared to other approaches.","UPDATE (after rebuttal):

Thanks for your feedback.

1. Tease out D1, D2: I think it's good to include these nevertheless (at least in supplemental information) as it's an interesting piece of experimental evidence.

3. Share parameters D1 and D2: Please include these results in the final version as well.

I have increased the score to 8.

--------

The paper proposes dual discriminator GAN (D2GAN), which uses two discriminators and a different loss function for training the discriminators. The approach is clearly explained and I enjoyed reading the paper. The experiments on MoG, MNIST-1K, CIFAR-10, STL-10, ImageNet support the main claims. Overall, I think this is a good paper and I vote for acceptance.

I have a few questions / suggestions for improvement:

Eq (1) uses D1 and D2 as well as a new loss function for the discriminator. Have you tried using only D1 or D2? I think it'd be useful to report this baseline so that we can tease out the two effects. Interestingly,  the loss function obtained by using only D1 in eq (1), has an interesting connection with the KL importance estimation procedure (KLIEP). See also eq (12) in the paper:
Learning in Implicit Generative Models
Shakir Mohamed, Balaji Lakshminarayanan
https://arxiv.org/pdf/1610.03483.pdf

IIUC, you trained two separate networks for the discriminators D1 and D2. Have you tried sharing the parameters and just modifying the loss function? This reduces the number of parameters and makes the method more appealing. In particular, D1 targets p_data/p_g and D2 targets p_g/p_data. Since they are targeting the inverses of each other, it might be useful to have one network to model the log ratio p_data/p_g and use a hybrid loss for the ratio. If two networks are really necessary, it'd be helpful to add some insight on why this is the case (is it numerical issue, does it help the optimization etc)"
Beyond Parity: Fairness Objectives for Collaborative Filtering,"Sirui Yao, Bert Huang",https://proceedings.neurips.cc/paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html,"In this paper the authors explore different metrics for measuring fairness in recommender systems.  In particular, they offer 4 different metrics that measure if a recommender system over-estimates or under-estimates how much a group of users will like a particular genre.  Further, they show that by regularizing the by the discrepancy across groups does not hurt the model accuracy while improving fairness on the metric used for the regularization (as well as undiscussed slight effects on the other fairness metrics).

In contrast to the work of Hardt et al, this paper focuses on a regression task (rating prediction) and explores the difference between absolute and relative errors.  This is an interesting point in recommender systems that could easily be overlooked by other metrics.  Further, the clear demonstration of bias on MovieLens data and the ability to improve it through regularization is a valuable contribution.  Additionally, the simplicity of the approach and clarity of the writing should be valued.

However, the metrics are also limited in my opinion.  In particular, all of the metrics focus on average ratings over different groups.  It is easy to imagine a model that has none of the biases discussed in this paper, but performs terribly for one group and great for the other.  A more direct extrapolation of equality of opportunity would seem to focus on model accuracy per-rating rather than aggregates.  I do not think this ruins the work here, as I mentioned above that the directionality of the errors is an important observation, but I do think it is a limitation that will need to be developed upon in the future.

In this vain, a significant amount of recommender systems research is not cited or compared against.  Data sparsity, missing not at random, learning across different subgroups, using side information, etc. have all been explored in the recommender systems literature.  At the least, comparing against other collaborative filtering models that attempt to improve the model accuracy by side information would provide better context (as well as reporting accuracy of the model by both gender and genre).  A few pieces of related work are included below.


""Collaborative Filtering and the Missing at Random Assumption"" Marlin et al 
""Beyond Globally Optimal: Focused Learning for Improved Recommendations"" Beutel al
""It takes two to tango: An exploration of domain pairs for cross-domain collaborative filtering"" Sahebi et al
","The paper focuses on algorithmic fairness in the context of recommender systems (in te framework of collaborative filtering), which has been barely studied up to date. The main contribution of the paper are four measures of fairness suitable for recommender systems, and a high level idea of the real-world contexts for which they are suitable. 

On the positive side, the paper is in general well written, easy to follow, and provides intuition about the different measures of fairness they propose together with their pros and cons. Also, the running example on recommendations in education serves as a real-world motivation and explanation for the proposed measures. Finally, the thorough experimental section provides  good intuition on which measure are more appropriate for the different real-world scenarios.

As potential criticism, the technical contributions of the paper are scarce.  I believe that the paper lack of  a theoretical/empirical study of the trade-off between accuracy and fairness, as well as of the increase in computational complexity when extending the formulation of the recommender system (matrix factorization) to account for fairness. The authors focus on proposing novel fairness measurements but they do not study how the computational efficiency of matrix factorization learning algorithm changes from Eq. (4) to Eq. (11) under the different definitions of fairness, which might be important on real world applications. However, given that fairness is a pretty novel topic in the machine learning (ML) community, I believe that simple ideas/approaches as the one presented in the paper are of general interest and minimize the gap between theoretical and practical aspects of ML. ","
          The paper looks at fairness in the context of recommendation systems. It attempts to define metrics that determine whether recommendations (as measured by a suggestion for a class, or a movie) are skewed based on group identification. It then proposes algorithms to design recommenders that optimize for fairness.
          
          The main contributions are four different fairness measures that are all based on the idea of looking at errors in the outcome conditioned on group identification. The authors optimize for each of these using standard optimizations and then present results on synthetic and real data sets.
          
          The authors don't point this out, but the formulation of the fairness measures is a bit more general than stated: essentially the formulation generalizes the idea of balancing errors to a regression setting, and then formulates variants based on what kinds of regression errors are important (signed vs unsigned, overestimates vs underestimates). Viewed that way, this paper is less abuot recommendation systems per se, and more about how do to regression fairly.
          
          One interesting little tidbit in the experiment appeared to be the difference in effectiveness with observation vs population bias. This doesn't appear to have been explored further, but might be useful to understand because one might argue that observation bias is fixable, but population bias requires deeper interventions (if it's even possible).
          
          Overall, it's a decent and natural generalization of prior work to the regression setting, with results that aren't too surprising.
      "
A simple neural network module for relational reasoning,"Adam Santoro, David Raposo, David G. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap",https://proceedings.neurips.cc/paper/2017/hash/e6acf4b0f69f6f6e60e9a815938aa1ff-Abstract.html,"The paper proposes a plug and play module (called Relation Networks (RNs)) specialized for relational reasoning. The module is composed of Multi Layer Perceptrons and considers relations between all pairs of objects. The proposed module when plugged into traditional networks achieves state of the art performance on the CLEVR visual question answering dataset, state of the art (with joint training for all tasks) on the bAbI textual question answering dataset and high performance (93% on one task and 95% on another) on a newly collected dataset of simulated physical mass-spring systems. The paper also collects a dataset similar to CLEVR to demonstrate the effectiveness of the proposed RNs for relational questions. 

Strengths:

1. The proposed Relation Network is a novel neural network specialized for relational reasoning. The success of the proposed network is extensively shown by experimenting with three different tasks and clearly analyzing the effectiveness for relational questions by collecting a novel dataset similar to CLEVR. 

2. The proposed RNs have been shown to be able to work with different forms of input -- explicit state representations as well as features from a CNN or LSTM.

3. The paper is well written and the details of model architecture including hyperparameters are provided.

4. As argued in the paper, I agree that relational reasoning is central to intelligence and since RNs are shown to be able to achieve this reasoning and a result perform better at tasks requiring such reasoning than existing networks, RNs seem to be of significant importance for designing reasoning networks.

Weaknesses:

1. Could authors please analyze and comment on how complicated relations can be handled by RNs. Is it the case that RNs perform well for single hop relations such as ""what is the color of the object closest to the blue object"" which requires reasoning about only one hop relation (distance between blue object and all other objects), but not so well for multiple hop relations such as ""What shape is the small object that is in front of the yellow matte thing and behind the gray sphere?"". From the failure cases in table 1 of supplementary material, it seems that the model has difficulty in answering questions involving multiple hops of relations.

2. L203-204, it is not clear to me what do authors mean by ""we tagged ... support set"". Is this referring to some form of human annotation? If so, could authors please elaborate on what happens at test time?

3. All the datasets experimented with in the paper are synthetic datasets. Could authors please comment on how they expect the RNs to work on real datasets such as the VQA dataset from Antol et al.?

Post-rebuttal comments:

Authors have provided satisfactory response to my question about multi-hop reasoning. However, I would still like to see experiments on real VQA dataset to see how effective RNs are at dealing with the amount of variation real datapoints show (in vision as well as in language). So it would be great if authors could include results on the VQA dataset (Antol et al., ICCV 2015) in camera-ready.","This paper presents the relational network module, which when included as part of a larger network architecture is able to essentially solve the CLEVR VQA task despite its simplicity. The model is also tested over other synthetic tasks involving both vision and language; I hope that in the future the authors can also demonstrate its effectiveness on real-world tasks. While at this point I don't think results on bAbI are particularly informative, especially with the strange way the bAbI task is set up in this paper (what does ""up to 20 support sentences"" mean? was there some rule-based or ML method used to select these support sentences? why not use all sentences as support?), the model is an interesting and effective way to think about relational QA problems, and I hope that the paper is accepted. That said, I have some questions/comments about the paper:

- were functions other than simple summation in Eq.1 experimented with? 
- what is the function of the ""arbitrary coordinate"" in the ""dealing with pixels"" section? How is it ""arbitrary"" if it is supposed to indicate relative position? there is not enough detail to understand how it is implemented, and Figure 2 offers no insight despite being referenced. Is this crucial to making the RN work? If so, it needs to be stated in the paper. 
- is the reason why all support sentences aren't used in bAbI due to computational constraints (since some of the bAbI tasks have large contexts), and is this a potential limitation of the RN?
- the model doesn't make as much sense for NLP tasks where ""objects"" aren't clearly defined, and there is an obvious order to the objects (there is discourse-level information in the sequential ordering of sentences, even in bAbI). Couldn't the model be applied across different units of text (e.g., treat words, phrases, sentences, paragraphs all as objects)? Any thoughts on how this could be implemented in for example bAbI? 
- the CNN used in the CLEVR experiments is very simple compared to prior work, which utilized VGG features. since the CLEVR images are artificially constructed, it strikes me that a simpler CNN is better suited for the problem. What happens if you run the RN on CLEVR but utilize feature maps from the last conv layer of VGG as objects? this would be a more fair comparison than what is described in the paper. "
Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks,Arash Vahdat,https://proceedings.neurips.cc/paper/2017/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html,"This submission employs a conditional random field model to handle the label noise. EM algorithm is designed and empirical experiments verified the effectiveness. However, in the current version, we don't theoretically know why modern deep convolutional neural networks can gain robustness against label noise. Pls provide some theoretical guarantees. Also, the submission contains too many tricks, e.g., conditional random filed, a small set of clean labels, semi-supervised learning, and auxiliary distributions. We don't know which one really helps, which significantly weaken the paper. I also have a minor concern about how to choose the size of the subset of clean labels.
","The paper proposes a method for incorporating data with noisy labels into a discriminative learning framework (assuming some cleanly labeled data is also available), by formulating a CRF to relate the clean and noisy labels. The CRF sits on top of a neural network (e.g., CNN in their experiments), and all parameters (CRF and NN) are trained jointly via an EM approach (utilizing persistent contrastive divergence). Experiments are provided indicating the model outperforms prior methods on leveraging noisy labels to perform well on classification tasks, as well as inferring clean labels for noisily-labeled datapoints.

The technique is generally elegant: the graphical model is intuitive, capturing relationships between the data, clean/noisy labels, and latent ""hidden"" factors that can encode structure in the noisy labels (e.g., presence of a common attribute described by many of the noisy labels), while permitting an analytically tractable factorized form. The regularization approach (pre-training a simpler unconditional generative model on the cleanly labeled data to constrain the variational approximation during the early part of training) is also reasonable and appears effective.

The main downside of the technique is the use of persistent contrastive divergence during training, which (relative to pure SGD style approaches) adds overhead and significant complexity. In particular the need to maintain per-datapoint Markov chains could require a rather involved setup for truly large scale noisy datasets with hundreds of millions to billions of datapoints (such as might be obtained from web-based or user-interaction data).

The experiments are carefully constructed and informative. I was particularly happy to see the ablation study estimating the contribution of different aspects of the model; the finding that dropping the data <-> noisy-label link improves performance makes sense in retrospect, although wasn't something I was expecting. I would have been curious to see an additional experiment using the full COCO dataset as the ""clean"" data, but leveraging additional Flickr data to improve over the baseline (what would be the gain?).

The paper was quite readable, but could benefit from another pass to improve language clarity in a few places. For instance, the sentence beginning on line 30 took me a while to understand.

Overall, I think the paper is a valuable contribution addressing an important topic, so I recommend acceptance.","The authors propose a strategy for dealing with label noise, particularly in the setting where there are multiple target labels per instance, which distinguishes it from much of the previous work in the area. Overall, the paper is well-written and clear to follow. I only had a few minor comments which are listed below:

- In Equation 3: I think it would be interesting to examine whether weighting the relative contributions from the clean and noisy labeled data affects overall performance. I think this would be particularly interesting if the relative amounts of clean vs. noisy labeled data are varied.

- In the results section, could the authors also report the results in terms of p-values after performing a significance test so that it's clearer how strong the relative differences between the various systems are. In particular, at Line 274 the authors mention, ""Note that the 20.0% gain in terms of mAP is very significant"". Please mention p-values and how statistical significance was computed here.

- There have been a few previous works that have examined incorporating neural networks to predict CRF potentials from x, which the authors could consider citing as part of the relevant previous work, e.g.:
* J. Peng, B. Liefeng, J. Xu. ""Conditional neural fields."" Advances in neural information processing systems. 2009.
* T.-M.-T. Do and T. Artieres. ""Neural conditional random fields."" International Conference on Artificial Intelligence and Statistics (AI-STATS). 2010.
* R. Prabhavalkar and E. Fosler-Lussier. ""Backpropagation training for multilayer conditional random field based phone recognition."" IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2010.
* L. Van der Maaten and M. Welling and L. K. Saul. ""Hidden-unit conditional random fields."" International Conference on Artificial Intelligence and Statistics. 2011.

Minor comments and Typographical errors:
1. Line 20: ""Most previous work in this area have focused"" --> ""Most previous work in this area has focused"". Also, please cite relevant works (which appear later in the paper here as well)
2. Line 59-60: ""Misra et al. [4] targets the image multilabeling problem but models"" --> ""Misra et al. [4] target the image multilabeling problem but model""
3.Line 64: ""obtained from web"" --> ""obtained from the web""
4. Line 270: ""mAP"". Please introduce terms before using acronyms for the first time.

"
Stochastic Mirror Descent in Variationally Coherent Optimization Problems,"Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, Peter W. Glynn",https://proceedings.neurips.cc/paper/2017/hash/e6ba70fc093b4ce912d769ede1ceeba8-Abstract.html,"This paper analyzes the stochastic mirror descent (SMD) algorithm for a specific class of non-convex functions the variational coherence (VC) assumption. The authors study mirror descent by establishing a correspondence between the SMD iterates to an ordinary differential equation. They introduce a new divergence measure named Fenchel coupling and exploit its monotonicity property (due to the VC assumption) to demonstrate the SMD algorithm decreases its objective value until it gets close to the optimum.

The paper is clearly written and - omitting certain details -the proof is relatively easy to follow. The paper however has some significant problems in its presentation:
1) Interest for machine learning community: The main contribution of this paper is a proof of asymptotic convergence of SMD for certain non-convex functions. Although this is an interesting result for the optimization community, I’m however not sure this will be of a broad interest for the ML community which is typically more interested in deriving convergence rates. The authors do not give examples of ML problems where this new analysis could be relevant. There is also no experiment results. Can you please try to address this issue in the rebuttal?

2) Related work: The paper does not cover existing work in optimization for non-convex functions, not does it cover prior work relating ODEs to numerical optimization. I think existing proofs of convergence for stochastic gradient descent and proximal methods should be mentioned, e.g. Ghadimi, Saeed, and Guanghui Lan. ""Stochastic first-and zeroth-order methods for nonconvex stochastic programming."" SIAM Journal on Optimization 23.4 (2013): 2341-2368.
For ODEs, see references in [18] as well as
Stochastic Approximation and Recursive Algorithms and Applications
Kushner, Harold, Yin, George
I’m sure the authors are familiar with this line of work so please cite it appropriately.

Fenchel coupling
The paper uses the Fenchel coupling to quantify the distance between primal and dual variables thereby serving as an energy function to prove convergence. Can the authors comment on the difficulty of directly using the discrete primal variable to prove convergence? e.g. using a proof similar to SGD for non-convex functions (Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming, Saeed Ghadimi, Guanghui Lan)
Can the authors comment on whether other types of divergences would be appropriate for the analysis? It seems to me that the monotonicity property is due to the variational coherence assumption (as can be seen in E 3.39 in the appendix), can you confirm this? If so, then I wonder if the role of the Fenchel coupling is really that important for the analysis to hold?

Convergence rate: As mentioned earlier, the ML community would probably be more interested in a convergence rate. Can the authors elaborate on the difficulty of establishing a rate using their ODE technique?

Theorem 3.4: This result was at first puzzling to me. It says that the SMD algorithm oscillates in a region near the optimum. This is known to happen for SGD with constant step-size while using a decreasing step size (+ assumptions mentioned in the paper) will then guarantee convergence to an optimum. Can the authors clarify this?

Analysis:
Page 5 appendix above line 92, should be sum_k \alpha_k = + \infty
Line 174: you are citing a book, could you give a more precise reference?

--------------
Post-rebuttal:

I find the answers in the rebuttal convincing and updated my recommendation score accordingly. Overall, I think the paper would benefit from some clarifications which I think is easily do-able between now and the camera ready version. This includes:
 
1) Importance of the Fenchel coupling. The use of the Fenchel coupling (instead of the commonly used Bregman divergence) is a nice trick to get convergence for the type of non-convex functions considered in the paper. Also the ODE technique used in this paper could potentially push the development of similar techniques for different optimization techniques so this could really be valuable for the ML community. The submitted version of the paper did not explain this well but the answer provided in the rebuttal is convincing. I would recommend developing this aspect even more in the revised version. 

2) Interest regarding the ML community, they should develop this more in the introduction. Adding experimental results as they promised to R3 would be valuable as well 

3) As R2 pointed out, the intuition behind the analysis is not always clear. Given the rather convincing answers in the rebuttal, I think the authors can easily improve this aspect in the revised version. 

4) Minor modifications: Fix the numbering and clarify the use of certain terms whose meaning might differ for different communities, e.g. linear decrease, last iterate.
","Review: Stochastic Mirror Descent for Non-Convex Optimization

This paper discusses applying stochastic mirror descent (SMD) method on a class of problems beyond traditional convex ones. The authors show that global (non-ergodic) convergence of the SMD is possible for a class of problems called Variationally Coherent problems. The authors have also derived convergence rates, as well as analyzed the local behavior of the SMD for this problems. The paper contains lots of derivations. My general impression is that too many results are packed in the paper, and many of them are stated in a rather vague manner so the main points/contributions are somewhat made obscure. I have a few general concerns about the work.

1)	The title of the paper is misleading. The authors should be very specific and say “Stochastic Mirror Descent for Variationally Coherent Optimization Problems”. Claiming what the paper is doing is to perform SMD on general non-convex problems is a stretch. 
2)	In the introduction and perhaps in the abstract, the authors need to be more precise on what they meant by the “last-iterate”. This could be very misleading since the authors are not actually proving that the “last-iterate” of the algorithm to the global/local mean. 
3)	Before Eq. (3.3), the authors mentioned that F(x^*,\Phi_t) will decrease no slower than linearly, however I don’t see where such a “linear” decrease is in Eq. (3.3). 
4)	It is not clear from the main part of the paper what is the intuition behind Step 2 in page 6, which the reviewer believes is one of the key step of the paper. 
5)	Please give concrete examples for the locally variationally coherenet (LVC) problem. Also, from the definition of the LVC problem, the (LVC) property is only required to hold in one region. However, it is not clear why the SMD algorithm will be able to reach such a region. Is it possible that SMD will be stuck at some saddle points that are not LVC? From the proof I can say that this is a local result meaning the Algorithm 1 must start inside LVC region, but from the statement of the Theorem it is not clear (the authors simply say that “X_t be the iterate generated by Algorithm 1”). Please clarify.
6)	I am not convinced by the result sin Section 5. I think the author should remove it, and use the space to give better results, and possibly some more meaningful derivation, for the other parts of the proof. The authors need to make it more clear about the relationship between problems with sharp minimum, and “generic” linear program. What are other problems that contained in this class? And why the solution of such generic linear program is unique (Corollary 5.3)? Also from the proof of  Theorem 5.4, it appears that the authors call Theorem 3.3 (or Theorem 3.4 in the main paper) to argue for X_t converges to x^* also. However, Theorem 3.3 is based on the fact that the problem is globally (not locally) variationally coherent. But it appears, as the author stated, that the problem considered in Section 5 is locally VC (line 284: … that a sharp minimum is locally VC). 
7)	There is a mismatch between the theorem number in the supplementary material and that in the main paper. Therefore in the proof when the authors refers to the theorem number sometimes it is confusing. 
","This paper proved that stochastic mirror descent is non-ergodically but asymptotically convergent on a class of non-convex problems named variationally coherent problems. It also showed that stochastic mirror descent is still convergent with high probability if the problem is only locally variationally coherent.

I have the following concerns about this paper. 

First, all the convergence results in this paper are asymptotic. It would be much better if non-asymptotic convergence result with specific convergence rate can be given. 

Second, in convex optimization, the most common reason to use mirror descent in that it can lead to smaller constant factors in convergence rate. However, this paper failed to justify why mirror descent is still preferred over other common methods like gradient descent in non-convex settings. There is no such theory in this paper due to the lack of convergence rate. 

Third, no empirical experiments were conducted to show the superiority of this method. So it is not clear whether the proposed method is of practical interests. 



(Updated after Rebuttal)
I read the authors' rebuttal and decided to increase my evaluation. "
Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication,"Qian Yu, Mohammad Maddah-Ali, Salman Avestimehr",https://proceedings.neurips.cc/paper/2017/hash/e6c2dc3dee4a51dcec3a876aa2339a78-Abstract.html,"This paper extends the coded distributed computing of matrix multiplication proposed in Lee et al, 2016. The idea is to distribute the matrices by pre-multiplying them with the generator matrices of Reed Solomon code. Since Reed-Solomon codes are MDS codes, any K out of N distributed components are sufficient to recover the desired matrix multiplication.

I am somewhat confused by this paper. Reed-Solomon codes ARE defined to be evaluations of polynomials over finite field (or any fields). So why the authors say that they define something new called polynomial code and then follow up by saying the decoding is just like Reed-Solomon code? Reed Solomon codes are MDS code (in fact, Lee et al also used Reed Solomon codes, they just called it MDS codes).

The key idea of using codes for both matrices in matrix-matrix multiplication has appeared recently in Lee et al, 2017. I think there is a major problem from an application point of view in this paper that was not there for a simple matrix-vector multiplication of Lee et al of 2016. In case of matrix-vector multiplication A times x of Lee et al, 2016, the matrix A was fixed and x was variable. The matrix A was coded and distributed to the workers. x was uncoded. So the encoding is done only one time. It does indeed speed up the distributed computing. On the other hand, in this paper both the matrices A and B that are to be multiplied are being coded up. As a result when two matrices are given to multiply, a central node has to do a lot of polynomial evaluations and then distribute the evaluations among workers. The overhead for the central node seem to be way more than the actual computing task. Notably, this has to be done every time; as opposed to the one-time-encoding of Lee et al, 2016, or Dutta et al, 2016. 

 Also surprising is the lack of experimental results. Most of the related literature is interesting because of the experimental superior performance they exhibit in real systems. This paper does not make any attempt in that direction.  However since theoretically it is not that novel, and given the overhead in evaluating the polynomials, one really needs experimental validation of the methods.


========================
After the rebuttal:
I did not initially see the making of the product MDS as a big deal (i.e. the condition that no two terms in the expression (16) have same exponent of x is not that difficult to satisfy). But on second thought, the simplicity should not be deterrent, given that the previous papers kept this problem open.

There is one other point. What I understand is that each matrix is encoded using polynomials such that the product becomes codewords of Reed_Solomon codes. I do not necessarily agree with the term Polynomial code and the claim that a new class of code has been discovered.

Finally, it is not easy to approximate matrix multiplications over reals with finite field multiplication. I don't know why that is being seen as straight-forward quantization.

","The paper presents optimal error correcting based technique to speed up matrix-matrix multiplication. The authors follow a new line of work that uses ideas from coding theory to alleviate the effect of straggler (eg slower than average) nodes in distributed  machine learning. The main idea of coding in the context of distributed matrix multiplication is the following (a simple 3 worker example): Let A = [A1;A2], and a setup of 3 workers and a master. If a worker is computing A1*B, a second worker is computing A2*B, and a third is computing  a (A1+A2)*B , then from any 2 workers a master node could recover A*B, hence not having to wait for all 3 of the nodes.  This idea can both provably and in practice improve the performance of distributed algorithms for simple linear problems like matrix multiplication, or linear regression.

In this work, the authors present information-theoretically optimal codes for matrix-matrix multiplication, with respect to the number of nodes the master has to wait to recover the full matrix multiply. The novelty in the presented work is that coding is performed on both A and B matrices, and most importantly it is done in such a way that if the master can recover the A*B result from as few nodes as information theoretically possible: eg if each worker can store a 1/r fraction of A and B, then by waiting for only r^2 nodes the master can recover the results.

The novel coding technique used to achieve the above optimal result is called polynomial codes, and comes with several benefits, such as fast decoding complexity due to its Reed-Solomon structure. The authors couple their “achievability” result with a lower bound, essentially showing that polynomial codes are optimal with respect to the number of workers the master needs to wait for before recovering the full matrix multiplication result.

Overall, the paper is well written, and the theoretical contribution is strong and of interest to a new and emerging research direction on the intersection of Coding Theory and Distributed Machine Learning. My only question that is not addressed by the paper, is how well does the presented theoretical scheme work in practice. It would be useful to see some experiments that validate the theoretical results.","Summary:
   The authors consider the problem of efficient distributed multiplication of two matrices when there are many workers and some of them could be straggling. One simple metric the authors consider is optimizing the minimum number of workers that need to finish before they can recover the full matrix product. Suppose matrices that are being multiplied are A and B. Suppose there are N workers. Suppose A and B are divided into m and n parts each respectively.

The previous best bounds suggested that \Theta(sqrt(N)) workers need to finish before recovery. In this work, authors propose a construction called polynomial code by which one needs only mn workers to finish which is independent of N. Further it is also information theoretically optimal. They also show that a similar construction can work for convolution which requires only m+n-1 workers to finish. The authors show that in the multiplication case, because it is optimal with respect to this measure, it is also optimal when one takes into account any computational latency model for delays in the workers and also in terms of communication load.

The key idea is for every worker to evaluate a polynomial in the A-parts with degree profile increasing in steps of 1 and evaluate another polynomial in the B-parts with degree profile increasing in steps of m. The worker multiplies both polynomial evaluations of A-parts and B-parts to master node. It turns out that all mn product parts of A and B, are coefficients of a polynomial of degree mn. If mn workers return evaluations, it is evaluated in mn points which helps the master to recover using polynomial interpolation or Reed Solomon decoding.

Strengths:
    a) The idea of using polynomial evaluations for both A-parts and B-parts is the key idea and the authors propose a simple solution for distributed multiplication and convolution.
   
Weaknesses:

My only main concern with the paper is as follows:

   a) The main weakness is that it lacks some real experiments with real matrices. The theory is for finite fields. Authors mention that in Remark 6 (also elsewhere) that one can quantize reals in finite number of bits and then actually consider the quantized bits as elements of a finite field and perform such multiplication using polynomial codes. However there are hidden issues of dynamic range of real entries involved, size of the field required and presumably polynomial interpolation algorithms become difficult to implement in with large field size owing to quantization. If such things are taken into account, then clearly more discussion is needed. Very classical Reed Solomon codes in practice are implemented with field size of 256 (8 bit symbol). Its not clear if there are issues with scaling the field size beyond this. A practical experiment (with synthetic examples) could have ratified this point. This is because I can only see the most practical use case in ML system to be a problem with reals.





"
From Bayesian Sparsity to Gated Recurrent Nets,"Hao He, Bo Xin, Satoshi Ikehata, David Wipf",https://proceedings.neurips.cc/paper/2017/hash/e6cbc650cd5798a05dfd0f51d14cde5c-Abstract.html,"This paper proposes a recurrent network for sparse estimation inspired on sparse Bayesian learning (SBL). It first shows that a recurrent architecture can implement a variant of SBL, showing through simulations that different quantities have different time dynamics, which motivates leveraging ideas from recurrent-network design to adapt the architecture. This leads to a recurrent network that seems to outperform approaches based on optimization in simulations and two applications. 

Strengths: The idea of learning a recurrent network for sparse estimation has great potential impact. The paper is very well written. The authors motivate their design decisions in detail and report numerical experiments that are quite thorough and indicate that the technique is successful for challenging sparse-decomposition problems. 

Weaknesses: A lot of the details about the implementation of the method and the experiments are deferred to the supplementary material, so that the main paper is a bit vague. I find this understandable due to length limitations. ","The authors explored a connection between Bayesian sparsity and LSTM networks and then extended the work to gated feedback networks. Specifically, the authors first discussed the relationship between sparse Bayesian learning and iterative reweighted l1 regularization, then discussed the relationship with LSTM, and finally extended to gated feedback networks. Experimental results on synthetic and real data sets were reported.
","The paper presents an idea of casting the sparse Bayesian learning as a recurrent neural network structure, which enables the learning of the functions without having to hand-craft the iterations.

The paper is well written and clearly presented. The presented idea is interesting and aligns with some recent works presented in the literature on establishing links between sparse representation and deep neural networks, such as sparse encoder, LISTA, etc.

The experiments on DOA estimation and 3D geometry reconstruction are interesting and show the diversity of the potential applications of this technique.

"
Compatible Reward Inverse Reinforcement Learning,"Alberto Maria Metelli, Matteo Pirotta, Marcello Restelli",https://proceedings.neurips.cc/paper/2017/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html,"This paper extends compatible value function theorem in RL for constructing reward function features in IRL. Specifically, the main contributions of this paper are: (1)  ECO-Q, which constructs the feature space of the (unknown) Q-function by computing the null space of the policy gradient, (2) ECO-R, which constructs the feature space of the reward function by applying Bellman equation to the feature space of the Q-function. 
Overall, the proposed method is sound and has several advantages. It does not require environment dynamics and solving the forward problem. Going further from the existing work considering first-order necessary condition, it also exploits second order criterion which penalizes the deviation from the expert's policy. Finally, the algorithm generates feature functions of reward (or value) automatically, which eliminates the need to hand-crafted feature.
In summary, this paper tackles the continuous control IRL in a systematic way and provides promising results in experiments. I vote for acceptance of this paper.
","This paper proposes an approach for behavioral cloning that constructs a function space for a particular parametric policy model based on the null space of the policy gradient. 

I think a running example (e.g., for discrete MDP) would help explain the approach. I found myself flipping back and forth from the Algorithm (page 6) to the description of each step. I have some lingering confusion about using Eq. (4) to estimate discounted vectors d(s,a) since (4) only computed d(s). I assume a similar estimator is employed for d(s,a).

My main concern with the paper’s approach is that of overfitting and lack of transfer. The main motivation of IRL is that the estimated reward function can be employed in other MDPs (without rewards) that have similar feature representations. Seeking to construct a sufficiently rich feature representation that can drive the policy gradient to zero suggests that there is overfitting to the training data at the expense of generalization abilities. I am skeptical then that reasonable generalization can be expected to modifications of the original MDP (e.g., if the dynamics changes or set of available actions changes) or to other MDPs. What am I missing that allows this approach to get beyond behavioral cloning?

My understanding of the Taxi experiment in particular is that data is generated according to the policy model under line 295 and that the proposed method uses this policy model, while comparison methods are based on different policy models (e.g., BC instead estimates a Boltzmann policy). This seems like an extremely unfair advantage to the proposed approach that could account for all of the advantages demonstrated over BC. What happens when likelihood is maximized for BC under this actual model? Experiments with data generated out of model and generalization to making predictions beyond the source MDP are needed to improve the paper.

In summary, though the method proposed is interesting, additional experiments that mimic “real-world” settings are needed before publication to investigate the benefits of the approach suitability to noisy data and for generalization purposes.","This paper developed a model-free inverse reinforcement learning (IRL) algorithm to recover the reward function that explain the expert demonstrations. Different from earlier work, the method developed in the paper (CR-IRL) automatically constructs the basis functions for the reward function by exploiting the orthogonality property between the policy and value spaces. The method also uses a second-order condition to rank the reward functions. The method is novel and the presentation of the paper is clear.

Here are a few questions to be clarified by the authors:
*The method developed in the paper is a batch method, i.e., it applies method like SVD to the statistics over the entire dataset. This approach may have some scalability issue when considering problems with large amount of data. Is it possible to extend the method so that it is more scalable by using algorithms like stochastic gradient descent? Or it is possible to extend to the streaming data case?
*It is interesting to observe that the CR-IRL outperforms the true reward function in terms of convergence speed. Could the authors discuss more about the reasons?
"
Consistent Robust Regression,"Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, Purushottam Kar",https://proceedings.neurips.cc/paper/2017/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html,"++++++++
Summary:
++++++++
The paper presents a provably consistent, polynomial-time algorithm (that they call consistent robust regression, or CRR) for linear regression with corrupted samples under the oblivious adversary model. The algorithm is a simple iterative hard thresholding (IHT) procedure, and the authors show that this algorithm exhibits linear convergence. The analysis is supported by some representative synthetic numerical results.

++++++++
Strengths:
++++++++
Quality: Somewhat surprisingly, this method appears to be the first consistent procedure for robust linear regression, i.e., the parameter estimation error vanishes as the number of samples tends to infinity. Other estimation procedures (such as LASSO-type methods) seem to only provide solutions with error comparable to the noise level, even in the large sample limit.

Clarity: The paper is nicely written. The authors succinctly state their contributions, put them in the context of existing literature, and provide high level intuition as to what’s happening behind the scenes. 

++++++++++
Weaknesses:
++++++++++ 
Novelty/Significance: The reformulation of the robust regression problem (Eq 6 in the paper) shows that robust regression is reducible to standard k-sparse recovery. Therefore, the proposed CRR algorithm is basically the well-known IHT algorithm (with a modified design matrix), and IHT has been (re)introduced far too many times in the literature to count. 

The proofs in the appendix seem to be correct, but also mostly follow existing approaches for analyzing IHT (see my comment below). Note that the “subset strong convexity” property (or at least a variation of this property) of random Gaussian matrices seems to have appeared before in the sparse recovery literature; see “A Simple Proof that Random Matrices are Democratic” (2009) by Davenport et al.

Couple of questions:
- What is \delta in the statement of Lemma 5? 
- Not entirely clear to me why one would need a 2-stage analysis procedure since the algorithm does not change. Some intuition in the main paper explaining this would be good (and if this two-stage analysis is indeed necessary, then it would add to the novelty of the paper).

+++++++++
Update after authors' response
+++++++++
Thanks for clarifying some of my questions. I took a closer look at the appendix, and indeed the ""fine convergence"" analysis of their method is interesting (and quite different from other similar analyses of IHT-style methods). Therefore, I have raised my score.

","abstract

This   paper  introduces   an  estimator   for  robust   least-squares
regression. First, a comprehensive state of  the art is made to recall
the existing  robust estimators, the computational  techniques used and
the  theoretical   guarantees  available.    Then,  the   estimator  is
introduced,  motivated by  a  simple yet  smart  reformulation of  the
robust least-squares problem. An  iterative hard thresholding approach
is  used  to  adjust  the  model. The  obtained  estimator  is  proved
consistent  and compared  to  some state-of-the-art  competitors in  a
numerical study.

comments

I  enjoyed reading  this  paper.   I found  it  well  written, with  a
comprehensive state of the art. Theory of Robust regression is gently
introduced, with  relevant and not  too technical introduction  of the
statistical guarantees of the  existing estimators.  The contributions
are clearly  stated and, to  my knowledge,  they are important  in the
field  since no  robust estimator  has been  proved consistent  in the
literature.  The hard thresholding approach is well motivated and easy
to follow. Simulations are rather  convincing. I would have been happy
to see, however, some experiments in the high dimensional setting.

As a conclusion, I recommend acceptance for this very good manuscript.","Summary. This submission studies robust linear regression under the settings where there are additive white observation noises, and where \alpha=\Omega(1) fraction of the responses are corrupted by an oblivious adversary. For this problem, it proposes a consistent estimator, called CRR, which is computationally easy because it is based on iterative hard thresholding. It is shown to be consistent (i.e., converging to the true solution in the limit of the number of responses tending to infinity) under certain conditions (Theorem 4).

Quality. In Theorem 4, I did not understand how \epsilon is defined, and consequently, the statement of the theorem itself. Is it defined on the basis of the problem or algorithm setting? Or does the statement of the theorem hold ""for any"" \epsilon>0, or does ""there exist"" \epsilon for which the statement hold? The same also applies to \epsilon and e_0 in Lemma 5.
In Figure 1, the results with d=500, n=2000, \sigma=2, and k=600 appear in the subfigures (a), (b), and (c), but those in the subfigure (a) seem different from those in (b) and (c), which might question the validity of the results presented here.

Clarity. I think that this submission is basically clearly written.

Originality. Although I have not done an extensive survey in the literature, proposing a computationally efficient algorithm, with consistency guarantee in the limit of the number n of responses tending to infinity, for the robust linear regression problem with Gaussian noise as well as with oblivious adversary, when the fraction \alpha of adversarial corruptions is \Omega(1).

Significance. Even though the fraction \alpha of adversarial corruptions for which CRR is shown to be consistent is quite small (\alpha \le 1/20000), it is still nontrivial to give a consistency guarantee even if a finite fraction of responses are corrupted. It would stimulate further studies regarding finer theoretical analysis as well as algorithmic development.

Minor points:
Line 64: incl(u)ding more expensive methods
Line 76: Among those cite(d)
Line 82 and yet guarantee(s) recovery.
Line 83: while allowing (allow)
Lines 61, 102, and 115: The notation E[...]_2 would require explicit definition.
Equation (5): The equation should be terminated by a period rather than a comma.
Line 164: (more fine -> finer) analysis
Line 173: the (Subset) Strong Smoothness Property
Line 246: consequently ensure(s)
Lines 254-264: I guess that all the norms appearing in this part should be the 2-norm, but on some occasions the subscript 2 is missing.
Lines 285-286: Explicitly mention that the Torrent-FC algorithm was used for comparison. Simply writing ""the Torrent algorithm"" might be ambiguous, and also in the figures the authors mentioned it as ""TORRENT-FC"".
Line 293: faster runtimes tha(n) Torrent
Line 295: both Torrent (and) ex-Lasso become(s) intractable with a(n) order
Line 296: Figure(s) 2 (c) and 2(d) show(s)
Line 303: Figure 2 (a -> b) shows
Line 310: we found CRR to (to) be"
Scalable Variational Inference for Dynamical Systems,"Nico S. Gorbach, Stefan Bauer, Joachim M. Buhmann",https://proceedings.neurips.cc/paper/2017/hash/e71e5cd119bbc5797164fb0cd7fd94a4-Abstract.html,"he paper concerns identification of nonlinear ODEs. The method combines some previous ideas (Calderhead, Dondelinger) with a variational approximation. A mean-field approximation over the states and parameters is assumed. Standard mean-field updates are derived. On a Lotka-Volterra model, the method outperforms the state-of-the-art, and on a problem with many hidden states a solution can be found (presumably this is beyond the capabilities of the state of the art).

I confess that I found this paper a challenging read. There is quite a bit of background material, which the authors do seem to cover in some depth, but nonetheless the space constraints make some explanations rather curt. I suggest annotating the equation between 7 and 8 to explain which terms come from where.

The mixture-of-experts approach to combining the GP prior and the ODE seems to be a crucual part of the background material. If I understand correctly, a previous approach (Dondelinger) sampled the resulting distribution. This paper proposes a variational mean-field approximation. Since in general one might expect a variational approximation to perform *worse* than MCMC, the results in section 5.1 are rather surprising: the variational approximation appears to fit better than MCMC-based approaches. Without analysis and comment on this, the experiment feels rather unfinished.

Another confusing aspect of the paper is the treatment of the parameters \theta. eq. 10 implies that the authors are intending to estimate the parameters via MAP, integrating over the states X. However the equations between 12 and 13 imply that a mean-field posterior is assumed for theta.

I applaud the authors for stating up-front the class of problems for which their approximation applies. It seems like good-science top be upfront about this. However, it's not clear to me why the method should be restricted to this case, or how it might be extended to further cases. I feel that this aspect of the paper needs more explanation.

In its current form, I can;t recommend this paper for acceptance at NIPS. I think there's some valuable work in here, but the authors need to expplain their ideas, the background and the results with more clarity.

*** POST DISCUSSION EDIT ***
I have read the author rebuttal and increased my score. The authors have identified parts of their paper to clarify, and having seen also the other reviews I think the paper makes a good contribution. 

Despite some clarity flaws, the paper continues an important line of work and deserves discussion in the NIPS community. ","This paper proposes variational inference for models involving dynamical systems whose dynamics is described by a set of differential equations. 
The paper builds upon previous work on gradient matching and Gaussian processes (GPs); the idea is to interpolate between observations using GPs to obtain approximate solutions that when plugged back into the differential equations lead to some nice properties (derivatives of GPs are GPs) that can be exploited to carry out optimization of differential equation parameters without the need to solve the system explicitly.
This work extends previous work in this domain by proposing variational inference for the states and optimization of the lower bound to estimate differential equation parameters.

While the paper reports some interesting improvements with respect to previous gradient matching approaches and the paper is interesting and well written, I think that the lack of novelty is a major issue. 
Gradient matching has been extensively studied in various works referenced in the paper, so there is not much novelty from the modeling perspective.
Also, the proposed inference framework is a rather straightforward application of mean field variational inference, so again there is no particular innovation apart from the specialization of this particular inference technique to this model. 

** After rebuttal

In light of the rebuttal and the comments from the other reviewers, I've decided to increase the score to 6.","This paper proposes a mean-field method for joint state and parameter inference of ODE systems. This is a problem that continues to receive attention because of its importance in many fields (since ODEs are used so widely) and the limitations of existing gradient-based methods. The solution described in the paper is said to outperform other approaches, be more applicable, and scale better. 

The paper lays out its contribution clearly in the introduction. It does a good job of discussing the merits and drawbacks of previous approaches, and explaining how it fits in this context. The proposed methodology appears sound (although it might be useful to explain some of the derivations in a little more detail) and the results indicate that it performs much better than the state of the art. The main advantage of the new method is its execution time, which is shown to be orders of magnitude faster than methods with comparable (or worse) accuracy. In particular, the scalability analysis appears very promising for the applicability of this method to larger models.

Overall, I found this to be a strong paper. My comments mainly concern the presentation, and most are minor.
- The biggest comment has to do with the derivations of the evidence lower bounds and Equation (16). I believe it would be useful to have some more details there, such as explicitly pointing out which properties they rely on. It is also not fully clear to me how the E-M steps of Algorithm 1 make use of the previous derivations (although that may be because I have limited experience in this field). There appears to be some space available for expansion but, if necessary, I suppose that sections 2 and 3 could be combined.
- Besides that, there is a statement that appears contradictory. Lines 82-86 describe how parameters and state are alternately sampled in Calderhead et al. and Dondelinger et al. However, lines 29-30 make it seem like these approaches cannot infer both state and parameters. I would suggest either to rephrase (perhaps remove the ""simultaneously""?) or explain why this is a problem.
- As far as the experiments are concerned, I think it would be interesting to run the Lotka-Volterra example of Section 5.1 for longer, to confirm that the inferred state displays periodicity (perhaps comparing with the other methods as well). However, I believe the results do a good job of supporting the authors' claims.


Minor comments:
--------------
l2: grid free -> grid-free (also later in the text)
l52: the phrasing makes it sound like the error has zero mean and zero variance; perhaps rephrase to clarify that the variance can be generally elliptical / different for each state?
Equations 3 - 8: There is a small change in the notation for normal distributions between Eqs 3-4 and 5-8. For the sake of clarity, I think it is worth making it consistent, e.g. by changing the rhs of (3) to N(x_k | 0, C) instead of N(0,C) (or the other way around).
l71: What is the difference between C' and 'C ?
Equation 9: no upper bound on the sum
l99: contains -> contain
l99: ""arbitrary large"" makes me think that a sum term can include repetitions of an x_j (to effectively have power terms, as you would have in mass-action kinetics with stoichiometry coefficients > 1), but it is not fully clear. Is this allowed? (in other words, is M_{ki} a subset of {1,...,K}, or a multiset of its elements?)
l139: what does ""number of states"" refer to here? the number of terms in the sum in Eq 9?
Figure 1: in the centre-right plot, it seems like the estimate of \theta_4 is missing for the spline method
l155: Predator is x_2, prey is x_1
l163: move the [ after the e.g. : ""used in e.g. [Calderhead et al. ...""
l168: usining -> using
l171: I would add commas around ""at roughly 100 times the runtime"" to make the sentence read easier
l175: partial -> partially
l187: (3) -> (Figure 3)
l195-196: Figure 2 does not show the noise changing; I think having two references would be more helpful: ""[...] if state S is unobserved (Figure 2) and [...] than our approach (Figure 3).""
l198: perhaps ""error"" is better than ""bias"" here? Unless it is known how exactly the misspecification biases the estimation?
l202: Lorentz -> Lorenz (and later)
l207: stochstic -> stochastic
l208: number states -> number of states
l209: Due the dimensionality -> Due to the dimensionality,
l212: equally space -> equally spaced
l214: correct inferred -> correctly inferred
Figure 4: In the last two lines of the caption: wheras -> whereas; blue -> black (at least that's how it looks to me); is corresponding to -> corresponds to
l219: goldstandard -> gold standard
l253: modemodel -> modelling"
Learning multiple visual domains with residual adapters,"Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi",https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html,"Paper address the problem of learning a joint recognition model across variety of domains and tasks. It develops a tunable deep network architecture that, by means of adapter residual modules, can be steered for a variety of visual domains. Methods archives good performance across variety of domains (10 in total) while having high degree of parameter sharing. Paper also introduces a corresponding challenge benchmark - Decathlon Challenge. The benchmark evaluates the ability of a representation to be applied across 10 very different visual domains. 

Generally the paper is very well written and presents an interesting approach and insights. I particularly applaud drawing references to related works and putting the approach in true context. Additional comments are below. 

> References

Generally the references are good and adequate. The following references are thematically related and should probably be added:

Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification,
Tong Xiao, Hongsheng Li, Wanli, Ouyang, Xiaogang Wang,
CVPR, 2016.

Expanding Object Detector's Horizon: Incremental Learning Framework for Object Detection in Videos, 
Alina Kuznetsova, Sung Ju Hwang, Bodo Rosenhahn, and Leonid Sigal, 
CVPR, 2015.

> Clarifications

Lines 201-202 talk about shrinking regularization. I think this is very interesting and could be a major benefit of the model. However, it is unclear if this is actually tested in practice in experiments. Can you please clarify.

Also, authors claim that the proposed model ensures that performance does not degrade in the original domain. This does not actually appears to be the case (or supported experimentally). For example, performance on ImageNet in Table 1 goes from 59.9 to 59.2. This should be discussed and explained.

> Experiments

In light of the comments above, Table 2 should include the proposed model (or models), e.g., Res. adapt. decay and Res. adapt. finetune all.

Finally, explanation of Scratch+ is unclear and should be clarified in Lines 303-305. I read the sentence multiple times and still unsure what was actually done. 



","The paper presents a CNN architecture able to learn over different domains
with a limited amount of parameters and without the need to forget previous
knowledge. An original ResNet is modified by adding multiple modules composed 
by residual connections and batch normalization layers: the paper shows that
by only learning the parameters of these modules the network is able to 
produce good performance on different domains.

+ the paper is easy to read, the objectives as well defined and the method is clearly explained
+ besides the method, the author introduce a visual decathlon challenge that
may be used as reference for other domain generic methods in the future

few points that need some discussion:

- on page 4, lines 148-155 the paper explains that the idea proposed here is inspired by a 
previous work where a low rank filter decomposition was used instead of the introduction of
separated modules. Despite this practical difference it sounds like it would be possible to
test this method on the decathlon challenge, but it is not used here as reference in the expers.

- the method has been designed to cope with domains where the samples do not only differ in 
terms of style but also in their class set. Still nothing avoids to test the method in the 
easier domain adaptation setting with shared classes across domains. What would happen there?
Would it produce better or worse results than sota deep DA methods?

- as far as I'm concerned the FT procedure can be done in different ways depending on the 
number of layers that are kept fixed (frozen) and the number of layers for which the 
parameters can be updated. Thus I think FT is not necessarily a 10x process in terms of 
number of parameters with respect to the original ResNet. What would happen by updating
less parameters?

- In line 337 of page 8 the sota result on CIFAR is mentioned. Why no of the other databases
sota results are provided as reference? It could be useful to understand how the method compares
with them.
","This paper explores how to learn feature extractors that support learning in several different image domains.   The general idea is to learn a residual adapter module and use it to parameterize the standard residual network architecture.  there is a high-degree of parameter sharing between domains.  One particular advantage is to enable the learning of new domains sequentially without forgetting the previous learned results.

I find the 'avoid forgetting' part of the residual network model to be interesting, but this part is too short and the discussions at a high level, that I cannot figure out how much the 'forgetting' problem is addressed by this model.  For example, can there be some theoretical guarantee on how much forgetting is prevented?  Can there be some comparison on how much more is gained using the residual network model?

The experiments in Section 5 are detailed and well done, but it is a pity that the authors did not give a sequential learning setting similar to lifelong learning, where the learning curve is shown as a function of domains.   Some permutation that can result in better gain in learning performance can be discussed as well."
Incorporating Side Information by Adaptive Convolution,"Di Kang, Debarun Dhar, Antoni Chan",https://proceedings.neurips.cc/paper/2017/hash/e7e23670481ac78b3c4122a99ba60573-Abstract.html,"This paper proposes adapting CNNs to specific subtasks through the use of side networks that generate filters for the primary network.  In particular, a side network takes as input a small number of variables (for example, camera angle), and produces as output filter parameters that the main network will use when processing an input image to solve a particular task (for example, crowd counting).  The paper demonstrates use of such adaptive side layers to parametrize multiple CNN layers.

Strengths of this paper are mixed.  In terms of novelty, one might view the proposed design as a specific instantiation of hypernetworks [17]; the paper acknowledges this connection. Experiments seem to be well-designed and actually establish the utility of the proposed side network design in terms of both performance and efficiency.  In particular, the side network design outperforms baseline methods which only append the side knowledge to the input.  The side network designs are also able to achieve such performance with fewer parameters than the baseline networks.  Experiments demonstrate utility across multiple tasks.
","The paper proposes a new method for incorporating side information into Convolutional Neural Networks (CNNs) through adaptive convolution.  The idea is to provide an adaptive convolutional layer with filter manifold network (FMN) that uses the auxiliary input to generatethe filter weights that are convolved with the input maps.  The paper performs experiments on several tasks where data has undergone manipulation and show that the adaptive convolution provides better effectiveness that baseline CNN or my incorporating side information as an extra channel.  

The strength of the paper is that the work is well motivated by an application of people counting in video, where the cameras provide a wide range of perspectives, viewpoints and scales in capturing the real world scenes.  The paper shows that the adaptive convolution method improves accuracy by incorporating side information that modifies the filter weights.

The weakness of the paper is that other experiments not sufficiently grounded in real data with real corruption requiring the incorporation of side information.  Instead, the authors have manipulated data sets by adding salt and pepper noise or by blurring.  On one hand these manipulations are too simple, and they do not reflect the extent to which images may be corrupted in real applications.

The paper will be improved by extending the experiments to cover genuine scenarios with real data.  Since the problem is sufficiently motivation by needs in practice, it should be possible to use real data and scenarios that allow side information.","Summary of the Paper:
This work proposes to use adaptive convolutions (also called 'cross convolutions') to incorporate side information (e.g., camera angle) into CNN architectures for vision tasks (e.g., crowd counting). The filter weights in each adaptive convolution layer are predicted using a separate neural network (one network for each set of filter weights) with is a multi-layer perceptron. This network is referred to as 'Filter Manifold Network' which takes the auxiliary side information as input and predicts the filter weights. Experiments on three vision tasks of crowd counting, digit recognition and image deconvolution indicate the potential of the proposed technique for incorporating auxiliary information. In addition, this paper contributes a new dataset for crowd counting with different camera heights and angles.


Paper Strengths:
- A simple yet working technique for incorporating auxiliary information into CNNs with adaptive convolutions.
- With this technique, a single network is sufficient for different dataset settings (for instance, denoising with different blur kernel parameters) and authors demonstrated generalization capability of the proposed technique to interpolate between auxiliary information.
- Experiments on 3 different vision tasks with good results compared to similar CNN architectures with non-adaptive convolutions.
- A new dataset on crowd counting.


Weaknesses:
- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?
- From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.
- The improvements on image deconvolution is minimal with CNN-X working better than ACNN when all the dataset is considered. This shows that the adaptive convolutions are not universally applicable when the side information is available. Also, there are no comparisons with state-of-the-art network architectures for digit recognition and image deconvolution.


Suggestions:
- It would be good to move some visual results from supplementary to the main paper. In the main paper, there is almost no visual results on crowd density estimation which forms the main experiment of the paper. At present, there are 3 different figures for illustrating the proposed network architecture. Probably, authors can condense it to two and make use of that space for some visual results.
- It would be great if authors can address some of the above weaknesses in the revision to make this a good paper.


Review Summary:
- Despite some drawbacks in terms of experimental analysis and the general applicability of the proposed technique, the paper has several experiments and insights that would be interesting to the community.


------------------
After the Rebuttal:
------------------

My concern with this paper is insufficient analysis of 'filter manifold network' architecture and the placement of adaptive convolutions in a given CNN. Authors partially addressed these points in their rebuttal while promising to add the discussion into a revised version and deferring some other parts to future work. 

With the expectation that authors would revise the paper and also since other reviewers are fairly positive about this work, I recommend this paper for acceptance."
Hierarchical Clustering Beyond the Worst-Case,"Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn",https://proceedings.neurips.cc/paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html,"This paper studies the problem of hierarchical clustering in a beyond-the-worst-case setting, where the data is a random graph generated by a Hierarchical Stochastic Block Model (HSBM). It proposes an SVD+linkage algorithm and proves that in certain regimes it returns a solution that is a constant approximation to the cost function of hierarchical clustering proposed by Dasgupta. It also considers an algorithm that combines the SDP relaxation approach and recursive sparsest cut approach in previous works to get a constant approximation in larger regimes. 

Roughly speaking, the HSBM model considered assumes some underlying tree T* over k leaves (each leaf corresponding to a bottom cluster containing a certain number of vertices). Each node in the tree has some weight in (0,1) and a node's weight should not be larger than its descendant's. Then generate a graph on the vertices where an edge (i,j) is added with probability equal to the weight of their lowest common ancestor (for those inside the bottom cluster, use the weight of the leaf). The algorithm is quite simple: first use SVD to get spectral embeddings of the vertices, and use single linkage on the Euclidean distances between the embeddings to get k clusters (aiming to recover the bottom clusters); then use single linkage to merge the k clusters but using the edge weights. The analysis uses the guarantee of the spectral embeddings and the assumptions that the bottom clusters are large enough to show that the bottom clusters are recovered. Then the algorithm will output a tree roughly corresponding to the underlying tree T*, which will have cost close to that of T* which is optimal. 
The result for the second algorithm follows from the existing results for the SDP relaxations in the random and semi-random settings, and those for the recursive sparsest cut approach to building the tree.
The authors also provided an empirical evaluation on synthetic and real-world data. The results provide quite positive support for using the algorithm.

The presentation is clear, though the notations are a bit heavy. Related works are properly cited and discussed. 

Overall it provides nice algorithms and analysis for hierarchical clustering assuming the data is from a reasonable random model. One of the algorithms is actually quite intuitive and has good empirical performance, thus provides a probable explanation for why simple hierarchical clustering heuristics work well in practice.

minor:
-- Line 172: very similar -> is very similar
-- Line 191: \tilde{T}_k should be \tilde{T}?
-- Line 230: delete ""with""","Summary of paper:
The paper studies hierarchical clustering in the recent framework of [1], which provides a formal  cost function for the problem. [4] proposed a hierarchical stochastic block model (HSBM) for this problem, and showed that under this model, one can obtain a constant factor approximation of Dasgupta’s cost function [1]. 
The present work generalises the HSBM model slightly, where they do not restrict the number of clusters to be fixed, and hence, the cluster sizes may be sub-linear in the data size. The authors prove guarantees of a proposed algorithm under this model. They further extend their analysis to a semi-random setting where an adversary may remove some inter-cluster edges.
They also perform experiments on some benchmark datasets to show the performance of the method.

Quality:
The paper is of decent quality. While the paper is theoretically sound, the experiment section is not very convincing. Why would one consider 5 flat cluster datasets and only one hierarchical dataset in a hierarchical clustering paper? Even for the last case, only a subset of 6 classes are used. Further, 3 out of 5 methods in comparison are variations of Linkage++, the other two being two standard agglomerative techniques. What about divisive algorithms?

Clarity:
Most parts of the paper can be followed, but in some places, the authors compressed the material a lot and one needs to refer to cited papers for intuitive understanding (for instance, Definition 2.1). There are minor typos throughout the paper, example in lines 41, 114, 126, 179, 201, 342. 

Originality:
The paper derives a considerable portion from [4]. Though some extensions are claimed, I wonder if there is any technical challenges in proving the additional results (I did not see the details in supplementary).
For example, is there any difference between Linkage++ and the method in [4]? Apparently I did not find any significant difference.
The analysis on HSBM may allow growing k, but is there truly a challenge in doing so. At least for first step of algorithm, achieving n_min > \sqrt{n}  does not seem a big hurdle.
Analysis on the semi-random setting is new, but I wonder if there are any challenges in this scenario. Or does it immediately follow from combining HSBM analysis with standard techniques of semi-random graph partitioning?

Significance:
In general, hierarchical clustering in the framework of [1] has considerable significance, and [4] is also a quite interesting paper. However, I consider the present paper a minor extension of [4], with the only possible interesting aspect being the result in the semi-random setting. 

---------------------------
Update after rebuttal:
I thank the authors for clarifying the technical challenges with proving the new results. Hence, I have now increased my score (though I am still concerned about the algorithmic and experimental contributions).
Regarding the experiments, I do agree that there are not many well-defined benchmark out there. The papers that I know are also from early 2000s, but the authors may want to check the datasets used here:
1. Zhao, Karypis. Hierarchical Clustering Algorithms for Document Datasets. Data Mining and Knowledge Discovery, 2005
2. Alizadeh et al. Distinct types of diffuse large B-cell lymphoma identified by gene expression profiling. Nature, 2000
Apart from these, UCI repository has Glass and Reuters that I know have been used for hierarchical clustering. Perhaps, there are also more recent document / gene expression datasets in recent data mining papers.","This paper studies the hierarchical clustering cost function recently introduced by Dasgupta in a specific setting of both a general Hierarchical Stochastic Block Model (HSBM) and a semi-random extension. To achieve this they introduce an algorithm Linkage++ where first a single-linkage of an SVD-projection of the data is used for a first partitioning, and then these are clustered by single-linkage using edge-weights of the original data. Guarantees on the almost optimal cost of the result are proved. Empirical evaluations are performed both on the synthetic HSBM as well as on real-world data.

The paper answers an important question of how well the cost can be approximated in a natural setting by a more-or-less standard algorithm. The benchmarks show that small costs can be achieved also for real-world data sets. I appreciate that the authors conclude that one should consider an ensemble of different algorithms to obtain decent approximations of the cost.

The paper is written in a nice, clean, and understandable way.

(As an observation: it is remarkable that single-linkage yields uniformly the best classification error for the real-world data sets in these experiments.)

Questions
---------

* For the synthetic data sets: can the true optimal cost be estimated?

* Were the real-world data sets centered before calculating the cosine similarity?

* How many singular values were considered?

* Do I understand correctly that the difference in the experiments between ""single-linkage"" and ""density-based linkage"" algorithms is that the former uses say euclidean distance whereas the latter uses the cosine similarity?

* Can you say something concretely on the run times of your experiments?

Minor points
------------

* Definition 1.1: To me it is not clear what is meant by ""MINIMAL ultrametric"". If it is important it should be described quickly.

* lines 318-321: log() seems to mean the binary logarithm, maybe write log_2() ?

* line 201: ""The expected graph _as the_ is the..."" delete ""as the"""
"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference","Geoffrey Roeder, Yuhuai Wu, David K. Duvenaud",https://proceedings.neurips.cc/paper/2017/hash/e91068fff3d7fa1594dfdf3b4308433a-Abstract.html,"This paper proposes a new gradient estimator for Variational Bayes with zero variance when the variational distribution is exactly equal to the true distribution of latent variables. The paper is written well and clear. I think it is acceptable after some minor revisions.

It will be nicer if the case where the variational distribution is different from the true distribution p(z|x) is analyzed in addition. In that case, the expectation of score function is not zero anymore and therefore the proposed estimator introduces some bias. How adversarial can be the effect of this bias? 

Also, for the mixture ELBO in line 188, shouldn't there be a variational distribution over \pi_c? I don't see any explicit definition of z_c in the manuscript. How do you choose them in the variational mixture and how does that choice affects reparameterization?","This submission proposes a method that often reduces the variance of stochastic gradients for variational inference. With the reparameterization trick, the score function is effectively included in the stochastic gradients. Though it may serve as a control variate, more often it increases the variance of the stochastic gradients. Excluding the score function does not bias the stochastic gradients, and often leads to better empirical results.

The topic the paper addresses is important: the reparameterization trick has greatly expanded the applicability of variational inference. Yet there has been little systematic evaluation of the different ways of computing stochastic gradients with the reparametrization trick. 

In addition to addressing an important topic, this paper presents somewhat surprising results: 1) a Monte Carlo approximation to the entropy term may lead to lower variance overall than using the exact entropy term, 2) the score function adds variance unnecessarily to the stochastic gradients.

The paper does not require novel mathematical arguments to make its case, or even much math at all. The empirical results are appreciated, but the improvements appear modest. Overall, the paper is well written.

I presume equations 5--7 are all correct, but it would help to have them clarified. Why are we differentiating just the integrand of (1), when the measure of the integrand itself involves the variational parameters? How does equality 7 follow? If it’s just the chain rule, are there notational changes that would make that more clear? Or some additional text to explain it? Notational challenges may be partly why no one has noticed that the score function is adding unnecessary variance to the gradients until now.","The authors analyze the functional form of the reparameterization trick for obtaining the gradient of the ELBO, noting that one term depends on the gradient wrt. the sample z, which they call the path derivative, and the other term does not depend on the gradient wrt sample z, which they (and others) call the score function. The score function has zero expectation, but is not necessarily zero at particular samples of z. The authors propose to delete the score function term from the gradient, and show can be done with a one-line modification in popular autodiff software. They claim that this can reduce the variance—in particular, as the variational posterior approaches the true posterior, the variance in the stochastic gradient approaches zero. They also extent this idea nontrivially to more complicated models.

While this is a useful and easy-to-use trick, particularly in the way it is implemented in autodiff software, this does not appear to be a sufficiently novel contribution. The results about the score function are well-known, and eliminating or rescaling the score function seems like a natural experiment to do.

The discussion of control variates in lines 124-46 is confusing and seemingly contradictory. From my understanding, setting the control variate scale c = 1 corresponds to including the score function term, i.e. using the ordinary ELBO gradient. Their method proposes to not use a control variable, i.e. set c = 0. But lines 140-6 imply that they got better experimental results setting c = 1, i.e. the ordinary ELBO gradient, than with c = 0, i.e. the ELBO gradient without the score function term. This contradicts their experimental results. Perhaps I have misunderstood this section, but I’ve read it several times and still can’t come to a sensible interpretation.

The experimental results show only marginal improvements, though they are essentially free."
"Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos","Gerasimos Palaiopanos, Ioannis Panageas, Georgios Piliouras",https://proceedings.neurips.cc/paper/2017/hash/e93028bdc1aacdfb3687181f2031765d-Abstract.html,"The paper studies the dynamics associated with the multiplicative weights meta algorithm in the context of congestion games. The main question motivating this study is whether applying MWU with constant learning rate yields convergence to exact Nash equilibria. 

By connecting the dynamics to Baum-Eagon inequality, the authors prove a positive result for the standard MWU algorithm. Interestingly, it is shown that when a different variant of the algorithm is applied with constant step size, it leads to limit cycles or even chaotic behavior. 

The applicability of this result is questionable; indeed, applying MWU using the doubling trick yields optimal rates and does not incur any overhead. Furthermore, only asymptotic bounds are obtained.

However, I still find the contribution interesting and even enlightening. The proofs are elegant and possible connections to other fields such as computational complexity and distributed computing make the contribution even more appealing. ","The paper revisits the convergence result of multiplicative weighted update (MWU) in a congestion game, due to Kleinberg, Piliouras, and Tardos (STOC 2009), and establishes a connection between MWU and Baum-Welch algorithm in a neat and elegant style. By showing the monotonicity of the potential function in a congestion game, the authors prove that any MWU with linear updating rule converges to the set of fixed points, which is a superset of all the Nash equilibria of the congestion game. The Baum-Eagon inequality offers a new interpretation of the dynamics of the linear updating rule in MWU and their results in congestion games are quite general, and so are the conditions on initialization and isolated Nash equilibrium in a congestion game with finite set of agents and pure strategies.

The results in this paper hold for any congestion game irrespective of the topology of the strategy sets by the nature of their game, however,  one should emphasize the assumption that, individual earning rates $\epsilon_i$ are bounded above, in order for Baum-Eagon inequality to work in their context. An elaborate analysis on the upper bounds would be nicer, since for any $i$, this paper requires that
$$\frac{1}{\epsilon_i} >  \sup_{i,\mathbf{p}\in \Delta,\gamma\in S_i} \{c_{i\gamma}\}\geq \max_{e} c_e(|N|),$$ which can be huge when the number of agents is large enough.

Although Theorem 3.7 holds, the proof presented by the authors is flawed. To prove any point $\mathbf{y} \in \Omega$ is a fixed point of $\zeta$, one could guess from the argument in the paper that the authors intend to show that $\Phi(\mathbf{y}) = \Phi(\zeta(\mathbf{y}))$, thus by monotonicity of $\Phi$, $\zeta(\mathbf{y})=\mathbf{y}$. However, they applied the notation $\mathbf{y}(t)$, even though $\mathbf{y}$ is, according to their definition, the limit point of an orbit $\mathbf{p}(t)$ as $t$ goes to infinity. The subsequent argument is also hard to follow. 

A easy way to prove theorem 3.7 is by contradiction. Suppose that $\mathbf{y}$ is not a fixed point, then by definition (line221), there exists some $i,\gamma$ s.t., $y_{i\gamma} > 0$ and $c_{i\gamma}\neq \hat{c}_i$.  If $c_{i\gamma} > \hat{c}_i$, then by convergence and continuity of the mapping, $\exists t_0$ and $\epsilon_0 > 0$, s.t., $\forall t > t_0$, $p_{i\gamma}(t) \geq \epsilon_0 > 0$, and $c_{i\gamma}(t) \geq  \hat{c}_i(t)+ \epsilon_0.$ Then it's not hard to see that
$$p_{i\gamma}(t+1) = p_{i\gamma}(t) \frac{\frac{1}{\epsilon_i} - c_{i\gamma}(t) }{\frac{1}{\epsilon_i} - \hat{c}_{i}(t)} \leq  p_{i\gamma}(t) (1- \epsilon_0 \epsilon_i),$$ which yields $\lim_{t\to \infty} p_{i\gamma}(t) = y_{i\gamma}=0$, contradicting with $y_{i\gamma} > 0$.

If $c_{i\gamma} < \hat{c}_i$, then by convergence and continuity of the mapping, $\exists t_0$ and $\epsilon_0 > 0$, s.t., $\forall t > t_0$, $p_{i\gamma}(t) \geq \epsilon_0 > 0$, and $c_{i\gamma}(t) + \epsilon_0 <  \hat{c}_i(t).$ Then it's not hard to see that $$p_{i\gamma}(t+1) = p_{i\gamma}(t) \frac{\frac{1}{\epsilon_i} - c_{i\gamma}(t) }{\frac{1}{\epsilon_i} - \hat{c}_{i}(t)} \geq  p_{i\gamma}(t) (1+ \epsilon_0)$$. Then $\lim_{t\to \infty} p_{i\gamma}(t)$ doesn't lie in the simplex. 

In Remark 3.5, the authors claim that Lemma 3.3 and thus Theorem 3.4 and 3.1 hold in the extension of weighted potential games. However, a underlying step in the proof of Lemma 3.3 might not hold without adding extra conditions: the non-negativity of coefficients in $Q(p)$ in a weighted potential game. Similar to the proof of Lemma 3.3, in the weighted case, $\frac{\partial Q}{\partial p_{i\gamma}} = \frac{1}{\epsilon_i} - \frac{c_{i\gamma}}{w_i}$. Whether the RHS is non-negative is unknown simply by the definition of $\epsilon_i$ nor the fact that $w_i \in [0,1]$.

On Line 236, the constant in $Q(p)$ should rather be $Q(p)=const -\Phi$ and $const = \sum_{i\in N} 1/{\epsilon_i} + (|N|-1) 1/\beta$.
","This paper studies multiplicative weight update algorithm in congestion games. The typical multiplicative weights algorithm updates the probability by an action by 1 - \eps C(\gamma) (linear update) or (1 - \eps)^C(\gamma) (exponential update). In either case, generally the parameter \eps is also updated as the algorithm progresses. In this paper, the authors study what happens when one does not update the parameter \eps. 

In congestion games, the authors show that the linear update always converges. On the other hand, they construct examples of congestion games where for any values of \eps between (0, 1) the exponential update does not converge but instead exhibits something called Li-Yorke chaos. 

The paper is very well written and uses novel techniques. 

Minor nits:
* Provide a  reference for ""it is well known that to achieve sub-linear regret, the learning rate eps must be decreasing as time progresses.""  Also how does sub-linear regret relate to convergence?
* Should Corollary 3.2 be called corollary - I am assuming it generalizes Baum-Eagon but follows using the same/similar proof. "
QMDP-Net: Deep Learning for Planning under Partial Observability,"Peter Karkus, David Hsu, Wee Sun Lee",https://proceedings.neurips.cc/paper/2017/hash/e9412ee564384b987d086df32d4ce6b7-Abstract.html,"The paper introduces a policy network architecture, the QMDP-net, that explicitly contains a separate filter and planning module, where the filter is needed for environments with partial observability. The QMDP-net is tested on multiple partially observable problems, where it has good performance on all of them.

The paper is an interesting addition to the literature on training policy networks, where the network structure is designed in a way to introduce a ""prior"" on the type of computations the network is expected to need to fulfil its task. The partially observable markov decision process is a problem type that has a clear significance also in practical cases.

The authors make a good effort at comparing their model to alternative architectures, such as a generic RNN and CNN+LSTM. I think the paper could have been even stronger if the authors had found some problem that would already have been tackled by others, and compare their model to that. I am not familiar enough with the literature to know whether a good such set exists, though.

In general the paper is well written and seems methodologically sound. As a minor comment, the abstract seems to be in a smaller font (possibly due to the italicised font) than the rest of the text, I would consider revising that.","The paper proposes a novel policy network architecture for partially observable environments. The network includes a filtering module and a planning module. The filtering module mimics computation of the current belief of the agent given its previous belief, the last action and the last observation. The model of the environment and the observation function are replaced with trainable neural modules. The planning module runs value iteration for an MDP, whose transition function and reward function are also trainable neural modules. The filtering model and the planning module are stacked to resemble the QMDP algorithm for solving POMDP. All the modules are trained jointly according to an imitation learning criterion, i.e. the whole network is trained to predict the next action of an expert given the past observations and actions. The experimental evaluation shows better generalization to new tasks.

The approach can be as an extension of Value Iteration Networks (VIN) to partially observable environments by means of adding the filtering network. The paper is in general well written. I did not understand though why the internal model of the network does not use the same observation and action spaces as the real model of the world. Sentences like ""While the belief image is defined in M, action inputs are defined in \hat{M}"" were therefore very hard to understand. The choice of notation f^t_T is quite confusing. 

The experimental evaluation seems correct. I would suggest a convolutional LSTM as another baseline for the grid world. A fully-connected LSTM is obviously worse eqipped than the proposed model to handle their local structure. I am somewhat suspicious if the fact that expert trajectories came from QMDP could give QMDP-net an unfair advantage, would be good if the paper clearly argued why it is not the case. 

One limitation is this line of research (including VIN) is that it assumes discrete state spaces and the amount of computation is proportional to the number of states. As far as I can I see this makes this approach not applicable to real-world problems with high-dimensional state spaces.

Overall, the paper proposes a novel and elegant approach to build policy networks and validates it experimentaly. I recommend to accept the paper.


","This paper proposes a deep learning system for planning in POMDPs. The system attempts to combine model-based and model-free methods in an end-to-end differentiable recurrent policy network.

The paper suffers from circuitous writing. Inexplicably, the authors are not upfront about the problem addressed. For example, they don't reveal until the bottom of the third page that this work primarily addresses imitation learning and not the general planning setting. Notation is also introduced a bit clumsily. 

The authors devise an architecture based on the QMDP algorithm. The architecture consists of one component which is meant to calculate an agents belief over the current state at any step and another that given these beliefs generates actions. The ideas are interesting but they could be presented much more clearly. I would have much preffered for the authors to just enumerate what precisely takes place at each step of training. A model with this many non-standard components requires clearer explanation.

I suspect the authors are doing some interesting work here but cannot accept the paper in its current form. I suggest that the authors give this paper a full rewrite and articulate precisely what they do, sufficiently clearly that any expert reader could re-implement the work from scratch. "
Deep Supervised Discrete Hashing,"Qi Li, Zhenan Sun, Ran He, Tieniu Tan",https://proceedings.neurips.cc/paper/2017/hash/e94f63f579e05cb49c05c2d050ead9c0-Abstract.html,"The paper proposes a deep hashing approach that assumes a setting where there is a training set with class annotations. The model parameters are learned using a loss based on pairwise similarities and class separability under a linear model. The main novelty is in handling the binary embedding directly during learning, without relaxation, using alternating minimization. Results seem better than all the many baselines presented, across two different datasets. There is also a comprehensive analysis of different options. The writing is clear.

I'm not an expert in this area, so my main concern is novelty. It seems like the proposed optimization has been proposed before ([9, 17, 21]), so i assume the difference is that in this paper is that there's also the linear classification component of the loss. The overall approach seems also quite complex and potentially very slow to train (how slow?). ","The paper proposes a supervised hashing algorithm based on a neural network. This network aims to output the binary codes while optimizing the loss for the classification error. To jointly optimize two losses, the proposed method adopts an alternating strategy to minimize the objectives. Experiments on two datasets show good performance compared to other hashing approaches, including the ones use deep learning frameworks.

Pros:
The paper is written well and easy to follow. Generally, the idea is well-motivated, where the proposed algorithm and optimization strategy are sound and effective.

Cons:
Some references are missing as shown below, especially that [1] shares similar ideas. It is necessary to discuss [1] and carry out in-depth performance evaluation to clearly demonstrate the merits of this work. 

[1] Zhang et al., Efficient Training of Very Deep Neural Networks for Supervised Hashing, 
CVPR16

[2] Liu et al., Deep Supervised Hashing for Fast Image Retrieval, CVPR16

There are missing details for training the network (e.g., hyper-parameters), so it may not be able to reproduce the results accurately. It would be better if the authors will release the code if the paper is accepted.

Since the method adopts an alternating optimization approach, it would be better to show the curve of losses and demonstrate the effectiveness of the convergence.
The study in Figure 1 is interesting. However, the results and explanations (Ln 229-233) are not consistent. For example, DSDH-B is not always better than DSDH-A. In addition, the reason that DSDH-C is only slightly better than DSDH-A does not sound right (Ln 231-232). It may be the cause of different binarization strategies, and would require more experiments to validate it.

Why is DSDH-B better than DSDH in Figure 1(a) when using more bits?
","The paper propose a new method for learning hash codes using label and similarity data based on deep neural networks. The main idea of the paper is eq(6) where the authors propose a loss function that takes into account both pairwise similarity information and label information. 

The experiment section seems adequate and examines several aspect of the algo, e.g. importance of adding label information and comparison with state-of-the-art.

couple of points:

  - there is a similar work in 
K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen. Deep learning of binary hash codes for fast image retrieval.
It would be interesting to see a comparison both in terms of network structure and performance.

 -  in eq(4) is it possible to use a softmax classifier instead of the linear one?





"
Approximation Algorithms for $\ell_0$-Low Rank Approximation,"Karl Bringmann, Pavel Kolev, David Woodruff",https://proceedings.neurips.cc/paper/2017/hash/e94fe9ac8dc10dd8b9a239e6abee2848-Abstract.html,"The paper studies the problem of l_0-Low rank approximation in which the goal is to approximate an input matrix A (m\times n) with a rank-k matrix A’ whose entry-wise l_0 distance is minimum. The paper provides the first biceriteria approximation algorithm for this problem: for any k > 1 in poly(mn), provide a rank O(k\log(mn)) such that ||A-A’||_0 \leq poly(k log(mn)) \times OPT_k where OP_k denotes the closets rank-k matrix to A.

Moreover, they provide a (2+\eps)-approx for the case k=1 in sublinear time and further provide an algorithm for the case of binary matrices which seem to be more applicable in practice. However, the approximation ratio depends on the value of optimal solution and works well when OPT is O(||A||_0). The ratio makes the result a bit questionable.

The problem theoretically seems natural and standard; however, its application is not well-motivated. The paper also lacks experiments and solid explanation of the application of problem in practice. The results are clean and standard. However, there are some assumptions for which not sufficient evidences have been provided:
•	Why do they need to have the assumption that sum of rows and column can be accessed in O(m+n)? (lines 138-140). Assuming ||A||_0 and the adj list access you can easily compute them. Apparently, it is required for Theorem 13 and 14 where one can spend ||A||_0 in running time and compute them and still achieve the same running time guarantees.
•	The conjecture or the inapproximability result for Boolean l_0 rank-1 seems strong without any evidence.

Minor comments:
•	Line 31: rank-k -> rank-$k$
•	Line 33: min(mn^2, mn^2) -> probably min(mn^2, m^2n)
•	Line 131-137: you can mention incident list access model which is a very natural access model in the area of sublinear time algorithms design
•	Line 168: S^(0) = [n]   S^(0) = [m]
•	Line 234: The -> we

To sum up, the paper has studied and interesting theoretical problem and provides reasonable progress on that. However, its applicability in practice is not well supported.
","The paper proposes algorithms for finding a rank-k ell_0-minimizing approximation of a given matrix A. It gives approximation algorithms for the real case when p=1 and p > 1 and it also studies the boolean case. For all these cases the authors provide approximation algorithms (in some cases surprisingly sublinear) and in others bi-criteria approximations.

Strengths:
-- The algorithms are interesting and the proof techniques involved have some interesting twists.
-- The paper is well written.

Weakenesses:

-- The lack of experimental evaluation makes it hard to see the applicability of these algorithms in practical scenarios.
-- It would be interesting to reduce the space occupied for the extensive literature review and provide some experimental evaluation.","
This paper considers the problem of approximating matrices when no clear metric is present on the data and the l_0 norm is used (the number of non-zero elements in the difference between the original data and the approximation).
A low-rank solution is proposed.

The paper doesn't seem to give a practical case where this is useful.
In fact, ""low rank"" implies some linearity and hence implicitly a metric, which lay not make a lot of sense if we can't interprete the data with a metric to start with.  E.g., for k=1 one can easily make the first row and then one element in each other row of A-A' equal to zero (so 2n-1 elements), but then predicting linearly the values of the other elements seems arbitrarily.
Also, if the elements in A are continuous variables drawn from a distribution with no linear properties, the probability that for any low rank approximation A' more than these minimal number of elements of A-A' can be made equal to *exactly* zero (the requirement of the l_0 norm) is equal to 0.

In that sense, if the data is drawn randomly from a continuous distribution, with very high probability, OPT^{(k)} is something like (n-k)(m-k).  If k is ""low"", then theorems as Theorem 2 talking about multiples of OPT^{(k)} are rather trivial (because O(k^2) OPT^{(k)} > 2.OPT^{(k)} > mn) for ""random"" data.

So maybe the problem is to ""discover"" that by some external cause the data is very close already to low rank, in the sense that a lot of elements happen to match exactly the linear low rank relation to other elements.
In amongst others line 272, the authors too indicate that the work is most interesting if A is already close to low rank.

The mathematical elaboration and randomized algorithms are quite interesting from a theory point of view.
Moving some details to the supplementary material, and making a consistent story connecting the presented algorithms to clear learning problems, could make this a nice paper.

Some details:
* Line 48: supplementary -> supplementary material
* Line 244: having two times ""w.h.p."" in the same statement is not really necessary (but could be a compact way to present the result).

"
ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization,"Yi Xu, Mingrui Liu, Qihang Lin, Tianbao Yang",https://proceedings.neurips.cc/paper/2017/hash/e97ee2054defb209c35fe4dc94599061-Abstract.html,"The paper considers to accelerate ADMM using adaptive penalty parameter.  The paper developed a new adaptive scheme of penalty parameters for ADMM and established theoretical guarantees for both deterministic and stochastic ADMMs. Overall, the paper is interesting in the use of local error bound but the contribution is not significant enough. 

1.  The new adaptive penalization scheme turns out to be loop warm start algorithms. The algorithm could be practically slow, though faster convergence is claimed. 

2.  Increasing penalty parameter after each outer-loop iterate could greatly slow down the algorithm when K is large. ","Summary: This paper shows that O(1/eps) iteration complexity of ADMM can be improved to O(1/eps^(1-theta)) where theta is a  parameter that characterizes how sharply the objective function increases with respect to increasing distance to the optimal solution. This improvement is shown under a locally adaptive version of the ADMM where the penalty parameter is increased after every $t$ steps of ADMM. The method is extended to stochastic ADMM whose O(1/eps^2) iteration complexity is shown to similarly improve.  The results are backed by experiments on generalized Lasso problems.

Overall, the paper is well written and makes an important contribution towards improving the analysis of ADMM under adaptive penalty parameters. On a practical note, I would have liked to see some comparisons against other ""adaptive-rho"" heuristics used the literature (see Boyd's monograph). As background, it may be valuable to some readers to see how the local error bound relates to the KL property, and some examples of ""theta"" for problems of interest in machine learning. 

There are lots of grammatical errors, typos and odd phrases in the the abstract: ""tremendous interests"" --> ""tremendous interest"", ""variants are""-->""variants is"", ""penalty scheme of lies at it is..."",  ""iterate message""..

The LADMM-AP behavior in plot 1(e) is somewhat strange. Any explanations?

If factor of 2 in LA-ADMM optimal in any sense? In practice ADMM stopping criteria include primal-dual tolerance thresholds. Shouldnt LA-ADMM use those instead of fixed t? 

With regards to linearized ADMM, Eqn 7, please comment on its relationship to Chambelle-Pock updates (https://hal.archives-ouvertes.fr/hal-00490826/document)



","This paper exploits a local sharpness property to propose deterministic and stochastic (linearized) ADMM algorithms with adaptive penalty parameters that provably yield faster convergence rates than the respective algorithms with fixed parameters. 

The paper is well motivated, well written, and contains very interesting theoretical contributions. I recommend to accept the paper. 

The only complaint I have is that the experimental evaluation is very limited, particularly in the deterministic case. Instead of the nuclear norm minimization, I would have preferred the minimization of a polyhedral energy (ell^1 regularization + ell^1 data term) with a log-plot that verifies the linear convergence predicted by the theory. From the current experiment it is difficult to see if the adaptive step sizes yield a practical advantage. The authors could furthermore compare to simple adaptive schemes such as residual balancing. Finally, I am wondering about the computational overhead of the adaptive scheme over the classical ADMM: How would the plots in Fig. 1 look if the x-axis showed 'time' instead of 'iterations'?

Minor comments: 
- The reference ""Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation"" by
Z. Xu, M. Figueiredo, X. Yuan, C. Studer, and T. Goldstein, CVPR 2017, could be of interest to you (but could be similar to [28] - I haven't read the paper in detail yet). 
- Please proofread the manuscript - there are several typos, e.g. l. 4 ""are"" -> ""is"", l. 13 rewrite ""... scheme of lies at it is ..."", l. 64 ""stocahstic"", Assumption 1(c) -> for all y?, l. 258 -> ""the local sharpness parameter is \theta = 1"", ""enjoys"" -> ""enjoy"". "
A Learning Error Analysis for Structured Prediction with Approximate Inference,"Yuanbin Wu, Man Lan, Shiliang Sun, Qi Zhang, Xuanjing Huang",https://proceedings.neurips.cc/paper/2017/hash/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Abstract.html,"
There are some related works with learning-theoretic guarantees that go unmentioned in this manuscript:
1) London et al, ""Collective Stability in Structured Prediction: Generalization from One Example"", ICML 2013
2) Honorio and Jaakkola, ""Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms"", UAI 2016
3) McAllester and Keshet, ""Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss"", NIPS 2011

The constant \rho introduced in Definition 1 seems to act as an approximation constant (in theoretical computer science terms.) Recall that in general, inference is NP-hard, and in several instances, it is also NP-hard to approximate within a constant approximation factor (e.g., when Y is the set of directed acyclic graphs.) Authors should comment on this and provide more intuition behind the constant \rho.

Finally, given the PAC-Bayes results on McAllester 2007, the proof of Theorem 4 and 6 seem to follow straightforwardly.

=== After rebuttal

I think that the technical part could be improved a bit, by using specific examples (directed trees for instance) and plugging specific worst-case \rho's. All this from the theoretical point of view, in order to better appreciate the results.
      ","This is a multi-faceted paper: it contributes new theory, new algorithms, and experiments to evaluate the algorithms. Unfortunately, I do not have the background to evaluate the details of your proofs. However, I found your motivation of the theoretical question good and well-explained. For underestimation you provide an attractive framework for new learning algorithms. Do you have any recommendations for how to leverage the insights of this paper to improve learning with overestimation?

The improvements in your experiments are minor, but they help illustrate your point. The main shortcoming of the experiments is that there is no exploration of overestimation.  

The introduction and abstract would be improved significantly if you defined overestimation and underestimation much earlier on. Perhaps introduce them informally and then define them later with math. For overestimation, I would emphasize early on that this often comes from some LP relaxation of a combinatorial problem.  
","This paper is on the important topic of learning with approximate
inference. Previous work, e.g., Kulesza and Pereira (2007), has demonstrated the
importance of matching parameter update rules and inference approximation
methods. This paper presents a new update rule based on PAC Bayes bounds, which
is fairly agnostic to the inference algorithm used -- it assumes a
multiplicative error bound on model score and supports both over and under
approximations.

The example given in section 3.2 is a great illustration of how approximation
error is more subtle than we might think it is. Sometimes an approximate
predictor can fit the training data better because it represents a different
family of functions!

The experiments presented in the paper are on NLP problems. However, in NLP the
""workhorse"" approximate inference algorithm used is beam search. I strongly
encourage the authors to extend their experiments to include beam search
(varying the beam size to control the approximation quality).

    If, for some reason, beam search does not match their method, it should be
    made very clear.

    Related to beam search is work done by Liang Huang (e.g.,
    http://www.aclweb.org/anthology/N12-1015) about how perceptron with beam
    search can diverge and how to fix the learning algorithm (see ""early update""
    and ""max violation"" updates).

    This paper is pretty influential in NLP.

    This is a case where the learning rule is modified. I think the NLP
    community (at least) would be very interested if beam search could actually
    work with your approach.

Overall, I like this paper and I think that it could have a real impact because
learning with approximate inference is such an important topic and the approach
describe here seems to be pretty effective. Unfortunately, the current state of
the paper is a bit too rough for me to recommend acceptance. (If only we had the
option to ""accept with required revisions"")


Question
========

I'm confused about Theorem 6.

  a) Isn't this bound looser than the one in theorem 4?  Should making *more*
     assumptions (about \rho *and* \tau) lead to a tighter bound?

  b) I expected \rho to be replaced with a bound based solely on \tau. What's
     the deal?

Am I missing something?


Lower-level comments
====================

The authors assume that the reader is already familiar with the concepts of
under-estimation and over-estimation. I do not believe most readers will know
about this. I strongly recommend defining these terms very early in the paper.
I would also recommend explaining early on why these two classes of
approximations have been studied in the past (and why they are typically studied
separately) -- this would be much more useful than the current discussion in the
related work section.

bottom of page 1: Please define risk? In particular is e(h) /empirical/ risk or
true risk?

39: typo? ""We investigate three wildly used structured prediction models""
    -> I think you meant ""widely"", but I suppose ""wildly"" also works :P

41: At this point in the paper, I'm very surprised to see ""text classification""
    on a list of things that might use approximate inference... It might be
    useful to explain how this is relevant -- it seems as if the answer is that
    it's just a synthetic example.

    There are some settings, such as ""extreme classification"", where the number
    of labels is huge and approximate inference might be used (e.g., based on
    locality sensitive hashing or approximate nearest neighbors).

42: typo ""high order"" -> ""higher-order""

45: It's tough to say that ""Collins [5]"" was ""the first"" since many results for
the /unstructured/ multi-label case technically do transfer over.

47,49,50,etc: Citations are not nouns use use \citet instead of \citep and
\usepackage{natbib}.

   ""[37] provided..."" -> ""Collins (2001) provided ...""

49: ""Taskar’s bound"" is undefined, presumably it's in one of the citations in
    the paragraph. (Also, you probably mean ""Taskar et al.'s bound"".)


There are many excellent citations in related work (section 2), but without much
discussion and clear connection to the approach in the paper.


68: ""w the parameter."" -> ""w is the parameter vector."" (probably want to say $w
\in \mathbb{R}^d$ as well)


70: clearly over and under estimation method do not partition the space of
approximate inference algorithms: what sorts of algorithms does it omit? does it
matter? Also, what is the role of \rho? Do we need to know it? does it have
constant for all x? (Looks like later, e.g., theorem 4, it's allowed to depend
on the example identifier) Why are multiplicative bounds of interest? Which
approx methods give such bounds? (Later it is generalized based on
stability. Probably worth mentioning earlier in the paper.)

The notation h[w](x) is unusual, I'd expect/prefer h_w(x), or even h(w, x).

73, 74: definitions of H, H- and H+ are a little sloppy and verbose. The
definition of H is pretty useless.

73: The typesetting of R doesn't mach line 68. Please be consistent.

section 3.1
 - L(Q,S,h[w]) seems unnecessary (it's fine to just say that the expectation is over S).
 - Also, S is undefined and the relationship between S and x_i is not explained
   (I assume x_i \in S and |S|=m). Alternatively E_{x \sim S} would do the
   trick.

Lemma 2: Missing citation to PAC-Bayes Theorem.

  Please explain the relationship between the prior p(w') and Q(w'|w). The use
  of a prior comes out of the blue for readers who are unfamiliar with PAC Bayes
  bounds.

  A few words about why I should even be thinking about priors at this point
  would be useful.

  Also, if I don't care about priors can't I optimize the prior away?

77: Should technically pass Q(.|w) to KL divergence, i.e., $D(Q(.|w) || P(.))$.

80: It might be more useful to simply define under and over estimation with this
definition of margin.

theorem 4:
 - assume h'[w] be a ..."" -> ""theorem 4: assume h'[w] is ...""
 - Should be a \rho_i approximation of h[w](x_i)?
 - what is \bar \cal Y ?
 - why bother subscripting \lambda_S? just use \lambda

90: a better/more-intuitive name for square root term?

107: definition 8, ""contains"" is not a good term since it's easy to mistaking it
for set containment or something like that. Might be better to call it
""dominates"".

Section 3.2 could be summarized as approximate inference algorithms give us a
/different/ family of predictors. Sometimes the approximate inference family has
a better predictor (for the specific dataset) than the exact family.

   Giving high-level intuitions such as the one I provided with resonate more
   with the reader.

Figures 1 and 2 take a little too much effort to understand.

    Consider hoisting some of the discussion from the main text into the
    caption.

    At the very least, explain what the visual elements used mean mean (e.g.,
    What is the gray arc? What are the columns and rows?)

159: ""... or by tuning."" on development data or training data?

246: ""non-comparableness"" -> ""incomparability""
"
Simple strategies for recovering inner products from coarsely quantized random projections,"Ping Li, Martin Slawski",https://proceedings.neurips.cc/paper/2017/hash/ea159dc9788ffac311592613b7f71fbb-Abstract.html,"
Paper Summary:
The authors study the distortion induced on inner products from applying a quantization over random projection. They show that even a highly coarse quantization (of 3-5 bits) only results in negligible to distortion overall (but significantly more distortion is induced on pairs of points with high cosine similarity). 

Review:
I am not too familiar with the quantization after random projection literature to comment on the originality or significance of this work.

Overall I like the approach to compress the data even further to gain significant speedups without compromising on quality. I believe that the work presented here can help practitioners gain better understanding on when and how simple quantization can help as a post processing step after random projection.


","********************************
* Summary 	               *
********************************

The paper investigates theoretically and empirically different strategies for recovery of inner products using quantized random projections of data instances. Random projections are often used in learning tasks involving dimensionality reduction. The goal of the additional quantization step is data compression that allows for a reduction in space complexity of learning algorithms and more efficient communication in distributed settings.


********************************
* Theoretical contributions    *
********************************

The main focus of the paper is on studying a linear strategy for recovery of inner products from quantized random projections of the data. The strategy approximates the inner product between two instances from the instance space with the inner product of the corresponding quantized random projections divided by the dimension of the projection space. The main theoretical contribution is a bound on the bias of such approximations (Theorem 1). In addition to this strategy, the paper considers recovery of inner products from random projections that are normalized (i.e., having unit norm) prior to quantization. For such approximations, the paper expresses the bias and variance in terms of the relevant terms of the linear strategy for recovery of inner products (Proposition 1). The paper also provides a bound on the variance of recovery of inner products with values close to one and strategies based on quantization with finitely many bits (Theorem 2).


********************************
* Quantization 		       *
********************************

The quantization of random projections is performed using the Lloyd-Max quantizer. The method resembles one dimensional K-means clustering where interval end-points determine the clusters and the centroid is given as the conditional expectation of the standard normal random variable given the interval, i.e., c_k = E [ z | z \in [t_k, t_{k+1}) ], where c_k is the cluster centroid or quantization value, t_k and t_{k + 1} are interval end-points of the kth interval, and the total number of intervals K is given with the number of quantization bits B = 1 + \log_2 K.


********************************
* Empirical study 	       *
********************************

# Figure 2
The goal of this experiment is to numerically verify the tightness of the bound on the bias of the linear strategy for recovery of inner products from quantized random projections. The figure shows that the bound is tight and already for the recovery with 5-bit quantization the bound almost exactly matches the bias. The paper also hypothesizes that the variance of the linear strategy with finite bit-quantization is upper bounded by the variance of the same strategy without quantization. The provided empirical result is in line with the hypothesis.

# Figure 3
The experiment evaluates the mean squared error of the recovery of inner products using the linear strategy as the number of quantization bits and dimension of the projection space change. The plot indicates that quantization with four bits and a few thousand of random projections might suffice for a satisfactory recovery of inner products.

# Figure 4
- In the first experiment, the normalized strategy for recovery of inner products from quantized random projections is compared to the linear one. The plot (left) indicates that a better bias can be obtained using the normalized strategy. 
- In the second experiment, the variance of the quantized normalized strategy is compared to that without quantization. The plot (middle) indicates that already for quantization with 3 bits the variance is very close to the asymptotic case (i.e., infinitely many bits and no quantization). 
- The third experiment compares the mean squared error of the normalized strategy for recovery of inner products from quantized random projections to that of the collision strategy. While the collision strategy performs better for recovery of inner products with values close to one, the normalized strategy is better globally.

# Figure 5
The experiment evaluates the strategies for recovery of inner products on classification tasks. In the first step the random projections are quantized and inner products are approximated giving rise to a kernel matrix. The kernel matrix is then passed to LIBSVM that trains a classifier. The provided empirical results show that the quantization with four bits is capable of generating an approximation to kernel matrix for which the classification accuracy matches that obtained using random projections without quantization. The plots depict the influence of the number of projections and the SVM hyperparameter on the accuracy for several high-dimensional datasets. The third column of plots in this figure also demonstrates that the normalized strategy for recovery of inner products from quantized random projections is better on classification tasks than the competing collision strategy.


********************************
* Theorem 1 		       *
********************************

Please correct me if I misunderstood parts of the proof.

# Appendix B: Eq. (4) --> Bound
Combining Eq. (6) with Eq. (4) it follows that 

E[\rho_{lin}] - \rho = -2 \rho D_b + E[(Q(Z) - Z)(Q(Z') - Z')] >= -2 \rho D_b    ==>    2 \rho D_b >= \rho - E[\rho_{lin}] .

To be able to square the latter without changing the inequality one needs to establish that \rho - E[\rho_{lin}] >= 0. Otherwise, it is possible that | \rho - E[\rho_{lin}] | > 2 \rho D_b and \rho - E[\rho_{lin}] < 0.

# Appendix B: Eq. (6)
- A proof of the left-hand side inequality is incomplete, E[(Q(Z) - Z)(Q(Z') - Z')] >= 0. At the moment the term is just expanded and it is claimed that the expansion is a fact. If so, please provide a reference for this result. Otherwise, an explanation is needed for why it holds that E[ Z Q(Z') ] - E[ Q(Z) Q(Z') ] <= E[ Z Z'] - E[ Z Q(Z') ].
- For the proof of the right-hand side inequality, it is not clear why it holds that E[ Z Z' ] + E[ Q(Z) Q(Z') ] - 2 E[ Z Q(Z') ] <= E[ Z Z' ] - E[ Z Q(Z') ].

### My decision is conditional on these remarks being addressed properly during the rebuttal phase. ###


********************************
* Minor comments 	       *
********************************

- line 79: bracket is missing ( || z ||^2 + || z' ||^2 ) / k
- Appendix A, Eq. (2): the notation is not introduced properly","In this paper, authors investigate the dimensionality reduction technique with random projection followed by subsequent coarse quantization, which compresses data further. They find that with only 3-5 bits per each projection, the loss in accuracy becomes generally acceptable for classification tasks. Their approach does not require complicated steps such as maximum likelihood estimation that is required by previous work.

The proposed normalized linear estimator comes with a variance at most 2x that of previous MLE based method. And the analysis of quantization clearly shows that just a few bits would reduce the variance asymptotically towards that with infinite precision.

Generally speaking, it would be very interesting if authors can apply some data dependent feature learning techniques such as autoencoders and compare the performance under same bit length."
Trimmed Density Ratio Estimation,"Song Liu, Akiko Takeda, Taiji Suzuki, Kenji Fukumizu",https://proceedings.neurips.cc/paper/2017/hash/ea204361fe7f024b130143eb3e189a18-Abstract.html,"This paper studies a robust approach for density ratio estimation, which is an important problem. The paper is well-written. The paper establishes consistency under different settings. The authors further provide interesting numerical studies to backup their methods.

My only question is: under the high-dimensional setting, what scaling can the method tolerate? The authors should explicitly list this scaling.
","Summary: This paper proposes a ""trimmed"" estimator that robustly (to outliers) estimates the ratio of two densities, assuming an a exponential family model. This robustness is important, as density ratios can inherently be very unstable when the denominator is small. The proposed model is based on an optimization problem, motivated by minimizing KL divergence between the two densities in the ratio, and is made more computationally tractable by re-expressing it in terms of an equivalent saddle-point/max-min formulation. Similar to the one-class SVM, this formulation explicitly discards a portion (determined by a tuning parameter) of ""outlier"" samples. The density-ratio estimator is shown to be consistent in two practical settings, one in which the data contains a small portion of explicit outliers and another in which the estimand is intrinsically unstable. Finally, experiments are presented in the context of change detection, including a synthetic Gaussian example and a real relative object detection example.




Main Comments:

The paper is fairly clearly written, and the derivation of the estimator is well motivated throughout. While the application is somewhat specific, the method seems practically and theoretically strong, and I appreciate that the theoretical results are tailored to the application settings of interest. The experients aren't a particularly thorough comparison, but nevertheless effectively illustrate the behavior and benefits of the proposed estimator.




Minor Comments:

Line 22: ""a few sample"" should be ""a few samples""

Line 29: small typo: ""uses the log-linear models"" should be ""uses the log-linear model"" or ""uses log-linear models""

The placement of Section 3.2 is a bit odd, as it interupts the flow of deriving the estimator. Perhaps this could go immediately before or after the experimental results?

Line 131: ""The key of"" should be ""The key to""

Figure 3: Perhaps include a note saying that this figure is best viewed in color, as I found this figure very confusing in my greyscale printout.","Trimmed Density Ratio Estimation

This paper proposes a convex method for robust density ratio estimation, and provides several theoretical and empirical results. I found the method to be well motivated, and of course Figure 3 is quite fun. Based on the evidence in the paper, I would feel comfortable recommending the method to others.

I would be happy to see these results published; however, I found the presentation to be fairly terse, and thought the paper was very difficult to follow. My main recommendation for the writing would be to focus more on core contributions, and to remove some side remarks to allow for longer discussions.

Moreover, the paper was missing some references.

Noting these issues, my current recommendation for the paper is a middle-of-the-road “accept”; however, if the authors are willing to thoroughly revise their paper, I would be open to considering a higher rating. Below are some specific comments.

Comments on motivation + literature:

- The paper opens the abstract by describing density ratio estimation (DRE) as a “versatile tool” in machine learning. I think this substantially undersells the problem: DRE is a a core statistical and machine learning task, and any advance to it should be of broad interest. I would recommend stressing the many applications of DRE more.

- On a related note, DRE has also received quite a bit of attention in the statistics literature. For example, Efron and Tibshirani (1996) use it to calibrate kernel estimators, and Fithian and Wager (2016) use DRE to stabilize heavy-tailed AB-tests. Fokianos (2004) models the density ratio to fit several densities at once. This literature should also be discussed.

- Are there some applications where trimming is better justified than others? Engineering applications relating to GANs or anomaly detection seem like very natural places to apply trimming; other problems, like two-sample testing, may be more of a stretch. (In applications like anomaly detection, it’s expected to have regions where the density ratio diverges, so regularizing against them is natural. On the other hand, if one finds a diverging density ratio in two-sample testing, that’s very interesting and shouldn’t be muted out.)

Comments on relationship to the one-class SVM:

- I found Section 2 on the one-class SVM to be very distracting. It’s too short to help anyone who isn’t already familiar with the method, and badly cuts the flow of the paper. I think a version of the paper that simply removed Section 2 would be much nicer to read. (The discussion of the one-class SVM is interesting, but belongs in the discussion section of a long journal paper, not on page 2 of an 8-page NIPS paper.)

- The use of the one-SVM in the experiments was interesting. However, I think the median reader would benefit from a lot more discussion. The point is that the proposed method gets to leverage the “background” images, whereas one-SVM doesn’t. So even though one-SVM is a good method, you bring more data to the table, so there’s no way one-SVM could win. (I know that the current draft says this, but it says it so briefly that I doubt anyone would notice it if they were reading at a regular speed.)

- As a concrete recommendation: If the authors were to omit Section 2 and devote the resulting space to adding a paragraph or two to the intro that reviews and motivates the DRE literature to the intro (including the stats literature) and to thoroughly contrasting their results with the one-SVM in the results section, I think the paper would be much easier to read.

Other comments:

- In the setting of 3.1, Fithian and Wager (2016) show that if we assume that one class has much more data than the other, then density ratio models can be fit via logistic regression. Can the authors comment on how the proposed type of trimming can be interpreted in the logistic regression case?

- Section 3.2 is awkwardly placed, and cuts the flow. I recommend moving this material to the intro.

- I wasn’t sold by the connection to the SVM on lines 120-129. The connection seems rather heuristic to me (maybe even cosmetic?); I personally would omit it. The core contributions of the paper are strong enough that there’s no need to fish for tentative connections.

- The most classical approach to convex robust estimation is via Huberization (e.g., Huber, 1972). What’s recommended here seems very different to me —- I think readers may benefit from a discussion of how they’re different.

- The approach of assuming a high-dimensional linear model for the theory and and RBF kernel for the experiments is fine. Highlighting this choice more prominently might help some readers, though.

References:
Efron, Bradley, and Robert Tibshirani. ""Using specially designed exponential families for density estimation."" The Annals of Statistics 24.6 (1996): 2431-2461.
Fithian, William, and Stefan Wager. ""Semiparametric exponential families for heavy-tailed data."" Biometrika 102.2 (2014): 486-493.
Fokianos, Konstantinos. ""Merging information for semiparametric density estimation."" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 66.4 (2004): 941-958.
Huber, Peter J. “Robust statistics: A review."" The Annals of Mathematical Statistics (1972): 1041-1067.

##########
UPDATE

In response to the authors’ comprehensive reply, I have increased my rating to an “8”."
Adaptive Batch Size for Safe Policy Gradients,"Matteo Papini, Matteo Pirotta, Marcello Restelli",https://proceedings.neurips.cc/paper/2017/hash/ea6b2efbdd4255a9f1b3bbc6399b58f4-Abstract.html,"Summary:

This paper derives conditions for guaranteed improvement when using policy gradient methods. These conditions are for stochastic gradient estimates and also bound the amount of improvement with high probability. The authors then show how these bounds can be optimized by properly selecting the step size and batch size parameters. This is in contrast to previous work that only considers how the step size can be optimized. The result is an algorithm that can guarantee improvement with high probability.

Note: I did not verify all of the math, but at an intuitive level the theorems and lemmas in the main body are believable.

Major:

1. What am I supposed to take away from this paper? Am I supposed to think ""I should use policy gradient methods with their safe step size & batch size approach if I need a safe RL algorithm""? Or, am I supposed to think ""This paper is presenting some interesting theoretical insights into how step sizes and batch sizes impact policy gradient methods""?

If the answer is the former, then the results are abysmally bad. The axes are all scaled by 10^7 trajectories. Even the fastest learning methods are therefore impractically slow. This doesn't appear to be a practical algorithm. Compare to the results in your reference [23], where high confidence improvement is achieved using hundreds to thousands of episodes. So, the proposed algorithm does not appear to be practical or competitive as a practical method.

This makes me think that the contribution is the latter: a theoretical discussion about step sizes, batch sizes, and how they relate to guaranteed improvement. This is interesting and I would recommend acceptance if this was the pitch of the paper. However, this is not made clear in the introduction. Until I reached the empirical results I thought that the authors were proposing a practical algorithm. I propose that the introduction should be modified to make it clear what the contribution is: are you presenting theoretical results (e.g., like a PAC RL paper), or are you proposing a practical algorithm? If the answer is the latter, then you need to compare to existing works or explain why it is not an issue that your method requires so much data.


2. The assumption that the policy is Gaussian is crippling. In many of the real-world applications of RL (e.g., dialogue systems) the actions are discrete, and so the policy is not Gaussian. Does this extend to other policy forms, like softmax policies? Is this extension straightforward, or are there additional challenges?


Medium:

1. Line 92. You say that convergence is assured because the angular difference is at most \pi/2. A similar claim was made by Peters and Schaal in their natural actor-critic paper, and it later turned out to be false (See ""GeNGA: A generalization of natural gradient ascent with positive and negative convergence results"" by Thomas at ICML 2014). Your claim is different from Peters and Schaal's, but due to the history of these off-hand claims tending to be false, you should provide a proof or a reference for this claim.

2. You use concentration bounds that tend to be wildly over-conservative (Chebyshev & Hoeffding) because they must hold for a broad class of distributions. This is obviously the correct first step. However, have you considered using bounds that are far tighter (but make false distributional assumptions), like Student's t-test? If the gradient estimate is the mean of n i.i.d gradient estimates, then by the Central Limit Theorem the gradient estimate is approximately normally distributed, and so Student's t-test is reasonable. Empirically, does it tend to result in improvements with high probability while requiring far less data? It seems like a practical implementation should not use Hoeffding / Chebyshev / Bernstein...

3. Page 7 is one log paragraph.


Minor:

1. Line 74 ""paramters""

2. Capitalization in section titles is inconsistent (e.g., section 3 capitalizes Policies but not Size or Scalar).

3. Line 91 ""but, being the \alpha_i non-negative"". Grammar error here.

4. Integrals look better with a slight space before the dx terms: $$\int_{\mathcal X} f(x) \, \mbox{d}x$$. The $\,$ here helps split the terms visually.

5. Line 102 ""bound requires to compute"" - requires who or what to compute something? Grammar.

6. The line split on line 111 is awkward.

7. Equation below 113, use \text{} for ""if"" and ""otherwise"".

8. Below 131, $\widehat$ looks better on the $\nabla$ symbol, and use $\max$ for max.

9. Line 140 say what you mean by the ""optimal step size"" using an equation.

10. Line 192, don't use contractions in technical writing (i.e., don't write ""doesn't"", write ""does not"").


AFTER AUTHOR REBUTTAL:

My overall review has not changed. I've increased to accept (7 from 6) since the authors are not claiming that they are presenting a practical algorithm, but rather presenting interesting theoretical results.","I enjoyed this paper, understood the importance of the basic idea, and think it is worthy of publication.

Some comments:

Some of the writing sounds strange, just in the introduction we have:
""performance of such policies strictly depends on the model accuracy"" strictly? maybe strongly is better.
""think about equipment wear in smart manufacturing"", I think a 'for example' would suffice.
""hinder the performance loss"" reduce the performance loss?
There are some other cases of unusual wording in the paper which make it a little more difficult to read.

Why is diag(\alpha_i) = \Lambda when a much more easily understood notation would be \diag(\lambda_i) = \Lambda?

Throughout the paper argmax is used as a function, but argmax actually returns the *set* of all indices that achieve the max. Usually this would be an unimportant distinction but in this paper the return value from 'argmax' is set to a non-zero value and all other indices set to zero, with no consideration for what to do in cases with more than one element in the argmax.

Theorem 3.3 is referenced several times but does not appear anywhere in the paper or supplementary material.

The proof for Corollary 3.1 needs fleshing out ""The function has no stationary points. Moreover, except from degenerate cases, partial maximization is possible only along a single dimension at a time. Candidates for the optimum are thus attained only on the boundary, and are obtained by setting to 0 all the components of the step size but one"" is not sufficient since it requires too much of the reader (why does it have no stationary points? why is maximization possible only along a single dimension? etc.).

Table 2: ""5% confidence intervals are reported"", surely this is meant to be 95% confidence intervals? If not then the results should be changed to be 95% intervals, 5% intervals are meaningless for all practical purposes.

Sect 5: ""We then proceed to test the methods described in Section 5."" Should be section 4 I think.

=== review update ===
Happy to recommend for publication.","--Brief summary of the paper:
The paper proposes a method to optimize the step size and batch size for policy gradient methods such that monotonic improvement of a policy is guaranteed with high probability.　The paper focuses on a class of linear-in-parameter Gaussian policies and first derives an adaptive step-size which results in a coordinate descent like update.　Then, the paper derives adaptive batch sizes for gradient estimation using different concentration bounds.　Empirical behaviors of these adaptive batch sizes are evaluated and analyzed on a simple experiment. 

--Major comments and questions:
The paper tackles important issues of policy gradient methods which are to choose step size and batch size, which are usually tackled by trial-and-error. The paper provides rigorous theoretical results for a class of linear Gaussian policy function.　The writing is also easy to follow and understand.　The paper's theoretical contribution to policy gradient framework is significant, although the practical contribution is mild. However, I have the following questions.
1) (Important question) In Theorem 4.1, the adaptive batch size N* depends on the gradient estimate. However, the gradient is usually estimated from a batch of trajectory samples. Is the gradient estimated before determining N*? If it is so, then what is the adaptive batch size N* used for? The same question is also asked for the initial search solution in Section 4.3 which again depends on the gradient estimate.
2) In the paper, the variance of Gaussian policy (sigma) is fixed and not learned. Intuitively, Gaussian policy with large variance often require a large batch size to accurately estimate the gradient. Does the theorem also support this? The paper should mention this viewpoint as well.
3) Since the performance improvement in Definition 4.1 is also a function of the policy variance through the constant $c$, can the theorems be extended to adaptively adjust the policy variance? This would be an important extension since in practice the policy variance should not be a common fixed value and should be determined through learning.

--Minor suggestions:
1) Natural policy gradient and actor-critic are important subclasses of policy gradient methods. Is it straightforward to apply the method to them? (For actor-critic I suppose the theorems need to consider an error bound of the Q-function as well, and this might be a promising future direction.)
2) The state value function V(s) used in line 66 should be properly defined. 
3) Having equation numbers will make the paper easier to read.
4) Some typos: Line 72, paramters -> parameters. Line 24: been -> be. Line 226: s_t ~ N(…) -> s_t+1 ~ N(…). Line 265: methods -> method. Duplicated references for [16] and [17], [21] and [22].
"
Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting,"Rebecca Morrison, Ricardo Baptista, Youssef Marzouk",https://proceedings.neurips.cc/paper/2017/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html,"In the paper ""Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting"", the authors proposed an algorithm which estimates a sparse multivariate non-Gaussian distribution from a set of observed variables. This targeted problem is not as well studied as in Gaussian MRFs or discrete MRFs, and therefore this is an important problem (one real-world example is the cloud cover formation problem mentioned by the authors). 

The design of the algorithm is based on a transport map, a concept that is not well-known in the machine learning and statistics community (as far as I know). On one hand, I feel such new concept should be encouraged and welcomed by the machine learning and statistics community. On the other hand, I feel the concept can be introduced in a better way by proving more intuition of the concept. For example, the authors can provide more insight about the lower triangular monotone increasing maps (introduced at the bottom of Page 2), Hermite polynomials (for c_k) and Hermite functions (for h_k). In the meantime, I'm a little concerned that some related work (e.g. [1]) is not published yet and I wonder whether the proposed method in the current submission relies heavily on other people's unpublished work (for accuracy and validity concern). 

During the rebuttal phase, the authors have addressed my concern in their *Additional clarifications* section. ","This paper presents a method for identifying the independence structure of undirected probabilistic graphical models with continuous but non-Gaussian distributions, using SING, a novel iterative algorithm based on transport maps. The authors derive an estimate for the number of samples needed to recover the exact underlying graph structure with some probability and demonstrate empirically that SING can indeed recover this structure on two simple domains, where comparable methods that make invalid assumptions about the underlying data-generating process fail.


Quality.

The paper seems technically sound, with its claims and conclusions supported by existing and provided theoretical and empirical results. The authors mostly do a good job of justifying their approach, but do not discuss potential issues with the algorithm. For example, what is the complexity of SING? Since it has a variable-elimination-like aspect, is complexity exponential in the treewidth of the generated graph? If so, how is this approach feasible for non-sparse graphs? In that vein, I would like to see how SING fares empirically when applied to more realistic and higher-dimensional problems and in comparison to other standard MRF-learning approaches. The latter typically make overly strong assumptions, but can still perform well given that there are rarely enough data points for true structures to be recovered.


Clarity.

The writing is good, and each subsection is clear, but the authors should better define their notation and spend more time explaining the algorithm as a whole and gluing the pieces together. In particular, what exactly happens in subsequent iterations? A quick example or a brief high-level walk-through of the algorithm would be helpful here to assemble all of the pieces that have been described into a coherent whole. 

Also, is the amount of sparsity that is estimated monotonic over iterations? If SING over-estimates the amount of sparsity present in the data, can it then reduce the amount of sparsity it is estimating? The sparsity pattern identified on line 8 of Alg. 1 is used in line 2, correct? (Note that you use I_{S_{i+1}} and S^B_{I_{i}}, respectively, on these lines – is I_i the same as I_{S_{i+1}} from the previous iteration?) It’s not clear to me how this sparsity pattern gets used in the estimation of the transport map. You should more clearly define where the original and induced graphs come from. From what I can tell, the original graph is the result of the thresholded \Sigma (but no graph is discussed in Alg. 1), and the induced graph comes from some variable-elimination-like algorithm, but it’s not clear how this actually happens. Does Alg. 1 perform variable elimination? What are the factors? Why do you need to do this?


Originality.

The approach seems novel and original, although I am not that familiar with transport maps and their corresponding literature.


Significance.

The presented approach seems interesting and potentially useful, but lacks sufficient evaluation as the authors do not evaluate or discuss the complexity of their algorithm and only evaluate on two simple problems. It would also be useful to properly conclude section 5 by better explaining and contextualizing the result. In particular, is the number of samples needed high or low relative to other works that estimate similar types of graph structures, or is this completely novel analysis?


Overall.

With additional results, editing, and a more cohesive explanation of their approach, this could be a successful publication, but I do not think it has reached that threshold as it stands.


Detailed comments:
- Please define your notation. For example, in the introduction you should define the independence symbol and \bf{x}_{jk}. In section 2, you use both d_j d_k for (I think) partial derivatives, but then later use d_{jk} for (I assume) the same quantity, but define neither. You should define det() and the circle, which is typically used for function composition.  The V on line 154 should be defined. The projection (?) operator on the equation below line 162 should be defined. Etc.

- Line 75: inline citations should be Author (Year), instead of the number (use \citet instead of \citep if using natbib)

- There are a few typos and duplicated words (e.g., lines 86, 191).

- The font sizes on your figures are barely legible when printed – these should be increased.

- Alg 1., line 1, “DECREASING = TRUE” does not show when printed, perhaps due to font choice or embedding.

- Lines 156 and 173: These should be referred to as Equation (6) and Equation (7), not Line (6) and (7).

- Figure 3d shows SING(B=1), but the B=1 case has not been explained before this, so it looks like a failure case of the algorithm. Better would be to put the success case in Figure (3d) and then show the failure case in Figure (4) (separate from the others) and discuss the effect of B=1 there.

- Figure 5: it’s confusing to show the true graph (5a) as the ‘recovered’ graph. You should show the recovered adjacency matrix for SING as an added sub-figure of figure 5. Further, the x- and y- axis labels are just numbers, meaning that it’s not clear which numbers correspond to the hyperparameters, so lines 232-233 are confusing.

- Figures 3-6: would be useful to have a legend or explanation somewhere that blue means an edge and white means no edge.


---- 

After reading the author response, I have updated my score to 'weak accept' as I agree that the NIPS community will benefit from this work and my main concerns of complexity with respect to the use of variable elimination have been satisfied. However, the exact use of the variable elimination heuristics still remains somewhat unclear to me and the authors should clarify and be more explicit about how it is used in their algorithm.","Review of ""Beyond normality. Learning sparse probabilistic 
graphical models in the non-Gaussian setting""

The paper describes a new approach for structure estimation 
in continuous-valued non-Gaussian graphical models using 
""transport maps"". This appears to be an very interesting 
higher-order generalization of copulas -- representing 
a joint density of interest as a multivariate (not coordinatwise) 
transformation of a jointly Gaussian density. The authors argue
that transport maps have sparsity properties corresponding 
to the Markov graph of the graphical model, which allows
efficient estimation and inference. The authors also use a new
measure of conditional independence based on a generalization of 
the information matrix applicable to non-Gaussian models. 

I think this is a very exciting paper, providing a new (at least 
for the machine learning community) approach to deal with high-dimensional 
non-Gaussian undirected graphical models. I think that while there are many 
challenges to make the approach practical -- e.g. learning high-dimensional 
multivariate polynomials from samples will be hard to scale to much more 
than 10 variables considered in the paper -- but the paper opens up 
a new direction of research. Perhaps some form of approximate pairwise
polynomial expansion (or other parametereizations) may be fast enough to 
learn non-trivial models. 

Also  it appears that the authors refer  to a nontrivial amount of 
existing work -- but this literature is new to me -- and as far as I know it
has not been presented to the ML community -- appearing in physics and 
geoscience journals and the like. 

The paper is well written, and easy to follow.

The key question is the scalability of the Hermite polynomial representation 
of the multivariate transport maps.  Is there any hope that useful approximations
may be obtained using pairwise expansions (it would already be a generalization 
of the univariate copula transformations)? 


Detailed comments: 

1) notation x_{-jk} needs to be explained (or perhaps use x_{\jk} which is more common
to refer to conditioning on all other variables in the graph. 

2) Sec. 1. page 1. ""algorithms are consistent for sparse recover with few samples""... It's worth being 
more precise -- what do you mean by ""few samples""?

3) Sparse l1-penalized logistic regression is also used in the discrete setting (in addition to [12]). 
""HIGH-DIMENSIONAL ISING MODEL SELECTION USING l1-REGULARIZED LOGISTIC REGRESSION"", Ravikumar, Wainwright, Lafferty, 2010. 

4) Cloud cover formation:  what is the data? Satellite images? 

5) For your definition of Omega_jk( x ) -- is there a notion of scale -- to be able to argue about ""small"" or ""large""
values?  Is it possible to normalize it to be [-1,1]  -- similar to correlation coefficients?  Does this matrix
have any properties -- e.g. positive-definiteness or cpd? 

6) There are principled thresholding based approaches for estimating sparse inverse covariance matrices, e.g. 
""Elementary Estimators for Sparse Covariance Matrices and other Structured Moments"", Yang, Lozano, Ravikumar, ICML 2014. 

7) In (2) and (3) -- what conditions are there on the two maps?  Is it fully general (or needs smoothness and strict 
positivity)?

8) The monotonicity of __the__ each component (line 86)... drop __the__...

9) I am assuming you use multivariate Hermite polynomial expansions? What degree do you consider (are you able 
to handle) in practice -- there's a combinatorial explosion of multivariate terms that appears daunting.  How 
many multivariate basis functions do you use -- how much time does it take for the computational 
examples considered in the paper? 

10)  Please give the intuition why the problem is convex in a few words?  Do you only need second-order moments of pi, 
which depend linearly on the hermite coefficients? 

11) Sec 3.2 The lower bounds from the markov structure to the sparsity of the transport maps -- is this not an exact match 
due to fill-in in variable elimination? 

12) In figure 2 (b) -- why is bias non-zero -- but depends on the number of samples? How do you define and measure bias?

13) In algorithm 1 -- does the sparsity profile of the transport map increase monotonically with iterations?  Can it oscillate? 
Are there any guarantees / empirical observations? 

14) What is the delta method? reference?

15) We consider the case in which the sparse graph is returned after a single iteration... ?  How can you assume this, and 
how often does it happen in practice?   Perhaps it's better to say that you're analyzing convergence after identifying 
the correct graph structure?

16) Figure 3 (b) -- very small labels. 

17) What it SING with l1-penalties?  Is there a reference? "
"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models","George Tucker, Andriy Mnih, Chris J. Maddison, John Lawson, Jascha Sohl-Dickstein",https://proceedings.neurips.cc/paper/2017/hash/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Abstract.html,"Summary

This paper proposes a control variate (CV) for the discrete distribution’s REINFORCE gradient estimator (RGE).  The CV is based on the Concrete distribution (CD), a continuous relaxation of the discrete distribution that admits only biased Monte Carlo (MC) estimates of the discrete distribution’s gradient.  Yet, using the CD as a CV results in an *unbiased* estimator for a discrete random variable’s (rv) path gradient as well as lower variance than the RGE (as expected).  REBAR is derived by exploiting the REINFORCE estimator for the CD and by observing that given a discrete draw, the CD’s continuous parameter (z, here) can be marginalized out.  REBAR has some nice connections to other estimators for discrete rv gradients, including MuProp.  Moreover, the CD’s temperature parameter can be optimized on the fly since it is no longer crucial for trading off bias and variance (as it is when using the CD alone).  Lastly, REBAR is applicable to both multi-level stochastic models as well as reinforcement learning.  Experiments showing the improvements from de-biased estimators and lower estimator variance are included.     

 
Evaluation

Method:  I find REBAR to be a quite interesting and novel proposal.  Using the CD as a CV is a nice idea and might have been worthy of publication on its own, but the additional idea of marginalizing out the CD’s continuous parameter (z) increases the worth of the contribution a step further.  Moreover, REBAR is simple to implement so I can see it finding wide use everywhere REINFORCE is currently used.  Discrete latent variables are not often used in stochastic NNs—-ostensibly b/c of the high variance that impedes training—-but REBAR may contribute here too, making discrete latent variables more competitive with continuous ones.  I am hard pressed to find a deficiency with the proposed method.  REBAR does not scale well to multiple stochastic layers, requiring a forward prop for each, but the authors propose workarounds for this in the appendix.

Experiments:  I find the experiments adequate to validate the method.  The toy experiment examining bias is a little contrived, but it’s nice to see that the unbiased approach helps.  A stochastic network experiment here would be much more impressive though.  The following experiments show lower variance and higher ELBO when training.  I appreciate the discussion of REBAR not directly resulting in improved held-out performance; this is something optimization-focused papers often leave out or fear to include. 

Presentation:  I find the presentation of the method to be the paper’s most noteworthy deficiency.  This is not all the authors’ fault for I recognize the page limit presents difficulties.  However, I think the authors push too much information into the appendix, and this hurts the paper’s ability to stand alone.  Without the appendix, REBAR’s derivation is quite hard to follow, I found.  I would prefer the connection to MuProp be pushed into the Appendix and more explanation included in the REBAR derivation.

Conclusions:  I found this paper interesting and novel and expect it’ll have significant impact given the difficulties with REINFORCE and other MC gradient estimators for discrete rvs.  Thus, I recommend its acceptance.  My only complaint is that the method is hard to understand without the appendix, hurting the paper as a stand-alone read. ","This paper introduces a control variate technique to reduce the variance of the REINFORCE gradient estimator for discrete latent variables. The method, called REBAR, is inspired by the Gumble-softmax/Concrete relaxations; however, in contrast to those, REBAR provides an unbiased gradient estimator. The paper shows a connection between REBAR and MuProp. The variance of the REBAR estimator is compared to state-of-the-art methods on sigmoid belief networks. The paper focuses on binary discrete latent variables.

Overall, I found this is an interesting paper that addresses a relevant problem; namely, non-expensive low-variance gradient estimators for discrete latent variables. The writing quality is good, although there are some parts that weren't clear to me (see comments below). The connections to MuProp and Gumble-softmax/Concrete are clear.

Please find below a list with detailed comments and concerns:

- The paper states several times that p(z)=p(z|b)p(b). This is confusing, as p(z|b)p(b) should be the joint p(z,b). I think that the point is that the joint can be alternatively written as p(z,b)=p(b|z)p(z), where the first term is an indicator function, which takes value 1 if b=H(z) and zero otherwise, and that it motivates dropping this term. But being rigorous, the indicator function should be kept. So p(b|z)p(z)=p(z|b)p(b), and when b=H(z), then p(z)=p(z|b)p(b). I don't think the derivations in the paper are wrong, but this issue was confusing to me and should be clarified.

- It is not clear to me how the key equation in the paper was obtained (the unnumbered equation between lines 98-99). The paper just reads ""Putting the terms together, we arrive at"", but this wasn't clear to me. The paper would definitely be improved by adding these details in the main text or in the Appendix.

- In the same equation, the expectation w.r.t. p(u,v), where u, v ~ Uniform(0,1) is also misleading, because they're not independent. As Appendix 7.5 reads, ""a choice of u will determine a corresponding choice of v which produces the same z"", i.e., p(u,v)=p(u)p(v|u), with p(v|u) being deterministic. In other words, I don't see how ""using this pair (u,v) as the random numbers"" is a choice (as the paper suggests) rather than a mathematical consequence of the procedure.

- The paper states that REBAR is equally applicable to the non-binary case (lines 44-45). I think the paper can be significantly improved by including the mathematical details in the Appendix to make this explicit.

- In the experimental section, all figures show ""steps"" (of the variational procedure) in the x-axis. This should be replaced with wall-clock time to get a sense of the computational complexity of each approach. 

- In the experiments, it would be interesting to see a comparison with ""Local expectation gradients"" [Titsias & Lazaro-Gredilla].

- In the experimental section, do the authors have any intuition on why MuProp excels in the structured prediction task for Omniglot? (Figs. 6 and 7)

- The procedure to optimize temperature (Sec 3.2) seems related to the procedure to optimize temperature in ""Overdispersed black-box VI"" [Ruiz et al.]; if that's the case, the connection can be pointed out.

- The multilayer section (Sec 3.3) seems a Rao-Blackwellization procedure (see, e.g., [Ranganath et al.]). If that's the case, the authors should mention that.

- In line 108, there seems to be a missing 1/lambda term.

- In lines 143-144 and 193, there are missing parenthesis in the citations.

- Consider renaming Appendix 7 as Appendix 1.
"
Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues,"Noga Alon, Moshe Babaioff, Yannai A. Gonczarowski, Yishay Mansour, Shay Moran, Amir Yehudayoff",https://proceedings.neurips.cc/paper/2017/hash/eddb904a6db773755d2857aacadb1cb0-Abstract.html,"The paper presents the first Glivenko Cantelli type submultiplicative bound and uses it to derive learning guarantees for the problem of revenue estimation. 

The paper is well written and the results are certainly not trivial my only reservation with this paper is the fact that the dependence on the confidence parameter \delta is not logarithmic. Normally other relative deviation type bounds achieve a logarithmic rate on the confidence parameter why not in this case? i.e. what makes this problem inherently harder?

I have read the rebuttal from the authors and still recommend for acceptance.","This paper provides a submultiplicative form of the Glivenk-Cantelli via an appropriately generalized Dvoretzky-Kiefer-Wolfowitz type inequality. Nearly matching lower bounds are given, as well as a zero-one law in terms of the finiteness of the first moment. The results appear to be correct and technically non-trivial. The paper will be of broad interest to the NIPS community.","This paper provides a generalization of the Glivenko-Cantelli theorem, a ""submultiplicative"" compromise between additive and multiplicative errors. The two main results go hand in hand, the first dealing with existence of a sufficient index which guarantees submultiplicativity and the second providing an upper bound on such an index in order to provide ease in applying the results. It is clear that only submultiplicative results are possible due to a simple counterexample given. The proofs of the main results are technical, but mathematically clear. 

Throughout the paper, the author(s) familiarity with previous work with generalizing the Glivenko-Cantelli theorem is verified, and the novelty of the work is demonstrated.

The author(s) apply these results to the problem of estimating revenue in an auction via empirical revenues. It is clear that understanding revenue estimation better will lend itself to revenue maximization. Moreover, investigating the connection between finite moments and uniform convergence in this example can offer a framework for exploring other estimators in a similar way.

While the paper is well-written, and I believe the results to be significant and even fundamental, I felt that the example given, while interesting, was quite specialized and detracted somewhat from generality of the paper. Even more detail in the discussion section on the potential impact to the learning theory community would be great.

Additional notes:
The definition of F_n(t) on lines 22 and 23 is hard to read. 
The phrasing on lines 66-68 is awkward. 
Between lines 104 and 105, why not continue with notation similar to that introduced in Theorem 1.3 for consistency?"
Tensor Biclustering,"Soheil Feizi, Hamid Javadi, David Tse",https://proceedings.neurips.cc/paper/2017/hash/ede7e2b6d13a41ddf9f4bdef84fdc737-Abstract.html,"The paper solves the tensor biclustering using tensor (un)folding+spectral clustering. This is an interesting paper and addresses a relevant problem mainly seem in computational biology/genomics and in domains having multiple data modalities. 

a) It would benefit the reader if you can mention in the Introduction section itself that you achieve tensor biclustering via tensor (un)folding followed by spectral clustering. 

b) It took me time to understand Lines 38-43 and it could be that I have misunderstood your intent. Please clarify: We can perform Tensor bi/tri clustering on an asymmetric tensor, right? Given this, could you please explain Line 40. Again, you mention clustering based on similar trajectories. How did you get ‘similar’ trajectories? In triclustering, you get a resultant cuboid in which all the 3 axes are similar based on some similarity measure. From Eq (3), you create a folded tensor but taking into account ‘all’ trajectories. When does the ‘similar’ trajectories kick in? If dimensions of C_2 and C_1 are n2 x n2 and n1 x n1 respectively then how/where are you capturing the m-dimensional similarity structure? 

c) Minor comment - Eq 2: T_(j_1,1). What does 1 denote? ","
	This paper studies the tensor biclustering problem, which is
	defined as follows: given an order-3 tensor that is assumed to
	be of rank 1 with additive Gaussian noise, we want to find
	index sets along with mode 1 and mode 2 where the factor
	vectors have non-zero entries. The paper considers several
	methods and analyzes their recovery performance. The
	statistical lower bound (i.e. the best achievable performance)
	is also evaluated. The performance is also evaluated by experiments. 

	The paper is clearly written, self-contained, and easy to follow.

	Overall, this paper aims to develop a relatively minor
	problem, tensor biclustering, theoretically in depth, and it
	seems to be succeeded. For computationally efficient methods
	(the proposed algorithms), the asymptotic order of variance
	where the non-zero indices are correctly recovered is
	identified. In addition, the optimal bound of performance is
	evaluated, which is helpful as a baseline. 

	Still, I feel there is room for improvement. 
	
	1. The model formulation (1) looks less general. The strongest
	restriction may be the setting q=1. Although the paper says
	the analysis can be generalized into the case q>1 (Line 83),
	it is not discussed in the rest of paper. For me, the analysis
	for q>1 is not trivial. It is very helpful if the paper
	contains discussion on, for q>1, how we can adapt the
	analysis, how the theoretical results change, etc.

	2. Synthetic experiments can be elaborate. I expected to see
	the experiments that support the theoretical results
	(e.g. Theorems 1 and 2). For example, for various n and
	\sigma_1^2, you can plot monochrome tiles where each tile
	represents the recovery rate at corresponding n and \sigma_1^2
	(e.g. the tile is black if the rate is 0 and is white if the
	rate is 1), which ideally shows the phase transition that the
	theorems foreshow.
	
      "
On the Model Shrinkage Effect of Gamma Process Edge Partition Models,"Iku Ohama, Issei Sato, Takuya Kida, Hiroki Arimura",https://proceedings.neurips.cc/paper/2017/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html,"This paper presents several variants of GP-EPM to solve the shrinkage problem of GP-EPM for modelling relational data. The proposed models are demonstrated to have better link prediction performance than GP-EPM by estimating the number of latent communities better. 

Note I always point out this problem to my students as:  the model is
""non-identifiable"".   Don't get me wrong, the whole body of work,
EPM, etc., is fabulous, but identifiability was always an important lesson in class.

The paper is well-written and easy to follow. The proposed models are well-motivated empirically with synthetic examples and then with theoretical analysis. To me, the main contribution of the paper is it points out that the obvious unidentifiability issue in GP-EPM can, interestingly, be a problem in some real cases. The proposed CEPM is just a naive model, while DEPM is a simple but intuitive solution and DEPM to IDEPM is like mapping the Dirichlet distribution to Dirichlet process. The solutions are not novel but straightforward to the problem. Finally the experimental results support the main claims, which makes it an interesting paper.

Note, in 4.3 (line 187), you say ""one remarkable property"".  This *should* be well known in the non-parametric community.  The result is perhaps the simplest case of the main theorem in ""Poisson Latent Feature Calculus for Generalized Indian Buffet Processes"", Lancelot James, 2014.  The terms in $K_+$ in (5) are the marginal for the gamma process.  The theorem can be pretty much written down without derivation using standard results.

This is a good paper cleaning up the obvious flaws in EPM and its implementation. It covers good though routine theory, and experimental work.  Section 4.3 should be rewritten.

However, I belief the important issue of identifiability should be better discussed and more experiments done.  For instance, a reviewer points out that the problem is partially overcome more recently using stronger priors.  Clearly, identifiability is something we require to make theory easier and priors more interpretable.  It is not strictly necessary, though it is routinely expected in the statistics community.
Anyway, would be good to see more discussion of the issues and further experimental investigation.
      ","This paper introduces refinements to edge partition model (EPM), which is a nonparametric Bayesian method for extracting latent overlapping features from binary relational data. The authors use constraints over node-feature association distributions in order to improve the shrinkage effect in the EPM. The following concerns need to be addressed in my opinion:

I am not completely convinced about the argument made in section 3 about the source of poor shrinkage in EPM, where the main reason has been stated as arbitrary values for hyperparameters of feature distributions. Can we simply overcome this by assuming narrow priors? On the other hand, for CEPM, the constants $C_1$ and $C_2$ have been set equal to dimensions for fair comparison. However, I assume the amount of shrinkage will depend on their choice in practice. How do you optimally set them?

Regarding DEPM and its infinite version, I think it's a good idea to use Dirichlet distributions for features. However, it's not clear for me how much of performance improvement of DEPM compared to EPM in link prediction is due to shrinkage improvement and not resolving unidentifiability problem using Dirichlet distributions. 

Finally, while it is clear based on the experiments that EPM has the least shrinkage among the methods, it would be nice if this problem was illustrated on a simulated dataset, were the real number of latent factors is known.","The paper proposes some refinement to the infinite edge partition model (EPM) that may suffer from relatively poor shrinkage effect for estimating the number of network components. The paper is an easy read, the observations about this particular limitation of EPM are correct and the experiments are well designed. However, the paper has the following limitations which must be addressed:

1. Please use different notations for ""a."" and ""b."" in the sentence on line 80 and for ""m_{i,k}"", ""m_{j,k}"" on line 203. 
2. The observations about the shrinkage are correct in the paper. However, by imposing priors on the parameters of the gamma distribution, some of these undesired behaviors can be avoided. For example, please review the following papers and the experimental setup therein:
-- http://jmlr.org/papers/v17/15-633.html
-- https://link.springer.com/chapter/10.1007%2F978-3-319-23528-8_18
-- https://mingyuanzhou.github.io/Papers/DLDA_TLASGR_v12.pdf
Perhaps, a more detailed experimental analysis would reveal that the problem with shrinkage is not that critical and can be mitigated.
3. Moreover, several papers have already used the Dirichlet priors to get around this specific problem of shrinkage and identifiability. For example, please review the following paper:
-- https://www.cse.iitk.ac.in/users/piyush/papers/topic_kg_aistats.pdf
4. The infinite DEP model is novel. However, the derivations in Section 4.3 are related to the usage of Dirichlet-multinomial distribution. So while the introduction of the collapsed sampler is novel, the derivation of the same may not be considered that novel.
5. Please fix the typo ""an another important theorem..""."
Estimating Mutual Information for Discrete-Continuous Mixtures,"Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",https://proceedings.neurips.cc/paper/2017/hash/ef72d53990bc4805684c9b61fa64a102-Abstract.html,"This paper deals with estimation of the mutual information of two measures under very weak assumptions, allowing for continuous, discrete and mixture of continuous and discrete distributions. The estimator is based on comparing the probability masses of Euclidean balls with identical radii for the three measures (namely, P_{XY}, P_X and P_Y), where the common radius is chosen based on the distance of the k_th nearest neighbor of the pair (X,Y). 

Consistency of the estimator is proven. It is argued that the estimator allows for more general distributions than previous attempts, especially those based on three differential entropies.

The proofs are technical but seem to be correct and the paper is well written, offering a very elegant procedure under natural and general assumptions. I recommend its acceptance at NIPS.","Summary: This paper proposes an estimator for the mutual information of a pair (X,Y) of random variables given N joint samples. While most previous work has focused separately on the purely discrete and purely continuous methods (with significantly different methods used in each setting), the novel focus here on the case of combinations or mixtures or discrete and random variables; in this case, the mutual information is well-defined but cannot always be expressed in terms of entropy, making most previous estimators unusable. The proposed estimator generalizes the classic KSG estimator by defining the behavior at samples points with k-NN distance 0. The L_2 consistency of the estimator is proven, under a sort of continuity condition on the density ratio that allows the existence of atoms, and suitable scaling of k with N. Finally, experimental results on synthetic and real data compare the proposed estimator to some baseline estimators based on perturbing the data to make it purely discrete or continuous.

Main Comments:

This paper addresses a significant hole that remains in the nonparametric dependence estimation literature despite significant recent progress in this area; many current estimators are difficult or impossible to use with many data sets due to a combination or mixture of discrete and continuous components. While the proposed estimator seems like a fairly simple correction of the KSG estimator, and the consistency result is thus likely straightforward given the results of [16], the empirical section is strong and I think the work is of sufficient practical importance to merit acceptance. The paper is fairly well-written and easy to read.

Minor Comments/questions:

1) As far as I understand 3H estimators can work as long as each individual dimension of the variables is fully discrete or full continuous, but fail if any variable is a mixture of the two (i.e., if an otherwise continuous distribution has any atoms). It might be good to clarify this, and also to compare to some 3H estimators (e.g., in experiment II).

2) Lines 39-42: Perhaps there is a typo or I am misreading something, but it isn't clear to my why the given definition of X isn't entirely discrete (it appears to be supported on {0,1,2,...}).

3) Lines 129-130: could be probably clearer ""the average of Radon-Nikodym derivative"" should be ""the average of the logarithm Radon-Nikodym derivative of the joint distribution P_{XY} with respect to the product distribution P_XP_Y.""

4) As discussed in Appendix A, a key observation of the paper is that, to be well-defined, mutual information requires only the trivial condition of absolute continuity of the joint distribution w.r.t. the marginal distribution (rather than w.r.t. another base measure, as for entropy). I think it would be worth explicitly mentioning this in the main paper, rather than just in the Appendix.

5) Although this paper doesn't study convergence rates, could the authors conjecture about how to quantify parameters that determine the convergence rate for mixed discrete/continuous variables? For continuous distributions, the smoothness of the density is usually relevant, while, for discrete distributions, the size of the support is usually relevant. However, mixture distributions are discontinuous and have infinite support.

6) Theorem 1 assumes that k -> infinity as N -> infinity. Can the authors conjecture whether the proposed estimator is consistent for fixed values of k? I ask because of recent work [4,16,43] showing that the KL and KSG estimators are consistent even for fixed k.

7) For some reason, the PDF is encoded such that I cannot search, highlight, or copy text. Perhaps it was converted from a postscript? Anyhow, this made the paper significantly more difficult to review.


#### AFTER READING AUTHOR REBUTTAL ####
""H(X,Y) is not well-defined as either discrete entropy or continuous entropy"" - I think ","Estimating mutual information has plenty of applications and have received renewed attention in recent years. In this paper, the authors focus on a particular case of estimating mutual information when the distributions are mixtures consisting of discrete and continuous components. The authors argue that this special case shows up in various applications, and point out that existing estimators work very badly in these cases, if they work at all. The proposed estimator works by identifying discrete points (by checking if the exact same point appeared k times) and uses a plug in estimator of the Radon-Nikodym derivative at these points. At continuous points, the estimator approximates the densities by considering how many samples lie within a ball of an appropriate radius (as suggested by the distance of the k-the nearest neighbour), and then estimates the Radon-Nikodym derivative at these point by using a KSG-like estimator. The two results are combined to arrive at the estimated mutual information. The authors show consistency of the proposed estimator under some conditions. The paper proposes a novel estimator, analyzes it theoretically, and also has compelling simulations."
Reconstructing perceived faces from brain activations with deep adversarial neural decoding,"Yağmur Güçlütürk, Umut Güçlü, Katja Seeliger, Sander Bosch, Rob van Lier, Marcel A. J. van Gerven",https://proceedings.neurips.cc/paper/2017/hash/efdf562ce2fb0ad460fd8e9d33e57f57-Abstract.html,"This paper presents a very interesting work of reconstructing a visual stimuli from brain activity, measured by fMRI, in a generative adversarial network (GAN) framework. Overall, the proposed idea is clearly described and the exhaustive experiments and analysis well supports the idea.

However, there are still some points that should be clarified.

- Eq. (5): The notations of ${\bf B}$ and $\beta_{-}$ are not defined.

- In Eq. (6), from line 1 to line 2, to this reviewer’s understanding, it utilizes the relations of $y=B^{T}z$, $B^{-1}B=I$, and $B^{T}(B^{T})^{-1}=I$. However, as specified in the line 83 of the same page, $B\in\mathbb{R}^{p\times q}$, i.e., asymmetric. Hence, it’s necessary to explain how the relation between line 1 and line 2 in Eq. (6) holds.

- In “fMRI dataset”, it is not clear what values from the deconvolve fMRI signals were used as the response $y$. What is the dimension of $y$?

- Table 1: As reference, it would be informative to provide the upper limit of the three metrics obtained by the latent feature $z$ obtained by $\phi(x)$.","The authors propose a brain decoding model tailored to face reconstruction from BOLD fMRI measurements of perceived faces.

There are some promising aspects to this contribution, but overall in its current state there are also a number of concerning issues.

Positive points:
- a GAN decoder was trained on face embeddings coming from a triplet loss or identity-predicting face embedding space to output the original images. Modulo my inability to follow the deluge of GAN papers closely, this is a novel contribution in that it is the application of the existant imagenet reconstruction GAN to faces. This itself may be on the level of a workshop contribution.

- The reconstructed faces often coincide in gender and skin color with the original faces, indicating that the BOLD fMRI data contained this information. Reconstructing from BOLD fMRI is a very hard task and requires good data acquisition. The results indicate that a good job has been done on this front.

- the feature reconstruction numbers indicate that using a GAN for face reconstruction is a step forward with respect to other (albeit much older and inherently weaker) methods.

Points of concern:
No part of the reconstruction method proposed seems novel (as claimed) and validation is shaky in parts.

- The Bayesian framework seems superfluous. The reconstruction is performed using ridge regression with voxel importance weighted by inverse noise level. The subsection of deriving the posterior can be cut. [Notational quirk: If I understand correctly, B is not necessarily square nor necessarily invertible if it were. If the pseudoinverse is meant, it should be written with a different symbol, e.g. +]

- No reason is given as to why PCA is performed on the face embeddings and how the number of components was chosen. (If the full embedding had been chosen, without PCA, would the reconstructions of embeddings look perfect? Is the application of PCA the ""crucial aspect that made this work""? If so, please elaborate with experiments as to why this could be - projecting properly into the first two (semantically meaningful) axes does seem like an important thing to be able to do and could add robustness.

- No information is provided as to which voxels were used for reconstruction. Were they selected according to how well they are predicted? If so, was the selection explicit or implicit by using the inverse noise variance weighting? If explicit (e.g. noise level cutoff), how many were selected and according to which cutoff criterion? Were these voxels mostly low-level visual or high-level visual? Could a restriction to different brain areas, e.g. known face-selective vs areas of the low-level visual hierarchy be used and evaluated?
This question is not only neuroscientifically relevant. It would provide insight into how the decoding currently works.

- The evaluation of the reconstruction seems to hang in the void a bit, as well as its comparison with other (incorrigibly worse) methods.

The GAN that was trained outputs faces. It was trained to do this and it seems to do it well. This means that the baseline correlation of *any* output of the GAN will have a relatively high correlation with any face due to alignment of features. The permutation test of significance is appropriate and this outcome should be highlighted more (how about comparing to reconstructions from random vectors also?). A histogram of correlation scores in supplementary material would also provide insight.

The comparison against the other methods seems a bit unfair. These other methods are (in modern view) clearly ""all-purpose hacks"", because there was not much else available to do at the time. Nobody expects eigenfaces to do well in any measure, nor does anybody expect mean and some restricted form of (diagonal?) covariance to be sufficient to model natural image data. These methods are thus clearly lacking in expressiveness and could never yield as good reconstructions as the neural network model. There are other face representations using active appearance modeling that could have been tried, which perform much better reconstruction. (No fMRI reconstruction has been attempted with these and thus it could be an interesting extra comparison.)
This outcome is good for the vggface/GAN couple in terms of face representation and reconstruction (and as stated, this may be worth writing about separately if it hasn't been done before).
It is however only the increase in feature decoding accuracy that shows that the representation is also slightly better for BOLD data (as opposed to reconstruction accuracy).

However, looking at some of the eigenface/identity reconstructions, they also sometimes get core aspects of the faces right, so they are not doing that badly on certain perceptual axes.

Lastly, why is it that the GAN reconstruction gets to have a ""best possible reconstruction from this embedding"" column, whereas eigenfaces and identity reconstruction do not get this privilege? Why does Table 1 not have the equivalent ratios for the other methods?

- The title of this contribution is wildly inappropriate and must be changed to reflect the content of the work. It would be less unacceptable if there had also been a reconstruction from general natural images to parallel the face reconstruction, or if there had been several other restricted data types. In the current state the title must contain sufficient information to infer that it revolves around face reconstruction and nothing else.

- Other possible axes of comparison: What about neural image representations trained on imagenet, such as Alexnet convolutional layers? How well does a GAN reconstruction from those do on faces? What about trying the face representation and reconstruction of the original DCGAN paper?

All in all, the contribution is significant but could be much more so, if more experiments had been made.





","Summary:  The authors deploy an adverserial training process to improve decodability of faces from brain data.

Reaction:
In general, I don't have a lot to say about this paper, because it seems quite straightforward.  It seems like a solid implementation of a nice idea.  The explication is very clear, and the results are reasonably cleanly quantified.  Obviously the results are imperfect (as can be seen from Table 1 and Fig 2) but they seem to be a clear improvement over other approaches.    

One minor confusion: the text describes the results of Table 1 as follows: ""Table 1 shows three reconstruction accuracy metrics for both subjects in terms of the ratio of the reconstruction accuracy from the latent features to the reconstruction accuracy from brain responses.""  Isn't that reversed?  Shouldn't it read: ""Table 1 shows three reconstruction accuracy metrics for both subjects in terms of the ratio of the reconstruction accuracy from the brain responses to the reconstruction accuracy from latent features.""?   After all, the brain responses seem to construct significantly less good responses than the original latent features.  

I have a few concerns:

(1) The set of faces used for testing seems rather limited -- eg. only faces from a single face-on angle, at a standard size and central position.   How well would this same technique work on faces with more variation, e.g. at various rotations?  If the technique become significantly less accurate, would that make the results less impressive?  Also, it would interesting to see if the network could learn to produce the head-on image of a face seen at a non-standard angle.  I understand that addressing this concern is probably not immediately possible, because of a limitation of the underlying neural dataset.  

(2) It's not clear that fig 3 or 4A add much to the overall story.   I know that with variational latent models interpolations and feature tilings are often computed.   Fig 3 shows that the model has the same apparent qualitative interpretability for latent feature dimensions as many other such models, and is not a surprise therefore.  Fig 4A shows qualitatively the interpolation is smooth-ish both for the latent model and (kind of) for brain responses.   I'm not sure what is really added by this about brain decodability by these analyses.   How much ""better"" are these interpolations that simple pixel morphing?     (And on what axes of betterness?)  In general I find such interpolation illustrations hard to put into context (as to how ""good"" such interpolations are) even in ""usual"" pure machine-learning adversarial generation papers, and that doubly applies here.  

(3) Fig4B is more interesting, showing that reasonable and diverse samples of faces arise from sampling response priors of brain data.   That's cool. I wish it was better quantified.  In particular, I was some kind of quantitative analysis of the diversity and quality of these samples was developed, and then a comparison on such metrics made between results from their model and other approaches to brain decoding (e.g. the ones tested in Fig 5).    I'd much rather see this comparison than the stuff in Fig 3 and 4A.  "
An inner-loop free solution to inverse problems using deep neural networks,"Kai Fan, Qi Wei, Lawrence Carin, Katherine A. Heller",https://proceedings.neurips.cc/paper/2017/hash/f016e59c7ad8b1d72903bb1aa5720d53-Abstract.html,"An interesting paper that solves linear inverse problems using a combination of two networks: one that learns a proximal operator to the signal class of interest, and the other that serves as a proxy for a large scale matrix inversion. The proximal operator is reusable whenever the signal domain is unchanged. One would need to retrain only the matrix inversion network when the underlying problem is changed. This is a significant advantage towards reusability of the training procedure.

Strengths
+ Novelty 
+ A very interesting problem

Weaknesses
- An important reference is missing
- Other less important references are missing
- Bare-bones evaluation

The paper provides an approach to solve linear inverse problems by reducing training requirements. While there is some prior work in this area (notably the reference below and reference [4] of the paper), the paper has some very interesting improvements over them. In particular, the paper combines the best parts of [4] (decoupling of signal prior from the specific inverse problem being solved) and Lista (fast implementation). That said, evaluation is rather skim — almost anecdotal at times — and this needs fixing. There are other concerns as well on the specific choices made for the matrix inversion that needs clarification and justifications.

1) One of the main parts of the paper is a network learnt to invert (I + A^T A). The paper used the Woodbury identity to change it to a different form and learns the inverse of (I+AA^T) since this is a smaller matrix to invert. At test time, we need to apply not just this network but also ""A"" and ""A^T"" operators.

A competitor to this is to  learn a deep network that inverts (I+A^T A). A key advantage of this is that we do not need apply A and A^T during test time. It is true that that this network learns to inverts a larger matrix ... But at test time we have increased rewards. Could we see a comparison against this ? both interms of accuracy as well as runtime (at training and testing) ?

2) Important reference missing. The paper is closely related to the idea of unrolling, first proposed in, “Lista”
http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf

While there are important similarities and differences between the proposed work and Lista, it is important that the paper talks about them and places itself in appropriate context. 

3) Evaluation is rather limited to a visual comparison to very basic competitors (bicubic and wiener filter). It would be good to have comparisons to 
- Specialized DNN: this would provide the loss in performance due to avoiding the specialized network.
- Speed-ups over [4] given the similarity (not having this is ok given [4] is an arXiv paper)
- Quantitative numbers that capture actual improvements over vanilla priors like TV and wavelets and gap to specialized DNNs.


Typos
- Figure 2: Syntehtic 
","This paper proposes an efficient method for applying ADMM to solve image processing problems involving neural networks.  The proposed method avoids solving large matrix inversions directly.

I have a few criticisms of this work:
1) The authors should discuss the relationship between their proximal operators, and those used in 4.  A main contribution of this paper is the use of a neural network to replace the proximal operator, but this was already done in [4], although with different methods.

2) I'm unclear about the details of the synthetic experiments.  It seems like you trained a neural network to replicate the behavior of the shrink operator?  I think?  Also, It's unclear how the runtime of the network implementation is comparable to the shrink operator.

3)  Major weakness of this paper: they don't compare to any other modern methods (they compare to simple things like a Weiner filter, but this is a *really* low bar and easy to beat).  What about other variational deblurring method (like TV or wavelet based methods), and what the methods discussed in [4], which can handle the super-resolution problems posed here.
    I like the ideas in this paper, but the experimental comparisons just seem fairly weak, which is why I can't strongly recommend this paper for publication. 

note:  I mentioned reference [4] several times because of its close relationship to the work under review.  I am not an author on that paper, nor do I have any direct affiliations with the authors or their institutions. ","In recent literature, there are studies showing that the testing-time inference can be approximated by the forward-pass of deep neural network. This paper falls into this category. In particular, two neural networks are proposed to approximate the inner loops of the ADMM arising in the inverse image processing problems (e.g., super resolution and deblurring). Techniques and devises are sound; empirical results are satisfactory. 

There is minor concern about the title, which seems over-claiming/mis-leading: the proposed approach is data-dependent and not really aims at general ADMM numeric optimization. 

Another minor suggestion is that more expositions can be used to justify the cPSDAE (discussions at line 126 and line 206; at least make the contents self-contained here)."
A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control,"Fanny Yang, Aaditya Ramdas, Kevin G. Jamieson, Martin J. Wainwright",https://proceedings.neurips.cc/paper/2017/hash/f0204e1d3ee3e4b05de4e2ddbd39e076-Abstract.html," The paper looks at continuous improvement using a sequence of A/B tests, and proposes instead to implement adaptive testing such as 
      multi-armed bandit problem while controlling the false discovery rate. This is an important problem discussed in statistical literature, but still unsolved.

      The approach proposed in this paper seems to apparently solve the issues. This is a very interesting paper, that despite minor concerns listed below, could lead to a potentially new avenue of research.
      
      Lines 24-37: There are well-known issues, and it would be desirable to add citations. Although authors clearly focus on CS/ML literature, there is also a relevant body of literature in biometrics, see e.g. survey by Villar, Bowden and Wason (Statistical Science 2015), the references therein and the more recent papers citing this survey.
      Line 37: ""testing multiple literature"" -> ""multiple testing literature"" 
      Line 38-47: A similar concept exists in biometrics, called ""platform trials"" - please describe how your concept differs
      Line 112: ""and and"" -> ""and""
      Line 115: please provide reference for and description of LUCB
      Line 153: ""samplesas"" -> ""samples as""
      Line 260: ""are ran"" -> ?
      Line 273: It is not clear what ""truncation time"" is and why it is introduced - it seems to have a huge effect on the results in Figure 2
      Line 288-291: While this motivation is interesting, it seems to be mentioned at an inappropriate place in the paper - why not to do it in the introduction, alongside website management and clinical trials?
      
      ","If no proper correction is applied, the repetition of individually well-grounded high-confidence tests is known to lead irremediably to absurd ""statistically certified"" discoveries.
False discovery rate control (FDR) is an alternative of family-wise test corrections like Bonferroni's which are known to be too conservative.

This paper propose a general framework for repeated A/B/n tests which integrates several concrete ideas:
- introduce a ""best-arm identification with control"" variant of the (epsilon,delta)-PAC best-arm identification problem that we could also name ""better-arm identification"";
- replace inefficient static A/B/n testing procedures by adaptive PAC ""better-arm"" algorithms;
- propose a variant of the LUCB algorithm for this purpose;
- integrate anytime p-values calculus for continuous FDR control.

The proposed ""better-arm identification"" algorithm is analyzed in section 3.2.
The proposed meta-procedures are analyzed and shown to guarantee this FRD control.
Some experiments are provided at the end of the paper.

At first sight I was a bit surprised by this strongly application-oriented cross-discipline paper, but I really liked the fresh ideas and the concrete perspectives they give for MABs both on the applicative and theoretic grounds.

Typo:
l37 ""testing multiple"" -> ""multiple testing ""
l153 ""samplesas"" -> ""samples as""

","The paper studies sequential hypothesis testing problems, where the decision maker processes hypotheses sequentially (as opposed to processing them jointly). The objective is to control false discovery rate (FDR) or modified FDR (mFDR), and also keep the best arm discovery rate (BDR) high. 

The proposed algorithm is based on a combination of pure-exploration multi-armed bandit (MAB) methods and results from online FDR control. Using an appropriate pure exploration method ensures that BDR is high, while the online FDR method ensure low FDR. The main contribution of the paper is an appropriate combination of these two components that guarantees simultaneous control of FDR and BDR. 

Overall, the paper is well-written and the ideas are very clear and easy to follow. The technical contribution is interesting and relevant to NIPS community. 


"
Interactive Submodular Bandit,"Lin Chen, Andreas Krause, Amin Karbasi",https://proceedings.neurips.cc/paper/2017/hash/f0935e4cd5920aa6c7c996a5ee53a70f-Abstract.html,"The papers builds on CGP-UCB. Although one can apply CGP-UCB to a submodular set function of a growing set of selected actions, the original notion of regret is about individual actions of adding elements to the sets. The paper manages to express the theoretical results for CGP-UCB for a notion of regret that is meaningful for the interactive submodular setting, comparing the final values of each set to the (1 - 1/e) approximation guarantee of greedy item selection. Similarly, the main assumption for the regret bound is expressed in terms of the marginal gains for the submodular functions in a RKHS. Experimental results are provided on a variety of problems.

The paper is well written and generalizes upon multiple recent works. This appears to have many practical applications.

Regret is not explicitly mentioned in the experiments section. On some of them, \sqrt{T} regret is not too obvious as the gap between greedy and SM-UCB appears constant. This may be a small-sample effect. There is also no exploration on the influence of the kernel on the performance in those datasets (e.g., how ""low"" is actually the norm of the marginal gain functions with respect to the reproducing kernel in those cases? Does this correlate with better performance in practice?)

The paper repeatedly mentions that the proposed method outperforms all prior art. This is a bit misleading, as the only such baseline is ""random subset selection"". Other baselines are introduced by dumbing-down the proposed method, although this does not quite qualify as prior art. 

Remarks

L48: ""of the different between""
L226: ""Thr rows""
","        Pros:

        - The paper combines submodular function maximization with contextual bandit, and propose the problem of interactive submodular bandit formulation and SM-UCB solution. It includes several existing studies as special cases.

        - The empirical evaluation considers several application contexts and demonstrates the effectiveness of the SM-UCB algorithm.

        Cons:

        - The discussion on the key RKHS condition is not enough, and its applicability for actual applications is unclear. More importantly, there is no discussion about constant B for the RKHS norm for various applications, and also the maximum information gain \gamma_t, but both of them are needed in the algorithm.

        - The technical analysis is heavily dependent on prior study, in particular [33]. The additional technical contribution is small.




        The paper studies a contextual bandit learning setting, where in each round a context \phi is presented, which corresponds to an unknown submodular set function f_\phi. The user needs to select one item x to add to the existing selected set S_\phi, with the purpose of maximizing the marginal gain of x given S_\phi. After selecting x, the user obtains a noisy feedback of the marginal gain. The regret is comparing against the (1-1/e) fraction of the optimal offline solution, when all the submodular functions for all contexts are known. The key assumption to make the learning work is that the marginal contribution \Delta in terms of the subset S, the new item x, and the context \phi, has a bounded RKHS norm. Intuitively, this norm makes sure that similar contexts, similar subsets, and similar item would give similar marginal results. 

        The authors propose the SM-UCB algorithm to solve the problem. The proposed algorithm and its regret analysis is heavily based on the prior work on contextual Gaussian process bandit optimization [33]. In fact, it looks like all the authors need to do is to treat existing subset S already selected as part of the context, and then plugging it into the algorithm and the analysis of [33]. The proof of Theorem 1 given in the supplementary material is only to connect their current regret definition with the regret definition of [33], using the submodularity property. Then they just apply Theorem 1 of [33] to obtain the result. Therefore, from the technical point of view, the paper's new contribution is small.

        [Note: After reading the authors' feedback, I get a bit clearer understanding, in that the difference in model from [33] seems to be that the current action may affect future contexts. However, I am still not quite sure how much the analysis differs from [33], since there is only a two-and-a-half page analysis in the supplementary material, and it mostly relies on [33] to achieve the main result. Please clarify the technical contribution of the current paper, and what are the novelty in the analysis that differs from [33]]

        Thus, the main contribution of the paper, in my view, is to connecting submodular maximization with prior work on contextual bandit and give the formulation and the result of interactive submodular bandit problem. Their empirical evaluation covers several application cases and demonstrate advantages of the AM-UCB algorithm, giving readers some concrete examples on how to apply their general results in different contexts. 

        However, one important thing the authors did not discuss much is the key assumption on the bounded RKHS norm. First, RKHS norm should be better defined and explained. Second, its implication on several application contexts should be discussed. Third, the choice of kernel functions should be discussed. More importantly, the bound B should be evaluated for the applications they discuss and use in the experiment section. This is because this complexity measure B is not only used in the regret bound, but also appears as part of \beta_t in the algorithm. Similar situation occurs for the maximum information gain paramter \gamma_t. The authors do not discuss at all how to obtain these parameters for the algorithm. Without these parameters, it is not even clear how the authors evaluate their SM-UCB algorithm in the experiments. This is a serious ommission, and the readers would not be able to apply their algorithm, or reproduce their experiments without knowing how to properly set B and \gamma_t. 

        Furthermore, In the empirical evaluation section, the authors use linear kernel and Jaccard kernel, but why these are reasonable ones? What are their implications to the application context? What are the other possible kernels to use? In general, I do not have a good idea how to pick a good kernel function for an application, and what are the impact and limitations of this assumption. I think this should be the key contribution of the paper, since the other technical contribution is minor, but the authors fail to provide satisfactory answers in this regard.

        Other minor comments:

        - line 133, ""T_i"" --> ""T_j""

        - line 164, I(y_A | \Delta) --> I(y_A; \Delta)

        - Related work

        It would be nice if the authors could compare their approach with other related approaches combining multi-armed bandit with combinatorial optimization tasks, such as

        a) combinatorial multi-armed bandit with application to influence maximization and others: Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms Wei Chen, Yajun Wang, Yang Yuan, Qinshi Wang, in Journal of Machine Learning Research, 2016
        b) Online influence maximization. Siyu Lei, Silviu Maniu, Luyi Mo, Reynold Cheng, and Pierre Senellart. In KDD, 2015.
        c) Stochastic Online Greedy Learning with Semi-bandit Feedbacks. Tian Lin, Jian Li, Wei Chen, In NIPS'2015 


","This paper proposes a general framework for interactive submodular optimization with bandit feedback.  Specifically, the algorithm must maximize over a collection of submodular functions.  At each iteration, the algorithm receives context corresponding to which submodular function it is currently maximizes.  The algorithm selects an item to add the set for that function, and receives a noise corrupted feedback corresponding to the marginal gain.  Furthermore, one assumes a kernel that captures the correlations between (context, set, action) pairs, and the regret bounds depend on maximum information gain of this kernel.

One uncertainty I have about this submission is its overall novelty.  What do the authors view as the novel part of their contribution?  Is it the unified framework?  Is it the analysis?  From my vantage point, the unified framework, while nice, is not that surprising.  The proof has some surprising elements to it, namely that one can allow the future feedback to be influenced by the current chosen actions.  That was not entirely obvious (to me) before.

A minor point -- I don't think the proposed approach fully generalizes the Linear Submodular Bandits setting. In that work, the algorithm must select L items before receiving any feedback, and the proposed framework doesn't deal with such delayed feedback.  However, I think the generalization is straightforward.  

The above minor point also accentuates my uncertainty about the novelty of the contribution.  Each way that the proposed framework generalizes previous work is, in my opinion, relatively incremental. One can keep adding more pieces to the framework and claim a broader generalization, but it's not clear that this paper shifts the way people think about research in this direction.  I'm happy for the authors to correct my misconception in this regard, if indeed I am being shortsighted here.

The notation is kind of sloppy.  For example, u_i is not formally defined.  The actual iterative procedure is also not well specified.  For example, what is the step-by-step sequence of interactions between the algorithm and the environment?  Does it first receive a u_i, then a phi_i, then chooses an item x?  One can sort of infer this from the algorithm pseudo-code, but it would've been much clearer to formally define the problem interaction setup.


** RESPONSE TO AUTHOR FEEDBACK **

I thank the authors for their response.  For revising the paper, I think the comparison w.r.t. [33] should be made more explicit in terms of the exact technical contributions"
Hash Embeddings for Efficient Word Representations,"Dan Tito Svenstrup, Jonas Hansen, Ole Winther",https://proceedings.neurips.cc/paper/2017/hash/f0f6ba4b5e0000340312d33c212c3ae8-Abstract.html,"This work describes an extension of the standard word hashing trick for embedding representation by using a weighted combination of several vectors indexed by different hash functions to represent each word. This can be done using a predefined dictionary or during online training. The approach has the benefit of being easy to understand and implement and greatly reduces the number of embedding parameters.

The results are generally good and the approach is quite elegant. Additionally, the hash embeddings seem to act as an effective regularizer. However, table 2 would benefit from describing the final selected vocabulary sizes as well as the parameter reduction provided by the hash embedding. Table 3 is also missing a joint state of the art model for the DBPedia dataset [1].

Line 255 makes the claim that the ensemble would train in the same amount of time as the single large model. However, this would only be true if each model in the ensemble had an architecture with fewer weights (that were not embedding weights). From the description, it seems that the number of non-embedding weights in each network in the ensemble is the same as that in the large model so that training time would be significantly larger for the ensemble.

Table 3 highlights the top 3 best models, however, a clearer comparison might be to split the table into embedding only approaches vs RNN/CNN approaches. It would also be interesting to see these embeddings used in the more context-sensitive RNN/CNN models.

Minor comments:
L9: Typo: million(s)
L39: to(o) much
L148: This sentence is awkwardly rewritten.
L207: What is patience?
L235: Typo: table table
Table 4: Were these obtained through summing the importance weights? What were the order of the highest/lowest weights?
[1] Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow. ""Virtual adversarial training for semi-supervised text classification."" ICLR 2017.","This paper uses hashed embeddings to reduce the memory footprint of the embedding parameters. Their approach is simple, where they learn a vector of importance parameters, one for each component vectors. Both the trainable matrices: the matrix of importance parameters and tThe embedding matrix of component vectors is much smaller than a regular embedding matrix. 

To obtain an embedding for a word, they first hash the token id into row of the importance parameters and then hash each component of the importance parameters into the component vector embedding. 

Their results show that just using bow models, their ensemble of  hash embeddings do better than previous bow models on classification tasks. This is a simple yet effective approach to decrease the number of parameters in the model and can also serve as a regularizer. 

The authors should point out the compute time required for inference and if hashing is more expensive than an embedding gather on the GPU. ","This paper proposes hash embeddings to obtain vector representations of words using hashing tricks. This model uses a set of hash functions to map a word to a small set of shared embedding vectors. This technique enables to reduce the cumulative number of parameters and to avoid difficulties the conventional word embedding model have. This model is simple, easy to implement, and achieves state-of-the-art experimental results.

First of all, the question I have is about the difficulty of training hash embeddings. This model has to train both a set importance weights on the hash function and shared embeddings. In my intuition, it makes the training difficult to obtain a reasonable local minimum because these two parameters affect each other. If possible, it is better to show how robust this model is regarding initialization points and hyper-parameters, and give a good intuition about how to find a good local minimum.

The second question is the relationship between ""discourse atoms"" [1] and shared embedding trained from hash embeddings. ""discourse atoms"" also try to reconstruct word embeddings by a linear combination of a small set of atom vectors. As a result, some atoms could capture higher concepts about words (like ontology), and I guess these characteristics can also be captured in shared embeddings in hash embeddings. Do the authors have some insights about the relationship between these ""atoms"" and hashed representations.

[1] Linear Algebraic Structure of Word Senses, with Applications to Polysemy https://arxiv.org/abs/1601.03764
"
Learning Low-Dimensional Metrics,"Blake Mason, Lalit Jain, Robert Nowak",https://proceedings.neurips.cc/paper/2017/hash/f12ee9734e1edf70ed02d9829018b3d9-Abstract.html,"The paper presents theoretical work towards learning low-dim/low rank and sparse metrics, deriving the minimax bounds on the generalization error and shows sample complexity based on ‘d' and ‘p'. 
The paper is well written and revolves around/builds upon \ref 2 and \ref 3 whilst filling the gaps and providing more in-depth theoretical justifications specific to low-rank and sparse metrics. 

Section 2.3 provides identification bounds but what are the authors thoughts on trying out, say for example, the Bregman divergence for comparing K and K^*? 

  ","Summary of the paper:
The paper considers the problem of learning a low-rank / sparse and low-rank Mahalanobis distance under relative constraints dist(x_i,x_j) < dist(x_i,x_k) in the framework of regularized empirical risk minimization using trace norm / l_{1,2} norm regularization. The contributions are three theorems that provide 1) an upper bound on the estimation error of the empirical risk minimizer 2) a minimax lower bound on the error under the log loss function associated to the model, showing near-optimality of empirical risk minimization in this case 3) an upper bound on the deviation of the learned Mahalanobis distance from the true one (in terms of Frobenius norm) under the log loss function associated to the model.


Quality:
My main concern here is the close resemblance to Jain et al. (2016). Big parts of the present paper have a one-to-one correspondence to parts in that paper. Jain et al. study the problem of learning a Gram matrix G given relative constraints dist(x_i,x_j) < dist(x_i,x_k), but without being given the coordinates of the points x_i (ordinal embedding problem). The authors do a good job in explaining that the ordinal embedding problem is a special case of the considered metric learning problem (Section 2.4). However, I am wondering whether such a relationship in some sense holds the other way round too and how it could be exploited to derive the results in the present paper from the results in Jain et al. (2016) without repeating so many things: can’t we exploit the relationship G=X^T K X to learn K from G? At least in the noise-free case, if G=Y^T Y we can simply set K=L^T L, where L solves Y^T = X^T L^T?  

I did not check the proofs in detail, but what I checked is correct. Also, many things can be found in Jain et al. (2016) already.

In Line 16, d_K(x_i,x_j) is defined as (x_i-x_j)^T K (x_i-x_j), but this is not a metric. To be rigorous, one has to take the square root in this definition and replace d_K(x_i,x_j) by d_K(x_i,x_j)^2 in the following. 

In Corollary 2.1.1, the assumption is max_i \|x_i\|^2 = O(\sqrt{d}\log n), but in Theorem 2.1 it is max_i \|x_i\|^2 = 1. How can we have a weaker assumption in the corollary than in the theorem?

There is a number of minor things that should be corrected, see below.


Clarity:
At some points, the presentation of the paper could be improved:

Apparently, in Theorem 2.3 the set K_{\lambda,\gamma} corresponds to l_{1,2} norm regularization, but the definition from Line 106 has never been changed. That’s quite confusing.

Line 76: “a noisy indication of the triplet constraint d_{\hat{K}}(x_i,x_j) < d_{\hat{K}}(x_i,x_k)” --- in the rest of the paper, hat{K} denotes the estimate? And, if I understand correctly, we have y_t=-1 if the constraint is true. From the current formulation one would expect y_t=+1 in this case.


Originality:
I am missing a comparison to closely related results in the literature. What is the difference between Theorem 2.1 and Inequality (5) in combination with Example 6 in [3] (Bellet et al., 2015---I refer to the latest arxiv version)? The authors claim that “past generalization error bounds ... failed to quantify the precise dependence on ... dimension”. However, the dimension p only appears via log p and is not present in the lower bound anymore, so it’s unclear whether the provided upper bound really quantifies the precise dependence.

Huang et al. (2011) is cited incorrectly: it DOES consider relative constraints and does NOT use Rademacher complexities.


Significance:
In the context of metric learning, the results might be interesting, but a big part of the results (formulated in the setting of ordinal embedding --- see above) and techniques can be found already in Jain et al. (2016).


Minor points:
- Line 75, definition of T: in a triple (i,j,k), we assume j < k
- Line 88: that’s an equation and not a definition
- Line 98: if I_n is used to denote the identity, why not use 1_n?
- Line 99: XV instead of VX
- Line 100/101: also define the K-norm used in Line 126 and the ordinary norm as Euclidean norm or operator norm 
- Be consistent with the names of the norm: in Line 101 you say “the mixed 1,2 norm”, in Line 144 “a mixed l_{1,2} norm”.
- In Corollary 2.1.1: shouldn’t the log n-term be squared?
- Line 190: remove the dot
- Line 212: I think “rank of the” is missing
- Typos in Line 288, Line 372, Line 306, Line 229, Line 237, Line 240, Line 259
- Line 394: “We” instead of “I”
- Line 335: for K^1 \noteq K^2  ","In this paper, the author has investigated the learning of low-dimensional metrics. In general, the technique of this paper is sound and the paper is well written. My concern is mainly focused on the experimental part. 
(1)	The author adopt the simulation dataset for performance comparison. What about the performance on large-scale real datasets, e.g., image, text.
(2)	Nuclear norm and L_{1,2} norm are adopted for comparison. To validate the effectiveness, the author should list more state-of-the-art methods here  and compare their performance from a different perspective.
(3)	The conclusion part is missing. The abstract part as well as the contributions claimed in part 1 need improvement. The author should clear state the contributions of paper and convincingly demonstrate how each of these contributions advance the state-of-the-art.
"
Unsupervised Sequence Classification using Sequential Output Statistics,"Yu Liu, Jianshu Chen, Li Deng",https://proceedings.neurips.cc/paper/2017/hash/f1981e4bd8a0d6d8462016d2fc6276b3-Abstract.html,"This paper proposes an algorithm to learn a sequence classifier without labeled data by using sequential output statistics (language model). The proposed formulation is hard to optimize in its functional form and a stochastic primal dual gradient method is developed to efficiently solve this problem. Compared to earlier work this formulation is less inclined to get stuck at a trivial solution and doesn’t require a generative model. Experimental results on two real world datasets show that the proposed method results in lower error rate compared to its base line methods.

Here are few comments.

1. In figure 2(a), what does the two axis represent (lambda_d and lambda_p)?

2. The basic idea of the paper and its primal dual stochastic gradient formulation seems convincing. It would be nice however, to include other baseline comparisons, namely from [11] and [30]. Also it would be convincing to add more datasets.

3. It would be nice to provide explicit gradient formulas (step 5 of algorithm 1).","This paper studies the problem of learning a sequence classifier without labeled data by using sequential output statistics. 
This is motivated by applications where labels are costly to obtain, while sequential output statistics can be obtained easily. One typical example is OCR, where the output statistics are the language models. It proposes an unsupervised learning cost function, which is intuitive: view the mapping from the input sequence to the output sequence as transforming the input distribution to a distribution in the output domain, and the cost is the cross entropy between the output statistics and the transformed distribution. 
The authors discuss some nice properties of the cost: though it's non-convex, compared to existing works it has the property of coverage seeking that is better for optimization. It also transforms the cost to its primal-dual form and proposes a stochastic gradient method. Experiments on two real applications show that this approach works well (gets results close to that of supervised learning), while the alternatives fail miserably.

The presentation is very clear. Detailed comparisons are given to related works, though I'm not familiar enough with this direction to see if it covers all. 

Overall, the proposed approach is intuitive and the paper gives detailed discussions about advantages over existing works. Experimental results provide strong support for the effectiveness of the method. 

minor:
--Line 283: rate rate --> rate","This paper presents a method for predicting a sequence of labels without a labeled training set. This is done by incorporating a prior probability on the possible output sequnces of labels, that is a linear classifier is sought such that the distribution of sequnces it predicts is close (in the KL sense) to a given prior. The authors present a cost function, an algorithm to optimize it, and experimental results on real data of predicting characters in two tasks: OCR and spell correction.
	  The paper is technically sound and generally clear. The idea is novel (some prior work is cited by the authors) and interesting. The main reason for my score is a concern regarding the difficulty of obtaining a useful prior model p_LM. Such models usually suffer from sparseness of data, that is unless the vocabulary is quite small, it is common for some test samples to have zero probability under the prior/LM learned from a training set. It is true that this isuue is well known and solutions exist in the literature, however I would like the authors to adress the influence of it on their method. Another aspect of the same issue is the scaling of the algorithm to large vocabularies and/or long sub-sequences (parameter N in the paper). Indeed, the experiments use quite a small vocabulary (29 characters) and short sub-sequences (Ngrams with N=2,3). In NLP data at word level, the vocabulary is orderes of magnitude larger.
	  Some more comments:
	  - I would also like the authors to adress the application of their method to non-NLP domains.
	  - The inclusion of the supervised solution in the figures of the 2D cost raises a question - it seems the red dot (the supervised solution) is always in a local minimum of the unsupervised cost - I don't understand why this should be the case.
	  - If it is assumed that there is a structure in the output space of sequences, why not incorporate this structure into the classifier, that is why use a point prediction p(y_t|x_t) of a single label independently of its neighbors?
	  Specific comments: 
	  (*) lines 117-118, 'negative cross entropy', isn't the following formula the cross entropy (and not the negative of it)?
	  (*) line 119, this sentence is somewhat confusing, if I understand correctly \bar p is the expected frequency of a *given* sequence {i_1,...i_n}, and not of *all* sequences.
	  (*) lines 163-166, here too the sign of the cross entropy seems to be reversed, also when referred to in the text, e.g in line 163, if p_LM goes to 0, then -log(0) is +inf and not -inf as written in the paper, and it indeed suits the sentence in line 167, since you expect that to penalize a minimization problem, some term goes to +inf and not to -inf. 
	  Typos:
	  (*) lines 173 and 174, ""that p_LM"" seems wrong, did the authors mean ""for which p_LM"" ?
	  (*) line 283, word 'rate' appears twice.
	  "
Deep Sets,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R. Salakhutdinov, Alexander J. Smola",https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html,"The paper presents a way to design neural networks that are invariant / equivariant to permutations, making them suitable for set-valued data. A proof is given that essentially any invariant function can be written in the proposed form rho(sum_x phi(x)), where phi is a learned feature transform and rho processes the summed representations. Furthermore, such a function is always invariant.

The paper contains extensive experiments, that show the proposed architecture is applicable to a wide range of problems and yields competitive results across the board. The experiment on digit addition shows very good performance, but it should be noted that addition is directly baked into the model, so this is perhaps not very surprising.

The paper is fairly well written and very well structured.

Section 2.3 discusses well known related results. It took me quite a while to understand what the relation to the present work is. I think this section could be made clearer.

Lemma 3 is correct, but implicitly assumes that the representation of the symmetric group acting on the input should be the same representation as the one acting on the output. A simple example of an alternative equivariant map could be one where the output is a scalar that changes sign when an odd permutation is applied and otherwise stays the same. Such a map could do pairwise comparisons at each layer, which I suspect would improve results for e.g. point cloud prediction.

Lemma 3 was derived before in “Equivariance through parameter sharing” and other papers by Ravanbakhsh.

The paper by Ravanbakhsh on “estimating cosmological …” appears twice in the reference list.

In figure 1, why does the curve for SDM stop before the DeepSets curve?

In summary, I think this is a solid paper with a strong theoretical foundation and extensive experimental validation.

Minor comments:
3: “and are” -> “that are”
30: “in field” -> “in the field”
47: “not to be indifferent” -> “to be indifferent”
50-57: several typos / grammar mistakes
84: it is not immediately clear what M_0 is; it would be good to have a line about it.
156: matrices
","The paper characterises functions that are permutation invariant and equivariant (the latter under certain restrictions); it then derives form these characterisation principle for the design of deep neural networks that operate on set-like objects. A number of diverse experiments validate the approach.

The problem of learning to classify or regress from sets is an important one. The diversity of applications considered in the experiments is notable, although some of them are ""mere"" toy problems.

Nevertheless, I was still slightly turned off by the nature of some of the results. Theorem 2 indicates that any function of the power set of a countable set has an additive decomposition (by the way, it is never said that the functions $Phi$ in the decomposition can be a scalar one, but that is the case from the appendix). On a glance, this is an exciting result. However, it is obtained by constructing a surjective mapping from the power set of N to the real numbers. This construction does not show that such an additive decomposition is a reasonable one in a learning setting. For example, representing the function that computes the maximum element in a set (which is obviously permutation invariant) may be very inconvenient to code in this manner. Furthermore, the proof itself may be a bit problematic as the proposed mapping does not seem to be surjective after all; for example, 2^-1 = sum_{k=2}^{+inf} 2^-k, so the sets {1,0,0,0,...} and {0,1,1,1,1,...} would have the same code.

Lemma 3 is also very restrictive. Here the authors show that a fully connected layer which multiplies all input components by the same scalar and sums to that a scaled average of the components is the only one that can be permutation equivariant. This is fine, but at the same time does not allow the neural network to do much.

Most importantly, these two theoretical results do no appear to suggest genuinely new approaches to permutation-invariant data processing in neural networks. The idea of using commutative pooling operators (sum or pooling), in particular, is completely standard. 

The sum of digits experiment is a bit suspicious. In the text case, in particular, it would be enough to map each digit to its corresponding numerical value (a triviality) to allow the sum operator solve the problem exactly -- thus is not exactly surprising that this method works better than LSTM and GRU in this case. The image case is only slightly harder.

The rebuttal is informative and answers the most important questions raised in the review well. Therefore I am slightly upgrading my already positive score. By the way, I understand the necessary condition nature of Lemma 3. My point is that this results also shows that there isn't all that much that can be done in terms of data processing if one wants to be permutation invariant.

","Summary of Review:
Although the techniques are not novel, the proofs are, and the paper seems well done overall.
I’m not familiar with the experiments / domains of the tasks, and there’s not enough description for me to confidently judge the experiments, but they appear fairly strong.
Also, the authors missed one key piece of related work (“Learning Multiagent Communication with Backpropagation” - Sukhbaatar et al. 2016).

Overview of paper:
This paper examines permutation-invariant/equivariant deep learning, characterizing the class of functions (on countable input spaces) and DNN layers which exhibit these properties.
There is also an extensive experimental evaluation.
The paper is lacking in clarity of presentation and this may hide some lack of conceptual clarity as well; for instance, in some of the applications given (e.g. anomaly detection), the input is properly thought of as a multiset, not a set.  
The proof of Theorem 2 would not apply to multisets.

Quality:
Overall, the paper appears solid in motivation, theory, and experiments. 
The proofs (which I view as the main contribution) are not particularly difficult, impressive, or involved, but they do provide valuable results.
The other contributions are a large number of experiments, which I have difficulty judging.
The take-aways of the experiments are fairly clear: 
1) expanding deep learning to solve set-based tasks can (yet again) yield improvements over approaches that don’t use deep learning
2) in many cases, a naive application of deep learning is bound to fail (for instance, section 4.3, where the position of the anomaly is, presumably, random, and thus impossible for a permutation *invariant* architecture, which I presume the baseline to be), and permutation invariance / equivariance *must* be considered and properly incorporated into the model design.


Clarity:
While the overall message is clear, the details are not, and I’ll offer a number of corrections / suggestions, which I expect the authors to address or consider.
First of all, notation should be defined (for instance, this is missing in de Finetti’s theorem).
Section 2.3 should, but does not explicitly show how these results relate to the rest of the paper.
Line 136-137: you should mention some specific previous applications, and be clear, later on in the experiments section, about the novelty of each experiment.
Line 144: which results are competitive with state-of-the-art (SOTA)?  Please specify, and reference previous SOTA.

As mentioned before, the tasks are not always explained clearly enough for someone unfamiliar with them (not even in the appendix!).
In particular, I had difficulty understanding the tasks in section 4.1.3 and 4.2.
In 4.2 I ask: How is “set expansion” performed? What does “coherent” mean? Is the data labelled (e.g. with ground-truth set assignments?) What is the performance metric? What is the “Beta-Binomial model”?

detailed comments/questions:
There are many missing articles, e.g. line 54 “in transductive” —> “in the transductive”
The 2nd sentence of the abstract is not grammatical.
line 46: “not” should be removed
line 48: sets do not have an ordering; they are “invariant by definition”.  Please rephrase this part to clarify (you could talk about “ordered sets”, for instance).
Line 151: undefined notation (R(alpha), M).
Lines 161-164 repeat the caption of Figure 1; remove/replace duplicate text.
the enumeration in lines 217-273 is poorly formatted
Line 275 d) : is this the *only* difference with your method?  Max-pooling is permutation invariant, so this would still be a kind of Deep Set method, right??
line 513: the mapping must be a bijection for the conclusions to follow!

As I mentioned, set and multiset should be distinguished; a multiset is permutation invariant but may have repeated elements.  
Multisets may be more appropriate for some machine learning tasks.
BTW, I suspect the result of Theorem 2 would not hold for uncountable sets.

Bayesian Sets is cited twice (35 and 49).


Originality:
I’m a bit out of my depth in terms of background on the tasks in this paper, and I’m not 100% sure the proofs are novel; I would not be particularly surprised if these results are known in, e.g. some pure mathematics communities.
My best guess is that the proofs are novel, and I’m fairly confident they’re novel to the machine learning or at least deep learning community.


Significance:
I think the proofs are significant and likely to clarify the possible approaches to these kinds of set-based tasks for many readers.
As a result (and also due to the substantial experimental work), the paper may also play a role in popularizing the application of deep nets to set-based tasks, which would be quite significant.
"
Optimal Shrinkage of Singular Values Under Random Data Contamination,"Danny Barash, Matan Gavish",https://proceedings.neurips.cc/paper/2017/hash/f231f2107df69eab0a3862d50018a9b2-Abstract.html,"This paper consider the problem of matrix de-noising under the mean square-loss via estimators that shrink the singular values of a contaminated version of the matrix to be estimated. Specifically, the authors seek asymptotically optimal shrinkers under various random contamination models, extending beyond additive white noise.

Unfortunately, I could not determine the novelty of the techniques used as there is no clear indication in the paper of what results are new or rather simple extensions of previous results, such as in Nadakuditi (2014) and Gavish & Donoho (2017). As a non-expert, as far as I can see, the results in this paper are rather simple extensions of those given in Gavish and Donoho. A clear indication of what is new and a detailed comparison to what is already known is needed. In addition, I found many sentences to be unclear and inadequate, making it a difficult read.

Some more specific questions/comments:

In line 213 it is not clear whether you prove that the a.s. limit exists or assume it exists.

It is unclear under what class of estimators is yours optimal. Only a clue is given in the very last sentence of the paper.

In line 64, the variance of A's entries is taken to be the same as that of B, namely \sigma_B, while later it is changed to sigma_A. Thm. 2 however does not involve sigma_A. Is there a specific reason for choosing sigma_B = sigma_A?
In addition, it is unclear how can one estimate mu_A, sigma_A and sigma_B, which are needed for the method.

Lines 89 and 90 should come before Thm. 1.

The hard threshold shrinker is missing a multiplicative y_i all over the paper.

There are many typos.","This paper introduces a new model of i.i.d. noise in low-rank matrices, which subsumes additive and multiplicative noise, corruption, outliers, and missing entries. The paper derives exact optimal shrinkage and thresholding for this noise model, and demonstrates the results empirically. This unification is an improvement over previous literature in which a variety of specific noise models are studied; the model and analysis here make a nice contribution to the field. Indeed, this model could be of interest in other questions of low-rank recovery, such as sparse or non-negative PCA. A more detailed overview of prior work in singular value shrinkage would help the introduction.
","his paper is about how to design an optimal shrinkage function such that the latent low-rank matrix can be best reconstructed from the singular vectors of an observation matrix. Unlike existing works, which mainly focus on the additive white noise model, this paper studies a general contamination model that can involve various noise regimes such as missing data and outliers. The authors established a set of theorems that give us some useful messages, e.g., the optimal shrinkage function is unique and determined explicitly.
 
Overall, I think this is a solid work. However, it is unclear how the proved results can take effect in practice. The optimal shrinkage function depends on some extrinsic parameters of the contamination model, which is unknown. So, one may need to estimate those parameters based on the observations. But in this case there is no guarantee for the optimality of the shrinkage function. It would be better if the authors could test their results on some realistic datasets.

Moreover, the papers contain several typos. Please proof-read the paper. 
"
Learning Mixture of Gaussians with Streaming Data,"Aditi Raghunathan, Prateek Jain, Ravishankar Krishnawamy",https://proceedings.neurips.cc/paper/2017/hash/f24ad6f72d6cc4cb51464f2b29ab69d3-Abstract.html,"I think that this is a good paper, on a timely subject: the estimation of centres in a Gaussian mixture model, the streaming setting, and the theoretical guarantees obtained on a non-convex optimisation problem are all very interesting.

My two remarks would be the following:

- In the discussion following theorem 1, is zeta (the exponent of N in the first additive term) a constant greater than 1? It is only claimed that it is 'large', but that is not exactly clear, and could change the impact of the result.

- In any discussion regarding the lower bounds (intrinsic limitations of this problem), it is only explained that a constant fraction of the points will be attributed to the wrong cluster. It is not clear however that this will necessarily imply that the corresponding cluster is not well estimated. It would be interesting to comment on that. 

- p.2 'steaming' -> 'streaming'","The authors consider the problem of learning mixture of spherical Gaussians in the streaming setup. In particular, they assume a (usual) separation condition on the mixture components, and show that the mixture can be efficiently recovered in the streaming setup.

Providing efficient algorithms for learning mixture of Gaussians is an active line of research. The authors extend the state-of-the-art for learning spherical GMMs by studying the streaming setting. The paper is well-written, and the claims seem to be sound.

The provided analysis seems to show that the algorithm works when all the mixing weights are equal. The authors claim this can be extended to arbitrary mixing weights, yet I could not find sufficient support for this claim.

The sample complexity of the method -- as the authors also suggest -- does not seem to be optimal. It would be nice to compare it with the best known bounds for the offline case [e.g., see https://arxiv.org/pdf/1706.01596.pdf].","This paper proposes a LLoyd type method with PCA initialization to estimate means of Gaussians in a streaming setting. References seem to be severely lacking as the domain is wide. The key point of the algorithm seems to be the initialization and there is no discussion on this part (comparison with the literature on k-means initialization). While the technical results might be interesting I have difficulties commenting on them without the proper background on the subject. I find as well that the technical exposition of proofs is not very clear (notations, chaining of arguments). 


Other concerns

1. Line 116 : What about the estimation of variance and weights? 

2. Algorithm 1 : How do you know N in a streaming setting?

3. Theorem 1 : The error bound assumes that estimated centers ans true centers are matched. The bound is a k-means type bound. Hence, the title of the paper is misleading as it should talk about k-means clustering of Gaussian mixtures.  The estimation of sigma is vaguely discussed in Section 6.

4. Section 6 looks like a very partial result with no appropriate discussion on why it is interesting. 

"
Learning to Compose Domain-Specific Transformations for Data Augmentation,"Alexander J. Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher Ré",https://proceedings.neurips.cc/paper/2017/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html,"This paper tackles the data augmentation problem of generating more labeled training data to help the end discriminative model. It uses user-specified transformations and GAN for generating the sequence of such transformations by letting discriminator try to predict null-class. In the experimental results, the proposed approach outperforms other data argumentation baselines and the one without any data augmentation.

Data argumentation is an important problem and the idea to incorporate domain-knowledge with selecting transformations by GAN is interesting. The experimental results look to improve baselines and help the cases of small amount of labeled data. However, I concern that other GAN-based approaches [26,29] also achieved comparable performance. Also I was wondering if the proposed approach could also improve performance for 100% of data sets (MNIST and CIFAR-10), or even more interesting data sets like Imagenet. 


Detailed comments:
- Can you incorporate the end discriminative model in training? 
- You compare with other GAN approaches in only CIFAR-10, and how about other data sets with them?
- Missing reference: Sixt et al 2016 also tries to use GAN to generate more labeled training data even though being slightly different.

Typo:
line #145: as as -> as

Reference
[1] Sixt et al, RenderGAN: Generating Realistic Labeled Data, arxiv 2016","This paper addresses an interesting and new problem to augment training data in a learnable and principled manner. Modern machine learning systems are known for their ‘hunger for data’ and until now state-of-the-art approaches have relied mainly on heuristics to augment labeled training data. This paper tries to reduce the tedious task of finding a good combination of data augmentation strategies with best parameters by learning a sequence of best data augmentation strategies in a generative adversarial framework while working with unsupervised data.
      Plus points:
      1. The motivation behind the problem is to reduce human labor without compromising the final discriminative classification performance. The problem formulation is pretty clear from the text.
      2. The solution strategy is intuitive and based on realistic assumptions. The weak assumption of class invariance under data transformations enable the use of unsupervised training data and a generative adversarial network to learn the optimum composition of transformation functions from a set of predefined data transformations. To address the challenge of transformation functions being possibly non-differentiable and non-deterministic, the authors propose a reinforcement learning based solution to the problem.
      3. The experiments are well designed, well executed and the results are well analyzed. The two variants (mean-field and LSTM) of the proposed method almost always beat the basic and heuristics based baselines. The proposed method shows its merit on 3 different domains with very different application scenarios. The method experimentally proves its adaptability and usefulness of using domain specific transformation functions in the experiment with Mammography Tumor Classification experiment. The authors did a good job by showing the robustness of the method in the TF Misspecification experimentation (L279-286).
      Minus points:
      1. I have confusion regarding the role of the discriminator which I’m writing in detail below. I can understand that the generator's task is to produce less number of out-of-distribution null class (eqn. between L113 and 114). With the weak assumption (eq.(2)), this means that the generator's task is to produce mostly an image of the same class as that of the input. Then if I'm understanding correctly, the discriminator's task is to have low value of D(.) when a transformed example is passed through the discriminator (first term of eqn between L117-L118). At the same time, the discriminator's task is to have high value of D(.) when a non-transformed example is passed through the discriminator (second term of eqn between L117-L118). Thus the 'game' being played between the generator and the discriminator is that the generator is trying to (to propose a sequence of transformations to) generate in-class (i.e., non null-class) examples while the discriminator is trying to tell whether the input coming to it is a null-class (coming from a bad generator which produces null-class example) or not (coming from the real distribution of examples or from a good generator). If my understanding (above) is true, I'd request to put some explanatory texts along this line (the game analogy and what is the role of the discriminator i.e., when will the discriminator give a high value and when low value). And if my understanding is not in the right direction, then I'd like to see a better explanation of what the authors tried to achieve by the generative adversarial objective.
      Side comment: Please provide eqn. numbers to all equations even if it is not referred anywhere in the text. It helps the readers to refer to them.
      In light of the clear motivation of a new problem, succinct solution strategy and well designed and executed experiments I’m positive about the work. However, I’d like to see the authors’ rebuttal about the confusion I wrote above.

=======
After reading the fellow reviewers' comments and the the authors' response in detail, I'm more positive about this work. The authors' did a good job in clarifying my question about the role of the discriminator and have told that they would add a clarification along those lines. I agree with the authors that the focus of the work was on conducting a study of getting optimum sequence of transformation functions for data augmentation and data augmentation makes more sense when the availability of data is limited. Notwithstanding, the authors did a good job by providing the performance of their method fin the fully supervised scenario for the sake of completeness and on the request of the fellow reviewers. This also adds to the robustness of the proposed method. Thus, considering the novel problem definition, neat solution strategy and supportive experiments I am going for an even stronger accept. To me, it is a candidate for an oral presentation.","The paper proposes a technique for composing a set of given transformations to do data augmentation for supervised learning. It models the problem in a sequence modeling framework using LSTM which, given the current state, outputs at each step a probability distribution over the given set of transformations. Current state is comprised of the transformations applied so far and the transformed versions so far. The sequence model is trained using adversarial training: a ""discriminator"" models the distance between the distribution over transformed examples and the distribution over original examples and the sequence model is trained to minimize that distance. To avoid identity transformation, it has a diversity objective that penalizes the distance between the original example and its transformed version. To consider non-differentiable transformations, the paper uses a reinforcement learning approach (score-function based gradient estimate) to optimize the composition model. 

This seems to be the first work (afaik) to tackle this problem of automatically learning the order of composition the given domain specific transformations. The approach proposed is also natural and makes sense. So there is novelty in this aspect. 

However I am not sure if the significance of this problem is motivated well enough. The paper cites a few papers ([8,4,12]) for state-of-the-art results needing data augmentation through compositions of several transformations, without giving details on the datasets on which these SOTA results were obtained and library of transformations used there. A few specifics will be useful here (e.g on dataset X, SOTA results were obtained with model Y (resnet, etc) using random compositions of transformations A,B,C..). 

I also have reservations about the empirical evaluation. For most benchmark public datasets, it only considers semi-supervised learning setting (~ 10% of labeled data used for training). It will be useful to show results for fully supervised learning (with CIFAR-10 at least) to see if learning the composition for data augmentation helps at all in pushing the SOTA further. For semi-supervised learning, the results in Table 1 are not competitive I think -- MNIST with 10% labels (~500) is at 99.1 whereas SS-GAN [26] obtains about the same accuracy with 100 labels. Similarly for CIFAR-10 (table 2), the proposed approach using LSTM-RESNET obtains 81.32 with 10% labeled data (~5000) whereas SS-GAN obtains 81.37 with 4000 samples (table 2 should be corrected to reflect this discrepancy). Another difference is the classifier used -- SS-GAN uses a conv-net (though carefully designed I would guess) vs a Resnet used for the proposed method -- and some lift in the accuracy could be coming from Resnet. 

I also think the paper doesn't exploit the full potential of the approach for semi-supervised learning. Once a composition model is learned, it is possible to employ a smoothness regularizer using the unlabeled data for training the final classifier -- something like || f(x) - f(T(x))|| where f is the classifier and T(x) is the transformed example. This has the potential for further improving the results in Table 2 -- as it has been shown in some recent works ([1,2,3]). 

I like the experiments with transformation misspecification showing the robustness of the approach. 

Finally some more light on the use of score-function gradient estimator would be useful since it is known to have higher variance in the estimates and can finally converge to a bad solution if the model is not initialized properly. Do the authors use a more careful initialization of the LSTM model (pre-training etc)? How much time it takes to converge? What is the variance in the optimal transformation sequences (under the learned policy) for different examples? 

[1] TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING, 2017
[2] WEIGHT-AVERAGED CONSISTENCY TARGETS IMPROVE SEMI-SUPERVISED DEEP LEARNING RESULTS, 2017
[3] Improved Semi-supervised Learning with GANs using Manifold Invariances, 2017


 "
Preventing Gradient Explosions in Gated Recurrent Units,"Sekitoshi Kanai, Yasuhiro Fujiwara, Sotetsu Iwamura",https://proceedings.neurips.cc/paper/2017/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html,"This paper addresses an important problem in training a class of recurrent neural networks: the Gated Recurrent Units (GRU). Training these important class of neural networks is not an easy problem and this paper makes an important contribution that throws light on the gradient explosion issue. A well written paper, the review in section 2 gives a succinct summary of the model and previous work. The proposed algorithm is well-motivated and clearly explained. I believe I can implement this  immediately by following the steps. The discussion on computational cost is a little unclear, and the results shown in Fig. 2 is indeed disappointing (there really isn’t a saving, but the authors seem to give a spirited projection that this may be the case in section 4.4). Results shown in Tables 1 and 2 are important – they relate to success / otherwise of getting a network to learn. I think this is a problem easily hidden in a culture of only “reporting good news” – and that the gradient clipping method actually fails to find solutions is worthy of note. 


","Summary
The authors propose a method for optimizing GRU networks which aims to prevent exploding gradients. They motivate the method by showing that a constraint on the spectral norm of the state-to-state matrix keeps the dynamics of the network stable near the fixed point 0. The method is evaluated on language modelling and a music prediction task and leads to stable training in comparison to weight clipping.

Technical quality
The motivation of the method is well developed and it is nice that the method is evaluated on two different real-world datasets. I also like the plots that show the relation between the spectral radius of the linear approximation of the system and the norm of the gradient. However, one important issue I have with the evaluation is that the learning rate is not controlled for in the experiments. Unfortunately, this makes it hard to draw strong conclusions from the results. The starting learning rates seem to be quite high and this may very well be the reason why the clipping-based optimization performs so poorly. Experiments with a range of different starting learning rates could give more insights into the importance of the method in general. Of course it would still be a strength of the method that it is less sensitive to the choice of learning rate but if simply lowering the learning rate a bit solves most of the issues the clipping has, it lessens the impact of the method.

Clarity 
Despite the large number of equations, the argumentation is easy to follow. Except for some grammar mistakes, the paper is well-written in general.

Novelty
While recurrent neural networks have been quite thoroughly analysed in similar ways, this is to my knowledge the first work that does this for GRUs. I also don't know about any methods that combine SGD training with SVD to clip certain singular values. 

Significance
While the analysis is specific to GRUs, I don't see why it couldn't inspire similar analysis of other popular RNN architectures. GRUs are also quite popular and for that reason the proposed method could have practical value for people. That said, it is hard to predict whether some new architecture will show up and take over or not and whether this method will be significant in the long run. 

pros:
The method is well-motivated and the derivation can be useful by itself.
Very insightful plot of the relation between the gradient norm and spectral radius.
GRUs are very popular at the moment so if the method works well it may find widespread use.

cons:
Only works for GRUs
The experiments could be the result of the particular choice of starting learning rate
"
Streaming Sparse Gaussian Process Approximations,"Thang D. Bui, Cuong Nguyen, Richard E. Turner",https://proceedings.neurips.cc/paper/2017/hash/f31b20466ae89669f9741e047487eb37-Abstract.html,"This paper presents a variational and alpha-divergence approach specifically designed for the case of Gaussian processes with streaming inputs. 

The motivation given in the introduction is convincing. The background section is very well presented, sets up the methodology for section 3 and at the same time explains why a batch based approach would be inadequate. Section 3 which is the most important, is however more difficult to follow. It is not a problem to understand how one equation leads to the other, but many important details are left out of the description making it very difficult to understand in a conceptual level. In particular, it'd help to explain the concept behind starting from eq. (5). Further, what is p(a|b) in eq. (6)? It seems to be an important element for our understanding of the method, as it somehow involves the update assumption. I was also wondering whether it'd be possible to arrive at the same bound by starting with the joint distribution and simply applying Jensen's inequality together with (4). That would probably be a clearer presentation.

The extension to alpha-divergences and related conversation is interesting, although I would also expect to see a deeper comparison with [9] in the methodological level. This is important since the proposed method converges to [9] for alpha zero, which is what the authors use in practice. 

The experiments are structured nicely, starting from intuitive demonstration, moving to time-series and spatial data and finishing with classification data. The method does seem to perform well and run fast - it seems to be two time slower than SVI-GP, which still makes it a fast method. However, the experiments do not explore the hypothesis whether the SVI (batch-based) or the streaming based method performs better, simply because they use different approximations (we see that in time zero SVI is already much worse). Since the uncollapsed bound has been used for classification, I'd be keen to see it for the regression tasks too, to be able to compare with SVI-GP. Indeed, the SVI-GP seems to actually reduce the RMSE more than OSGP in Fig. 3.

Another useful property to explore / discuss, is what happens when the order of the data is changed. Further, the comparisons do not consider any other competitive method which is tailored to streaming data, such as [9] or [Gijsberts and Metta: Real-time model learning using incremental sparse spectrum GP Regression, 2013]. Finally, it'd be useful to either repeat the plots in Fig 2,3 with number of data in the x-axis (so all methods are aligned) or to simply put those and state the difference in running times.

Overall seems like a nice paper, which has good motivation and considers an important problem. However, the key points of the method are not demonstrated or explained adequately. I feel that re-working the presentation and experiments would significantly improve this paper. 

EDIT after rebuttal period: The authors have clarified most of my concerns and I have raised my score to reflect that.",The paper is very well written. The method and underlying motivations are described well. The experiments are clearly described. In particular the movie like figures clearly describe the behaviour of the method on a couple of toy problems.  ,"Title: Streaming Sparse Gaussian Process Approximations


Summary: The paper proposes a sparse GP approximation to an online setting where data arrives sequentially. The authors provide practical methods for learning and inference in this online setting. 


Clarity: The paper is well written and clearly introduces the concepts.



More detailed comments:
1) The exposition in section 3 is clear, and I also like the fact that the main contribution (3.2) is attached after the mathematical details for a simpler problem. Otherwise the paper would have been harder to read.
2) I am not yet convinced that a sparse method based on pseudo points is the right thing for an online setting: At some point we will run out of pseudo points, and then we will have to make compromises (moving window, ...). Figure 1 points in this direction already. Can you comment on this?
3) l208: Can you give some more details on the statement ""all models including full GP regression tend to learn bigger noise variances"". It is an observation, but I would like to have an explanation or hypothesis.
4) For the time-series data, are the hyper-parameters learned?
5) Fig. 4: Can you explain a bit the non-monotonic behavior? What is happening?


Summary of the review:
Overall, a good paper that addresses a problem that is largely ignored by the GP community. 


"
Differentially Private Empirical Risk Minimization Revisited: Faster and More General,"Di Wang, Minwei Ye, Jinhui Xu",https://proceedings.neurips.cc/paper/2017/hash/f337d999d9ad116a7b4f3d409fcc6480-Abstract.html,"Summary: The paper revisits the problem of differentially private empirical risk minimization and claims to provide algorithms with tighter gradient complexity (i.e., the number of gradient evaluations to obtain the optimal error). The main algorithm they use is a differentially private variant of the stochastic variance reduced gradient descent (SVRGD) algorithm. Furthermore, they provide excess empirical risk guarantees for non-convex loss functions that satisfy Polyak-Lojasiewicz condition.

Positive aspects of the paper: SVRGD has become very popular in the convex optimization literature, and this paper provides the first differentially private variant of it. Furthermore, the analysis for the non-convex case is very interesting.

Other comments: 

i) I believe all the bounds in Table 2 and Table 3 (in terms of gradient complexity) is already known in the literature (up to logarithmic factors). See, the paper ""Is Interaction Necessary for Distributed Private Learning?"". The main point is that differentially private gradient descent algorithms converge at the same rate as their non-private counter parts up to the optimal error. 

ii) I am unclear about the Polyak-Lojasiewicz condition. I am sure it is my ignorance of the topic, but the paper does not provide enough intuition into the condition.

Given that gradient complexity results are already known, I am worried about the impact of the paper.","Summary:
A large number of machine learning models are trained on potentially sensitive
data, and it is often import to guarantee privacy of the training data.
Chaudhuri and Monteleoni formulated the differentially private ERM problem and
started a line of work on designing differentially private optimization
algorithms for variants of ERM problems. Recent works have gotten nearly optimal
tradeoffs between the additional error introduced by the DP algorithm (the
privacy risk) and the privacy parameter, for a large class of settings. In this
work, these results are improved in the additional axis of computational
efficiency. For smooth and strongly convex losses, this work gets  privacy risk
bounds that are essentially the best known, but do so at a computational cost
that is essentially (n+ \kappa) gradient computaitons, instead of n\kappa, where
\kappa is the condition number. Similar improvements are presented for other
settings of interest, when the loss function is not strongly convex, or when the
constraint set has small complexity.

A different viewpoint on the results is that the authors show that DP noise
addition techniques and modern optimization methods can be made to work well
together. Speficially, one can use SVRG with noise addition at each step and the
authors show that this noisy SVRG also gets near optimal privacy risk. Similarly
for the case of constraint sets with small Gaussian width (such as l_1), where
previous work used noisy mirror descent, the authors show that one can use an
accelerated noisy mirror descent and get faster runtimes without paying in the
privacy cost.

I think the problem is very important and interesting. While the tools are somewhat
standard, I think this paper advances the state of the art sufficiently that I
am compelled to recommend acceptance.","This paper gives several algorithm for ERM in convex optimization that satisfy differential privacy. The algorithms improve on known bounds in terms of  number of necessary gradient computations and handle some some general settings such as non-convex functions satisfying a certain condition.
As far as I can tell from the presentation the results are obtained by plugging the known analyses of gradient perturbation using Gaussian noise into well-known faster algorithms than those previously considered (e.g. SVRG). The stochastic methods naturally deal with randomized estimates of the gradient so accounting for additional randomization due to Gaussian noise is relatively straightforward.
These results are useful to have but I think that both technical and conceptual contributions are not quite significant enough for publication in NIPS. (The paper does not contain any discussion of ideas they needed to employ to make the analysis go through so I assume there is not much to discuss).

Some minor additional comments:
32. The bound was not ""broken"" but rather refined using additional structural assumptions.
Table 1 caption: L is undefined (I guess it should be the smoothness parameter that you assume to be 1)
 "
Unbounded cache model for online language modeling with open vocabulary,"Edouard Grave, Moustapha M. Cisse, Armand Joulin",https://proceedings.neurips.cc/paper/2017/hash/f44ee263952e65b3610b8ba51229d1f9-Abstract.html,"This paper proposes a non-parametric method to cache previously seen contexts for language modelling. The basic idea is that at each point, the k nearest-neighbour states from previously seen contexts are retrieved, and a kernel density estimation method applied to generate a probability distribution over an open vocabulary. Thus, the cache model is unbounded, unlike methods such as pointer networks or continuous caches. Results demonstrate the performance of this method on language modelling with time and topic drift, over standard RNN language models.

This was a well-written paper, and the unbounded cache idea is intuitively appealing. I think that the proposed method could become useful for many tasks besides language modelling. 

I would have liked to see a comparison of this method against parametric or local cache methods such as the pointer-generator network. Also, how much slower is the proposed model at inference time? Querying 1024 nearest neighbours in order to estimate p_{cache} looks like it may be expensive.","This paper proposes a modification of recurrent networks (Elman network, LSTM or gated recurrent unit) for language modelling that is able to adapt to changes in the data distribution, by introducing an unbounded cache defined to deal with unseen words during training or uncommon words. Although there are other similar works in the literature (see [21],[50],[41]), this one allows to store all the previously seen words, instead of just the most recent ones by estimating the probability distribution of the words seen using a kernel density estimator (and an approximate knn to scale the process). From a theoretical point of view, the paper is interesting and addresses an issue that is present in many NLP applications. In the result sections, the authors show that using the cache is important to obtain good results. During the reviewing process they have added substantial empirical validation to the paper, showing that it improves on the state of the art. 

Opinion on the paper: 
+ the paper is very well written (although the authors should pass an english corrector) 
+ the topic is of great importance for every NLP practitioner
- the paper lacks proper numerical evaluation (although in the rebuttal process the authors provided a wide range of numerical results, they cannot be included in the final paper, thus not taken into account in the evaluation)
","This paper discusses an extensions to the recently proposed continuous cache models by Grave et al. The authors propose a continuous cache model that is unbounded, hence can take into account events that happened an indefinitely long time ago. While interesting, the paper fails to provide good experimental evidence of its merits. Its main statement is that this model is better than Grave et al., but then does not compare with it. It only seems to compare with cache models from the nineties (Kuhn et al.), although that is not even clear as they spend only one line (line 206) discussing the models they compare with. ""the static model interpolated with the unigram probability distribution observed up to time t"" does sound like Kuhn et al. and is definitely not Grave et al.

The authors also mention the importance of large vocabularies, yet fail to specify the vocabulary size for any of their experiments. I also don't understand why all the datasets were lowercased if large vocabularies are the target? This (and the above) is a real pity, because sections 1-3 looked very promising. We advise the authors to spend some more care on the experiments section to make this paper publishable.

Minor comments:
* line 5: stores -> store
* line 13: twice ""be""
* line 30: speach -> speech
* line 31: ""THE model""
* line 59: ""assumptions ABOUT""
* line 72: no comma after ""and""
* line 79: algorithm -> algorithmS
* line 79: approach -> approachES
* line 97: non-parameteric -> non-parametric
* line 99: ""THE nineties""
* line 110: ""aN update rule""
* line 127: Khun -> Kuhn
* line 197: motivationS
* line 197: ""adapt to A changing distribution""
* line 211: ""time steps""
* line 211: adaptative -> adaptive
* table 2: the caption mentions that the model is trained on news 2007, but in fact this varies throughout the table?
* line 216: interest -> interested
* line 235: millions -> million
* line 267: experimentS
* line 269: where do these percentages come from? they seem wrong...
* line 281: ""THE static model""
* line 283: Set"
Shape and Material from Sound,"Zhoutong Zhang, Qiujia Li, Zhengjia Huang, Jiajun Wu, Josh Tenenbaum, Bill Freeman",https://proceedings.neurips.cc/paper/2017/hash/f4552671f8909587cf485ea990207f3b-Abstract.html,"This paper contains a lot of ideas and methods packed into a tightly wound package - less would have been certainly more: The general aim is to build a system that can mimick the human ability to recognise shape and material objects from their sounds. Certainly a nice idea to explore within a NIPS community. To this end a generative model is defined, and a process for inference is proposed (""Physics-Based Audio Engine"") that uses a physics simulation, this physics simulation is coupled (somehow) to a sound generation algorithm. This appears to use (somehow) the physics simulation engine representation of the vibrating surfaces of the objects to render the sounds. This engine is then used to create training stimuli for 4 variants of machine learning algorithms to learn appropriate representations. Again the descriptions are rather curt, so it is difficult to understand how the algorithms actually operate and how they perform. Into the paper mix we also have now human experiments that aims to compare the algorithms with that of the human observer. A description of how the experiment was conducted with human users is missing, however human users are almost always outperformed by the model. While it is up to this point unclear why the authors did not use real data when assessing their models, we are treated to real at the end (Figure 7) but without any meaningful information for us to understand how their method performs. In my opinion these are the major issues
1. Too much information therefore too superficially presented.
2. It is unclear how good the generative model is, comparison to real data would have been help here in anchoring it first.
3. The use of 4 different algorithms for classification is a nice effort, but it remains unclear to me what the point is (do the authors have a clear hypothesis?)
4. The human data is in itself interesting, but at this point it appears unclear how it was obtained, and how it informs us further on the work done in the paper. 
 
","This paper presents a system to infer shape and material of falling objects from sound. The main system follows the generative approach: its pipeline consists of a physics engine for simulating rigid body dynamics (given latent variables that characterize various features of the object) and an audio synthesis system. Four different inference models are explored in the paper ranging from an unsupervised scheme, in which a Gibbs sampler is used to infer the latent variables that give rise to a given test audio clip; to a fully-supervised model in which a oracle model is trained from the ground-truth data is obtained from the generative model. Finally, a study was conducted to compare the performance on object recognition between human and the inference model, showing that the latter performs comparably/better.

PROS: 

- The first pages are relatively well written. Both the architecture and the experimental setups are well documented.

- The system reaches (super) human-level performance in shape and material recognition from (synthetic) sounds.

- The relation to previous literature is well documented.

CONS:

- Although the paper tackles a hard task using a sophisticated system, there is little to no significant ML contribution.

- Many typos. Paper gets quite confusing towards the last pages and feels like having been written up in a rush.

- There is no comparison to previous systems.","This paper describes an analysis-by-synthesis system for identifying properties of objects bouncing on a surface including material, shape, and height of fall.  A thorough investigation is conducted on synthetic data using various constraints on the training of the model, showing that it is accurately able to recover these basic facts  from new recordings.  An experiment on real data comparing human listeners with the system shows that it is comparable and may outperform the humans in the height task.  The system uses a surprisingly simple comparison method between the observed and synthesized signals, but it appears to work well.  The literature review is short, but quite good.

The paper has several minor weaknesses:
* It is not clear if the ability of the model to detect fall height is because of the absolute timing of the simulations.  Falling from a greater height leads to a longer delay before the first impact.  This is obvious to an algorithm analyzing fixed-sized wav files, but not to a human listening to sound files with somewhat unknown silent beginnings.  A fairer comparison would be to add a random amount of delay before starting the sounds for both listeners.
* The comparison method is changed between the synthetic and real tasks, which seems unfair.  If it is necessary to use a more complex comparison method for the real task, then also use it for the synthetic one.
* Line 226 reports several analysis parameters in samples, but never states the sample rate.  Please describe these quantities in seconds or ms or provide the sample rate so the reader can perform the conversion themselves.

Overall, this is a strong paper that has gotten a relatively old and appealing idea to work much better than in the past."
On the Consistency of Quick Shift,Heinrich Jiang,https://proceedings.neurips.cc/paper/2017/hash/f457c545a9ded88f18ecee47145a72c0-Abstract.html,"         This paper provides theoretical guarantees for outputs of Quickshift-based algorithms, in three estimation problem: mode estimation, cluster tree estimation and modal regression. Up to my knowledge, such results for quickshift are new (though the same rates are obtained for mode estimation in ref 8 and RSL achieves consistency for the cluster tree estimation ref 3). Proofs are on the whole sound, but I have a few remarks:

proof of Theorem 1, p.5, l.191: ""by restricting f to B(hat(x),tau)"". Since hat(x) depend on X_[n], this seems not that straightforward. You may easily get rid of this by proving (2) before (1). (2) yields that for every x0 there is a unique root in B(x0,2tau) and this root is the max of KDE over B(x0,2tau). So, coming back to (1), since you proved that hat(x) is tau close to some x0, then hat(x) must be the maximizer of kde over B(x0,2tau), the latter ball being not random and you can restric f.

proof of Theorem 2, p.6, l.222: 'r < eps gives us hat(x) in hat(M)'. I do not see why: you have to prove that hat(x) is KDEmax over B(hat(x), tau) that is included in B(x0,tau + r). So you have to prove that hat(f)(hat(x)) > sup B(x0,tau + r)/B(x0,tau) hat(f). You can do that using the same argument as below, ll.232-236.

p.6, Lemma 4: the n sufficiently large also depends on delta, no? 

p.7, Theorem 3: same remark as above concerning the dependency on delta. You should also mention the required hypothesis (Holder is enough? by the way I do not see where the assumption tau < r_M/2 matters)

p.7, Theorem 5: the 'uniformly in x' might be misleading. For instance, conditional mode estimation involves some r_M(x), so n has to be large enough so that tau < = r_M(x). Some clarification bout this uniformity might be useful.

Minor concerns
p.2, l.58: about the work of Arias-Castro et al., bottom of p.9 it is proved that the uniform difference between the 'mean-shift' trajectory and the ideal one is decreasing (for 'a' small enough). Since the ideal trajectory converges eventually to a true mode, I guess that 'mean-shift' too.
p.2, l.99: spurious '.'
p.2, algorithm 1: 'X_i' - > 'x_i'
p.4, Lemma 2: I guess A_x0 is the connected component that contains x0
P.4, l.162: 'folowing'
p.5, l.192: spurious '.'
p.5, l.200: 'intentially' is 'intentionally'?
p.5, l.200: 'particlar'
p.7, l.263: 'pairs vertices'
p.7, l.280: missing 'we', and 'conditiona'
p.8, l.298: 'there exist y0'   ","This paper provides finite sample bounds for Quick Shift in mode estimation, cluster recovery, and modal regression. It also shows that Quick Shift has the advantage of assigning sample points to their modes for clustering. These results are good statistical analysis for Quick Shift. However, there are two minor shortcomings in the presented analysis: 

1. The decay assumption for the kernel function k stated as k(t) \leq C_\rho exp(-t^\rho) with \rho >0 is rather strong since polynomially decaying kernels are excluded. 

2. In all the five theorems, the probability level is 1- 1/n which depends on the sample size and cannot be arbitrarily close to 1. 

The results in the paper are interesting in general. ","In this paper the authors submit proofs regarding various notions of consistency for the Quick Shift algorithm. They also introduce adaptions of the Quick Shift algorithm for other tasks paired with relevant consistency results. 

Quick Shift is a mode finding algorithm where one chooses a segmentation parameter \tau, constructs a kernel density estimator, and assigns a mode, m, at samples where the KDE magnitude at m is larger than the KDE magnitude evaluated at all samples within a \tau-ball of m. In this setting a true mode are those points in the sample space where the density is maximized within a \tau ball. Quick shift can also be used to assign samples to a mode by ascending a sample to the nearest sample where the KDE magnitude is larger and repeating until a mode is reached. The authors present proofs of consistency for both of these tasks: mode recovery and mode assignment.

The authors also propose an adaptation of Quick Shift for cluster tree recovery and modal regression for which they also prove consistency.

The proofs presented in this paper are dense and technical; I cannot claim to have to have totally validated their correctness. That being said I found no glaring errors in the authors' reasoning. My largest concern with this paper is reference [1]. [1] plays a central role in the proofs for individual mode recovery rates. [1] is an ""anonymous  ""unpublished manuscript"" on Dropbox. I spent a fair amount of time searching for any other reference to this manuscript and I could find none. The authors give no explanation for this manuscript. I'm inclined to think that I cannot reasonably accept a paper whose results rely upon such a dubious source.

Additionally there are many other smaller issues I found in the paper. Some of them are listed here by line:

99: period after ""we"" should be removed
101-103: The sentence ""For example, if given an input X the response y have two polarizing tendancies, modal regression would be able to adapt to these tendancies,"" is very strange and unclear. Also ""tendencies"" is misspelled.
103: ""indepth"" is not a word. Replace with ""in-depth"" or ""indepth""
106: ""require"" should be ""requires"" 
112: Why is the ""distribution \script{F}"" introduced? This is never mentioned elsewhere. We can simply proceed from the density. 
113: I'm guessing that the ""uniform measure"" is the same thing as the ""Lebesgue measure."" ""Lebesgue measure"" is more standard terminology in my opinion.
115: Why is the norm notation replaced with absolute value notation? This occurs later as well elements which are clearly vectors, line 137 and Lemma 2 for instance.
139: Remark 3 should have a citation.
145: C is never defined.
150: Your style changes here. Assumption 3 uses parentheses and here Assumption 4 uses square brackets.
158: The final ""x"" in the set definition should be ""x'""
242: ""f(x)"" should be to the right of ""inf""
280: ""conditiona"" should be ""conditional""
286: What exactly is meant by ""valid bandwidth choices?""

Update: I've bumped the paper by two points. I still have concerns with the numerous small technical errors I found"
Wasserstein Learning of Deep Generative Point Process Models,"Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, Hongyuan Zha",https://proceedings.neurips.cc/paper/2017/hash/f45a1078feb35de77d26b3f7a52ef502-Abstract.html,"The paper proposes using Wasserstein generative adversarial networks (WGAN) for point process intensity or hazard estimation. The paper demonstrates the usefulness of objective functions beyond MLE and proposes a computationally efficient distance measure. It uses a regularized dual formulation for optimization and an RNN for generator and discriminator functions. Details on the methods and data used in the real analyses are light and could be documented more clearly for reproducability. 

The results demonstrate that the WGANTPP performs reasonably in a variety of tasks using both simulated and real data. The comparison baselines appear unreasonably weak, e.g. the kernel mixture process uses 3 components and visibly has 3 humps in all results, and e.g. the self-exciting and self correcting processes use fixed baselines when there is a large literature on settings for semi-parametric hazard functions. Using more flexible and analogous comparison models, e.g. Goulding ICDM 2016, Weiss ECML 2013, Jing WSDM 2017, would make for better comparison.

The distance defined would not deal with noise in the form of extraneous events due to the alignment of the ith element with the ith element of each sequence. Might a distance based on local alignment would likely be more robust than the one proposed? In what specific applications do you expect this distance to be more useful than log likelihood.

Eq 3. perhaps the second summation should read \sum_{j=i+1}^m y_j.

Section 3.1 inhomogeneous process: \alpha and c settings switched?

Spelling e.g. demosntrate","This paper proposes to perform estimation of a point process using the Wasserstein-GAN approach.
More precisely, given data that has been generated by a point process on the real line, the goal is to build a model of this point process. Instead of using maximum likelihood, the authors proposed to use WGAN.
This requires to:
- define a distance between 2 realizations of a point process
- define a family of Lipschitz functions with respect to this distance
- define a generative model which transforms ""noise"" into a point process

The contribution of the paper is to propose a particular way of addressing these three points and thus demonstrate how to use WGAN in this setting.
The resulting approach is compared on a variety of point processes (both synthetic and real) with maximum likelihood approaches and shown to compare favorably (especially when the underlying intensity model is unknown).

I must admit that I am not very familiar with estimation of point processes and the corresponding applications and thus cannot judge the potential impact and relevance of the proposed method. However, I feel that the adaptation of WGAN (which is becoming increasingly popular in a variety of domains) to 
the estimation of point processes is not so obvious and the originality of the contribution comes from proposing a reasonable approach to do this adaptation, along with some insights regarding the implementation of the Lipschitz constraint which I find interesting.

One aspect that could be further clarified is regarding the modeling of the generator function: from the definition in equation (7) there is no guarantee
that the generated sequence t_i will be increasing. It is the case that the weights are constrained to be positive for example? or is it the case that the algorithm works even when the generated sequence is not increasing since the discriminator function would discriminate against such sequences and
thus encourage the generator to produce increasing ones?



","
        This paper presents a method for learning predictive (one-dimensional) point process models through modelling the count density using a W-GAN.

        Detailed comments:
        * Abstract: there are several approaches for doing full (or approximate) Bayesian inference on (Cox or renewal) point processes.  E.g. see arxiv.org/pdf/1411.0254.pdf or www.gatsby.ucl.ac.uk%2F~vrao%2Fnips2011_raoteh.pdf.
        * To me it appears that the approach will only work for 1D point processes, as otherwise it is hard to represent them via the count density?  If this is the case, it would be good to see this more explicitly stated.
        * This is the case for most inhomogenous point processes, but given only a single realisation of the point process, it would seem very hard to characterise a low variance estimate of distance from the generated count measure to the data?  Perhaps this is the main reason that the W-GAN performs so well---much like the GP based intensity-explicit models, a heavy regularisation is applied to the generator/intensity-proxy.
        * I don't understand the origin of 'real' in the real world evaluation metrics e.g. Figure 3?  How do you arrive at this ground truth?
        * A discussion of how easy/difficult these models are to train would have been interesting.
        
        Finally I am very interested to know how simple models compare to this: e.g. KDE with truncation, simple parametric Hawkes etc?  My main concern with this work would be that these models are all horrendously over-complex for the signal-to-noise available, and that therefore while the W-GAN does outperform other NN/RNN based approaches, a more heavily regularised (read simpler) intensity based approach would empirically outperform in most cases.

      "
Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,"Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer",https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html,"This paper proposes a new aggregation rule for gradient vectors received from different workers in distributed stochastic gradient descent to prevent Byzantine failure. It is proved in the paper that the normal aggregation rule like averaging cannot tolerant even one such bad failures. The paper defines a concept as Byzantine resilience to evaluate the tolerance of Byzantine failures and proves that the proposed aggregation function satisfies that. The experimental results shows that the proposed method works much better than without it with more and more Byzantine workers. However, the method will slow down the computation even when there is no such workers. So it seems that it’s important to trade off the robustness and efficiency. 

I am not quite familiar with distributed stochastic descent. My question is more related with machine learning tasks itself. In figure 4, it seems Krum also achieves worse learning error than average. So I would be more interested in how large the difference of error could be. Is it bounded? How much performance cost we need to pay for being robust to Byzantine failures using Krum?


","This paper presents a formal condition for Byzantine fault-tolerant stochastic gradient aggregation, as well as an algorithm (Krum) that satisfies the condition. Given the definition, it is straightforward to see that aggregation based on linear combination (including averaging) is not Byzantine tolerant. Meanwhile the paper gives evidence that Krum is not only tolerant in theory but reasonably so in practice as well.

I found the paper to be clear, well-organized, self-contained, and altogether fairly thorough. The basic motivation is clear: the literature on distributed learning has focused on statistical assumptions and well-behaved actors, but what can we do in very pessimistic conditions? To my knowledge this is the first paper that explores this view of fault-tolerance (already established in other contexts). I did not verify the analysis.

A suggestion on exposition: The draft gives a bit of unusually extensive background on SGD by illustrating an unbiased stochastic gradients via cartoon figure (fig 1). This is nice but perhaps unnecessary for a stochastic optimization audience at NIPS. With that said, I see it as the authors' choice whether to keep this or try to reallocate the space to more main technical content.

My remaining remarks are about the experiments:

1. The experiments are not so well-matched to the paper's main setting, and could be significantly strengthened by looking at more modern datasets and models. Current experiments are on small-scale (""single-machine"") datasets for which distributed learning is not a relevant tool. The experiments in the current draft therefore serve more as a simulation or benchmark. In that sense, they do successfully show intriguing phenomena, but it is not shown that this behavior generalizes to settings where the algorithm is more likely to be used. Since the paper considers an MLP for MNIST, one natural suggestion for the next step would be a deep convolutional net for data-augmented CIFAR-10, which could take sufficiently longer to train on one machine that it would benefit from a distributed setup.

2. Why are correct workers given a mini-batch of size 3? Without explanation, this appears to be an arbitrary choice.

3. In the ""cost of resilience"" experiment (fig 5), reporting the error at round 500 seems to represent an early phase of training (and note indeed that the accuracies shown are not yet all that great for the task). At a batch size of 100 (on the larger end), in 500 rounds, 500*100 = 50K examples do not comprise a full pass through the training set. The experiment would be more complete with a second plot showing (or at least a remark describing) behavior closer to convergence.

4. In the ""multi-Krum"" experiment (fig 6), setting the parameter m to be n - f, the number of non-Byzantine machines, seems like a strongly omniscient choice (how should we know the number of adversaries f?). The experiment would be more complete by showing other values of m (for the same f). The writing suggests that setting m provides a tradeoff, so such a plot would actually serve to show the trade-off curve.
","Intro:

The paper introduces a novel algorithm, called Krum, that combines partially calculated gradients in a Byzantine failure tolerant way. Such an algorithm is meant to be useful in distributed training of machine learning models on very large data sets.

I am a machine learner, but not a distributed computing expert. Hence, I review the manuscript from a customer perspective.

---
Strengths:

 * The research question is very interesting. Distributed techniques start to play a more important role in machine learning as the scales of the data sets grow exponentially.

 * The paper is written in a sophisticated academic language. It is also nicely structured.

 * The proposed algorithm is supported with rigorous theoretical argumentations.

---
Weaknesses:

 * Results in Figure 4 are not charming. When there are no Byzantine failures, the proposed algorithm lags far behind simple averaging. It starts to show its power only after one third of the workers are Byzantine. This looks like an extremely high failure rate. Apparently, averaging will remain to be extremely competitive when the failure rates are realistically low, making the motivation of the proposed approach questionable.

 * While the paper introduces an idea that starts to become useful when the data set is dramatically large, the reported experiments are on extremely small data sets. MNIST has 60000 instances, spambase has 4601. The real effect of the divergence between the true and the stochastic gradient starts to become visible when the data set is large enough. With today's hardware technology, a modest workstation with a few-thousand-dollar GPU can be easily trained on MILLIONS of data points without any need for such protocols as Krum. Furthermore, whether more data points than a few million are needed depends totally on applications. In many, the performance difference between 10 Million and 100 Million is ignorably small. For a machine learner to be convinced to slow down her model for the sake of safe distributedness, the authors should pinpoint applications where there is a real need for this and report results on those very applications.

 * The proposed algorithm is for updating global parameters from stochastic gradients calculated on minibatches. Although this approach is fundamental to many machine learning models, the central challenge of the contemporary techniques is different. Deep neural nets require distributed computing to distribute operations across ""neurons"" rather than minibatches. The fact that the proposed algorithm cannot be generalized to this scenario reduces the impact potential of this work significantly.

Minor point: The paper would also be stronger if it cited earlier pioneer work on distributed machine learning. One example is:
 
C.T. Chu et al., Map-Reduce for Machine Learning on Multicore, NIPS, 2007

---
Preliminary Evaluation:

While I appreciate the nice theoretical work behind this paper and that distributed machine learning is an issue of key importance, the reported results shed doubt on the usefulness of the proposed approach.

---
Final Evaluation:
 
I do acknowledge that 33% Byzantine failure rate is a standard test case for general distributed computing tasks, but here our concern is training a machine learning model. The dynamics are a lot different from ""somehow"" processing as large data bunches as possible. The top-priority issue is accuracy, not data size. According to Figure 4, Krum severely undermines the model accuracy if there is no attack. This literally means a machine learner will accept to use Krum only when she is ABSOLUTELY sure that i) a 10-20 million data point subset will not be sufficient for satisfactory accuracy (hence distributed computing is required), and ii) at least 33% of the nodes will act Byzantine (hence Krum is required). As a machine learner, I am trying hard but not managing to find out such a case. Essentially, it is not the readership's but the authors' duty to bring those cases to attention. This is missing in both the paper and the rebuttal. I keep my initial negative vote."
Protein Interface Prediction using Graph Convolutional Networks,"Alex Fout, Jonathon Byrd, Basir Shariat, Asa Ben-Hur",https://proceedings.neurips.cc/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html,"The authors propose a new framework for applying convolutional filters to graph-structured data in the context of classification tasks.  The novel aspect of the method is the introduction of edge features into the computation of 
the convolutional activation.  The edge features are represented in two ways: through sum-coupling, where the edge features are added to the node features; and through product-coupling, where the edge features are multiplied with the node features.  In contrast with sum coupling, product coupling retains the association between nodes and their incident edges.  The representation is rotation invariant (i.e. invariant with respect to permutation of the indices of the nodes).  Sum-coupled GCNs offer the best performance in terms of the median area under the ROC curve when applied to a protein interaction classification task.

Quality:
The paper is of high quality.  The paper offers a well-explained model that outperforms strong baselines, including state-of-the-art SVM and graph-based neural network approaches.

Clarity:
The paper is very well written and enjoyable to read.  The model is clearly delineated.  The authors do a good job of summarizing the spatial approach to convolutional neural networks on graphs.  However, I would suggest that they add more detail on spectral approaches, particularly [1] and [2] which offer good performance and appear to be widely used.

Originality:
The paper is moderately original.  As the authors mention, the approach shares some similarity with existing spatial techniques for applying convolutional filters to graph-structures data.  

Significance:
This paper is likely to be of interest to the NIPS community.  The authors report good results and compare with strong baselines, and the dataset is compelling.

Overall impression: A clear, well-written paper that builds on existing work to generate strong results.  Accept.

[1] Thomas Kipf and Max Welling. ""Semi-supervised classification with graph convolutional networks."" arXiv preprint arXiv:1609.02907 (2016).
[2] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. ""Convolutional neural networks on graphs with fast localized spectral filtering."" Advances in Neural Information Processing Systems. 2016.","The authors present a novel method for performing convolutions over graphs, which they apply to predict protein interfaces, showing clear improvements over existing methods. The methodological contribution is strong, and the paper mostly clearly written. However, the authors should evaluate compute costs and more alternative methods for performing graph convolutions.

Major comments
=============
The authors highlight that their method differs from related methods by being designed for pairs (or collections) of graphs (l104). However, the two protein graphs a convolved separately with shared weights, and the resulting node features merged afterward. Any other method can be used to convolve proteins before the resulting features are merged. The authors should clarify or not highlight this difference between their and existing methods.

The authors should compare their method to a second graph convolutional network apart from DCNN, e.g. Schlichtkrull et al or Duvenaud et al.

The authors should clarify if they used the features described in section 3.1.1 as input to all methods. For a fair comparison, all methods should be trained with the same input features.

The authors should compare the memory usage and runtime of their method to other convolutional methods. Does the method scale to large proteins (e.g. >800 residues each) with over 800^2 possible residue-residue contacts? The authors should also briefly describe if computations can be parallelized on GPUs and their method be implemented as a user friendly ‘graph convolutional layer’.

The authors should describe more formally (using equations) how the resulting feature vectors are merged (section 2.3). The authors should also clarify how they are dealing with variable-length proteins which result in a variable number of feature vectors. Are the merged feature vectors processed independently by a fully connected layer with shared weights? Or are feature vectors concatenated and flattened, such that the fully connected layer can model interactions between feature vectors as suggested by figure 2? The authors should also clarify if the same output layer is applied independently to feature vectors or jointly.

Section 3.2: The authors should describe more clearly which hyper-parameters were optimized, both for GCN, PAIRpred, and DCNN. For a fair comparison, the most important hyper-parameters of all methods must be optimized.

l221-225: The authors used the AUC for evaluation. Since labels are highly unbalanced, the authors should also compare and present the area under precision-recall curve. The authors should also describe if performance metrics were computed per protein complex as suggested by figure 3, or across complexes.

Minor comments
=============
l28-32: This section should be toned down since the authors use some of the mentioned ‘hand-crafted’ features as input to their own model.

l90: The authors should clarify what they mean by ‘populations’.

l92-99: The authors should mention that a model without downsampling results in higher memory and compute costs. 

l95: typo ‘classiy’

Table 1: The authors should clarify that ‘positive examples’ and ‘negative examples’ are residues that are or are not in contact, respectively.

l155-162: The authors should mention the average protein length, which is important to assess compute costs (see comment above).

l180: The authors should clarify if the ‘Gaussian function’ corresponds to the PDF or CDF of a normal distribution.

Table 2: The authors should more clearly describe in the main text how the ‘No Convolutional’ model works. Does it consist of 1-4 fully connected layers that are applied to each node independently, and are the resulting feature vectors merged in the same way as in GCN? If so, which activation function was used and how many hidden units? Is it equivalent to GCN with a receptive field of 0?

Since the difference between the mean and median AUC is not clear by looking at figure 3, the authors should plot the mean and median as vertical lines. Since the figure is not very informative, I suggest to move it to the appendix and to show instead more protein complexes as in figure 4.

l192: Did the authors both downsample negative pairs (caption table 1) and give 10x higher weight to positive pairs? If so, it should be pointed out in the text that two techniques were used to account for class-imbalance.

l229: What are ‘trials’? Did the authors use different train/test split, or did they train models multiple times to account for the randomness during training?","The authors propose to use graph convolutional networks for protein interface prediction. A translation and rotation-invariant input feature representation is proposed. A small architecture search shows that for the dataset used, a 2-layer GCN works best. This model outperforms the previous SOTA (an SVM-based method).

This is an interesting and useful new application of graph convolutional networks, but in my opinion, the authors overstate the novelty of their approach when they write “We present a general framework for graph convolution [...]”. Firstly, it is not clear to me that a general framework is presented, rather than a particular architecture. Secondly, I think the architecture that is presented is a special case of what was presented in [1]. The claim that the ability to handle variable-sized graphs is novel is also not true, as this was already done by Duvenaud et al., reference [9] in the paper. 

One important point made in [1] is that it is more useful to do a proper comparison of the many possible GCN architectures for a new problem, rather than present a new architecture just because it hasn’t been used before. This paper only compares a few architectures / depths. Performing a (much) more exhaustive search would significantly strengthen the paper in my opinion. Given the small dataset size this should be doable computationally.

It would be nice to see the standard deviation of the AUC values under re-training. Right now it is hard to get a sense of how significant the difference between the different models is.

The paper is well written.

This paper seems to be a useful contribution to the literature on protein docking, showing a modest improvement over the state of the art. As such, I think the paper would be well-suited for publication in a molecular biology venue, or perhaps as an application paper at NIPS. The main weakness of the paper in my view is that it is a fairly straightforward application of an existing technique (GCNs) to a new domain (plus some feature engineering). As such I am leaning towards a rejection for NIPS.

[1] Neural Message Passing for Quantum Chemistry, Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl
"
Convergence rates of a partition based Bayesian multivariate density estimation method,"Linxi Liu, Dangna Li, Wing Hung Wong",https://proceedings.neurips.cc/paper/2017/hash/f55cadb97eaff2ba1980e001b0bd9842-Abstract.html,"Note: Below, I use [#M] for references in the main paper and [#S] for references in the supplement, since these are indexed differently.

Summary: This paper proposes and analyzes a Bayesian approach to nonparametric density estimation. The proposed method is based on approximation by piecewise-constant functions over a binary partitioning of the unit cube, using a prior that decays with the size of the partition. The posterior distribution of the density is shown to concentrate around the true density f_0, at a rate depending on the smoothness r of f_0, a measure in terms of how well f_0 can be approximated by piecewise-constant functions over binary partitionings. Interestingly, the method automatically adapts to unknown r, and r can be related to more standard measures of smoothness, such as Holder continuity, bounded variation, and decay rate of Haar basis coefficients. As corollaries, posterior concentration rates are shown for each of these cases; in the Holder continuous case, this rate is minimax optimal.



Major Comments:

The theoretical results of this paper appear quite strong to me. In particular, the results on adaptivity (to unknown effective dimension and smoothness) are quite striking, especially given that this seems an intrinsic part of the estimator design, rather than an additional step (as in, e.g., Lepski's method). Unfortunately, I'm not too familiar with Bayesian or partition-based approaches to nonparametric density estimation, and my main issue is that the relation of this work to previous work isn't very clear. Looking through the supplement, it appears that most of proofs are based on the results of [9M] and [14M], so there is clearly some established closely related literature. I'd likely raise my score if this relation could be clarified, especially regarding the following two questions:

1) Is the prior introduced in Section 2.2 novel? If so, how does it differ from similar prior work (if such exists), and, if not, which aspects, precisely, of the paper are novel?

2) Much of the proof of Theorem 3.1 appears to depend on results from prior work (e.g., Lemmas A.2, A.4, B.1, and B.2, and Theorem A.3 are from [4S] and [6S]). Corrolaries 3.2, 3.3, and 3.4 all follow by combining Theorem 3.1 with results from [4S] and [1S] that relate the respective smoothness condition to smoothness in terms of approximability by piecewise-constant functions on binary partitions. Thus, much of this work appears to be a re-working of previous work, while Lemma B.4 and the remainder of Theorem 3.1 appear to be the main contributions. My question is: at a high level, what were the main limitations of the previous results/proof techniques that had to be overcome to prove Theorem 3.1, and, if it's possible to summarize, what were the main innovations required to overcome these?



Minor Comments:

Lines 36-44: It would help to mention here that rates are measured in Hellinger divergence.

Line 40-41: (the minimax rate for one-dimensional Hölder continuous function is (n/log n)^{−\beta/(2\beta+1)}): If I understand correctly, the log factors stem from the fact that \beta is unknown (I usually think of the minimax rate as n^{−\beta/(2\beta+1)}, when \beta is treated as known). If this is correct, it would help to mention this here.

Line 41: small typo: ""continuous function"" should be ""continuous functions""

Lines 60-72: There's an issue with the current notation: As written, \Omega_2,...,\Omega_I aren't well-defined partition by this recursive procedure. If we split \Omega_j at step i, then \Omega_j should be removed from the list of partitions and replaced by two smaller partitions. I think I understand what is meant (Figure 1 is quite clear), but I don't see a great way to explain this with concise mathematical notation - the options I see are (a) describing the process as a tree with a set at each node, and then taking all the leaves of the tree or (b) using a pseudocode notation where the definition of \Omega_j can change over the course of the recursive procedure.

Line 185: I believe the minimax rate for the bounded variation class is of order n^(-1/3) (see, e.g., Birgé, Lucien. ""Estimating a density under order restrictions: Nonasymptotic minimax risk."" The Annals of Statistics (1987): 995-1012.) Perhaps this is worth mentioning?

Lines 157-175: Section 3.1 considers the case of a weak-\ell^p constraint on the Haar basis coefficients of the density. The paper calls this a spatial sparsity constraint. I feel this is misleading, since the sparsity assumption is over the Haar basis coefficients, rather than over spatial coordinates (as in [1M]). As a simple example, the uniform distribution is extremely sparse in the Haar basis, but is in no way spatially concentrated. I believe this is actually a smoothness assumption, since Haar basis coefficients can be thought of as identifying the discontinuities of a piecewise constant approximation function. Indeed, the weak-\ell^p constraint on the Haar coefficients is roughly equivalent to a bound on the Besov norm of the density (see Theorem 5.1 of Donoho, David L. ""De-noising by soft-thresholding."" IEEE transactions on information theory 41.3 (1995): 613-627.)

Line 246: small typo: ""based adaptive partitioning"" should be ""based on adaptive partitioning"".

Line 253: Citation [1M] is missing the paper title. I believe the intended citation is ""Abramovich, F., Benjamini, Y., Donoho, D. L., & Johnstone, I. M. (2006). Special invited lecture: adapting to unknown sparsity by controlling the false discovery rate. The Annals of Statistics, 584-653.""","Summary: The paper presents a Bayesian approach to density estimation. The approach
first constructs a binary partition tree on the domain and then uses a piecewise
constant estimator which is constant on each cell of the partition.
The main theorem gives the rate of posterior concentration in terms of a parameter r
which specifies how well a class is approximated by the partitioning scheme. The
authors bound r for different smoothness classes for the density.

While I did not read the proofs in the appendix, I found the technical exposition
substantial and interesting.

On the truncation of the Dirichlet prior:
- Can you elaborate what exactly the technical condition in Line 127 is? Ideally,
  you should include this detail in the main text.
- I am assuming that the posterior uses the truncated prior? How difficult is it
  to compute this?
- Lines 131-132 and 148-149 suggest picking a large eta, but what do you lose if you
  pick a very large eta? Does this, for instance, affect the rate? (e.g. a 1/eta
  term in front of the rate?)
- If the Dirichlet coefficients are bounded away from 0, then it means that you are
  putting non-zero mass across the entire domain. In instances where the support of
  f0 is a very small set, it appears that such an estimate might be very bad?
  Can you explain why you get the same rates even for such an f0?

Some comments/questions/suggestions:
- It might make sense to separate the discussion about [13] at the end of section 2.2
  into a separate subsection or paragraph. When reading it at first, I had assumed
  that it was an extension of the prior.
- Can you generalise the definition of bounded variation to higher dimensions?
- The Holder definition for Multivariate densities used in Section 3.3 is somewhat
  nonstandard, see for e.g Defintion 3 in [1]. Can you bound r for this definition
  too?
- I thought that the observations in lines 196-199 was quite interesting.

Experiments:
- The main theorems talk about the posterior, but not any specific estimate? What is
  the estimate used in the experiments? Is it the mean of the posterior?

References
[1] Kandasamy et al, Nonparametric Von Mises Estimators for Entropies Divergences and
Mmutual Informations.","The paper presents the concentration rate of partition based Bayesian density estimation. The paper proposes a suitable prior for densities on a binary adaptive partition, and shows explicitly the concentration rates in some specific situations, including the density being spatially sparse, belonging to the space of bounded variation, and being Holder continuous. The theory is well explained and a few relevant concepts are well reviewed. The theoretical contribution is remarkable and numerical examples are provided to illustrate and support the methodological contributions."
Avoiding Discrimination through Causal Reasoning,"Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, Bernhard Schölkopf",https://proceedings.neurips.cc/paper/2017/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html,"This paper formulates fairness for protected attributes as a causal inference problem. It is a well-written paper that makes an important connection that discrimination, in itself, is a causal concept that can benefit from using causal inference explicitly.

The major technical contribution of the paper is to show a method using causal graphical models for ensuring fairness in a predictive model. In line with social science research on discrimination, they steer away from thinking about a gender or race counterfactual which can be an unwieldy or even absurd counterfactual to construct---what would happen if someone's race is changed---and instead focus on using proxy variables and controlling causal pathways from proxy variables to an outcome. This is an important distinction, and allows us to declare proxy variables such as home location or name and use them for minimizing discrimination. To avoid using this proxy variable for prediction, the proposed method first assumes a likely graphical model for all relevant variables and then uses parametric assumptions to cancel out the effect of proxies. 

While the authors make unique theoretical contributions in connecting do-calculus interventions to fairness, in practice, this reveals limitations in using their method: first, we need to be able to construct a valid causal model. This can be hard in practical scenarios of race or gender discrimination when there can be many variables that affect an individual's outcome, and almost all of them may be connected to each other and to the outcome. Second, the statistical correction for proxies depends critically on the assumed functional form of causal relationships, which again can be arbitrary. I would encourage the authors to discuss these limitations briefly, if possible. That said, every other method (even non-causal) suffers from at least one of these limitations implicitly, so in comparison this can be a sound strategy to employ. Further, laying down the causal model enables the research community and policy makers to be explicit about their assumptions, which can enhance clarity and limitations of any predictive method.

In addition,  the authors give a clear (though artificial) example of how causal assumptions are necessary for detecting discrimination: they show that two different causal models can lead to the same observed data distribution, even though only one of them is a discriminatory causal model. I found this quite insightful.

","The authors point out that a causal inference viewpoint is a good way to view algorithmic discrimination and fairness, and then carry through this viewpoint to note its various insights that are unavailable via observational data analysis through standard machine learning and data mining.  This is a good piece of work, with good writing, correct math, and important start to an area of study that clearly needs to be studied.

There is a bit too much material of a tutorial nature.

The authors have not cited https://arxiv.org/abs/1608.03735.  ","The paper proposes a causal view on the question of fair machine learning. Its main contribution is the introduction of causal language to the problem, and specifically the notion of ""resolving variable"": a variable that mediates the causal effect of a protected attribute in a way the user deems ""fair"". An example would be the variable ""choice of departments"" in the famous Simpson's paradox college-admission scenario. 

Overall I think this paper has a lot of potential, but still needs some work on clarifying many of the concepts introduced. Fairness is a difficult subject as it clearly goes beyond the math and into societal and normative discussions. As such, I am looking forward to the authors replies to my comments and questions below, hoping to have a fruitful discussion.

Pros:
1. This paper present an important contribution to the discussion on fairness, by introducing the notion of resolving variable. I have already found this term useful in discussions about the subject.

2. The paper shows how in relatively simple scenarios the proposed definitions and methods reduce to very reasonable solutions, for example the result in Theorem 2 and its corollaries. 

Cons:
1. No experimental results, not even synthetic. Other papers in the field have experiments, e.g. Hardt Price & Srebro (2016) and Chulechova (2016).

2. Lack of clarity in distinction between proxies and non-resolving variables.

3. I am not convinced of the authors view on the role of proxy variables.

4. I am not convinced that some of the proposed definitions capture the notion of fairness the authors claim to capture.


Specific comments:

1. What exactly is the difference between a proxy variable and a non-resolving variable? Does the difference lie only in the way they are used?

2. Why is the definition of proxy discrimination (def. 3) the one we want? 
Wouldn't the individual proxy discrimination be the one which more closely correlates with our ideas of fairness?
E.g., say name (variable P) affects past employment (variable X), which in turn affects the predictor. Couldn't averaging this effect over X's (even considering the do-operator version) lead to a predictor which is discriminating with respect to a specific X?

3. Why is the notion of proxy variables the correct one to control for? I think on the contrary, in many cases we *do* have access to the protected attribute (self-declared race, gender, religion etc.). It's true that conceiving intervention on these variables is difficult, but conceiving the paths through which these variables affect outcomes is more plausible. The reason gender affects employment goes beyond names: it relates to opportunities presented, different societal attitudes, and so far. The name is but one path (an unresolved path) through which gender can affect employment. 

4. The wording in the abstract: ""what do we want to assume about the causal DGP"", seems odd. The causal DGP exists in the world, and we cannot in general change it. Do the authors mean the causal DGP *that gives rise to a predictor R* ? 

5. Why have the mentioned approaches to individual fairness not gained traction?

6. The paper would be better if it mentioned similar work which recently appeared on arxiv by Nabi & Shpitser, ""Fair Inference on Outcomes"". In general I missed a discussion of mediation, which is a well known term in causal inference and seems to be what this paper is aiming at understanding. 

7. I am confused by the relation between the predictions R and the true outcomes Y. Shouldn't the parameters leading to R be learned from data which includes Y ?

8. In figure 4, why does the caption say that ""we generically do not want A to influence R""? Isn't the entire discussion premised on the notion that sometimes it's ok if A influences R, through a resolving variable? 

Minor comments:

1. line 215 I think there's a typo, N_A should be N_P?

"
Alternating Estimation for Structured High-Dimensional Multi-Response Models,"Sheng Chen, Arindam Banerjee",https://proceedings.neurips.cc/paper/2017/hash/f60bb6bb4c96d4df93c51bd69dcc15a0-Abstract.html,"Bounds on the estimation error in a multi-response structured Gaussian linear model is considered, where the parameter to be estimated is shared by all m responses and where the noise covariance may be different from identity. The authors propose an alternating procedure for estimation of the coefficient vector that essentially extends the GDS (generalized Dantzig selector). The new procedure accommodates general ""low-complexity"" structures on the parameter and non-identity covariance of the noise term. 

The paper is outside my field of expertise, so I apologize in advance for any mistakes due to my misunderstanding. 
The paper is very technical, and without the appropriate background, I was not able to really appreciate the significance of the contribution. That said, the theme of the results is quite clear, especially with the effort of the authors to supplement the results with intuitive explanations. 

1. The paper is very technical, maybe too technical, and it is hard for a non-expert like myself to make sense of the conditions/assumptions made in the different theorems (despite the authors' evident effort to help with making sense of these conditions). 
2. Again, I was not able to appreciate if the different assumptions and results are sensible or whether they are mostly ad-hoc or artifacts of the proofs, but overall the contribution over GDS and existing methods does not appear to be sufficiently significant for acceptance to NIPS. 
3. The manuscript is not free of grammatical errors or bad wording, for example:
-l.168: ""at the same order""
-caption of Fig 2: ""The plots for oracle...implies...""
-caption of Fig 2: ""keep unchanged"" (better: ""remain unchanged"")

## comments after reading author rebuttal:
The authors have provided a clear non-technical summary of the paper, which is much appreciated. I would like to apologize for not being able to give a fair evaluation when originally reviewing the paper (this is because I did not have the technical background to appreciate the results). 
","The paper studies the problem of estimation in high dimensional multiple response regression. 
The author proposes a new estimation procedure, and give a non-asymptotic finite sample error bound, 
which is the first for this type of problem, except for related and more restricted 
non-asymptotic guarantees shown in [21].  
Overall, the paper is clearly written and seems to provide new interesting results for this problem. 

The problem seems very related to multiple regression problem in statistics, where some structure
is assumed to hold for the beta coefficients for different responses. It would be good to refer
to the relevant statistics literature. 

The upper-bound on estimation error in Theorem 3 is complicated and involves many terms. It is not 
clear if all of them are necessary/tight, how does each one affect the actual estimation error etc. 

The authors also show simulation results, showing favourable accuracy close to an oracle procedure. 
It is however not explained what is the oracle estimator, defined in eq. (27), assuming, 
and why is it impossible to use it in practice. 


Minor: 
Line 248: Throughout out -> Throughout 

","The authors propose an alternating estimation (AltEst) procedure to estimate
the model parameters of the problem of learning high-dimensional multiresponse linear models, based on the generalized Dantzig selector (GDS). Though it is a straightforward way to ""solve"" a non-convex problem, the authors also show its statistical guarantee and verify the results with experiments. 

I would say that showing the alternating estimation procedure is statistically sound might be more interesting that the problem itself, and may help us understanding some other similar problems. "
Multimodal Learning and Reasoning for Visual Question Answering,"Ilija Ilievski, Jiashi Feng",https://proceedings.neurips.cc/paper/2017/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html,"Summary -- 
The paper introduces a novel modular neural network for multimodal tasks such as Visual Question Answering. The paper argues that a single visual representation is not sufficient for VQA and using some task specific visual features such as scene classification or object detection would result in a better VQA model. Following this motivation, the paper proposes a VQA model with modules tailored for specific tasks -- scene classification, object detection/classification, face detection/analysis -- and pushes the state-of-the-art performance.


Strengths -- 
-- Since VQA spans many lower level vision tasks such as object detection, scene classification, etc., it makes a lot of sense that the visual features tailored for these tasks should help for the task of VQA. According to my knowledge, this is the first paper which explicitly uses this information in building their model, and shows the importance of visual features from each task in their ablation studies.

-- The formulation of their “module” structure is quite generic and many other modules corresponding to other low level computer vision tasks can be added to further improve the model performance.

-- The approach has been explained in detail.


Weaknesses --

-- The authors claim that their model is interpretable, but do not support this claim by any studies/analyses/qualitative examples. I agree that the ablation studies show the significance of each of the modules used in the proposed model, but it is not clear if the model is actually using the visual features from the correct module for a given question, e.g. for the question “Does the woman look happy?”, is it really the face analysis module which provides the correct signal to accurately predict the answer? This can be studied by analysing (e.g. making a pie chart of) the questions whose predictions were incorrect when the face analysis module was absent but are correct after adding the face analysis module. Similar studies can be done for all the modules. Without any such analysis, I am not convinced if the model is actually doing what the motivation of the paper says. If the author can report any studies/analyses supporting this, I am happy to increase my rating.

-- The paper does not talk about the weaknesses of the proposed model. Discussions, ideally using failure cases, about limitations of the proposed model and what is needed to improve it are very important for continuous research in this area and should be an integral part of any paper.

-- I am confused about the mathematics of some equations. In Eq 1, how is the matrix multiplication of 2 matrices P and W resulting in a vector c? In Eq 3, what dimension is the max pooling happening over? In Eq 4, should it be transpose of r_{k} which is being premultiplied with tensor W_{k}?

-- In Table 2, the MCB numbers reported are test-dev numbers instead of test-standard. MCB only reports test-standard numbers for their ensemble model with data augmentation. Please fix.

-- For classification modules, have the authors tried using soft probability scores instead of picking top k class labels?

-- Minor:
- A lot of cited papers have been published. It might be better to cite the conference version instead of arXiv.
- r_{q} and r^{l} has been used interchangeably. At many places, subscripts and superscripts have been used interchangeably. Please fix.
- Line 9: “does not limit” → “limits”

After author rebuttal: I thank the authors for the additional analyses. The rebuttal addresses all my concerns. I think this paper introduces an interesting and novel approach for VQA, which would be useful to the VQA research community. Therefore, I recommend the paper for acceptance. I have changed my rating to 7.","Summary
This paper proposes an approach that combines the output of different vision systems in a simple and modular manner for the task of visual question answering. The high level idea of the model is as follows. The question is first encoded into a bag of words representation (and passed through an MLP). Then different vision systems (which extract raw features, or compute attention on images using object detection outputs or face detection outputs or scene classification outputs) are all condensed into a representation compatible with the question. Finally the approach takes an outer product between the image representations from various tasks and the question, and concatenates the obtained representations. This concatenated feature is fed as input to an answering model which produces distributions over answer tokens. The entire model is trained with max-likelihood. Results on the VQA 1.0 as well as VQA 2.0 datasets show competitive performance with respect to the state of the art.

Strengths
1. At a high level the proposed approach is very well motivated since the vqa task can be thought of as an ensemble of different tasks at different levels of granularity in terms of visual reasoning. The approach has flavors of solving each task separately and then putting everything together for the vqa task.
2. The results feature ablations of the proposed approach which helps us understand the contributions of different modules in achieving the performance gains.
3. It is really encouraging that the approach obtains state of the art results on VQA. Traditionally there has been a gap between modular architectures which we “feel” should be good / right for the task of VQA and the actual performance realized by such models. This paper is a really nice contribution towards integrating different vision sub-problems for VQA, and as such is a really good step.

Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.

2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.

3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.

4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.

5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.

6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?

7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences). 

8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?


Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (aren’t we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?

Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*). 


[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. “Neural Module Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. “Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. “Simple Baseline for Visual Question Answering.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167."
Generative Local Metric Learning for Kernel Regression,"Yung-Kyun Noh, Masashi Sugiyama, Kee-Eung Kim, Frank Park, Daniel D. Lee",https://proceedings.neurips.cc/paper/2017/hash/f69e505b08403ad2298b9f262659929a-Abstract.html,"This paper follows the idea of “Generative local metric learning for nearest neighbor classification”, and analyze how metric can help for kernel regression with Nadaraya-Watson (NW) estimator. Although linear transformation (metric) has no impact on NW-kernel regression when the number of instances goes to infinity, it will reduce the bias given finite samples. The authors analyze that metric learning can significantly reduce the mean square error (MSE) in kernel regression, particularly for high-dimensional data. Together with a generative model, the method combines both the global and local information together. Experiments on both synthetic and real datasets show the effectiveness of the proposed method.

The whole paper is well-written and easy to follow. Analyses, discussion, and comparisons of the proposed method and its relativeness with other methods are clear. 

Two suggestions on the paper:
1. Fig.1 gives a direct illustration of the impact of metric in kernel regression. There need more statements on the effect of \nabla y. 
2. There needs a generative model to estimate the probability of data before computing the metric, and a single Gaussian model is used. Some of the following derivations are based on this assumption, the authors should make clear which computations are general and which are specific. 
","The idea of optimizing metric matrix used for NW regressor by explicitly counteracting the leading term of the bias of regressor is interesting. The (essentially) two-rank metric matrix construction is nice and experiments supports the proposed method well. 

My main concern is that two assumptions in Prop.2, 
1) p(x) is away from zero, and 
2) \nabla p(x) and \nabla y(x) are not parallel,
are guaranteed or validated. 
The possibility of the violation of the assumption 2) could be negligible, but it is highly possible that p(x) is close to zero and the bias term diverges. Does the regularization in eq.(21) safely solves the problem ?

Minor point:
in supplementary material, eq.(11), the right most L^{-1} must be L^{T}. eqs.(11) and (12), subscript x for L should be removed. 
","Metric learning is one of the fundamental problems in person re-identification. This paper presents a metric learning method using Nadaraya-Watson (NW) kernel regression. The key feature of the work is that the NW estimator with a learned metric uses information from both the global and local structure of the training data. Theoretical and empirical 9 results confirm that the learned metric can considerably reduce the bias and MSE for kernel regression even when the data are not confined to Gaussian.

The main contribution lies in the following aspects: 

1.	Provided a formation on how metric learning can be embedded in a kernel regression method.
2.	Proposed and proved a theorem that guarantees the existence of a good metric that can eliminate the leading order bias at any point regardless of assumption of Gaussian distribution of the data.
3.	Under Gaussian model assumption, provided an efficient algorithm that can guarantee to achieve an optimal metric solution for reducing regression error. 

Overall, this paper’s reasoning is rigorous. Description is clear. The experiments sufficiently demonstrate the author’s claims for the proposed method.

However, it needs major revision before it is accepted. My comments are as follows:

1. The structure of the paper is confused. I suggest the authors add contributions of the papers in the introduction section. The usage and importance of this paper needs to be further clarified.

2.  I also find some grammar problems in this paper. Author needs to carefully check these mistakes, which is very important for readers. 

3. The limitations of the proposed method are scarcely discussed.

4. The works also cannot describe the latest developments since there are too few references after 2016.

5. The authors only show very few experiments results in section 5, I think it is not enough. Except MSE, the authors also should provide other comparisons, such as Root mean squared error of classication accuracy(RMSE) or MSE, etc. In this way, the results of the paper will be more reasonable.


"
Overcoming Catastrophic Forgetting by Incremental Moment Matching,"Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, Byoung-Tak Zhang",https://proceedings.neurips.cc/paper/2017/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html,"The authors propose several methods for achieving continual learning by merging Gaussian posteriors of networks trained on several separate tasks: either take the mean or the mode. The techniques are simple but well motivated. Experiments suggest that these methods improve catastrophic forgetting in several tasks.

The paper has a nice review of related work, and the derivations and justifications are good. I found the paper an interesting read. The experiments appear to be well done and sufficient detail is included that they should be reimplementable.

I have one question concerning (7) and (8): practically, as there is a matrix inverse involved, did the authors do something the ensure numerical stability?
","[UPDATE] 
After digging through the supplementary material I have managed to find at least a hint towards possible limitations of the method being properly acknowledged. Not including an objective evaluation of limitations is a flaw of this otherwise well written paper, especially when the method relies crucially on weight transfer (as the authors point out outside the main paper, i.e. supplementary text and rebuttal). However, weight transfer is known to be an inadequate initialization technique between different problem classes and the authors don't clearly address this issue, nor do they properly qualify the applicability of the method.

In balance, this paper does give sufficient evidence that weight transfer and some form of parameter averaging are promising directions of future investigation, at least in a subset of interesting cases. Perhaps with publication chances of expanding this subset increase.

[REVIEW TEXT]
The paper evaluates the idea of incremental moment matching (IMM) as a solution for catastrophic forgetting in neural networks. The method is thoroughly benchmarked, in several incarnations, against state-of-the-art baselines on standard ‘toy’ problems defined on top of MNIST, as well as more challenging ImagNet2CUB and the Lifelog dataset. A new parameterization, dubbed ‘drop-transfer’ is proposed as an alternative to standard weight initialization of model parameters on new tasks. 

Pros:
- Comprehensive experiments are provided which show superior performance on standard MNIST classification variants, in comparison to previous methods.
- Encouraging results are shown on natural image datasets, although here many earlier methods were not applied, so there were few literature results to compare against.

Cons:
- Although differences exist between the pairs or triplets of problems considered to illustrate ‘catastrophic forgetting’, technically speaking, these problems are extremely related, sharing either the input support or the class distribution to some degree; this is in contrast to current challenges in the field, which consider sequences of arbitrarily different reinforcement learning problems, such as different Atari video games (Kirkpatrick et al. 2016, Rusu et al., 2016). Therefore, I find it difficult to tell how proficient the method is at reducing catastrophic forgetting in the general case. I recommend considering more standard challenges from the literature. 
- Another important consideration for such works is evaluating how much learning is possible with the given model when following the proposed solution. Since catastrophic forgetting can be alleviated by no further learning throughout most of the model, it follows that proficient learning of subsequent tasks, preferably superior to starting from scratch, is a requirement for any ‘catastrophic forgetting’ solution. As far as I can tell the proposed method does not offer a plausible answer if tasks are too different, or simply unrelated; yet these are the cases most likely to induce catastrophic forgetting. In fact, it is well understood that similar problems which share the input domain can easily alleviate catastrophic forgetting by distilling the source task network predictions in a joint model using target dataset inputs (Li et al., 2016, Learning without Forgetting).

In summary, I believe variations of the proposed method may indeed help solve catastrophic forgetting in a more general setting, but I would like to kindly suggest evaluating it against current standing challenges in the literature.
"
Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent,"Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu",https://proceedings.neurips.cc/paper/2017/hash/f75526659f31040afeb61cb7133e4e6d-Abstract.html,"This paper studies a decentralized parallel stochastic gradient descent (D-PSGD) and shows that decentralized algorithms might outperform centralized algorithms in this case. In this decentralized setting, every node performs SGD and exchanges information with neighbors using decentralized gradient descent (DGD). D-PSGD applies stochastic on the gradient information in DGD. With some assumptions on the functions, the authors show the theoretical result on when it converges. The numerical experiments comparing with C-PSGD is very interesting and promising. 
	  
The stepsize depends on the number of iterations, which is mostly used in analysis of stochastic algorithms. A too small stepsize usually means a slow algorithm and a large stepsize will provide a solution that is too far from the optimum.  Therefore, diminishing stepsizes are used in DGD and SGD, one question is what happens when the diminishing stepsize is used in D-PSGD.
","The authors study the convergence rate of decentralized algorithms for parallel gradient descent methods. They show with their analysis that the decentralized methods have less communication complexity than the centralized methods. To support their analysis the authors also provide computational results, where they demonstrate that decentralized parallel algorithms run faster than the centralized parallel algorithms.

The analysis is correct. However, it begs for more explanation. For instance, if I understand correctly, Corollary 2 states that K should be larger than the maximum of the two right-hand sides in relations (5) and (6). This implies that K may become too large and hence the step length may become very small. Then do we still have convergence with a constant step length? I may be missing a point here. Along this line, can the analysis be generalized to decreasing step lengths case?

Can the results be improved if the authors concentrate on (strongly) convex objective functions?
","In this paper, the authors present an algorithm for decentralized parallel stochastic gradient descent (PSGD). In contrast to centralized PSGD where worker nodes compute local gradients and the weights of a model are updated on a central node, decentralized PSGD seeks to perform training without a central node, in regimes where each node in a network can communicate with only a handful of adjacent nodes. While this network architecture has typically been viewed as a limitation, the authors present a theoretical analysis of their algorithm that suggests D-PSGD can achieve a linear speedup comparable to C-PSGD, but with significantly lower communication overhead. As a result, in certain low bandwidth or high latency network scenarios, D-PSGD can outperform C-PSGD. The authors validate this claim empirically.

Overall, I believe the technical contributions of this paper could be very valuable. The authors claim to be the first paper providing a theoretical analysis demonstrating that D-PSGD can perform competitively or even outperform C-PSGD. I am not sufficiently familiar with the relevant literature to affirm this claim, but if true then I believe that the analysis provided by the authors is both novel and intriguing, and could have nontrivial practical impact on those training neural networks in a distributed fashion, particularly over high latency networks.

The authors additionally provide a convincing experimental evaluation of D-PSGD, demonstrating the competitiveness of D-PSGD using modern CNN architectures across several network architectures that vary in scale, bandwidth and latency.

I found the paper fairly easy to read and digest, even as someone not intimately familiar with the parallel SGD literature and theory. The paper does contain a number of small typographical errors that should be corrected with another editing pass; a small list of examples that caught my eye is compiled below.

-- Minor comments --
Line 20: ""pay"" -> ""paying""
Line 23: ""its"" -> ""their"", ""counterpart"" -> ""counterparts""
Line 33: ""parameter server"" -> (e.g) ""the parameter server topology""
Algorithm 1: the for loop runs from 0 to K-1, but the output averages over x_{K}.
Line 191: ""on"" -> ""in""
Line 220: ""of NLP"" -> ""of the NLP experiment""
"
Gradient Descent Can Take Exponential Time to Escape Saddle Points,"Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Aarti Singh, Barnabas Poczos",https://proceedings.neurips.cc/paper/2017/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html,"It has recently been shown that, when all the saddle points of a non-convex function are ""strict saddle"", then gradient descent with a (reasonable) random initialization converges to a local minimizer with probability one. For a randomly perturbed version of gradient descent, the convergence rate can additionally be shown to be polynomial in the parameters. This article proves that such a convergence rate does not hold for the non-perturbed version: there exists reasonably smooth functions with only strict saddle points, and natural initialization schemes such that gradient descent requires a number of steps that is exponential in the dimension to find a local minimum.


I liked this article very much. It answers a very natural question: gradient descent is an extremely classical, and very simple algorithm. Although it is known not to be the fastest one in many situations, it is widely used in practice; we need to understand its convergence rate. The proof is also conceptually simple and elegant, and I found its presentation very clear.

My only criticism is that it seems to me that there are a lot of miscalculations in the proof, notably in the proof of Lemma B.2. I absolutely do not think they are a problem for the correctness of the proof, but I would appreciate them to be corrected before the article is published.


Minor remarks and typos:
- l.91: ""postpone in"" should be ""postpone to""
- l.109: """"strict"" defined"" should be """"strict"", which is defined""?
- l.115: ""the approximation to"" should be ""approximate""?
- Paragraph ""initialize far, far away"": some condition on h is needed to ensure that x_2 does not suddenly increase when x_1 is between -2 and -1. (I understand that this discussion is informal anyway, but it could be briefly mentioned.)
- l.163: ""in previous"" should be ""as in previous""
- l.166: why [-1/2;1/2] and not [-1;1]?
- l.167: ""has"" should be ""have""
- l.173: ""from of"" should be ""from""
- l.191: no comma before the closing parenthesis
- Corollary 4.3: ""The for""? ""satisfies"" should be ""that satisfies""; ""same of"" should be ""same as in""
- l.217: ""Guassian""
- l.218, ""where ..."": a verb is missing.
- Equation (3), second and third line, right hand side: x should be x_1.
- l.241: ""still satisfy the assumptions"" should be ""the assumptions are still satisfied""
- l.250: ""constraint"" should be ""constrained""
- l.251 and 438: ""make as"" should be ""make""?
- l.252: remove ""both coordinate"".
- l.261: ""(14)and"" should be ""(14) and""
- l.281: ""when if"" should be ""if""
- I know that the conjunction ""that"" can often be omitted after verbs like ""know"", ""note"", ""imply"" ... However, I would personally find the sentences easier to understand if ""that"" was not omitted when it introduces a complex proposition (lines 274, 388, 391 ...).
- l.367: I do not understand ""We connnect ... in a spline"".
- l.376: ""where"" should be ""which""
- Page 12: the number of the figure should be written in the caption.
- l.397: ""above lemma"" should be ""lemma below""
- l.403: remove ""for GD""
- l.406: ""the iterate"" should be ""the iterates""
- Page 13: it seems to me that we need to know that the iterates stay in D_0 in order for the first equations (after line 414) to be valid (which is proved after the equations).
- Equation after l.421, second line: I think that the inequality given by Lemma B.2 yields a \geq, not \leq.
- Line after this equation: ""where the"" should be ""where for the"".
- Equation after l.426, second line: a ""2"" is missing.
- I do not think that the definition in Equation (12) is correct (because, with this definition, D_a does not depend on a).
- Equation between (15) and (16): shouldn't a distinction also be made on the sign of x_{i+1}?
- Step 2: why couldn't we simply define f(x_1,\dots,x_d) by f(x_1,\dots,x_d)=f(|x_1|,\dots,|x_d|)? It seems to me that it is more or less what is done, but written in a more complicated way. Maybe I did not understand this part.
- l.460: ""agree"" should be ""agree with""
- l.471: maybe also say that there is no noise added when \tau \leq x_k\leq 2\tau?
- l.472: I think the assumption should be r \leq \tau/20.
- l.478: ""same argument"" should be ""same argument as""
- l.496: the comma should be on the previous line.
- Theorem B.1: the assumption that f'(y_0)\leq 0 is also needed in order to ensure p(y) \leq f(y_0).
- According to my computations, g_1''(\tau) is equal to -4\gamma, while the continuity assumption on the first derivative requires -2\gamma.
- l.521: I do not see why -g_1(2\tau)+4L\tau^2 is equal to (17\gamma/6+11 L/2)\tau^2.
- I do not see why \nabla f_{i,2} is equal to \nabla f_{i+1,1} (for the derivative along x_i, I do not find the same value).
- Theorem B.3: it seems to me that this is an over-simplification of [Chang, 2015, Theorem 1.3] (in particular, but not only, because I think that the notion of C^m functions does not really make sense on generic closed subsets of \R^d).","The authors construct examples of smooth bounded non-convex optimization problems where standard gradient descent with random initialization takes exponential time (in dimension) to converge to second order stationary points. In comparison, for such functions, the perturbed version of gradient descent is known converge in polynomial time. 

The paper provides an important negative result on a significant problem of interest and I vote to accept. That said the examples presented are very special where there gradient path leads though a sequence of d contiguous saddle regions before reaching the local optima. It does not provide much intuition of the general class of problems where GD can or cannot have polynomial time convergence. 

Writing:
Fig 2: What values of tau are used? Also I think contour plots with the saddle regions marked out for construction in (4) and (3) might be easier to interpret. 

[Edit]: I have read the author response. 
","This paper proves that gradient descent (GD) method can take exponential time to escape strict saddle points. This answers an interesting open question closely related to [Lee et al 16] which showed that GD can escape strict saddle points. As a result, this paper implied that in terms of time complexity, there is indeed a large gap between GD and perturbed GD in the worst case. The answer to the complexity of GD escaping strict saddle points has been much needed for a while, and this paper gave a nice answer to the question. So in my opinion it is a clear acceptance. 
    I have one comment about the initialization. This paper tried hard to go from the “unnatural initialization” to “fairly natural random initialization”. Such an effort is great, and the result is strong and convincing. Nevertheless, choosing initialization method and choosing counter-examples is like a game. This paper first chooses an initialization scheme such as uniform over [0,1], standard Gaussian or an ell_infinity ball as in Corollary 4.3, then choose the counter-example. One immediate question is what if someone chose an initial point on the surface of a large ball containing the local-mins, or a uniform distribution of points in a large cube, say, from -10c to 10c. The answer seems unclear to me. One may argue that in practice people usually fix a simple initialization scheme; but I would say that is because they are already satisfied with that initialization scheme, and if they found the results too bad, experienced practitioners may change the initialization scheme — at least they may try increasing the initialization regime. I don’t expect the authors to analyze the performance of GD for “adaptive random initialization” — this question is just one of many possible questions (like smoothed analysis). A short comment in the paper or the response would be enough. "
Dual Path Networks,"Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng",https://proceedings.neurips.cc/paper/2017/hash/f7e0b956540676a129760a3eae309294-Abstract.html,"The paper proposes a new CNN architecture named Dual Path Network (DPN), which carefully blends elements from ResNets and DenseNets. The paper studies ResNets, DenseNets, and the newly proposed DPNs from the perspective of RNNs. It then proposes a careful hybrid design which combines the benefits of both ResNet and DenseNet models. The paper comes with a very strong experimental evaluation. The authors compare the proposed DPN with other state-of-art architectures (ResNet, Inception-ResNet, ResNeXt) and show favorable results for comparable or computational and memory cost, not only on ImageNet classification but also on other visual recognition tasks (object detection, semantic segmentation, etc).
Given the importance of high-performing network backbones in computer vision tasks and the strong results presented in this paper, I find that this paper should clearly be accepted for publication at this conference.","In this paper, the authors propose a new deep CNN architecture, dubbed Dual Path Networks (DPN). The authors first cast the two of the most popular cnn architectures (ResNet and DenseNet) in the HORNN framework and then propose a simple network augmentation that take the advantages of both architectures. The augmentation is fairly simple. The proposed system achieves good performance results in different vision tasks (classification, detection, semantic segmentation) and the architecture design allows lower complexity and faster training.

Although the paper does not have any big novelty (the proposed model is basically a mixture of resnets and densenets), the authors showed different SOA cnn architectures can be cast on the HORNN framework and they achieve SOA results (with improved training) in different vision tasks. 
The paper contains a bunch of typos and should be re-reviewed in terms of English. I also deeply recommend the authors to release the source code.","The authors propose a new network architecture which is a combination of ResNets and DenseNets. They introduce a very informative theoretical formulation which can be used to formulate ResNets, DenseNets and their proposed architecture. The authors present compelling results, analysis and statistics on compelling benchmarks.

Pros:
(+) The paper is well written with theoretical and empirical results
(+) The authors provide useful analysis and statistics
(+) The impact of DPNs is shown on a variety of computer vision tasks
(+) The performance of the DPNs on the presented vision tasks is compelling
Cons:
(-) Optional results on MS COCO would make the paper even stronger

Network engineering is an important field and it is important that it is done correctly, with analysis and many in depth experiments. The impact of new architectures comes through their generalization capabilities. This paper does a good job on all of the above. Even though there is no groundbreaking novelty in the proposed micro-blocks, the authors provide extensive analysis and statistics for their proposed model. They also present results on a variety of computer vision tasks which makes their paper complete. Especially the analysis of memory usage, training speed and number of flops is very revealing and informative. 

The authors present results on compelling computer vision tasks, such as ImageNet, Places365 and PASCAL VOC. These are traditionally the most difficult datasets and tasks in computer vision. It would make the paper even stronger if the authors presented results on MS COCO, which is harder than PASCAL VOC."
Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit,"Laurence Aitchison, Lloyd Russell, Adam M. Packer, Jinyao Yan, Philippe Castonguay, Michael Hausser, Srinivas C. Turaga",https://proceedings.neurips.cc/paper/2017/hash/f80bf05527157a8c2a7bb63b22f49aaa-Abstract.html,"This papers proposes an inference method of (biological) neural connectivity from fluorescence (calcium ) traces. The model includes the spiking model (GLM+low-rank factor) with an external input (optical stimulation) and a fluorescence model. The inference methods is based on variational Bayes, where the approximate posterior is modeled using a neural network. 

Novelty and originality: The methods in this paper are adequately novel and original, nicely combining various elements from previous work.

Technical issues: My main problem with this paper is that I can't really be sure that the proposed method is actually working well. It is very good that the authors tested their method on real data, but since there is no ground truth, I it is hard to estimate the quality of the inferred weights (see footnote (1) below). Given that there is no ground truth, what would really help to convince the reader that everything is working as it should, is some basic check on simulated data to show that the inferred weights are reasonably accurate. Also, it is hard to interpret Fig 5 due to its scale: how can the reader know that an improvement from 1.02 to 1.05 in the ELBO is significant? 

Clarity: The methods part seems a bit dense, with various details not completely clear from the text. I would suggest adding more complete details in the supplementary, such as the complete objective being optimized. This would also make it easier for the reader to understand the context of section 2.5. Also, from section 2.1 (e.g., lines 69-70) I thought that the weights are also treated as latent variables, and so have a recognition model. But since no recognition model is specified for the weights, so I guess I was wrong.  

Minor issues:
1) In Figure 2A, the approximate ""sum of ReLU"" softplus, we have c > 0, i.e. it to does not go to zero for low firing rates. Maybe using an exponential tail in that section would improve the approximation (at is still integrates with a Gaussian, and can upper bound a logistic tail). 
2) For Gaussians, the notation is supposed to be N(mu,sigma^2), not N(mu,sigma).
3) line 192: Fig 6B -> Fig 6E.
4) Figure 6 third line in caption: remove ""A.""
5) The index 'c' is for ""cell""? This should be made clearer. 

Footnote:
(1) The authors argue that their method works since the properties of the detected weights match the expected biological statistics (Fig. 6), but this does not seem convincing. Specifically, It is a good sanity check, but these results only indicate that the method does not fail completely: it is not hard to think of cases in which we get the same properties, yet the estimate weights are highly inaccurate. For example, the observed distance dependence (Fig 6B) could be related to partial mixing of fluorescence signal of nearby neurons. With stimulation this could also be related to the ""area affect"" of the stimulation (Fig 6F), though this is partially compensated by the method. Similarly, the correlation of weight strength to strong Spont. and Signal. correlations might be the result that all of these factors typically increase with the firing rate of the neuron. 


%%% After authors' feedback %%%
The new results and clarifications address my concerns. I therefore have modified my score. I would like to see the limit that the correlation -> 1 on the synthetic data set in the revised paper, to make sure the method is consistent.","This paper applies variational autoencoders (with all the latest tricks) to the problem of estimating connectivity between neurons from Calcium imaging data with and without optogenetic perturbations.
This is an excellent paper with no major flaws as far as I can tell.
My only issue with the paper is that it confounds sparse connectivity with biologically plausible connectivity in a few places (eg. Line 232). Just because the connectivity is sparse doesn’t mean that it’s anatomically accurate. The correlations in Fig 6 give some indication that the connectivity is realistic, but my guess is that something like graphical lasso would also show these patterns. What would be more compelling is some results on reconstruction accuracy (with leave-neuron-out cross-validation). This could help clarify the practical extent of the model improvements, but, otherwise, this is a great paper.
","Summary

This paper tackles the problem of inferring the functional connectivity of the brain, combining the recordings of neural activity by calcium imaging with optogenetic activity perturbations at a cellular level. The authors propose a Bayesian generative approach to jointly model the spiking neural activity, the optogenetic perturbations, the calcium imaging measurement process, the synaptic connectivity and a low-rank manifold representing the activity in the rest of the brain. They resort to stochastic variational inference to approximate the posterior using sampling and the reparametrization trick to compute expectations when they cannot be computed in closed form. Finally, they perform several experiments showing the relevance of the model.

Qualitative Assessment 

The paper is well written and it addresses a problem relevant in the neuroscience community. The introduction successes at motivating the problem of unraveling the functional connectivity of the brain based of optogenetic perturbations and calcium imaging recordings, and describe the key elements that a sensible model should tackle. In my opinion, the main novelty of the paper is the well-thought Bayesian generative model that they proposed. Regarding the inference approach, the authors use standard stochastic variational inference. My main concern is regarding the experiments that the authors propose to validate the model. In particular, in Figure 5 they use the ELBO to compare the different proposed models. The ELBO is just a lower bound of the evidence of the data given the model. However, when comparing different models with different complexity (different number of parameters/hyper-parameters) there is no guarantees that the tightness of the bound is similar to all of them, making the comparison to totally fair. 

%%% After authors' feedback %%%
In light of the clarifications provided by the authors, I have modified my score. In the revised version I would like to see the experiments with synthetic data as well as a more in depth discussion of the consequences of using the ELBO for model selection."
Universal consistency and minimax rates for online Mondrian Forests,"Jaouad Mourtada, Stéphane Gaïffas, Erwan Scornet",https://proceedings.neurips.cc/paper/2017/hash/f80ff32e08a25270b5f252ce39522f72-Abstract.html,"Summary: 
This paper proposes a modification of Mondorian Forest which is a variant of Random Forest, a majority vote of decision trees. The authors show that the modified algorithm has the consistency property while the original algorithm does not have one. In particular, when the conditional probability function is Lipschitz, the proposed algorithm achieves the minimax error rate, where the lower bound is previously known.

Comments:
The technical contribution is to refine the original version of the Mondorian Forest and prove its consistency. The theoretical results are nice and solid. The main idea comes from the original algorithm, thus the originality of the paper is a bit incremental. 
I’m wondering that in Theorem 2, there is no factor about K, the number of Mondrian trees, as authors mentioned in Section 5. In my intuition, it is natural that many K results more accurate and converges faster. Ultimately, Theorem 2 says the Mondrian forest works well even if K=1. Is there any theoretical evidence of advantage by using many Mondrian trees?
 Computational time of Algorithm 4 is not clear. It seems to me that increasing lifetime parameters leads more time consumption in SplitCell. 
 In theorem 2, it is natural to measure the difference between two distributions by KL divergence. Do you have any idea of using another discrepancy measure instead of the quadratic loss? Additionally, the proof, the variance of noise \sigma assumed to be a constant. Is it a natural setting?

Minor Comments:
Line 111 g_n(x,Z_k) -> \hat{g}_n(x,Z_k)
Line 117 \nu_{d_\eta} -> \nu_{\eta}
Line 3 of pseudocode of Algorithm 2 \tau_A -> \tau
Line 162 M_{\lambda^{\prime}} \sim MP(\lambda,C) -> M_{\lambda^{\prime}} \sim MP(\lambda^{\prime},C)


","In this paper, the authors study the consistency of an algorithm of Mondrian Forest. First, they propose to use an increasing lifetime parameters in Mondrian Forest, and develop an updating rule that works in online setting. Then, they establish simple conditions for consistency, and also prove that their algorithm achieves the minimax rate for the estimation of a Lipschitz regression function.

Strong Points:
1. This paper is well-motivated. 
2. The authors present conditions for consistency of their modified Mondrian Forest, which is novel.
3. The convergence rate is minimax optimal, which is quite strong.

Limitations:
1. A potential limitation is the explicit dependence on the dimensionality $d$.
2. There are no empirical studies of their modified algorithm.

Questions:
1. The results in this paper do not have any dependence on the number of trees $K$. Does it mean we can set $K=1$? If this is true, why do we need forest? A single tree is enough.
2. What value of $\lambda_n$ should we use in practice?","This paper consider the consistency of a (online) variant of Mondrian Forest. The authors amend the original Mondrian Forest algorithm where a fixed lifetime parameter hinders statistical consistency of the original procedure. The authors modify the modified Mondrian Forest algorithm grows trees with increasing lifetime parameters, and uses an alternative updating rule, allowing to work also in an online fashion. Then, the authors prove the statistical consistency of the modified Mondrian Forest algorithm, and achieve the minimax rate (optimal rate) for the estimation of a Lipschitz regression function.

This work is generally well-written, while I have the following concerns:

1) The proposed online algorithm is a modification of original Mondrian Forest with different parameter, and the statistical consistency depends on the choices on the parameter. How about the performance on real datasets? I think the authors should present some real experiments to show the effectiveness of the proposed online learning algorithm, especially for the generalization of the proposed algorithm. 

2) It is easy to prove the main statistical consistency and the minimax rate without new technical insights from theoretical view.
"
Gradient Episodic Memory for Continual Learning,"David Lopez-Paz, Marc'Aurelio Ranzato",https://proceedings.neurips.cc/paper/2017/hash/f87522788a2be2d171666752f97ddebb-Abstract.html,"The authors of the manuscript consider the continuum learning setting, where the learner observes a stream of data points from training, which are ordered according to the tasks they belong to, i.e. the learner encounters any data from the next task only after it has observed all the training data for the current one. The authors propose a set of three metrics for evaluation performance of learning algorithms in this setting, which reflect their ability to transfer information to new tasks and not forget information about the earlier tasks. Could the authors, please, comment on the difference between continuum and lifelong learning (the corresponding sentence in line 254 seems incomplete)?

The authors also propose a learning method, termed Gradient of Episodic Memory (GEM). The idea of the method is to keep a set of examples from every observed task and make sure that at each update stage the loss on the observed tasks does not increase. It seems like there are some typos in eq. (6), because in its current form I don’t see how it captures this idea.

GEM is evaluated on a set of real-world datasets against a set of state-of-the-art baselines. I have a few questions regarding this evaluation:
1. comparison to icarl: if I understand correctly, icarl solves a multi-class classification problem with new classes being added over time, i.e. by the end of training on CIFAR-100 it would be able to solve a full 100-classes problem, while GEM would solve only any of 20 5-classes problems. How was the evaluation of icarl performed in this experiment? Was it ever given a hint which of 5 classes to look at?
2. it also seems from table 1 that the results in figure 1 for GEM and icarl were obtained for different memory sizes. If this is true, could the authors comment on why?
3. on MNIST rotations task multi-task method has less negative transfer than GEM, but its accuracy is lower. Could the authors, please, comment on why? Or these differences are non-significant? 
4. It would be very interesting to see performance of baseline (multi-task for MNIST and/or multi-class for CIFAR) that gets all the data in a batch, to evaluate the gap that remains there between continuum learning methods, like GEM, and standard batch learning.
5. were the architectures, used for baseline methods (multi-task and icarl), the same as for GEM?
","The paper proposes an interesting approach to minimizing the degree of catastrophic forgetting with respect to old tasks while retaining a memory buffer of old examples for each task. The experiments are on public datasets relevant to recent work and compare against some reasonable baselines. However, I think the way the paper presents itself is misleading to the readers about where the core innovation of the paper is. I like the way the paper revisits various concepts in section 2, but it largely reads as if the authors are the first to propose these ideas that are already well understood by the community of researchers studying these problems. 

For example, I do not believe the development of a new term “continuum learning” used constantly throughout the paper is warranted given the fact that it is a very straightforward instantiation of lifelong learning. The concept behind this term continuum learning is not a novel idea. The authors stress the fact that they use a task descriptor, stating it is novel on line 67. I definitely do not think this is novel. Even multi-task learning as explored in the cited (Caruana, 1998) must send a task identifier with the training data in order to choose which task specific layers to use. This component of the system is just not always specifically called out. 
","This algorithm presents a framework for learning in the context of a series of
tasks that are presented in a certain order without repeating samples. It offers
an approach for dealing with non-iid input data, catastrophic forgetting, and
transfer learning as they are required in such a context. After detailing the
problem, the authors propose a solution that allows for learning while
protecting against catastrophic forgetting and show a simple algorithm for
learning based on this framework. They then validate the framework against other
recently proposed algorithms as well as single-task and multi-task approaches as
backups, and show that their model is less prone to catastrophic
forgetting. While I am not familiar with the corpus of work on this particular
problem, assuming they are the first to tackle it in this particular context I
find their idea to be quite interesting and worthy of inclusion in NIPS this
year. The applications of an approach that learns like a human, without needing
to review each data point many times and which does not need the tasks to be given
simultaneously, are at least clear to me in the case of EEG processing and
surely have other interesting applications as well.

My one concern has to do with the validation datasets. As they do not use the
standard MNIST and CIFAR100, I find it important to see what a benchmark accuracy
might be (trained without the rather strong constraint of only seeing each data
point once) to get a sense of where this method currently is in terms of
performance.

Further, there are some issues with the paper that I would like to better
understand:

1. How long to the various methods take? As is pointed out, the constraints of
their gradient steps require an inner optimization and it would be interesting
to see how that compares time-wise with more standard approaches. Figure 1 has
'training time' on the x-axis, but is this in terms of the number of samples
seen or the actual clock time?

2. Because they make a claim based on the relative size of p and M, it would be
nice to know exactly how big your CIFAR100 network is."
Variational Inference for Gaussian Process Models with Linear Complexity,"Ching-An Cheng, Byron Boots",https://proceedings.neurips.cc/paper/2017/hash/f8da71e562ff44a2bc7edf3578c593da-Abstract.html,"The authors propose a method to improve the scalability of Gaussian process based models. Their method is based on using an approximation similar to the one typically used based on inducing points. The key difference is that the authors have a different number of inducing points in the expression for the approximate GP mean than in the expression for the approximate variance. Because the cost of computing the mean is much smaller than for the variance, the authors can then afford to use a much larger number of inducing points to approximate the mean, without introducing a significant additional cost. the result is much better predictive performance than traditional sparse methods based on inducing inputs. The experiments performed illustrate the gains of the proposed method.

Quality

The paper seems technically sound, although I did not examine the theoretical content in detail.

Clarity

The paper is clearly written.

Originality

The proposed method is original up to my knowledge.

Significance

The proposed method is highly significant. In my opinion it is one of the most relevant advances in the area of scalable Gaussian processes in the last years.

Minor typos or comments:

1 - Figure 2, mujoco appears twice and kuka is missing.
2 - In line 67 it is said that the proposed approach over-estimates the variance. However it can be appreciated that it also underestimates it in other regions of the plot different from the right-most part.","In their paper the authors propose a sparse variational algorithm for Gaussian process inference which decouples the inducing points used for approximating mean and covariance of the posterior. As both time and space complexity scale linearly in the number of inducing points used for the mean, it can be increased to describe complex functions more accurately. Results on Gaussian process regression for robot data sets show a significant increase in the accuracy compared to other algorithms which use the same inducing points for modeling mean and covariance.

The paper is well written. The authors explain the derivation of the SVDGP model in great detail and give a rather short, but sufficient description of the actual online learning algorithm. However, some details are not as clear as possible: I guess that GPR means Gaussian process regression on a subset of N = 1024 data points and VSGPR has only applied to a (larger) subset of the data points (section 4). And then GPR should not have any variational parameters, only the hyperparameters are updated by conjugate gradient (CG) (table 1). I recommend to clarify this and give the run times of the algorithms explicitly. Separating the inducing points for mean and covariance is a novel concept based on the RKHS representation of Gaussian processes. As this approach reduces the computational complexity, I expect that it will be useful for big data applications, where Gaussian process models are to slow without it.

The changes proposed in the author feedback are suitable to improve the clarity of the paper.","Summary:  The paper employs an alternative view of Titsias's sparse variational inference technique for Gaussian process regression, treating the mean function and covariance function as linear operators in RKHS. By decoupling the basis functions that each of these functions uses, the variational lower bound can now be computed in a O(NMa + NMb^2) complexity, linear in N, and linear in Ma (number of mean parameters) and quadratic in Mb (number of covariance parameters). Previous approaches have O(NM^2) complexity. This means one now can use much bigger Ma compared to Mb without significantly increasing the complexity. The catch is, however, the posterior prediction is no longer **calibrated** and as good, i.e. smaller Mb results in larger predictive variances. The method is compared against existing approaches on several regression datasets, showing lower predictive errors and achieving higher variational lower bounds. 

Details: 

The catch is that the predictive variance is large if Mb is small resulting in a poor predictive performance if measured using the log-likelihood. This can be seen in figure 1, and is mentioned in the text but has not been fully experimented (as the paper only uses the squared error metric which does not use the predictive variances).

Unless I have missed it, the basis locations are not optimised but selected using several initial mini-batches. Would there be any problems if we optimise locations of the inducing points/bases as in Titsias's approach? 

The inducing points/inputs are grounded on some physical locations and can be plotted. Can we visualise the subspace parameterisation? For example, grounding some bases and plotting the samples generated from the corresponding GPs? 

And why is there a kink/sudden change in the traces for all methods in the middle of Figure 1? The learning curves for SVI are still going down after 2000 iterations -- does this use SNGD for the q(u)'s parameters? I think recent works on SVI for GP models (Hensman et al 2014, Bonilla et al 2014, 2015) all use SGD for the mean and Cholesky parameters. This seems to work well in practice, and avoid the need to deal with natural gradients or select a separate learning rate. It would be better to compare to these variants of SVI.

Extensions to non-Gaussian likelihoods seem non-trivial as computing the expected log likelihood would decouple the variational parameters. "
The Reversible Residual Network: Backpropagation Without Storing Activations,"Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse",https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html,"The authors introduce “RevNets”, which avoid storing (some) activations by utilizing computational blocks that are trivial to invert (i.e. y1=x1+f(x2),  y2=x2 + g(y1) ). Revnets match the performance of ResNets with the same number of parameters, and in practice RevNets appear to save ~4X in storage at the cost of a ~2X increase in computation.

Interestingly, the reversible blocks are also volume preserving, which is not explicitly discussed, but should be, because this is a potential limitation.  The approach of reconstructing activations rather than storing them is only applicable to invertible layers, and so while requiring only O(1) storage for invertible layers, succeeds in only a 4X gain in storage requirements (which is nevertheless impressive).

One concern I have is that the recent work on decoupled neural interfaces (DNI) is not adequately discussed or compared to (DNI also requires O(1) storage, and estimates error signals [and optionally input values] analogously to how value functions are learned in reinforcement learning). While DNI is not lossless as the authors mention, preliminary indications are that conditional DNI (cDNI, e.g. on labels) is quite effective https://arxiv.org/pdf/1608.05343.pdf, figure 7. DNI has other advantages as well, but indeed is not fully evolved. Nevertheless I think that DNI should be included in table 1, and discussed more thoroughly (If cDNI is highly effective on large scale tasks it would subsume Revnets).

Overall, I believe that this paper will be of interest to practitioners in the field, and the idea, while straightforward, is interesting.

Minor comments:

- Checkpointing is straightforward and general, and may require less overhead, and so should probably be directly compared to at least under the same constraints (store corr. non-invertible layers). More generally, the experiments section should be more explicit wrt the realized memory/compute trade-off.

“We empirically verified the memory gain by fitting at least twice the number of examples while training ImageNet”
- This confused me---is the gain not 4X?

---

Authors: Thank you for your feedback. I've updated my recommendation since 1) DNI is not yet officially published, and 2) the practical memory advantages of RevNets have/will be made clear in the final version of the paper. Good luck!
","The paper introduces a reversible block for residual networks that has the benefit of not needing to store all of the forward pass activations for the backward pass. This enables training of larger models or minibatches, as the memory footprint during training is smaller.

As the memory of GPUs is often a bottleneck when training very large models, the paper is a welcome addition to the line of literature on decreasing this footprint. By training on both CIFAR and ImageNet, the experiments focus on two widely used benchmarks. The experiments seem solid and achieve compelling results. The authors also confirm empirically that numerical instabilities are not a significant problem, which is good.

To make the paper even stronger, I would have been interested in seeing even more experiments. It is good that the model is tested both on CIFAR (10 and 100) and ImageNet, but for example SVHN would have been a fairly ""cheap"" addition to the set of standard benchmarks. The performance on RNNs, that the authors also discuss, would be very interesting to see.

In addition, the main concern I have with using the model in practice is the potential drawback of not discarding information (hence the reversibility) - I'm not sure exactly what form that discussion would take, but a slightly deeper discussion on that issue would be helpful. The second paper on NICE might give some ideas for how to do that.

A minor issue is also that it is somewhat unclear to the reader how large a typical memory footprint of the activations is compared to the footprint of the weights for a particular minibatch size in the resnets. This obviously depends on many factors, but it would be really helpful (for instance in the appendix) to make some back-of-the-envelope calculations that would give an indication of e.g. how much deeper models or how much larger minibatches one could train, given an optimized code, for typical model sizes.

The paper is generally well written, although some minor typos are still present (p1: ""simply by widening (increasing the convolutions' filter count) fewer layer"" should probably read ""...layers"", p2: ""...derivative defines the the effect..."" has one ""the"" too much, p2: ""change to v_i, taking into account the indirect effects through the descendants of v_k"" should probably read ""... descendants of v_i"") which will be easy to fix by carefully rereading the paper a few times.

The drawbacks are however fairly minor, and the strength of the paper is that the idea is fairly simple, has been shown with sufficient empirical results to work, and is well presented. In my view, this is an idea that would interest the NIPS audience."
Language Modeling with Recurrent Highway Hypernetworks,Joseph Suarez,https://proceedings.neurips.cc/paper/2017/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html,"Summary:

The paper augments Recurrent Highway Networks (RHNs) with a ""hypernetwork."" Results are presented on character-level language modelling on the PTB benchmark where the authors show a small improvement over several HyperLSTM variants (SOTA as they say). Furthermore, they discuss the problem of vanishing gradients in their architecture and compare those gradients to those of a stacked LSTM. 

Pros: 

- good and reasonable results
- reinforces the superiority of RHN over standard LSTM for language modelling on PTB using a gradient analysis. 

Cons:

- In terms of new contributions, the paper seems not that strong. 
- The experiments are limited to character-level modelling only. The authors do mention hardware limitations but fail to provide e.g. word-level results for PTB. 
- While they compare with an HyperLSTM they do not compare ith a plain RHN. It's implicitly assumed that the hypernetwork augmentation will improve a comparable RHN architecture but this has not been shown. Perhaps one could quickly set up additional experiments to test this. 
- Layer Norm was not used in the architecture but since LN has been shown to lead to significant improvements in combination with Hypernetworks this could have been tested empirically.

References:

- One cannot cite only the vanishing gradient work of Bengio et al (1994) without citing the original, essentially identical work of Hochreiter (1991), simply because the 1994 paper does not cite the original work. One way around this is to cite only the 2001 paper, where Hochreiter is 1st author, and Bengio 2nd. 

Hochreiter, Sepp. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, TUM, pp. 91, 1991.

- line 17-21: this is misleading - mention that vanilla LSTM consistently outperforms the cited LSTM variant called ""GRU"" on NLP and machine translation according to Google's cited analysis: https://arxiv.org/abs/1703.03906

Formulas: 

- The authors decided to use a notation different from the one of the authors of the Recurrent Highway Network and HyperNetworks paper and fail to properly define its meaning (e.g., h is not the state but s is). This makes it unnecessarily more difficult to understand their formulas. 


General recommendation: Slightly leaning towards accept, provided the comments are taken into account. Let's wait for the rebuttal phase. 
","The paper describes how to achieve state-of-the-art results on a small character prediction task. Some detailed comments below:

- Title: First of all, I would not call work on PTB using characters ""language modeling"", it is character prediction. Yes, theoretically the same thing but actual language model applications typically use much bigger vocabularies and much more data, and this is where most of the problems in language modeling lie. Replacing ""language modeling"" by ""character prediction"" would be a more appropriate title

- Results (experimental): I agree that PTB is OK to use as a database to test character prediction models. However, it would have been good to also test/show your findings on larger databases like for example the 1 Billion Google corpus or similar. Public implementations for training these relatively quickly on limited hardware exist and it would have made this a much stronger paper, as eventually I believe you hope that your findings will be in fact used for larger, real problems. To show in your paper that you can handle large problems is always better than finding reasons to not do it as you do in line 113-118 and in other places throughout the paper.

- Results (experimental): You list accuracy/perplexity/bpc -- it would be good to define exactly how these are calculated to avoid confusion. 

- 3.2: 2-3 days of training for PTB is quite long, as you note yourself. It would have been nice to compare the different models you compare yourself over time by plotting perplexity vs. training time for all the models to compare how efficiently they can be trained. 

- 5.1: You say that you are almost certainly within a few percent of maximum performance on this task -- why? Maybe that is true but maybe it isn't. Diminishing gains on an absolute scale can have many reasons, including the one that possibly many people do not work on PTB for character prediction anymore. Also, the last sentence: Why is accuracy better than perplexity to show relative improvement? BTW, I agree that accuracy is a good measure in general (because everybody knows it is bounded between 0 and 100%), but perplexity is simply the standard measure for these kinds of predictions tasks.

- 5.4: You say that how and when to visualize gradient flow has not been receiving direct treatment. Yes, true, but I would argue the main reason is that it is simply not so interesting -- what matters in training these networks is not a theoretical treatment of some definition of gradient flow but simply time to convergence for a given size model on a given task. This goes also back to my comment from above, what would have been more interesting is to show training time vs accuracy for several different networks and different sizes.

- 6: Why can you compare gradient norms of completely different architectures? I would argue it is quite unclear whether gradients of different architectures have to have similar norms -- the error surface could look very different. Also, even if you can compare them, the learning algorithm matters a lot -- SGD vs Adam vs Rprop or whatnot have all a completely different effect given a certain gradient (in some cases only the sign matters etc.). So overall, I would argue that gradient flow is simply not important, it would be better to focus to convergence optimizing learning algorithms, hyperparameters etc.
 


","This paper discusses a combination of recurrent highway networks (RHNs) and hypernetworks. Whereas simply joining 2 existing techniques may not seem pretty original, the paper does present some nice theoretical results for hypernetworks, nice experimental results for RHNs and a good amount of analysis. The authors refer to all the relevant papers and exhibit some interesting insights into models that have previously been explained in an overly complicated manner. I feel that this paper succeeds in what it intends to do: advocate RHNs as the default building block for recurrent architectures, rather than LSTMs.

Some minor comments:
* line 44: (e.g. on Penn Treebank [16] (PTB))
* line 58 and later occurrences: Lowercase ""where""
* equations 1-3: decide on s_t or s_i, don't mix
* footnote 2: purposed -> purpose
* equation 7: clarify M_{pl}, use math notation instead of duplicate function
* equation 7: where do s_h and s_n come from? they are not introduced in equation 6...
* figures 1, 2, 3: perhaps show later, as it is only discussed on the next page
* figures 2 and 3: label the weight layers, as it is not clear what is the first and the last layer. figure 3 seems to suggest that the 4th bar corresponds to layer 1, yet for figure 2 you mention that the gradients increase during back propagation which seems to indicate the opposite
* line 236: twice ""the""
* line 259: an -> a
* line 285: LSTM"
Parametric Simplex Method for Sparse Learning,"Haotian Pang, Han Liu, Robert J. Vanderbei, Tuo Zhao",https://proceedings.neurips.cc/paper/2017/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html,"This paper extends simplex algorithm to several sparse learning problem with regularization parameter. The proposed method can collect all the solutions (corresponding to different values of the regularization parameter) in the process of simplex algorithm. It is an efficient way to get the sparse solution path and avoid tuning the regularization parameter. The connection between path Dantzig selector formulation and sensitivity analysis looks interesting to me.

Major comments:

- The method used in this paper seems closely related to the sensitivity analysis of LP. What is the key difference? It looks like just an application of sensitivity analysis.

- The paper mentioned that the number of iterations is linear in the number of nonzero variables empirically. Is there any guarantee for such linear dependency?

Experiment:
- Table 1 is not necessary since PSM never violate the constraints.
- Simplex method is not very efficient for large scale LP. I did not see comparison with interior based approaches or primal dual approaches. Authors are expected to make such comparison to experiments more convincing.

Typo/question:
line 122: z^+, z^- => t^+, t^-
line 291: solutio => solution
line 195: what is the relation between the feasibility condition and the sign of b and c?
line 201: \bar{x}_B and \bar{z}_N are always positive? (otherwise how to always guarantee the feasibility when \lambda is large)
line 207: here we see an upper-bound for \lambda, it contradicts the claim that the feasibility will be guaranteed when \lambda is large enough. In addition, is it possible that \lambda_min > \lambda_max?
line 236: can we guarantee that there is no cycle in such searching?","This paper exposes the use of a modified simplex algorithm, the parametric simplex method, to solve sparsity-related problems such as the Dantzig selector, Sparse SVM or again differential network estimation, over a range of regularization parameters. Along with some experiments, this work provides the theoretical analysis of the algorithm concerning its correctness and the sparsity of the iterates.

I report some of my concerns below:
- My main concern is relative to the experiments, where only very sparse vectors are considered. This raises the question of how the method performs when the sparsity is not that low.
- In the context of machine learning, I would like to see how the method performs against coordinate descent-like algorithms (for instance on the Dantzig selector).
- Does the fact that the Basis Pursuit, which is also a linear program, is not included is because the authors believes there is enough work on it, or is it because the algorithm does not work well in this case ? It seems to me that BP is the standard linear program to check ... 

On the bright side, the paper is clearly written, the supplementary materials is reasonably long (3 pages), and everything is technically sounds.","In this paper, the authors propose a parametric simplex method for sparse learning. Several advantages over other competing methods are discussed in detail. More numerical experiments are provided to prove the efficiency of the proposed algorithm.

The authors claim that first order method work well only for moderate scale problems, but in the experiments, I do not think the author considered large scale case. As a result I suggest the authors to further modify and calculate large-scale case. My main concern for this paper is that the simplex method is just a classical method for linear programming. The authors have not carefully discuss their main contributions the parametric type modification and the novelty."
Filtering Variational Objectives,"Chris J. Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Teh",https://proceedings.neurips.cc/paper/2017/hash/fa84632d742f2729dc32ce8cb5d49733-Abstract.html,"This paper generalized the traditional ELBO in Variational Inference to a more flexible lower bound “Monte Carlo Objectives(MCO)”. It modifies IWAE, a previous MCO, to get a new lower bound FIVO based on particle filtering. The paper also gives some theoretical properties of MCO about consistency and bias. Looking beyond classical ELBO is one of most promising directions in study of Variational Inference.

However, it also has several problems or unclear explanation:
1.	It is better to include more details about the optimization. The influence of biased gradient need more theoretical study.
2.	What is the criterion used for resampling; Is it effective sample size? How frequently is the resampling carried out in the experiments? Will frequent resampling increases variance? Why the figure in appendix shows that the variance with resampling is higher than that without resampling?
3.	In Section 4.2, it is mentioned that the FIVO assumption cannot guarantee the sharpness. The trade off between this disadvantage and the advantage of FIVO is not thoroughly investigated. 
4.	How to choose the number of particles for FIVO? Why in figure 2, for IWAE(special case of FIVO without resampling), the largest particle size(N=16) achieves lowest likelihood score?","The paper extends Importance Weighted variational objective to sequential model where the posterior is over a sequence of latent variables. In doing so, the authors also present (1) an elegant and concise generalization of IW to the case of any unbiased estimator of the marginal likelihood with a bound on the asymptotic bias, (2) a demonstration that this idea works in the case of non-iid average as in the particle filter with resampling (where particles are coupled through the resampling step). All of these are novel, although it is perhaps not surprising once one realize the implication of (1).

Eq (11) seems to deal with the resampling (of discrete particles) through the usual reinforce trick which is known to have high variance. In fact, the authors do observe that particle filter-FIVO gradients computed in (11) has very high variance. However, they should mention this much earlier, right after eq (11), with a heads-up that only the first part of eq (11) is used, and discussion the theoretical implication if possible. For example is the actual gradient estimate no longer unbiased? would it cause any convergence issue?

Despite this main limitation, the empirical results seem to show that this approach is worthwhile. However, I would strong encourage the authors to pay more attention to this limitation, be up front about it, discuss potential pitfalls and whether they observed these issues in practice.

","This paper introduces Monte Carlo objective to improve the scaling of the IWAE and provides a tighter lower bound to the log likelihood than traditional ELBO. Various theoretical properties of Monte Carlo objectives are discussed. An example of Monte Carlo objective is provided to use particle filters for modeling sequential data. The paper is generally well written and I enjoyed reading it.

* For the FIVO algorithm, it'd be more self-contained if including a brief description of the resampling criteria or its exact procedure used in the experiment, so that the reference doesn't need to be consulted.

* Although not required, it'd be helpful to provide extra examples of Monte Carlo objectives other than applications of sequential data as FIVOs.

* I feel it's fairer to stay with a batch of 4 instead of 4N for ELBO to reduce the impact of different batch configurations, even if the computational budget does not match up (line 259). The authors could also consider reporting the running time of each method, and normalize it somehow when interpreting the results. This could also give readers some idea about the overhead of resampling.

* I'm a bit worried about the use of the biased gradients to train FIVO. Is it possible that part of the superior performance of FIVO over IWAE somehow benefits from training using the incorrect gradients? It'd be useful to also report the performance of FIVO trained with the unbiased gradients so that the readers will have the idea about the impact of the large variance coming from the resampling term.

* It's not clear to me why checking on the KL divergence between the inference model q(z|x) to the prior p(z) addresses the need to make use of the uncertainty (line 279 to 283). What's the connection between ""the degree to which each model uses the latent states"" and the KL divergence? How to interpret ELBO's KL collapses during training? Doesn't the inference model collapses onto the prior suggest that it carries less information about the data?

* Typo: it should be z ~ q(z|x) in line 111-112.
"
Cold-Start Reinforcement Learning with Softmax Policy Gradient,"Nan Ding, Radu Soricut",https://proceedings.neurips.cc/paper/2017/hash/faafda66202d234463057972460c04f5-Abstract.html,"The paper presents a new method for structured output prediction using reinforcement learning. Previous methods used reward
augmented maximum likelihoods or policy gradients. The new method uses a soft-max objective. The authors present a new 
inference method that can be used to efficiently evaluate the integral in the objective. In addition, the authors propose 
to use additional reward functions which encode prior knowledge (e.g. to avoid word repetitions). 

The paper seems to be promising from the sequence learning point of view as it can 
produce sequences of considerably higher quality. However, the authors seem to be ignorant to a large part of the 
reinforcement learning and policy search literature, please see [1,2] for good reviews of policy search with information-geometric
constraints. 
(1) The soft-max objective is common in the reinforcement learning literature. The given objective is the dual of the following 
optimisation problem: 

J = E_{p}[R] + KL(p||p_\theta). 

If we solve that for p we exactly get the sampling distribution q given in the paper. Plugging the solution back in 
the objective J yields the dual problem of the objective which is equivalent to equation 7. I think such insights
reveal much of the structure of the optimisation problem. Moreover, please note that he objective J is commonly
used in policy search, see for example the REPS algorithm, KL control, linearly solvable MDPs, trust region policy optimisation etc... There is an interesting
difference though, that commonly, we want to compute p and p_theta is the sampling distribution. In this algorithm, we optimise for 
p_theta, i.e., we change the sampling distribution. I think this connections to standard policy search algorithms need to be revealed
in this paper to provide a better understanding what the algorithm actually does. 
(2) The RAML algorithm is a special case of expectation-maximisation based policy search while the new algorithm is an 
instance of information-theoretic policy search, see [1].

Clarifying these relations should help to improve the paper significantly. 

While in general, I like the approach, I have one technical concern regarding the sampling procedure: 

The weight w_i for the reward function is either 0 (with probability p_drop) or W. The authors argue that this is 
equivalent to using a weight that is given by p_drop * W. However, this is incorrect as

E_p(drop)[exp(log p(z) + I(drop == 0) * W * reward)] is *not* the same as exp(E_p(drop)[log p(z) + I(drop == 0) * W * reward)] =
exp(log p(z) + p_drop * W * reward)] 

In fact, the distribution the authors are sampling from is rather a mixture model:

p_drop * p(z) + (1-p_drop) * exp (W * reward) / Z

This is a very different distribution from the one given in the algorithm so I do not think it is sound to use it. 
I think this issue need to be addressed before publishing the paper (maybe its also a misunderstanding on my side,
but the authors need to address my concerns and either proof my wrong or suggest a sound sampling method). Using KL trust
regions similar as in the REPS algorithms could for example be a sounder way of finding the weights w_i). 

In summary, this is in interesting paper, but I have a concern regarding the soundness of the sampling procedure which needs 
to be addressed. 

[1] Deisenroth, Neumann, Peters: A survey on policy search for robotics"", FNT 2013
[2] Gergely Neu, Anders Jonsson, Vicenç Gómez, A unified view of entropy-regularized Markov decision processes, 2017


","This paper studies the exposure-bias problem and the wrong-objective problem in sequence-to-sequence learning, and develop a new softmax policy gradient method, which could learn the model with cold-start. The approach combines the RAML and policy gradient method to effectively exploit the model prediction distribution and the reward distribution. An efficient approximate sampling strategy is also developed for estimate the gradient. The method is evaluated on text summarization task and image caption task, both of which show better performance than previous methods. This paper studies an important problem and presents interesting result. There are some minor points that the authors may clarify:

The statement “the large discrepancy between the model prediction … and the reward distribution R(z | y)…” (line 113-114) is not clear to me. Please clarify how to see this from Eq (6). Specifically, what does it mean by “the reward distribution R(z | y)”? A probability distribution of R(z | y)?

It is not clear how is the cost function (7) motivated. Although from the gradient formula (8), it can be viewed as a combination between the gradients of (2) and (4), a clearer justification and explanation of the meaning of this cost itself is necessary. For example, why softmax function is chosen to combine the model and reward together?  Please clarify more.
","This paper proposes a variant of the RAML method to train RNNs, with the weights containing both the exponentiated reward and the generative probability.  This last terms allows to avoid the need for warm starts (ML pretraining).

The paper is well written and the examples shown are convincing. 

A few points to address: 
1) The title is confusing, since it refers to ""reinforcement learning"" and not to sequence prediction. 
2) How is the decoding performed? Beam search?
"
Bridging the Gap Between Value and Policy Based Reinforcement Learning,"Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans",https://proceedings.neurips.cc/paper/2017/hash/facf9f743b083008a894eee7baa16469-Abstract.html,"SUMMARY:
The paper considers entropy regularized discounted Markov Decision Process (MDP), and shows the relation between the optimal value, action-value, and policy. Moreover, it shows that the optimal value function and policy satisfy a temporal consistency in the form of Bellman-like equation (Theorem 1), which can also be extended to its n-step version (Corollary 2).
The paper introduces Path Consistent Learning by enforcing the temporal consistency, which is essentially a Bellman residual minimization procedure (Section 5).


SUMMARY OF EVALUATION:
Quality: Parts of the paper is sound (Section 3 and 4); parts of it is not (Section 5)
Clarity: The paper is well-written.
Originality: Some results seem to be novel, but similar ideas and analysis have been proposed/done before.
Significance: It might become an important paper in the entropy regularized RL literature.


EVALUATION:
This is a well-written paper that has some interesting results regarding the relation between V, Q, and pi for the entropy regularized MDP setting. That being said, I have some concerns, which I hope the authors clarify.

- Enforcing temporal consistency by minimizing its l2 error is minimizing the empirical Bellman error. Why do we need a new terminology (Path consistency) for this?

- The empirical Bellman error, either in the standard form or in the new form in this paper, is a biased estimate of the true Bellman error. Refer to the discussion in Section 3 of

Antos, Szepesvari, Munos, “Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path,” Machine Learning Journal, 2008.

Therefore, for stochastic dynamics, minimizing Equation (15) does not lead to the desired result. This is my most crucial comment, and I hope the authors provide an answer to it.

- Claiming that this paper is bridging the gap between the value and policy based RL is beyond what has actually been shown. The paper shows the relation between value and policy for a certain type of reward function (which has the entropy of policy as an additional reward term).

- On LL176-177, it is claimed that a new actor-critic paradigm is presented where the policy is not distinct from the values. Isn’t this exactly the same thing in the conventional value-based approach for which the optimal policy has a certain relationship with the optimal value function? In the standard setting, the greedy policy is optimal; here we have relationship of Equation (10).

- It might be noted that relations similar to Equations (7), (8), (9), and (10) have been observed in the context of Maximum Entropy Inverse Optimal Control. See Corollary 2 and the discussion after that in the following paper:

Ziebart, et al., “The Principle of Maximum Causal Entropy for Estimating Interacting Processes,” IEEE Transactions on Information Theory, 2013.

Here the Bellman equations are very similar to what we have in this paper.

Ziebart et al.’s framework is extended by the following paper to address problems with large state spaces:

Huang, et al., “Approximate MaxEnt Inverse Optimal Control and its Application for Mental Simulation of Human Interactions,” AAAI, 2016.

This latter paper concerns with the inverse optimal control (i.e., RL), but the approximate value iteration algorithms used for solving the IOC essentially use the same Bellman equations as we have here. The paper might want to compare the difference between these two approaches.

- One class of methods that can bridge the gap between value and policy based RL algorithms is classification-based RL algorithms. The paper ignores these approaches in its discussion. ","The paper derives a new insight for entropy-regularized RL: a relation between the optimal policy and value function that holds for any sequence of states along a trajectory. This relation, in the form of a consistency equation, is used to define a novel loss function for actor-critic algorithms, that holds in an off-policy setting as well as on-policy.

The paper is clearly written and was a pleasure to read. Choosing to focus on deterministic transitions in the main text made it easier to focus on the main idea.

While entropy based RL has been explored extensively, the insight in this paper is, to my knowledge, novel, and leads to a very elegant algorithmic idea. The off-policy actor critic proposed here is only slightly different from previous work such as A2C (as noted in the paper), but makes much more sense theoretically. 

Due to its principled construction, I believe that the PCL algorithm proposed here would become a standard RL algorithm in the future, and strongly suggest the acceptance of this paper.

Comments:
120: Theorem 1: it would be interesting to follow up with the intuition of what happens when \tau -> 0 (line 102) for this result as well. I’d expect it to reduce to the hard Bellman backup, but for actions that are not optimal I cannot really see this.

74-77: Technically, the optimal policy is not necessarily unique. The uniqueness should be assumed for this paragraph to hold (this is not a strong assumption).

I have read the author feedback.","The paper is well-written. I have two issues. First, I am not sure that NIPS the best place for it. You do mention neural networks as a possible application in the introduction, but that is about it. It seems to me, also given the theoretical nature of the publication. that JMLR or ICML would be better suited.

In line 86 you describe two examples in which entropy regularisation has been shown to work. There are earlier examples:

Information-Theoretic Neuro-Correlates Boost Evolution of Cognitive Systems
by Jory Schossau, Christoph Adami and Arend Hintze
Entropy 2016, 18(1), 6; doi:10.3390/e18010006

K. Zahedi, G. Martius, and N. Ay, “Linear combination of one-step predictive information with an external reward in an episodic policy gradient setting: a critical analysis,” Frontiers in psychology, vol. 4, iss. 801, 2013. 

Houthooft, Rein, et al. ""Vime: Variational information maximizing exploration."" NIPS 2016.

It would be good, if the authors could expand on how their Equations 4 relates to other, previous approaches.
"
"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms","Yogatheesan Varatharajah, Min Jin Chong, Krishnakant Saboo, Brent Berry, Benjamin Brinkmann, Gregory Worrell, Ravishankar Iyer",https://proceedings.neurips.cc/paper/2017/hash/fb3f76858cb38e5b7fd113e0bc1c0721-Abstract.html,"This propose proposes an expert-defined factor graph for analyzing EEG signal and inferring properties of brain areas, such as whether they belong to abnormal areas. The authors then develop a scalable inference algorithm for their model. They evaluate it empirically on real data and show it outperforms other methods. The paper is well written and clear.

The role of G_n, the interaction between the channels, is unclear to me. It is mentioned throughout Section 3 yet I could not find it in the factor graph equations.

The factor graph takes into account previous states and the authors claim to handle temporal dependencies. Yet, inference seems to be performed on a single epoch at a time and inferred values for previous states Y are never questioned again. Is there does a medical or biological motivation for this choice? From an inference point of view this looks problematic to me. As the states influence themselves over time, both past and future Fns would influence the states at any epoch n. Therefore it seems more logical to me to perform inference on all states together. Would that be possible with your method?

Why not integrate the Maximum likelihood SOZ deduction within the model?

The empirical results look good and show the qualities of the approach. I am however not very familiar with EEG analysis so I cannot comment on the interest of the proposed method for these applications.

Suggestion: get rid of some lines in Table 2 so that you can have more space between the table and the following text.

typos:
l41: ""Majority"" --> A/The majority?","In the paper titled ""EEG-GRAPH: A Factor Graph Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"", the authors introduced a factor-graph-based model to capture the observational, temporal, and spatial dependencies observed in EEG-based brain activity data. From the methodology aspect, the techniques used in the paper is not new, but a combination of existing methods. The factor-graph model in the paper is quite similar to dynamic Bayesian networks. The MAP solution is achieved via min-cut algorithm. Note that, the graphical model is created manually, and therefore no structure learning is involved. 

From application aspect, this is a significant improvement in the fields of electrophysiology and brain computer interfaces because this is the first model which captures the three types of dependencies (claimed by the authors). The experiment design and evaluation is sound and solid. The proposed method significantly outperformed existing methods. The overall presentation is good and the paper is easy to follow. 

A few minor points:

1. It might be useful to mention whether the edge set E_n stays the same or not across different epochs. 
2. What's the difference between the proposed method and dynamic Bayesian networks? It may be helpful to compare them.
3. The proposed method is also quite similar to a previous work (titled ""High-Dimensional Structured Feature Screening Using Binary Markov Random Fields"" by Liu et al, AISTATS-2012) which also used graphical model to capture dependence between variables, and employed min-cut algorithm to identify the binary hidden state (MAP inference). It may be helpful to compare them.
4. In Figure 1a, one arrow goes from ""Current observation (events, rhythms)"" to ""Current state of a brain region"". However, I feel the direction of the arrow should be reversed. What do you think?
","SUMMARY:
========
The authors propose a probabilistic model and MAP inference for localizing seizure onset zones (SOZ) using intracranial EEG data.  The proposed model captures spatial correlations across EEG channels as well as temporal correlations within a channel.  The authors claim that modeling these correlations leads to improved predictions when compared to detection methods that ignore temporal and spatial dependency.

PROS:
=====
This is a fairly solid applications paper, well-written, well-motivated, and an interesting application.  

CONS:
=====
The proof of Prop. 1 is not totally clear, for example the energy in Eq. (4) includes a penalty for label disagreement across channels, which is absent in the the graph cut energy provided by the proof. The relationship between min-cut/max-flow and submodular pairwise energies is well established, and the authors should cite this literature (e.g. Kolmogorov and Zabih, PAMI 2004).  Note that the higher-order temporal consistency term can be decomposed into pairwise terms for every pair of temporal states.  It is unclear why this approach is not a valid one for showing optimality of min-cut and the authors should include an explanation.

What do the authors mean by ""dynamic graph"" (L:103)?  Also, the edge set $E_n$ is indexed by epoch, suggesting graph structure is adjusted over time.  It is not discussed anywhere how these edges are determined and whether they in fact change across epochs.

This estimate of SOZ probability is motivated in Eq. (5) as an MLE.  It isn't clear that (5) is a likelihood as it is not a function of the data, only the latent states.  The estimated probability of a location being an SOZ is given as an average over states across epochs, which is a valid estimator, and connections to MLE are unclear beyond that.

Generalizability claims of the approach (L:92-93) are weak.  The extent to which this is a general model is simply that the model incorporates spatio-temporal dependencies.  Specifying the factors encoding such dependencies in any factor graph will always require domain knowledge.

Some detailed comments:
  * Are there really no free parameters in the model to scale energy weights?  This seems surprising since the distance parameter in the spatial energy would need to be rescaled depending on the units used.
  * Authors claim no other work incorporates spatio-temporal EEG constraints but a recent paper suggests otherwise: Martinez-Vargas et al., ""Improved localization of seizure onset zones using spatiotemporal constraints and time-varying source connectivity."", Frontiers in Neuroscience (2017).  Please cite relevant material.
  * Lines in plots of Fig. 3 are too thin and do not appear on a printed version

"
Natural Value Approximators: Learning when to Trust Past Estimates,"Zhongwen Xu, Joseph Modayil, Hado P. van Hasselt, Andre Barreto, David Silver, Tom Schaul",https://proceedings.neurips.cc/paper/2017/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html,"This paper proposes a novel way to estimate the value function of a game state, by treating its previous reward and value estimations as additional input besides the current state. These additional input is the direct reason why value function depicts a non-smoothness structure (e.g., a sparse immediate rewards lead to a bump in the value function). By taking them into consideration explicitly, the value function can be more easily estimated.

Although the proposed idea is quite interesting, there are a lot of baselines that the proposed method might need to compare. As mentioned in Section 7, eligible trace estimates the cumulative return as a linear combination of k-step *future* returns with geometric weights. Also Generalized Advantage Estimation, as a novel way to estimate the overall return using future estimation, is another example. Comparing these previous approaches with the proposed method that uses past estimation is both interesting and important.  

In addition, there exist many simple approaches that also captures the history (and sparse reward that happened recently), e.g., frame stacking. A comparison will also be interesting. 

The paper starts with the story that the value function is smooth, if the non-smooth part is explained away by previous rewards and values. There are also motivating examples (e.g., Fig. 1 and Fig. 2). However, there is no experiments in Atari games showing that the estimated value function using proposed approach is indeed smoother than the baseline. 

Table. 1 shows that while median score shows a strong boost, mean score of the proposed approach is comparable to the baseline. This might suggests that the proposed approach does a good job in reducing the variance of the trained models, rather than giving higher performance. In the paper, there seems to be no relevant analysis. 

Some detailed questions:

1. According to the paper, the modified A3C algorithm uses N = 20 steps, rather than N = 5 as in the vanilla A3C. Did the baseline also use N = 20? Note that N could be an important factor for performance, since with long horizon the algorithm will see more rewards in one gradient update given the same number of game frames. 

2. Line 154 says ""all agents are run with one seed"". Does that mean the agents are initialized by the same random seed among different games, or the game environment starts from the same seed? Please clarify. ","[I have read the other reviews and the author feedback. I maintain my rating that this paper should be accepted.]

The paper proposes a novel way to blend ""projected"" TD errors into value function approximations and demonstrates downstream benefit in deep RL applications.
Some of the claims are over-reaching (e.g. statements like ""structure ... has never been exploited to obtain better value function approximations"" lead the reader down the wrong path in context of van Hasselt and Sutton [13] ) and the paper may benefit from explaining connections to eligibility traces, PSR and [13] much earlier.

That said, the core idea is simple, effective and easy to implement. Many questions I had (e.g. identifiability concerns between beta and v were heuristically handled by the stop-gradient operations in Eqn 5 and 6, perhaps an alternative way to do aggregate end-to-end training exists) were addressed in Section 6.

At present, Figure 4 does not add value in my opinion. I'm not convinced that E_v and E_beta are comparable (the regression targets Z_t depend on many confounding factors). I'd rather see an exploration of the limits of the proposed approach: e.g. intuitively it seems that natural value estimates could ""oscillate"" and present challenges for reliable convergence.

Minor:
128: instead *of* the 
134: actor's policy gradient
151: because *of* the rich
180: instead of exploration
215: *be* defined recursively
220: acknowledged that
233: detailed consideration","Review of submission 1276:
Natural value approximators: learning when to trust past estimates 

Summary:
The Nature Value Approximators (NVA) utilizes the regularity in the value functions of reinforcement learning approaches to make more accurate value predictions. NVA learns to mix direct value estimates and past estimates automatically. Empirical results on ATARI games show NVA reduces the value prediction error substantially.  

The paper is very well written and was generally a pleasure to read. 
 
Comments:
+ To the best of my knowledge, NVA is the first one leveraging the regularity of value functions in RL to learn to leverage past estimates to improve value prediction automatically. The idea of NVA is novel and interesting. 

+ NVA is essentially a low capacity memory mechanism. Memory mechanism has been investigated in RL to address partially observability and/or to reuse knowledge/skills in the setting of transfer learning/lifelong learning. It is interesting to see that memory can indeed help prediction beyond partial observability

+ Section 6 and 7 provides extensive discussion on NVA variants and related topics.  They are informative and useful.

- If we compare Figure 4 (Top) with Figure 3, the game Centipede has the greatest improvement on the value loss, but the worst performance gain. What property of the game Centipede caused this result?"
Active Exploration for Learning Symbolic Representations,"Garrett Andersen, George Konidaris",https://proceedings.neurips.cc/paper/2017/hash/fb89fd138b104dcf8e2077ad2a23954d-Abstract.html,"The paper proposes an active exploration scheme for more efficiently building a symbolic representation of the world in which the agent operates. This builds on earlier work that introduces a rich symbolic language for representing decision making problems in continuous state spaces. The current work proposes a two phase strategy - the first phase builds a Bayesian model for the options based on data gathered through execution of the options. The second phase uses of a MCTS based exploration strategy that drives the exploration based on the goodness of prediction of the occurrence of the symbols, given a set of options.  They demonstrate the utility of the approach on two domains - one inspired by asteroid game and the other a treasure hunting game. The line of inquiry of this paper is very promising and quite needed. A truly intelligent agent should be able to actively seek experience that would better enable it to ""understand"" the domain. 

The paper is somewhat well written. The basic setting of the work is well explained, but it is a little hard to understand the framework proposed in this work, since the writing in sections 3.1 and 3.2 is a bit dense and hard to follow. It is not entirely clear what is the specific feature that makes the approach data efficient. The comparisons are against random and greedy exploration. Given that the greedy algorithm is a count based exploration strategy, it is surprising that it does so poorly. Is there some explanation for this? It would be nice to see some interpretation of the nature of the models learned when the active exploration strategy is used. 

One nitpick: If one ignores the duration of option completion, then what we have is a MDP and no longer a SMDP. ","This is a very interesting paper, with multiple complementary ideas. It advocates model-based active exploration (model learning + seeking regions of uncertainty). Instead of doing this in raw state space, it proposes a method for abstracting states to symbols based on factoring and clustering the state space. The exploration is then done by MCTS-planning in a (sampled) symbolic model. The task setup evaluates pure exploration (ignoring all rewards) on a two different domains.

This approach to unsupervised hierarchical reinforcement learning is novel and ambitious, the paper is clear and well-written. The proposed method may be somewhat brittle in its current form, and it is unclear to what problem complexity it can scale, but that can be resolved by future work. My main recommendation is to add a thorough discussion of its weaknesses and limitations. 

Other comments:
* Section 2.1: maybe it’s not necessary to introduce discounts and rewards at all, given that neither are used in the paper?
* Section 3.1: the method for finding the factors seems very brittle, and to rely on disentangled feature representations that are not noisy. Please discuss these limitations, and maybe hint at how factors could be found if the observations were a noisy sensory stream like vision.
* Line 192: freezing the partitioning in the first iteration seems like a risky choice that makes strong assumptions about the coverage of the initial data. At least discuss the limitations of this.
* Section 4: there is a mismatch between these options and the desired properties discussed in section 2.2: in particular, the proposed options are not “subgoal options” because their distribution over termination states strongly depends on the start states? Same for the Treasure Game.
* Line 218: explicitly define what the “greedy” baseline is.
* Figure 4: Comparing the greedy results between (b) and (c), it appears that whenever a key is obtained, the treasure is almost always found too, contrasting with the MCTS version that explores a lot of key-but-no-treasure states. Can you explain this?
","Key question asked in the paper: Can an agent actively explore to build a probabilistic symbolic model of the environment from a given set of option and state abstractions? And does this help make future exploration easy via MCTS? This paper proposes a knowledge representation framework for addressing these questions.

Proposal: A known continuous state space and discrete set of options (restricted to subgoal options) is given. An options model is treated a semi-MDP process. A plan is defined as a sequence of options. The idea of a probabilistic symbol is invoked from earlier work to refer to a distribution over infinitely many continuous low-level states. The idea of state masks is introduced to find independent factors of variations. Then each abstract subgoal option is defined as a policy that leads to a subgoal for the masked states, for e.g. opening a door. But since there could be many doors in the environment, the idea of a partitioned abstract subgoal option is proposed to bind together subgoal options for each instance of a door. The agent then uses these partitioned abstract subgoal options during exploration. Optimal exploration then is defined as finding a policy via MCTS that leads to greatest reduction of uncertainty over distributions over proposed options symbolic model.


Feedback:

- There are many ideas proposed in this paper. This makes it hard to clearly communicate the contributions. This work builds on previous work from Konidaris et al. Is the novel contribution in the MCTS algorithm? The contributions should be made more explicit for unfamiliar readers. 

- It is difficult to keep track of what's going on in sec 3. I would additionally make alg1 more self-inclusive and use one of the working examples (asteroid or maze) to explain things. 

- Discuss assumption of restriction to subgoal option. What kind of behaviors does this framework not permit? discuss this in the paper

- Another restrictive assumption: ""We use factors to reduce the number of potential masks, i.e. we assume that if state variables i and j always change together in the observations, then this will always occur. An example of a factor could be the (x, y, z) position of your keys, because they are almost never moved along only one axis"". What are the implications of this assumptions on the possible range of behaviors?

- The experimental setup needs to be developed more to answer, visualize and discuss a few interesting questions: (1) how does the symbol model change with active exploration? (2) visualize the space of policies at various intermediate epochs. Without this it is hard to get an intuition for the ideas and ways to expand them in the future.

- I believe this is a very important line of work. However, clarity and articulation in the writing/experiments remains my main concern for a clear accept. Since some of the abstractions are fairly new, the authors also need to clearly discuss the implication of all their underlying assumptions."
Balancing information exposure in social networks,"Kiran Garimella, Aristides Gionis, Nikos Parotsidis, Nikolaj Tatti",https://proceedings.neurips.cc/paper/2017/hash/fc79250f8c5b804390e8da280b4cf06e-Abstract.html,"The paper considers the problem of balancing the information exposure in the social network. The authors consider two opposing views spreading through the network, and pose the problem of selecting nodes for infecting so that the number of imbalanced (i.e. infected by only one of the two views) nodes is minimized. The problem seems important, especially in the prevalent spreading of the fake news.

The model assumed by the authors is the independent-cascade model (ICM). It facilitates simple analysis, and it is Ok to start with simple models. However, I would say it is not a very realistic model, and more complex models exist which proved to capture social network phenomena well (e.g. Hawkes processes).

I am not particularly convinced by the settings taken by the authors. Why would it be reasonable to assume that we have probabilities for each node reposting a post? Even if we assume different probabilities for the two views, we are assuming independence of the two spreads. Shouldn't we rather have at least four probabilities for each edge? (i.e., probability of reposting story 1 if nothing was reposted yet, probability of reposting story 2 if nothing was reposted yet, probability of reposting story 1 if story 2 was already reposted, probability of reposting story 2 if story 1 was already reposted). That would sound more realistic to me, because if I already reposted supporting view on X, perhaps I am less likely to repost a support for Y?

The experimental evaluation is difficult to conduct for such a problem. I wonder how it could be improved to make it look less synthetic (since the real data provided the parameters for the random simulations). Perhaps a bit elaboration on this point would be good for appreciation of the experiments conducted.","
In this work the authors consider the problem of choosing seed sets subject to cardinality constrainsts such that the number of nodes that will be exposed to both or none of two campaigns will be maximized. The authors consider two different models of independent cascades, the heterogeneous and the correlated model. The authors propose a correlated independent cascade model where p1(e)=p2(e). Would it make sense for competing campaigns to consider a model where p1(e)=1-p2(e)? For instance, one may expect that a fully commited Democrat is less likely to retweet campaign tweets of Republicans, and vice versa.  Then, the authors prove that the problem is NP-hard. The reduction is provided in the appendix. The authors provide 3 algorithms, that come with guarantees under certain conditions. The first algorithm  decomposes the objective into two terms one of which is submodular, and optimizes this term. It would be nice to have a sentence (say in page 5, starting from line 190) describing in words the semantics of sets \Omega(S_1,S_2), \Psi(S_1,S_2). Since \Omega is submodular, the authors suggest in algorithm 0 to optimize only Omega. The other two algorithms are intuitive but work for the correlated model. Among the 3 proposed algorithms, the last one (Hedge) performs better in practice, even for the setting that the theoretical guarantees do not apply. However the improvement over Borodin's et al method is relatively small. The authors should provide the actual numbers in text since it's hard to tell precisely from  fig.2.  The experiments are nicely done, including the preprocessing of setting the probabilities on the edges.  Can the authors elaborate on when one expects the baseline HighDegrees to perform worse than Random?   Also, how different are the cloud of words produced by Borodin et al compared to your method? Some clouds in the Appendix would be helpful to understand not only the performance based on metrics.

Minor:  (a) Please provide a reference for the #P hardness (e.g., ""Scalable Influence Maximization for Prevalent ViralMarketing in Large-Scale Social Networks"" by Chen et al.). (b) Make sure to remove all typos (e.g., ctrl+F in appendix rewteets instead of retweets)

Overall, this is a solid paper, with interesting experimental findings. ","This paper proposes the problem of exposure balancing in social networks where the independent cascade is used as the diffusion model. The usage of symmetric difference is quite interesting and the approximation scheme to shovel the problem is plausible.

The experiments setup and the way the two cascades are defined and curated are interesting too.

It’s not clear if the two social network campaigns should be opposing or not? Is it a necessity for the model?

Furthermore, some hashtags are not really in opposition to each other.  For example, on obamacare issue, the tag @sentedcurze could represent many many topics related to Senator Ted Cruz including the anti-obamacare.

Any elaboration on the choice of \alpha=0 for the correlated setting.

Given the small cascades size, is the diffusion network learned reliable enough? It looks more like a heuristic algorithm for learning the diffusion network which might be of high variance and less robust given the small cascades size.

In Figure 3 why for some cases the difference between methods is increasing and for some cases it is decreasing?

Using the term “correlated” at the first though reminds the case that the two cascades are competing against each other to attract the audience. This looks to be a better definition of correlation than the one used in the paper. But in this paper, it’s not clear until the user reaches to page 3. I would suggest to clarify this earlier in the paper.

Showing standard deviations on the bar charts of figure 1 would have helped assess the significance of the difference visually. Furthermore, reporting significance levels and p-values could make the results stronger.

A few very related work are missing. Especially paper [1] has formulated the problem of exposure shaping in social networks. Furthermore, paper [2] considers two cascades who are competing against each other and optimizes one of them so they can hit the same number of people, i.e., trying to balance their exposure.  Given papers [1] and [2] I would say the current paper is not the first study of balancing g exposures in social networks. They are closely related.

Paper [3] has proposed a point-process based model for competing cascades in social networks that is related and finally paper [4]  proposes an algorithm to optimize the activities in social networks so that the network reaches to a specific goal.

[1] Farajtabar, Mehrdad, et al. ""Multistage campaigning in social networks."" Advances in Neural Information Processing Systems. 2016.

[2]  Farajtabar, Mehrdad, et al. ""Fake News Mitigation via Point Process Based Intervention."" arXiv preprint arXiv:1703.07823 (2017).

[3] Zarezade, Ali, et al. ""Correlated Cascades: Compete or Cooperate."" AAAI. 2017.

[4] Zarezade, Ali, et al. ""Cheshire: An Online Algorithm for Activity Maximization in Social Networks."" arXiv preprint arXiv:1703.02059 (2017)."
Nonlinear Acceleration of Stochastic Algorithms,"Damien Scieur, Francis Bach, Alexandre d'Aspremont",https://proceedings.neurips.cc/paper/2017/hash/fca0789e7891cbc0583298a238316122-Abstract.html,"The paper extends recent work of Scieur et al [2016] on nonlinear acceleration via extrapolation of sequences from deterministic to stochastic optimization. The work by Scieur itself  generalizes and extends results developed in the late 60s and 70s from quadratics to non-quadratics (whence the name “nonlinear”). Sequence extrapolation methods seem to have been “forgotten” or simply “not in use” by the ML and optimization community until recently, and have some interesting theoretical and practical properties. For instance, nonlinear regularized acceleration (NRA) is capable to accelerate the sequence of iterates formed by the gradient descent method and obtain the optimal accelerated rate. This is done via what essentially amounts to a “bootstrapping” extrapolation process.

Stochastic methods are the state of the art for optimization/training methods in ML, which is enough motivation to study sequence extrapolation methods for stochastic sequences, such as those formed from the iterates of SGD, SVRG, SDCA, SAGA, QUARTZ and so on. Hence, the topic of the paper is both timely and relevant.

Unfortunately, if theory is to be followed, each extrapolation steps is very expensive. When applied to the ERM problem, it is linear in the number of training points, and quadratic in k where k is the current iteration counter. Hence, the cost is extremely prohibitive, which means that the theoretical results are not useful. However, the authors show through experiments that practical benefits of their method are observed when a restarting strategy is used on consecutive sequences of iterates of small/constant length (experiments were done for k=10).  The paper would have been much stronger if these issues were resolved – if acceleration can be provably established for the restarting technique they use in practical experiments. 

Main issues:

1)	Sentence on line 25 does not make sense. How is 1 – kappa an accelerated rate?
2)	Why is \sqrt{S_\kappa(k,0)} equal to 1/kappa * [(1-sqrt{\kappa}) / (1 + \sqrt{\kappa}) ] ^k? I do not see a derivation of this in the paper. In other words, how does Cor 3.4 follow from (19)?
3)	Cor 3.4: a factor of 1/\kappa seems to be missing here.
4)	It should be mentioned in Section 3 which of the results were established in Scieur et al. [2016] and which are new. This is is not clear.
5)	Prop 3.3: are the polynomials supposed to have real coefficients?
6)	Why should we be interested in the sequence {x_t} produced by the linearized model (8)? We are primarily interested in the sequence provided by the original model x_{t+1} = g(x_t). This should be better motivated.
7)	Prop 3.6: Should the fraction defining the rate be (1-sqrt{\kappa}) / (1+ \sqrt{\kappa}) instead of (1 - \kappa ) / (1 + \kappa )? 


Small issues:
8)	27: method
9)	29 (and elsewhere): “performance” instead of “performances”
10)	A comma is missing at the end of  all displayed equations followed by “where”.
11)	57: an -> the
12)	What does O(x_0-x^*) mean in (14)? Should this be O(\|x_0-x^*\|)?
13)	68: The sentence needs to be reformulated. Say clearly that g is defined through (7)...
14)	Say Algorithm 1 and not Algorithm (1).
15)	78: compares
16)	Sometimes you use boldface I and sometimes standard I for the identity matrix.
17)	131: nonlinear or non-linear
18)	140: corresponds
19)	175 and 194: write \mathbb{R}^d, as in the intro. 
20)	199: after every k iterations?
21)	199-200: Is this what you do: You add one data pass after every 10 data samples? If so, that is too much!
22)	Many places: “least-square” -> “least-squares”
23)   226: like the -> like

=== post rebuttal feedback ===

I am keeping my ratings.","
Summary of the paper
====================

In principle, the extrapolation algorithm devised by Scieur et al. (2016) forms a scheme which allows one to speed up a given optimization algorithm by averaging iterates. The current paper extends this idea to the stochastic case by using the same the averaging technique only this time for noisy iterates. After recalling the results for the deterministic case (as shown in Scieur et al. (2016)), a theoretical upper bound is derived, based on theses results, by quantifying the amount of noise introduced into the optimization process. Finally, numerical experiments which demonstrate the speed-up offered by this technique for least-square and finite sum problems are provided. 



Evaluation
==========

Extending this technique for stochastic algorithms is a crucial step for this technique as this family of algorithms dominants many modern applications. From practical point of view, this seems to provide a significantly acceleration to some (but not all) of the state-of-the-art algorithms. Moreover, the theoretical analysis provided in the paper shows that this stochastic generalization effectively subsumes the deterministic case. However, this paper does not provide a satisfying analysis of the theoretical convergence rate expected by applying the Algorithm 1 on existing stochastic algorithms (especially, for finite sum problems). It is therefore hard to predict in what settings, for which algorithms and to what extent should we expect acceleration. In my opinion, this forms a major weakness point of this paper. 


General Comments
================
- As mentioned above, I would suggest providing a more comprehensive discussion which allows more intuition regarding the benefits of this technique with respect to other existing algorithms.
- Consider adding a more precise statement to address the theoretical convergence rate expected when applying this technique to a given optimization algorithm with a given convergence rate.


Minor Comments
==============
L13: f(x) -> f ?
L13: Smooth and strong convexity are not defined\addressed at this point of the paper.
L25: depends on
L31: Many algorithms do exhibit convergence properties which are adaptive to the effective strong convexity of the problem: vanilla gradient descent, SAG (https://arxiv.org/abs/1309.2388, Page 8, 'an interesting consequence').
L52: f(x) -> f ?
L55: Might be somewhat confusing to diverse from the conventional way of defining condition number.
L61: The linear approximation may not be altogether trivial (also may worth to mention that the gradient vanishes..)
L72: in the description of Algorithm 1, using boldface font to denote the vector of all ones is not aligned with vector notation used throughout the paper, and thus, is not defined a-priori.
L81: can you elaborate more on why 'Convergence is guaranteed as long as the errors' implies to 'ensures a good rate of decay for the regularization parameter'.
L103: lambda..
L118: I would split the technical statement from the concrete application fro SGD.
L121: Maybe elaborate more on the technical difficulty you encounter to prove a 'finite-step' version without lim?
L136: G is symmetric?
L137: corresponds?
L137: I'd suggest further clarifying the connection with Taylor reminder by making it more concrete.
L140: corresponds
L140: h in I-h A^\top A
L149: The
Figure 2 Legend: the 'acc' prefix is a bit confusing as 'acceleration' in the context of continuous optimization this term is used somewhat differently.
L227: Can you give more intuition on why 'the algorithm has a momentum term' implies 'underlying dynamical system is harder to extrapolate.'?





"
The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities,"Arun Suggala, Mladen Kolar, Pradeep K. Ravikumar",t,"This paper proposes a method to estimate a joint multivariate density non-parametrically by estimating a product of marginal conditional exponential densities. The marginal distribution of each variable is conditioned on the neighbors of that variable in a graph. The authors first discuss how their work relate to the literature. They study the consistency  of their approach in terms of the estimated marginals with respect to a joint distribution, by relying on a theorem in ref. 28. They also develop an estimation algorithm, study statistical properties of their approach and discuss the relationships with copulas. The paper ends with experimental results. The paper is well written.

1) Are there guarantees if the real distribution does not belong to the family of distribution estimated?

2) I am not very familiar with nonparametric density estimation. How practical is the assumption that the base measures are known?

3) The experiments seem well conducted an the baselines selected in a sensible manner. The evaluation is performed both on synthetic and real data. On synthetic data, the approach is only evaluated on models matching the assumption that the factors are at most of size 2. Imho, testing the approach on more general model would help understand its usefulness.

4) l119: why doesn't nonparametric estimation benefit from estimating a joint distribution as a product of conditional marginal distribution?

5) How easy/hard would it be to extend your approach to cliques of size greater than 2?

6) Will you make your code available?

7) Based on the definition of the density, shouldn't the edges of the graph E mentioned in the introduction correspond to non-independence rather than independence?

Typos:
You use both ""nonparametric"" and ""non-parametric""."
NeuralFDR: Learning Discovery Thresholds from Hypothesis Features,"Fei Xia, Martin J. Zhang, James Y. Zou, David Tse",https://proceedings.neurips.cc/paper/2017/hash/fec8d47d412bcbeece3d9128ae855a7a-Abstract.html,"The paper describes a new method for FDR control for p-values with additional information. 
For each hypothesis, there is a p-value p_i, and there is also a feature vector X_i. 
The method learns the optimal threshold for each hypothesis, as a function of the features vector X_i. The idea seems interesting and novel, and overall the paper is explained quite clearly. In several simulated and real data example, the authors show that their method can use the additional information to increase the number of rejections, for a given FDR control threshold. 

It seems to me to be important that the X_i's were not used to calculate the P_i, 
otherwise we get a problem of circularity. The authors mention this in an example, but there 
is no formal treatment of this - it is not clear what should be the probabilistic relationship between the X_i's and the p_i's. On the one hand, both the p_i's and the X_i's are likely to be dependent on the correctness of the null vs. alternative hypothesis, but on the other hand, if the X_i's were already used to calculate the p_i's, then they should not improve the decision boundary. 

The fact that X is multidimensional should not immediately rule out non-parametric methods as the authors claim - for example nearest-neighbours regression can still adapt to the 'true' intrinsic dimension of the regression function, even if the dimension of X is large. 

It is not clear to me how immune to overfitting is the cross-validation procedure proposed by the authors. The settings is not a standard supervised learning approach, since the 'label' FDR is unknown and is replaced by an estimator. It would be good to emphasize and elaborate this point more clearly. Then, since the estimator of FDR is noisy, this may yield to my understanding higher FDR for the test set. 
The authors do show in Theorem 1 a bound which grows with the number of folds in the cross-validation procedure. 

The mirror estimator proposed by the authors may have little bias but large variance if t(x) is small, since very few p-values will be between 1-t(x) and 1. This issue comes up in Story's approach where lambda parameter is chosen such that the null proportion, estimated using the interval [lambda, 1] should balance variance and bias. 
 

Minor:
======
Line 136: 'this' -> 'these'
","Multiple hypothesis testing is an important step of many high-throughput analyses. When a set of features is available for each of the tested hypothesis, one can use these features to weight the hypothesis and increase statistical power, a strategy that is known as independent hypothesis weighting. Specifically, this can be achieved by learning the discovery threshold on P-values as a function of the hypothesis features.

In this paper, the authors propose a new method (neuralFDR), where a neural network is considered to model this relationship. Considering simulations and two real data applications, the authors show that (i) neuralFDR can increase power while controlling the false discovery rate and (ii) it can be more powerful than existing weighting methods (especially when multiple hypothesis features are considered). The results are convincing, the paper is well written and the method has the potential to be used in high-throughput hypothesis testing problems. I have only two points that I would like to raise:

1) the authors do not discuss how computationally demanding their method is and how it compares to others. These considerations will certainly impact the choice of a potential user on which correction method to use;

2) In the application to the GTeX dataset, the authors showed that increasing the number of considered features, the number of discoveries of neuralFDR increases while the number of discoveries of IHW (the competitor method) decreases. In the most favourable scenario (3 features), they report 37,195 vs 35,598 discoveries, which corresponds to a 4.5% increase. Given this small percent increase, is there any reason why the authors did not include other features to further increase discoveries? In eQTL mapping there are plenty of variant features or gene features to choose from.

Minor comments:
Line 215. “800 more discoveries” the percent increase should also be reported
Table 2: I think that absolute and percent increases should be reported to improve readibility","This paper proposes a procedure to learn a parametric function mapping a set of features describing a hypothesis to a (per-hypothesis) significance threshold. In principle, if this function is learnt appropriately, the approach would allow prioritising certain hypotheses based on their feature representation, leading to a gain in statistical power. The function generating the per-hypothesis rejection regions is trained to control the False Discovery Proportion (FDP) with high probability, provided some restrictive assumptions are satisfied.

While multiple hypothesis testing is perhaps not a mainstream machine learning topic nowadays, key application domains such as computational biology or healthcare still heavily rely on multiple comparisons, making this a topic worth studying. A clear trend in this domain is the design of methods able to incorporate prior knowledge to upweight or downweight some hypotheses; the method described in this paper is one more example of this line of work.

Despite the fact that the topic is relevant, I have some serious concerns about the practical applicability of the proposed approach, as well as other aspects of the paper. 


***** Major Points *****

(1) In my opinion, the paper heavily relies on assumptions that are simply false for all motivating applications described in the paper and tackled in the experiments.

Firstly, the method assumes that triplets (P_{i}, \mathbf{X}_{i}, H_{i}) are i.i.d. If this assumption were not satisfied, the entire method would break down, as cross-validation would no longer be appropriate to validate the decision rules learnt on a training set.

Unfortunately, the independence assumption cannot be accepted in bioinformatics. For example, in genome-wide association studies, variants tend to be statistically dependent due to phenomena such as linkage disequilibrium (LD). In RNA-Seq analysis, gene expression levels are correlated according to complex co-expression networks, which are possibly tissue-specific. More generally, unobserved confounding variables such as population structure, batch effects or cryptic relatedness might introduce additional non-trivial statistical dependencies between the observed predictors. There exist straightforward extensions of the Benjamini-Hochberg (BH) procedure to guarantee FDR control under general dependence. Unless the authors extend their approach to allow for arbitrary dependencies between hypotheses, I do not believe the method is valid for the applications included in the experiments. Moreover, the assumption that all triplets (P_{i}, \mathbf{X}_{i}, H_{i}) are identically distributed is also hard to justify in practice.

Another questionable assumption, though perhaps of lesser practical importance when compared to the two previous issues, is the fact that the alternative distribution $f_{1}(p \mid \mathbf{x})$ must be non-increasing. The authors claim this is a standard assumption, which is not necessarily true (e.g. see [1] for a recent counter-example, where the alternative distribution is modelled as a beta distribution with learnable mean and precision). 

(2) The paper seems to make a heavy emphasis on the use of neural networks, to the point this fact is present in the title and name of the method. However, neural networks play a small role in the entire approach, as essentially all that is needed is a parametric function to map the input feature space describing each hypothesis to a significance threshold in the unit interval. Most importantly, all motivating examples are extremely low-dimensional (1D to 5D) making it likely that virtually any parametric function with enough capacity will suffice. 

I believe the authors should include exhaustive experiments to study how different function classes perform in this task. By doing so, the authors might prove to which extent neural networks are an integral part of their approach. Moreover, it might occur that simpler, easier to interpret functions perform equally well, in which case it might be beneficial to use those instead.

(3) While the paper contains virtually no grammar mistakes, being easy to read in that regard, the quality of the exposition could be significantly improved. 

In particular, Section 3 lacks detail in the description of Algorithm 1. Throughout the paper, the notion of cross-validation is rather confusing, as in this particular case distinct folds correspond to subsets of features (i.e. hypotheses) rather than samples used to compute the corresponding P-values. I believe this should be clarified early-on. Also, the variables $\gamma_{i}$ appear out of nowhere and are not appropriately discussed in the text. In my opinion, the authors should additionally adopt the notation used in Equation (8) in Supplementary Section 2 for Equation (3), in order to make it clearer that what they are truly optimising is only a smooth approximation to the true objective function.

The description of the implementation and training procedure in Supplementary Section 2 is also insufficient, as it is lacking a proper justification of the hyperparameter choices (e.g. network architecture, $\lambda$ values, optimisation scheme) and experimental results describing the robustness of their approach to such hyperparameters. In practice, setting these hyperparameters will be a considerable hurdle for many practitioners in computational biology and statistical genetics, which might not necessarily be familiar with standard techniques in deep learning. Therefore, I believe these issues should be described in greater detail for this work to be of use for such communities.

***** Minor Points *****

The use of cross-validation makes the results random by construction, which is an undesirable property in statistical association testing. The authors should carry out experiments to demonstrate the stability of their results across different random partitions and to which extent a potential underlying stratification of features might be problematic for the cross-validation process.

Decision rules in multiple hypothesis testing ultimately strongly depend on the number n of hypothesis being tested. However, the proposed approach does not explicitly depend on n but, rather, relies on the cross-validation set and the test set containing exactly the same number of hypotheses and all hypotheses being i.i.d. It would be interesting if the authors were able to relax that requirement by accounting for the number of tests explicitly rather than implicitly.

The paper contains several wrong or imprecise claims that however do not strongly affect the overall contribution:

 - “The P-value is the probability of a hypothesis being observed under the null” -> This is not a correct definition of P-value. A P-value is the probability of observing a value of the test statistic that indicates an association at least as extreme under the assumption that the null hypothesis holds.
 - “The two most popular quantities that conceptualize the false positives are the false discovery proportion (FDP) and the false discovery rate (FDR)” -> Family-Wise Error Rate continues to be popular and has a considerably longer history than any of those quantities. Moreover, while it is true that FDR is extremely popular and de-facto replacing FWER in many domains, FDP is relatively less frequently used in comparison to FWER.
- Definition 1 is mathematically imprecise, as it is not handling the case $D(t)=0$ properly.
- “Moreover, $\mathbf{x}$ may be multidimensional, ruling out the possibility of non-parametric modelings, such as spline-based methods or the kernel based methods, whose number of parameters grows exponentially with the dimensionality” -> Kernel-based methods can be applied in this setting via inducing variables (e.g. [2]). Additionally, like mentioned earlier, many other approximators other than neural networks require less an exponentially-growing number of parameters on the dimension of the feature space.
- Lemma 1 does not show that the “mirroring estimator” bounds the true FD from above, as Line 155 seems to claim. It merely indicates that its expectation bounds the expected FD, which is a different statement than what is discussed between Lines 143-151.
- As mentioned earlier in this review, the statement in Lines 193-194 is not completely correct. Five dimensions is definitely not a high-dimensional setting by any means. At least thousands of features should have been used to justify that claim.

The paper would benefit from proof-reading, as it contains frequent typos. Some examples are:

- Line 150: $(t(\mathbf{x}), 1)$ -> $(1-t(\mathbf{x}), 1)$
- Line 157: “approaches $0$ very fast as $p \rightarrow 0$” -> “approaches $0$ very fast as $p \rightarrow 1$”
- Line 202: “Fig. 3 (b,c)” -> “Fig. 4 (b,c)” 
- Lines 253-254: The alternative distribution $f_{1}$ is missing parenthesis
- Supplementary Material, proof of Theorem 1 and Lemma 2: All terms resulting from an application of Chernoff’s bound are missing parenthesis as well. While it might have been intentional, in my opinion it makes the proof a bit harder to read.

References:

[1] Li, Y., & Kellis, M. (2016). Joint Bayesian inference of risk variants and tissue-specific epigenomic enrichments across multiple complex human diseases. Nucleic acids research, 44(18), e144-e144.
[2] Dustin Tran, Rajesh Ranganath, David M. Blei - The variational Gaussian process. A powerful variational model that can universally approximate any posterior. International Conference on Learning Representations, 2016"
A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis,Tor Lattimore,https://proceedings.neurips.cc/paper/2017/hash/fed33392d3a48aa149a87a38b875ba4a-Abstract.html,"The paper provides both upper (Theorem 2)  and lower (Theorem 7)  bounds for the regret of a finite-armed stochastic bandit model and a UCB type algorithm that is based on confidence bounds for the median-of-means estimator, using work of Bubeck et al 2013. The analysis is done under a non-parametric setup, where the learner need to only assume the existence of a bound on the kurtosis of the noise. 

I read the paper and I found it to be well written and the topic to be very interesting. The author(s) did a wonderful job describing the importance of the work, and to delineate the proposed solution.The manuscript is in great shape. I can therefore recommend the acceptance of the paper subject to a minor revision:

The reference: 
Wesley Cowan and Michael N Katehakis. Normal bandits of unknown means and variances: Asymp- totic optimality, finite horizon regret bounds, and a solution to an open problem. arXiv preprint arXiv:1504.05823, 2015a. 

Should be replaced by the most recent: 

Wesley Cowan, Junya Honda, Michael N. Katehakis Normal bandits of unknown means and variances: Asymptotic optimality, finite horizon regret bounds, and a solution to an open problem. arXiv preprint arXiv:1504.05823v2, 2015a. 

2. In table 2 the authors should include the result for arbitrary discrete distributions of Burnetas and Katehakis (1996), with a complicated constant. (Section 4.3 of that paper). 
","Rebuttal
--------
I read the author feedback. There was no discussion, but due to the consensus, the paper should be accepted.

Summary
-------
The article considers a variant of the Upper Confidence Bound (UCB) algorithm for multi-armed bandit that is using the notion of kurtosis as well as median of means estimates for the mean and the variance of the considered distributions.

Detailed comments
-----------------
1) Table 1: I think the work by Burnetas and Katehakis regarding bandit lower bounds for general distributions deserves to be in this table.

2) It is nice to provide Theorem 7 and make explicit the lower bound in (special cases of) this setting.

3) The algorithm that is derived looks very natural in view of the discussion regarding the kurtosis.
One may observe that a bound kappa_0 on kappa is currently required by the algorithm, and I suggest you add some paragraph regarding this point:
Indeed on the one hand, a (naive) criticism is that people may not know a value kappa_0 in practice and thus one would now like to know what can be achieved when the kurtosis is unknown and we want to estimate it; This would then call for another assumption such as on higher order moments, and one may then continue asking similar questions for the novel assumption, and so on and so forth. 
On the other hand, the crucial fact about kurtosis is that unlike mean or variance, for many classes of distributions all distributions of a certain type share the same kurtosis. Thus this is rather a ""class"" dependent quantity than a distribution dependent quantity, and using the knowledge of kappa_0 \geq \kappa can be considered to be a weak assumption. I think this should be emphasized more.

4) Proof of Lemma 5: The proof is a little compact. I can see the missing steps; but for the sake of clarity, it may be nice to reformulate/expand slightly. (Applying Lemma 4 twice, first to the random variables Y_s, then to (Y_s-mu)^2, etc).

5) Proof of Lemma 6: end of page 4, first inequality: from (c), you can improve the factor 2 to 4/3  (1/ (1-1/4) instead of 1/(1-1/2))
Line 112 ""while the last uses the definition..."" and Lemma 4 as well?
With the 2/3 improvement, you should get \tilde \sigma_t^2 \leq 11/3 \sigma^2 + \tilde \sigma_t^2/3.

6) Without section 3, the paper would have been a little weak, since the proof techniques are extremely straightforward once we have the estimation errors of the median of means, which are known. Thus this section is welcomed.

7) Proof of Theorem 7: ""Let Delta_\epsilon = \Delta_\epsilon + \epsilon"" I guess it should be ""Let Delta_\epsilon = \Delta + \epsilon"".

8) There are some missing words on page 7.

9) Section 4:
i)Optimal constants. For the seek improvement, one should mimic the KL optimization, at least implicitly. I am not sure this can be achieved that easily.
Self-normalized processes would play a role in reducing the magnitude of the confidence bounds, I am not sure it will help beyond that.

ii) TS: I don't see any simple way to define a ""kurtosis-based"" prior/posterior. Can you elaborate on this point? Otherwise this seems like an empty paragraph and I suggest you remove it.

Decision:
--------
Overall, this is an article with a rather simple (a direct application of median of means confidence bounds for unknown mean and variance) but elegant idea, and the paper is nicely executed and very clear. I give a weak accept, not a clear accept only because most of the material is already known.","The paper provides a UCB-type algorithm for bandits with reward distribution with bounded kurtosis. The strategy of the paper is to use robust median of means estimators to get finite  time convergence. 

From lines 55 to 68, it is suggested that the classical empirical mean estimators won't do the job. I think it would be very helpful if the author gives a concrete example of an empirical estimator failing here.  

A major concern about the paper is that it has typos in its definitions and proofs. It can sometimes be frustrating to follow the arguments. As an example, I believe definition of T_i on line 72, and eqn (5) after line 118 to have this kind of issues. Moreover, after line 119, shouldn't u start from 0 and go to t?  Also line 142 doesn't make sense!

I suggest adding more explanations to the proof of Theorem 2 specially the few lines after 119. I wasn't able to follow some of the conclusions. 

"
Value Prediction Network,"Junhyuk Oh, Satinder Singh, Honglak Lee",https://proceedings.neurips.cc/paper/2017/hash/ffbd6cbb019a1413183c8d08f2929307-Abstract.html,"This paper propose a reinforcement learning planning algorithm, called VPN (value prediction network), that works for discrete action, continuous state problems. The framework is similar to the Dyna architecture, including a value prediction model, a learned transient model, and a planning module that do single step or multiple steps of rollouts. Experiments were performed on a collect domain and several Atari games.    

The paper cites Dyna, but the more relevant is the linear Dyna work:

Single step liner Dyna:
http://uai2008.cs.helsinki.fi/UAI_camera_ready/sutton.pdf

Multi-step linear Dyna:
https://papers.nips.cc/paper/3670-multi-step-dyna-planning-for-policy-evaluation-and-control.pdf

Both papers, though, does not deal with options. 

The paper seems to emphasize the difference of VPN to a similar algorithm called OPN. In VPN, the value is a function of the abstract state, but not the original observation. I agree this is true. Then in OPN, as discussed from line 200, the value is a function of the observation. I feel making such distinction is unnecessary. At the merit of making such a distinction is unclear. Taking Dyna and linear Dyna for example, Dyna uses a model between original observations (state is finite); linear Dyna uses a model between abstract models (continuous state problems). There is no agreement that Dyna or linear Dyna is better. Instead, they work for different problems. Making a comparison between models built on original observations or abstract state may depend on algorithmic implementation. 

Figure 7: planning depth study is interesting. Some intermediate, usually small number of planning steps performs the best. This is consistent with Yao’s results. In Figure 2 of the linked paper, 10 planning steps (instead of only 1 or infinite planning steps) performed the best. 

The option used in Ataris game are simple repeating primitive actions. This might be too simple except for algorithmic study convenience. 




","This paper proposes a new learning architecture, VPN, whose components can be
learned end-to-end to perform planning over pre-specified temporally extended actions. An advantage of this approach is that it explicitly avoids working in the original observation space and plans indirectly over a learned representation.
This is made possible by a applying a model-free DQN-like approach for learning values which are then used in combination with reward and duration models of options
to form planning targets. The proposed approach is shown to outperform DQN in a
variety of Atari games.

Model-based RL is a difficult problem when it comes to a large domain like ALE.
The development of new model-based methods which do not explicitly try to
reconstruct/predict the next observation seem to be going in the right direction.
VPN is a contribution in this new line of research.

However, there are a few conceptual elements which remain unclear.
First, why predicting the expected discount/duration separately from the
option-conditional expected cumulative return to termination ? Using the
Bellman-like equations for the reward model of options (see equation 17 of the options paper), the discount factor can effectively be folded into the reward prediction. This would avoid the need to learn a separate discount prediction for each option.

Second : The nature of the ""next abstract state"" (line 109) is a key element of the
proposed approach to avoid learning a transition model directly
in the original space. As you mention later on line 251, there is no incentive/constraint for the learned state representation to capture useful properties for planning and could in the extreme case become a tabular representation. The appendix then shows that you use a particular architecture for the transition module which predicts the difference. How much this prior on the architecture affects quality of the representation learned by the system ? What kind of representation would be learned in the absence of a residual connection ?

Can you also elaborate on the meaning of the weighting (1/d, d-1/d)  in equation (1) ? Why is it necessary ? Where does that come from ?

Line 21 : ""predict abstractions of observations""
This expression is unclear, but I guess that it means a feature/representation
of the state. You also write : ""not clear how to roll such predictions"", but isn't that exactly what VPN is doing ?

Typo : line 31 ""dyanmics""
"
Detrended Partial Cross Correlation for Brain Connectivity Analysis,"Jaime Ide, Fábio Cappabianco, Fabio Faria, Chiang-shan R. Li",https://proceedings.neurips.cc/paper/2017/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html,"In this work, the authors describe the use of detrended partial cross
correlation (DPCCA) as a quantity to capture short and long memory
connections among brain recordings, for connectivity analysis. DPPCA
is complemented with CCA to study the efficacy of detecting
connectivity on simulated data (generated with NatSim), and compared
to partial correlation and regularized inverse covriance (ICOV). On
real fMRI data, DPCCA is first used together with PCA to show
representative correlation profiles and perform dimensionality
reduction (with Isomap (Iso) and autoencorder (AutoE)). Second,
various combinations of DPCCA values and dimensionality reduction
methods are used as feature for predicting cocaine dependent class
from control.

The paper is sufficiently well written and most parts is described in
enough detail to reproduce the technical steps of the proposed
methodology. I appreciate the use of DPCCA which is definitely new to
the neuroimaging data analysis domain. The results provide significant
evidence to support the claims.

Even thoug the evidence supporting the claims is significant, the
effect size of the gain of the proposed method is pretty slim. For
example, in Table 3, the authors tested several combinations of
methods and reported the ""best"" classifier in each case. The final
result reported in the conclusion comments only the ""best of the best""
of them, with respect to ICOV. Given the small effect size (AUC=0.963
vs. AUC=0.948), I suspect that the positive difference may be
partially due to overfitting. When proposing multiple methods and
multiple classifiers, their AUC (or whatever other accuracy measure)
is used to rank them and decide the best one. Nevertheless, the best
AUC is slightly optimistically biased. In such setting, an unbiased
estimate of the AUC of the best method/classifier can be obtained only
on a further separate test set - which was not indicated by the
authors.

Minor issues:

- In Section 5, I'd like to see more discussion of the results in the
  paper (Fig.3, Tab.1, ecc.) and not only a small piece of Tab.3

- The descriptions in Section 4.3 are very dense and should be
  expanded more, to make them more accessible. Alternatively, some of
  them could be dropped, since they are not mentioned later on.

- Complementing DPCCA with CCA seems an ad-hoc step to show higher
  results in Table 1. The authors should work more on the motivations
  of such step.

- typo l.43: corrrelation
","The study presents a method of using DPCCA to discover brain connectivity patterns and differences, which can be used for neuropsychiatric condition prediction (cocaine addict or control). It is an interesting and important approach and the authors basically convince of its usefulness both in artificial and real fMRI data. However, I have a few questions/concerns:
- What is the rationale behind choosing the ROIs and would the results hold if other ROIs were chosen? (or for whole brain analyses)
- What is the number of PCs in fig.4? It's a bit strange that PC#3 explain only 0.1-0.2% of variance and the first two less than 1/4.
- What exactly do results in fig.4 suggest about the interactions between IFG & SMA and SMA & PC?
- Finally, are there any neural inferences that could be obtained using DPCCA that are not amenable using other approaches? ","The paper proposes a partialized version of DCCA, and applies it to synthetic and real fMRI data. While the contribution is incremental (this is a straight analogy to Whittaker's 1990 estimator of partial correlation), the results are at least on par with existing methods, and offer very interesting additional insights into connectivity across time scales. It also brings methods from statistical physics into neuroimaging, where they have been used (for DFA see e.g. e.g. Shen et al Clin Neurophysiol 2003 (EEG),  Mutch et al PLoS one 2012 (fMRI)), but not often for connectivity analysis. There are a few technical issues to address.

# Section 2.2.1

ROIs are derived from task fMRI on the whole dataset. This is problematic for the cocaine dependence prediction task (section 4.4) because it happens outside the cross-validation fold. The GLM should be re-trained within each fold using only training data, to compute ROIS which would then be applied to constructuct both training and testing graphs. Doing it outside the fold gives an unfair advantage and artificially boosts performance. Given the relatively large (by neuroimaging standards) sample size, this should not be too dramatic but it needs to be acknowledged in the text.

This is of course causes no problem in comparisons between connectivity methods since they all use the same ROIs.

# Section 3.1

Equation 1: in the denominator, should `C_{a,b} be C_{a,a}`, and likewise for `C_{b,b}` ?

Is there a guarantee that \rho in equation 2 is always invertible ?


# Section 3.2.2, 3.2.3

Why use the max over all time scales rather than some other statistic of the absolute value ? Figure 3 illustrates why the authors chose this, but this choice of an extreme statistic probably comes with a bias towards false positives. In this respect it is not clear why the authors say on lines 177-178 that DPCCA has low TPR?

Also, step 17 in algorithm 1 does not correct for multiple comparisons, using an uncorrected p-value while where are many hypothesis tests, which also contributes to false positives. This is not necessarily an issue but, together with the max operator of line 5, it is unclear how to assign proper p-values to each edge after algorithm 1, if that were desireable.

The null model chosen works for multi-subject studies only. It yields a very different (much lower) degree distribution. Is this a concern?


# Section 4.1

DCCA max is actually also located correctly at 10 s (fig 3e)

# Section 4.2

Please move hypothesis test results about DPCCA+CCA > ICOV from conclusion to this section. Given table 1 and large std it is not obvious at all that DPCCA+CCA is better, so having the test result here will be more convincing.

# Section 4.4.

As mentioned in my remarks on 2.2.1, ROIs are computed out-of-fold, which gives a small unfair edge to all methods, but does not affect comparison between methods computing connectivity on the same ROIs.

Using AUC is not recommended for comparing different classifiers (Hand, Machine Learning 77, 2009, but also Hanczar et al., Bioinformatics 26, 2010). Please show balanced accuracy as well. 

# Section 5

In table 3 it looks as if all methods (except Corr) do about the same, but the authors state in lines 241-242 that DPCCA+PCA works best. Given large std I don't think that the difference with, say ICOV or ParCorr is significant. Please provide a hypothesis test here.

"
